<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="NUTCH">
  <bug fixdate="2005-1-30 01:00:00" id="102" opendate="2005-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>jobtracker does not start when webapps is in src</summary>
      <description>When starting the jobtracker from NUTCH_HOME by bin/nutch-daemon.sh start jobtrackerThe jobtracker search for the webapps folder in NUTCH_HOME, but it is under src/When manually copy the webapps folder into NUTCH_HOME jobtracker starts without any problems. Exception in thread "main" java.lang.NullPointerException at org.apache.nutch.mapred.JobTrackerInfoServer.&lt;init&gt;(JobTrackerInfoServer.java:67) at org.apache.nutch.mapred.JobTracker.&lt;init&gt;(JobTracker.java:232) at org.apache.nutch.mapred.JobTracker.startTracker(JobTracker.java:43) at org.apache.nutch.mapred.JobTracker.main(JobTracker.java:1043)</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-1-11 01:00:00" id="1080" opendate="2011-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type safe members , arguments for better readability</summary>
      <description>Enable generics for some of the API, for better type safety and readability, in the process.</description>
      <version>None</version>
      <fixedVersion>1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-regex.src.test.org.apache.nutch.net.urlnormalizer.regex.TestRegexURLNormalizer.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.scoring.urlmeta.URLMetaScoringFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.Subcollection.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthenticationFactory.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthentication.java</file>
      <file type="M">src.plugin.feed.src.java.org.apache.nutch.parse.feed.FeedParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-24 01:00:00" id="1180" opendate="2011-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateDB to backup previous CrawlDB</summary>
      <description>Nutch currently replaces an existing CrawlDB with the new CrawlDB. By optionally keeping a previous version on HDFS users can easily revert in case of a mistake without relying on external backup mechanims.This should be enabled by default.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-2-24 01:00:00" id="1210" opendate="2011-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DomainBlacklistFilter</summary>
      <description>The current DomainFilter acts as a white list. We also need a filter that acts as a black list so we can allow tld's and/or domains with DomainFilter but blacklist specific subdomains. If we would patch the current DomainFilter for this behaviour it would break current semantics such as it's precedence. Therefore i would propose a new filter instead.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-4-14 01:00:00" id="1222" opendate="2011-12-14 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Upgrade to new Hadoop 0.22.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2005-1-4 01:00:00" id="131" opendate="2005-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-documented variable: mapred.child.heap.size</summary>
      <description>Got complaints about lack of heap space. Seems it was the children out of room for reduce of a updatedb.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2006-1-1 01:00:00" id="160" opendate="2006-1-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use standard Java Regex library rather than org.apache.oro.text.regex</summary>
      <description>org.apache.oro.text.regex is based on perl 5.003 which has some corner cases which perform poorly. The standard regular expression libraries for Java (1.4 and later) do not seen to contain these issues.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-regex.src.java.org.apache.nutch.net.RegexURLFilter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-19 01:00:00" id="1970" opendate="2015-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pretty print JSON output in config resource</summary>
      <description>We can use Jackson to pretty print the JSON output from the Nutch server.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.resources.JobResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.resources.ConfigResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2006-6-4 01:00:00" id="201" opendate="2006-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for subcollections</summary>
      <description>Subcollection is a subset of an index. Subcollections are definedby urlpatterns in form of white/blacklist. So to get the page intosubcollection it must match the whitelist and not the blacklist.Subcollection definitions are read from a file subcollections.xmland the format is as follows (imagine here that you are crawling allthe virtualhosts from apache.org and you wan't to tag pages withurl pattern "http://lucene.apache.org/" to be part of subcollectionlucene.&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;subcollections&gt; &lt;subcollection&gt; &lt;name&gt;lucene&lt;/name&gt; &lt;id&gt;lucene&lt;/id&gt; &lt;whitelist&gt;http://lucene.apache.org/&lt;/whitelist&gt; &lt;blacklist /&gt; &lt;/subcollection&gt;&lt;/subcollections&gt;plugin contains indexingfilter, query filter and supporting classes</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2006-4-5 01:00:00" id="244" opendate="2006-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent handling of property values boundaries / unable to set db.max.outlinks.per.page to infinite</summary>
      <description>Some properties like file.content.limit support using negative numbers (-1) to 'disable' a limitation.Other properties do not support this. I tried disabling the limit set by db.max.outlinks.per.page, but this isn't possible.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.WritableTestUtils.java</file>
      <file type="M">src.test.org.apache.nutch.parse.TestParseData.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-2 01:00:00" id="2490" opendate="2018-1-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sitemap processing: Sitemap index files not working</summary>
      <description>The sitemap processing feature does not properly handle sitemap index files due to a unnecessary conditional.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-3 01:00:00" id="2491" opendate="2018-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate sitemap processing and HostDB into crawl script</summary>
      <description>Add three new steps to the crawl bash script:1. Generate HostDB from CrawlDB2. Inject URLs from sitemaps URLs found in hosts from HostDb3. If given, inject sitemap URLs specified in a configuration file / in configuration files</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-3 01:00:00" id="2492" opendate="2018-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more configuration parameters to crawl script</summary>
      <description>Instead of having to copy and adjust the crawl script in order to specify the following configuration options allow the user to pass them in using arguments: numSlaves numTasks sizeFetchlist timeLimitFetch numThreads</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="2493" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add configuration parameter for sitemap processing to crawler script</summary>
      <description>While using the crawler script with the sitemap processing feature introduced in NUTCH-2491 I encountered some performance issues when working with large sitemaps.Therefore one should be able to specify if sitemap processing based on HostDB should take place and if so how frequently it should be done.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2006-5-4 01:00:00" id="264" opendate="2006-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tools for merging and filtering CrawlDb-s and LinkDb-s</summary>
      <description>This patch contains implementations and unit tests for two new commands: mergedb: merges one or more CrawlDb-s, optionally filtering urls through the current URLFilters. mergelinkdb: as above, only for LinkDb-s. Optional filtering is applied both to toUrls and fromUrls in Inlinks.</description>
      <version>0.8</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.protocol.Content.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2006-6-21 01:00:00" id="275" opendate="2006-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetcher not parsing XHTML-pages at all</summary>
      <description>Server reports page as "text/html" - so I thought it would be processed as html.But something I guess evaluated the headers of the document and re-labeled it as "text/xml" (why not text/xhtml?).For some reason there is no plugin to be found for indexing text/xml (why does TextParser not feel responsible?).Links inside this document are NOT indexed at all - no digging this website actually stops here.Funny thing: For some magical reasons the dtd-files referenced in the header seem to be valid links for the fetcher and as such are indexed in the next round (if urlfilter allows).060521 025018 fetching http://www.secreturl.something/060521 025018 http.proxy.host = null060521 025018 http.proxy.port = 8080060521 025018 http.timeout = 10000060521 025018 http.content.limit = 65536060521 025018 http.agent = NutchCVS/0.8-dev (Nutch; http://lucene.apache.org/nutch/bot.html; nutch-agent@lucene.apache.org)060521 025018 fetcher.server.delay = 1000060521 025018 http.max.delays = 1000060521 025018 ParserFactory:Plugin: org.apache.nutch.parse.text.TextParser mapped to contentType text/xml via parse-plugins.xml, but its plugin.xml file does not claim to support contentType: text/xml060521 025018 ParserFactory: Plugin: org.apache.nutch.parse.rss.RSSParser mapped to contentType text/xml via parse-plugins.xml, but not enabled via plugin.includes in nutch-default.xml060521 025019 Using Signature impl: org.apache.nutch.crawl.MD5Signature060521 025019 map 0% reduce 0%060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s, 060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s,</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.mime-types.xml</file>
      <file type="M">src.test.org.apache.nutch.util.mime.test.xml</file>
      <file type="M">src.test.org.apache.nutch.util.mime.mime-types.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2006-2-22 01:00:00" id="279" opendate="2006-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additions for regex-normalize</summary>
      <description>Imho needed:1) Extend normalize-rules to commonly used session-id's etc.2) Ship a checker to check rules easily by hand</description>
      <version>0.8</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2006-6-2 01:00:00" id="295" opendate="2006-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>More description for fetcher.threads.fetch property</summary>
      <description>Added some description to the fetcher.threads.fetch property to explain the number of threads running in a cluster. Patch is attached.</description>
      <version>0.8</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2006-11-9 01:00:00" id="305" opendate="2006-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update crawl and url filter lists to exclude jpeg|JPEG|bmp|BMP</summary>
      <description/>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.suffix-urlfilter.txt</file>
      <file type="M">conf.regex-urlfilter.txt.template</file>
      <file type="M">conf.crawl-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2006-9-28 01:00:00" id="332" opendate="2006-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>doubling score causes by page internal anchors.</summary>
      <description>When a page has no outlinks but several links to itself e.g. it has a set of anchors the scores of the page are distributed to its outlinks. But all this outlinks pointing to the page back. This causes that the page score is doubled. I'm not sure but may be this causes also a never ending fetching loop of this page, since outlinks with the status of CrawlDatum.STATUS_LINKED are set CrawlDatum.STATUS_DB_UNFETCHED in CrawlDBReducer line: 107. So may be the status fetched will be overwritten with unfetched. In such a case we fetch the page every-time again and also every-time double the score of this page what causes very high scores without any reasons.</description>
      <version>0.8</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2006-8-4 01:00:00" id="340" opendate="2006-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug(s) in 0.8 tutorial</summary>
      <description>There seems to be error(s) in whole web crawling section. This generates constantly (unneccessary) traffic to users list.</description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.src.documentation.content.xdocs.tutorial8.xml</file>
      <file type="M">site.tutorial8.pdf</file>
      <file type="M">site.tutorial8.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2006-8-12 01:00:00" id="347" opendate="2006-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build: plugins&amp;#39; Jars not found</summary>
      <description>While building Nutch, I noticed several places where various Jars from plugins' lib directories could not be found, for example:$ ant package...deploy: &amp;#91;copy&amp;#93; Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-log4j/lib-log4j.jar to copy.init:init-plugin:compile:jar:deps-test:deploy: &amp;#91;copy&amp;#93; Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-nekohtml/lib-nekohtml.jar to copy....The problem is, these "lib-XXXX.jar" files do not exist. Instead, those Jars are typically named with a version in the name, like log4j-1.2.11.jar. I could not find where this "lib-" prefix comes from, nor where the version is dropped from the name. Anyone knows?In order to avoid these errors I had to make symbolic links and fake things:e.g. ln -s log4j-1.2.11.jar lib-log4j.jarBut this should really be fixed somewhere, I just can't see where... Note that this doesn't completely break the build, but missing Jars can't be a good thing.</description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build-plugin.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2006-6-11 01:00:00" id="385" opendate="2006-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve description of thread related configuration for Fetcher</summary>
      <description>For some time I've been puzzled by the interaction between two paramters that control how often the fetcher can access a particular host:1) The server delay, which comes back from the remote server during our processing of the robots.txt file, and which can be limited by fetcher.max.crawl.delay.2) The fetcher.threads.per.host value, particularly when this is greater than the default of 1.According to my (limited) understanding of the code in HttpBase.java:Suppose that fetcher.threads.per.host is 2, and that (by chance) the fetcher ends up keeping either 1 or 2 fetcher threads pointing at a particular host continuously. In other words, it never tries to point 3 at the host, and it always points a second thread at the host before the first thread finishes accessing it. Since HttpBase.unblockAddr never gets called with (((Integer)THREADS_PER_HOST_COUNT.get(host)).intValue() == 1), it never puts System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. Thus, the server delay will never be used at all. The fetcher will be continuously retrieving pages from the host, often with 2 fetchers accessing the host simultaneously.Suppose instead that the fetcher finally does allow the last thread to complete before it gets around to pointing another thread at the target host. When the last fetcher thread calls HttpBase.unblockAddr, it will now put System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. This, in turn, will prevent any threads from accessing this host until the delay is complete, even though zero threads are currently accessing the host.I see this behavior as inconsistent. More importantly, the current implementation certainly doesn't seem to answer my original question about appropriate definitions for what appear to be conflicting parameters. In a nutshell, how could we possibly honor the server delay if we allow more than one fetcher thread to simultaneously access the host?It would be one thing if whenever (fetcher.threads.per.host &gt; 1), this trumped the server delay, causing the latter to be ignored completely. That is certainly not the case in the current implementation, as it will wait for server delay whenever the number of threads accessing a given host drops to zero.</description>
      <version>None</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2007-2-17 01:00:00" id="567" opendate="2007-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper (?) handling of URIs in TagSoup.</summary>
      <description>Doug Cook reported that TagSoup incorrectly handles some URI parameters. More discussion on the list and at TagSoup's mailing list.http://tech.groups.yahoo.com/group/tagsoup-friends/message/838I looked at the sources of TagSoup because I'm using it myself (although the URIs are not relevant for me). It seems like you can implement a naive workaround by remembering the parsing state and just avoiding entity resolution. Attached is the patch that does this.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.plugin.xml</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup.LICENSE.txt</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup-1.0rc3.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-9-18 01:00:00" id="621" opendate="2008-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch needs to declare it&amp;#39;s crypto usage</summary>
      <description>Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).See TIKA-118.</description>
      <version>0.7,0.7.1,0.7.2,0.8,0.8.1,0.9.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-9-19 01:00:00" id="651" opendate="2008-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bin/{start|stop}-balancer.sh from svn tracking</summary>
      <description>Files bin/{start|stop}-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version control.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-balancer.sh</file>
      <file type="M">bin.start-balancer.sh</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>