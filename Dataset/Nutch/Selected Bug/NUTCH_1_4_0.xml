<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="NUTCH">
  
  <bug fixdate="2011-7-23 01:00:00" id="1011" opendate="2011-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize duplicate slashes in URL&amp;#39;s</summary>
      <description>Many websites produce faulty URL's with multiple slashes e.g. http://cocoon.apache.org///////////////////////1.x/dynamic.htmlThis can be really nasty if the number of slashes varies, resulting in many URL's actually pointing to the same page and generating new (unique) URL's to the same or other duplicate pages.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.net.TestURLNormalizers.java</file>
      <file type="M">conf.regex-normalize.xml.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-28 01:00:00" id="1022" opendate="2011-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version number of Nutch agent in conf</summary>
      <description/>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-7-12 01:00:00" id="1043" opendate="2011-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pattern for filtering .js in default url filters</summary>
      <description>The Javascript parser is not used by default as it is extremely noisy, however the default URL filters do not filter out URLs ending in .js and the default parser (Tika) can't parse them. In a nutshell we are fetching URLS that we know can't be parsed.I suggest that we add a regex to the default URL filters. If people are interested in fetching and parsing .js files they can activate the plugin in their conf and remove the regex in the URL filters.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.regex-urlfilter.txt.template</file>
      <file type="M">conf.automaton-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-5-15 01:00:00" id="1053" opendate="2011-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parsing of RSS feeds fails</summary>
      <description>See discussion on http://lucene.472066.n3.nabble.com/RSS-feed-parsing-on-Nutch-1-3-td3166487.htmlHave been able to reproduce the problem and will look into it</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.feed.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-15 01:00:00" id="1054" opendate="2011-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make linkDB optional during indexing</summary>
      <description>Having a linkDB is currently mandatory for indexing, however not all users are interested in using the anchors. The linkDB should be optional while indexing</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-8-25 01:00:00" id="1069" opendate="2011-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Readlinkdb broken on Hadoop &gt; 0.20</summary>
      <description>reading the linkdb doesn't work on Hadoop 0.20+. It believes data is to be read from the _SUCCESS file that is written by newer Hadoop version.Quick fix is to remove the _SUCCESS file</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbMerger.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-23 01:00:00" id="1087" opendate="2011-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate crawl command and replace with example script</summary>
      <description>remove the crawl command add basic crawl shell scriptSee thread:http://www.mail-archive.com/dev@nutch.apache.org/msg03848.html</description>
      <version>1.4</version>
      <fixedVersion>1.6,2.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-1-24 01:00:00" id="1119" opendate="2011-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for index-static</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-static.src.java.org.apache.nutch.indexer.staticfield.StaticFieldIndexer.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-24 01:00:00" id="1125" opendate="2011-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for tld</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>2.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.CrawlDBScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-24 01:00:00" id="1126" opendate="2011-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for urlfilter-prefix</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.8,2.2.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-24 01:00:00" id="1127" opendate="2011-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for urlfilter-validator</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-7 01:00:00" id="1154" opendate="2011-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Tika 0.10</summary>
      <description>There have been significant improvements in Tika 0.10 and it would be nice to use the latest Tika in 1.4.</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.tika.TestRTFParser.java</file>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-11-2 01:00:00" id="1192" opendate="2011-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;/runtime&amp;#39; to svn ignore</summary>
      <description>Add '/runtime' to svn ignore. The .gitignore file already has the fix.</description>
      <version>None</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-4-22 01:00:00" id="1208" opendate="2011-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t include KEYS file in bin distribution</summary>
      <description>We should get rid of the KEYS file in the bin packaging (zip/tar) in 1.5.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-1-27 01:00:00" id="1237" opendate="2011-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve javac arguements for more verbose output</summary>
      <description>When trying to fix another problem I stumbled across this one. I think it is important to ensure that the javac outputs info regarding deprecation and unchecked operations.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>nutchgora,1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">default.properties</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-29 01:00:00" id="1239" opendate="2011-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webgraph should remove deleted pages from segment input</summary>
      <description>Webgraph's outlink job is currently unable to remove links. It should expand it's segment input and be able to remove nodes for pages that no longer exist.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.WebGraph.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.NutchWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-25 01:00:00" id="1259" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store detected content type in crawldatum metadata</summary>
      <description>The MIME-type detected by Tika's Detect() API is never added to a Parse's ContentMetaData or ParseMetaData. Because of this bad Content-Types will end up in the documents.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2012-3-14 01:00:00" id="1310" opendate="2012-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch to send HTTP-accept header</summary>
      <description>Nutch does not send a HTTP-accept header with its requests. This is usually not a problem but some firewall do not like it and will reject the request.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-2-2 01:00:00" id="1323" opendate="2012-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AjaxNormalizer</summary>
      <description>A two-way normalizer for Nutch able to deal with AJAX URL's, converting them to escaped_fragment URL's and back to an AJAX URL.https://developers.google.com/webmasters/ajax-crawling/</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-7-2 01:00:00" id="1327" opendate="2012-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryStringNormalizer</summary>
      <description>A normalizer for dealing with query strings. Sorting query strings is helpful in preventing duplicates for some (bad) websites.</description>
      <version>None</version>
      <fixedVersion>1.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-12-11 01:00:00" id="1331" opendate="2012-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>limit crawler to defined depth</summary>
      <description>there is a need to limit crawler to some defined depth, and importance of this option is to avoid crawling of infinite loops, with dynamic generated urls, that occur in some sites, and to optimize crawler to select important urls.an option is define a iteration limit on generate,fetch,parse,updatedb cycle, but it works only if in each cycle, all of unfetched urls become fetched, (without recrawling them and with some other considerations)we can define a new parameter in CrawlDatum, named depth, and like score-opic algorithm, compute depth of a link after parse, and in generate, only select urls with valid depth.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-1 01:00:00" id="783" opendate="2010-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IndexingFiltersChecker Utility</summary>
      <description>This patch contains a new utility which allows to check the configuration of the indexing filters. The IndexingFiltersChecker reads and parses a URL and run the indexers on it. Displays the fields obtained and the first 100 characters of their value.Can be used e.g. ./nutch org.apache.nutch.indexer.IndexingFiltersChecker http://www.lemonde.fr/</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-3-1 01:00:00" id="784" opendate="2010-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CrawlDBScanner</summary>
      <description>The patch file contains a utility which dumps all the entries matching a regular expression on their URL. The dump mechanism of the crawldb reader is not very useful on large crawldbs as the ouput can be extremely large and the -url function can't help if we don't know what url we want to have a look at.The CrawlDBScanner can either generate a text representation of the CrawlDatum-s or binary objects which can then be used as a new CrawlDB. Usage: CrawlDBScanner &lt;crawldb&gt; &lt;output&gt; &lt;regex&gt; &amp;#91;-s &lt;status&gt;&amp;#93; &lt;-text&gt;regex: regular expression on the crawldb key-s status : constraint on the status of the crawldb entries e.g. db_fetched, db_unfetched-text : if this parameter is used, the output will be of TextOutputFormat; otherwise it generates a 'normal' crawldb with the MapFileOutputFormatfor instance the command below : ./nutch com.ant.CrawlDBScanner crawl/crawldb /tmp/amazon-dump .+amazon.com.* -s db_fetched -textwill generate a text file /tmp/amazon-dump containing all the entries of the crawldb matching the regexp .+amazon.com.* and having a status of db_fetched</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-4-2 01:00:00" id="809" opendate="2010-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse-metatags plugin</summary>
      <description>Parse-metatags pluginThe parse-metatags plugin consists of a HTMLParserFilter which takes as parameter a list of metatag names with '*' as default value. The values are separated by ';'.In order to extract the values of the metatags description and keywords, you must specify in nutch-site.xml&lt;property&gt; &lt;name&gt;metatags.names&lt;/name&gt; &lt;value&gt;description;keywords&lt;/value&gt;&lt;/property&gt;The MetatagIndexer uses the output of the parsing above to create two fields 'keywords' and 'description'. Note that keywords is multivalued.The query-basic plugin is used to include these fields in the search e.g. in nutch-site.xml&lt;property&gt; &lt;name&gt;query.basic.description.boost&lt;/name&gt; &lt;value&gt;2.0&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;query.basic.keywords.boost&lt;/name&gt; &lt;value&gt;2.0&lt;/value&gt;&lt;/property&gt;This code has been developed by DigitalPebble Ltd and offered to the community by ANT.com</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.5</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2005-2-15 01:00:00" id="81" opendate="2005-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webapp only works when deployed in root</summary>
      <description>Index.jsp does a redirect (not forward) to the language folder.The links in the html however are relative to the language folder, not the application root.Not sure what the desired behavoir is, change the html (where is it generated?) or the redirect.</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.jsp.index.jsp</file>
      <file type="M">src.web.jsp.search.jsp</file>
    </fixedFiles>
  </bug>
</bugrepository>