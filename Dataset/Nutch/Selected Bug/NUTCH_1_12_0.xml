<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="NUTCH">
  
  <bug fixdate="2015-10-12 01:00:00" id="2137" opendate="2015-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add changes.txt and ALV2 headers to the Naive Bayes Parse Filter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.Train.java</file>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.Classify.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-16 01:00:00" id="2252" opendate="2016-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow phantomjs as a browser for selenium options</summary>
      <description>Adding phantomjs libraries to lib-selenium so you can choose this as a browser with the selenium option</description>
      <version>1.12</version>
      <fixedVersion>1.12</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-6 01:00:00" id="2321" opendate="2016-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexing filter checker leaks threads</summary>
      <description>Same issue as NUTCH-2320.</description>
      <version>1.12</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.AbstractChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-18 01:00:00" id="2327" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Seeds injected in REST workflow must be ingested into HDFS</summary>
      <description>Right now when one uses the REST POST /seed/create API, a directory is created within /var/some/path/here which is create if you are working locally with the Nutch server e.g. on one machine. It is however not suitable for using the REST API in distributed deployments where seeds needs to be present within HDFS. More documentation on this topic is available at https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI#Seed_List_creationThere are also various mailing list threads regarding use of the REST and this injector url issue described above needs to be addressed.sujenshah CC for context.http://www.mail-archive.com/user%40nutch.apache.org/msg14922.htmlhttp://www.mail-archive.com/user%40nutch.apache.org/msg14921.html</description>
      <version>1.12</version>
      <fixedVersion>1.13</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.resources.SeedResource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-2 01:00:00" id="2333" opendate="2016-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexer for RabbitMQ</summary>
      <description>A plugin to send the documents to a RabbitMQ server.</description>
      <version>1.12</version>
      <fixedVersion>1.14</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitMQConstants.java</file>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitIndexWriter.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-18 01:00:00" id="2353" opendate="2017-1-18 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Create seed file with metadata using the REST API</summary>
      <description>At the moment its not possible to create a seed file and specify any metadata when using the REST API. The file gets created but there is no option to add any metadata to the seed URLs.If we use a payload like this:{ "name":"name-of-seedlist", "seedUrls":[ { "url" : "http://example.com", "metadata" : { "key1" : "value1", "key2" : "value2", "key3" : "value3" } } ]}It should be easy to specify the desired metadata. Also this should keep BC with the previous array syntax if we only want to specify the list of URLs without any metadata at all.</description>
      <version>1.12</version>
      <fixedVersion>1.20</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.webui.model.SeedUrl.java</file>
      <file type="M">src.java.org.apache.nutch.service.resources.SeedResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.model.request.SeedUrl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-20 01:00:00" id="2354" opendate="2017-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hadoop dependencies to 2.7.4</summary>
      <description>This wednesday we experienced trouble running the 1.12 injector on Hadoop 2.7.3. We operated 2.7.2 before and we had no trouble running a job.2017-01-18 15:36:53,005 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.Counter, but class was expected at org.apache.nutch.crawl.Injector$InjectMapper.map(Injector.java:216) at org.apache.nutch.crawl.Injector$InjectMapper.map(Injector.java:100) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)Exception in thread "main" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.Counter, but class was expected at org.apache.nutch.crawl.Injector.inject(Injector.java:383) at org.apache.nutch.crawl.Injector.run(Injector.java:467) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.nutch.crawl.Injector.main(Injector.java:441) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:221) at org.apache.hadoop.util.RunJar.main(RunJar.java:136)Our processes retried injecting for a few minutes until we manually shut it down. Meanwhile on HDFS, our CrawlDB was gone, thanks for snapshots and/or backups we could restore it, so enable those if you haven't done so yet.These freak Hadoop errors can be notoriously difficult to debug but it seems we are in luck, recompile Nutch with Hadoop 2.7.3 instead 2.4.0. You are also in luck if your job file uses the old org.hadoop.mapred.* API, only jobs using the org.hadoop.mapreduce.* API seem to fail.</description>
      <version>1.12</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-31 01:00:00" id="2355" opendate="2017-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protocol plugins to set cookie if Cookie metadata field is present</summary>
      <description/>
      <version>1.12</version>
      <fixedVersion>1.13</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-27 01:00:00" id="2362" opendate="2017-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade MaxMind GeoIP version in index-geoip</summary>
      <description>Current version of GeoIP dependency is 2.8.1, we should upgradehttp://search.maven.org/#search|gav|1|g%3A%22com.maxmind.geoip2%22%20AND%20a%3A%22geoip2%22</description>
      <version>None</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-geoip.plugin.xml</file>
      <file type="M">src.plugin.index-geoip.ivy.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-30 01:00:00" id="2468" opendate="2017-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>should filter out invalid URLs by default</summary>
      <description>Some Nutch components, by default, should reject invalid URLs. This was recently discussed in the users mailing list and has affected my work for a while. Although there may be some special-purpose needs to collect invalid URLs, they are not generally useful for crawling.</description>
      <version>1.12</version>
      <fixedVersion>2.4,1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>