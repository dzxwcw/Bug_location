<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1232" opendate="2011-12-21 00:00:00" fixdate="2011-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove host field from index-basic</summary>
      <description>Either fields needs to be removed, it makes no sense to have two identical values for separate fields. I propose to get rid of the site field and leave the host field. This may be a breaking change for some installations however.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">conf.solrindex-mapping.xml</file>
      <file type="M">conf.schema.xml</file>
      <file type="M">conf.schema-solr4.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2429" opendate="2017-9-22 00:00:00" fixdate="2017-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Plugin System to allow protocol plugins to bundle their URLStreamHandlers</summary>
      <description>While trying to use the protocol-smb plugin (which is not part of the Nutch distribution) I realized there are four steps to successfully make use of a protocol plugin:1 - put the artifact into the plugins directory2 - modify Nutch configuration files to allow smb:// urls plus include the plugin to the loaded list3 - extract jcifs.jar and place it on the system classpath4 - run nutch with the correct system propertyWhile steps 1 and 2 seem obvious, 3 and 4 require knowledge of plugin internals which does not feel right for nutch and plugin users. Even more, the jcifs.jar would exist twice on the classpath and could even cause further problems during runtime.</description>
      <version>1.14</version>
      <fixedVersion>1.19</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitIndexWriter.java</file>
      <file type="M">src.plugin.indexer-csv.src.java.org.apache.nutch.indexwriter.csv.CSVIndexWriter.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.Any23ParseFilter.java</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.Any23IndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
      <file type="M">src.java.org.apache.nutch.util.NutchTool.java</file>
      <file type="M">src.java.org.apache.nutch.util.NutchJob.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.CrawlCompletionStats.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginRepository.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginManifestParser.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2457" opendate="2017-11-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Embedded documents likely not correctly parsed by Tika</summary>
      <description>While working on TIKA-2490, I think I found that Nutch's current method of requesting a mime-specific parser for each file will fail to parse embedded files, e.g. https://github.com/apache/tika/blob/master/tika-server/src/test/resources/test_recursive_embedded.docxThe fix should be straightforward, and I'll submit a PR once I can get Nutch up and running in my dev environment.</description>
      <version>1.14</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.TikaParser.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestEmbeddedDocuments.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestRTFParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestPdfParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestOOParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestMSWordParser.java</file>
      <file type="M">src.plugin.parse-tika.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2461" opendate="2017-11-14 00:00:00" fixdate="2017-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate passes the data to when maxCount == 0</summary>
      <description>The generator checks condition if (maxCount &gt; 0) : line 421 and stop the generation when amount per host exceeds maxCount( continue : line 455)but when maxCount == 0 it goes directly to line 465 :output.collect(key, entry);It is obviously not correct, the correct solution would be to add if(maxCount == 0){ continue;}at line 380.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2465" opendate="2017-11-27 00:00:00" fixdate="2017-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Broken Eclipse project. Classpaths and interactiveselenium should be fixed.</summary>
      <description>With the latest version of develop the Eclipse project doesn't work anymore.There are two sets of problem:1) Classpath problems 2) Incorrect usage of org.apache.nutch.protocol.interactiveselenium in the code. Should be replaced by org.apache.nutch.protocol.interactiveselenium.handlers</description>
      <version>1.14</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.InteractiveSeleniumHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefaultHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefaultClickAllAjaxLinksHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefalultMultiInteractionHandler.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2501" opendate="2018-1-22 00:00:00" fixdate="2018-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow to set Java heap size when using crawl script in distributed mode</summary>
      <description></description>
      <version>1.14</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug id="2502" opendate="2018-1-23 00:00:00" fixdate="2018-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Any23 Plugin: Add Content-Type filtering</summary>
      <description>It should be possible to filter based on a document's Content-Type when using Any23 extractors.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.any23.src.test.org.apache.nutch.any23.TestAny23ParseFilter.java</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.Any23ParseFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2505" opendate="2018-1-27 00:00:00" fixdate="2018-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nutch does not delete the .locked file, when the generator partition got an exception</summary>
      <description>nutch does not delete the .locked file when the generator partition got an exception. </description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2508" opendate="2018-1-31 00:00:00" fixdate="2018-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misleading documentation about http.proxy.exception.list</summary>
      <description>The description about http.proxy.exception.list states that domains as well as URLs can be configured to be excluded from being routed through a pre-configured proxy. This is misleading since only hosts are being checked when using this feature.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2523" opendate="2018-3-5 00:00:00" fixdate="2018-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateHostDB blocks usage of plugins unintentionally</summary>
      <description>UpdateHostDB blocks the use of urlnormalizer-host and urlfilter-domainblacklist (it check if they are configured and throws an exception) without any good reason.Quoting Markus: "I simply reused the job setup code and forgot to remove that check. You can safely remove that check in HostDB."</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDb.java</file>
    </fixedFiles>
  </bug>
  <bug id="2527" opendate="2018-3-7 00:00:00" fixdate="2018-4-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>URL filter: provide rules to exclude localhost and private address spaces</summary>
      <description>While checking the log files of a large web crawl, I've found hundreds of (luckily failed) requests of local or private content:2018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetching http://127.0.0.42/ ...018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetch of http://127.0.0.42/ failed with: java.net.ConnectException: Connection refused (Connection refused)URLs pointing to localhost, loop-back addresses, private address spaces should be blocked for a wider web crawl where links are not controlled to avoid that information is leaked by links or redirects pointing to web interfaces of services running on the crawling machines (e.g., HDFS, Hadoop YARN).Of course, this must be optional. For testing it's quite common to crawl your local machine.</description>
      <version>2.3.1,1.14</version>
      <fixedVersion>2.4,1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.regex-urlfilter.txt.template</file>
    </fixedFiles>
  </bug>
  <bug id="2555" opendate="2018-4-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>URL normalization problem: path not starting with a &amp;#39;/&amp;#39;</summary>
      <description>When an URL does not have a path but has GET parameters (for instance 'http://example.com?a=1') it should be normalized to add a '/' at the beginning of the path (giving http://example.com/?a=1). Our logs show that non-normalized URLs reach protocol-http, which then uses URL::getFile() to get the path, and tries to send an invalid HTTP request:GET ?a=1 HTTP/1.0instead ofGET /?a=1 HTTP/1.0 Example URL for which this poses a problem: http://news.fx678.com?171</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-basic.src.test.org.apache.nutch.net.urlnormalizer.basic.TestBasicURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-basic.src.java.org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.java</file>
      <file type="M">src.plugin.protocol-http.src.test.org.apache.nutch.protocol.http.TestBadServerResponses.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="2556" opendate="2018-4-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-http makes invalid HTTP/1.0 requests</summary>
      <description>protocol-http advertises its requests as being HTTP/1.0, but sends an Accept-Encoding request header, that is defined only in HTTP/1.1. This confuses some web servers Example: http://www.hansamanuals.com/main/english/none/theconf___987/manuals/version___82/hwconvindex.htm</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2558" opendate="2018-4-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-http cannot handle a missing HTTP status line</summary>
      <description>Some servers invalidly send an HTTP body directly without a status line or headers. Browsers handle that, protocol-http doesn't: Example: https://app.unitymedia.de/</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.test.org.apache.nutch.protocol.http.TestBadServerResponses.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="2559" opendate="2018-4-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-http cannot handle colons after the HTTP status code</summary>
      <description>Some servers invalidly add colons after the HTTP status code in the status line (they can send HTTP/1.1 404: Not found instead of HTTP/1.1 404 Not found for instance). Browsers can handle that.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.test.org.apache.nutch.protocol.http.TestBadServerResponses.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="2560" opendate="2018-4-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>protocol-http throws an error when an http header spans over multiple lines</summary>
      <description>Some servers invalidly send headers that span over multiple lines. In that case, browsers simply ignore the subsequent lines, but protocol-http throws an error, thus preventing us from fetching the contents of the page.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.test.org.apache.nutch.protocol.http.TestBadServerResponses.java</file>
    </fixedFiles>
  </bug>
  <bug id="2576" opendate="2018-5-9 00:00:00" fixdate="2018-6-9 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>HTTP protocol plugin based on okhttp</summary>
      <description>Okhttp is an Apache2-licensed http library which supports HTTP/2. jnioche's implementation storm-crawler#443 proves that it should be straightforward to implement a Nutch protocol plugin using okhttp. A recent HTTP protocol implementation should also fix (most of) the issues reported in NUTCH-2549.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-okhttp.src.test.org.apache.nutch.protocol.okhttp.TestBadServerResponses.java</file>
      <file type="M">src.plugin.protocol-okhttp.src.java.org.apache.nutch.protocol.okhttp.OkHttpResponse.java</file>
      <file type="M">src.plugin.protocol-okhttp.src.test.conf.nutch-site-test.xml</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-htmlunit.src.java.org.apache.nutch.protocol.htmlunit.HttpResponse.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.java.org.apache.nutch.net.protocols.Response.java</file>
      <file type="M">src.java.org.apache.nutch.metadata.HttpHeaders.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2577" opendate="2018-5-15 00:00:00" fixdate="2018-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-selenium can&amp;#39;t handle https</summary>
      <description>fetch of any https page is failing with: org.apache.nutch.protocol.ProtocolNotFound: protocol not found for url=httpsat org.apache.nutch.protocol.ProtocolFactory.getProtocol(ProtocolFactory.java:83)at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:687)</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-selenium.src.java.org.apache.nutch.protocol.selenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-selenium.plugin.xml</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.plugin.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2580" opendate="2018-5-21 00:00:00" fixdate="2018-6-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Improvements for Rabbitmq support</summary>
      <description>This one includes: Creation of lib-rabbitmq for common functionalities (publish-rabbitmq and indexer-rabbit). Update of the RabbitMQ's library version. Headers selection from NutchDocument's fields (for indexer-rabbit). Optional binding. A single or multiple documents into each message. Options for the creation of exchange, queue and binding. Simplify the configuration options.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexWriterParams.java</file>
      <file type="M">conf.index-writers.xml.template</file>
      <file type="M">src.plugin.publish-rabbitmq.src.java.org.apache.nutch.publisher.rabbitmq.RabbitMQPublisherImpl.java</file>
      <file type="M">src.plugin.publish-rabbitmq.plugin.xml</file>
      <file type="M">src.plugin.publish-rabbitmq.ivy.xml</file>
      <file type="M">src.plugin.publish-rabbitmq.build.xml</file>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitMQConstants.java</file>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitIndexWriter.java</file>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitDocument.java</file>
      <file type="M">src.plugin.indexer-rabbit.plugin.xml</file>
      <file type="M">src.plugin.indexer-rabbit.ivy.xml</file>
      <file type="M">src.plugin.indexer-rabbit.build.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2581" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Caching of redirected robots.txt may overwrite correct robots.txt rules</summary>
      <description>Redirected robots.txt rules are also cached for the target host. That may cause that the correct robots.txt rules are never fetched. E.g., http://wyomingtheband.com/robots.txt redirects to https://www.facebook.com/wyomingtheband/robots.txt. Because fetching fails with a 404 bots are allowed to crawl wyomingtheband.com. The rules is erroneously also cached for the redirect target host www.facebook.com which is clear regarding their robots.txt rules and does not allow crawling.Nutch may cache redirected robots.txt rules only if the path part (in doubt, including the query) of the redirect target URL is exactly /robots.txt.</description>
      <version>2.3.1,1.14</version>
      <fixedVersion>2.4,1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpRobotRulesParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="2583" opendate="2018-5-24 00:00:00" fixdate="2018-6-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrading Nutch&amp;#39;s dependencies</summary>
      <description>Hi, It would be nice to be able to upgrade all of Nutch's dependencies to the latest possible available.I've attached an Ivy.xml with the latest possible dependencies without breaking the compile. I've tested it with a few runs of the "crawl script", so far it seems to work, it generates, it fetches, it parses, it indexes to Solr..... Increasing any of this dependencies breaks the compile. PS: I haven't touched any of the Hadoop stuff and don't remember if I touched the testing part or not.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2595" opendate="2018-6-8 00:00:00" fixdate="2018-6-8 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade crawler-commons dependency to 0.10</summary>
      <description>See CHANGES</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2606" opendate="2018-6-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MIME detection is wrong for plain-text documents send as Content-Type "application/msword"</summary>
      <description>Plain-text documents send as Content-Type "application/msword" are tried to parse as Word documents. The MIME detection should be fixed, so that these are correctly identified as plain-text documents. See NUTCH-2603 and https://www.atnf.csiro.au/computing/software/gipsy/doc/update.doc</description>
      <version>1.14</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.TestMimeUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2611" opendate="2018-6-25 00:00:00" fixdate="2018-6-25 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add line-breaks when parsing HTML block-level elements</summary>
      <description>Currently, the HTML and Tika parser only add newlines following text-nodes that contain only whitespaces (e.g &lt;/span&gt; &lt;span&gt;), but not based on what the tags are, so for example a &lt;/div&gt;&lt;div&gt; will not add a new line.While some applications do not differentiate between a space and a new line, many others see the semantic difference (two following words in the same sentence are "near", but in separate sentences they are not).I believe adding newlines after block-level HTML elements, while not a panacea, will be an improvement on the current behavior.NUTCH-2318 is related to this.</description>
      <version>1.14</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2612" opendate="2018-6-26 00:00:00" fixdate="2018-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for sitemap processing by hostname</summary>
      <description>Add support to sitemap processor for processing just hostnames. Similar to the mapper eating sitemap URL's, but then with BaseRobotRules finding the sitemap URL's itself.Will upload patch soon.</description>
      <version>1.14</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2723" opendate="2019-6-19 00:00:00" fixdate="2019-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexer Solr not to decode URLs before deletion</summary>
      <description>URLs are indexed in their raw encoded form. But indexer-solr incorrectly decodes them just before they are sent for deletion, leading to a state where a bunch of URLs are never deleted.</description>
      <version>1.14</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-solr.src.java.org.apache.nutch.indexwriter.solr.SolrIndexWriter.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
