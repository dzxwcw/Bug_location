<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1232" opendate="2011-12-21 00:00:00" fixdate="2011-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove host field from index-basic</summary>
      <description>Either fields needs to be removed, it makes no sense to have two identical values for separate fields. I propose to get rid of the site field and leave the host field. This may be a breaking change for some installations however.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">conf.solrindex-mapping.xml</file>
      <file type="M">conf.schema.xml</file>
      <file type="M">conf.schema-solr4.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1945" opendate="2015-2-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Test for XLSX parser</summary>
      <description>Add a test for Excel spreadsheets (xlsx) files: because the are formally also zip files (as well as other composite files) the MIME type detection is crucial also for parsing, cf. NUTCH-1605 and NUTCH-1925.</description>
      <version>1.10,2.3.1</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestRTFParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestPdfParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestOOParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestMSWordParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestImageMetadata.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestHtmlParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestFeedParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestEmbeddedDocuments.java</file>
      <file type="M">src.plugin.parse-tika.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1994" opendate="2015-4-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Apache Tika 1.8</summary>
      <description>Tika 1.8 was released this morning.Lets upgrade then release Nutch trunk.</description>
      <version>1.10,2.3.1</version>
      <fixedVersion>1.10,2.3.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2149" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST endpoint to read Nutch sequence files</summary>
      <description>This endpoint enables reading of the webgraph data like nodes, links and any other sequence file in the Nutch ecosystem via a RESTful interface. The current API documentation for this Reader endpoint is available at - http://docs.nutchpytonutchrestapi.apiary.io/Thanks to https://github.com/ContinuumIO/nutchpy for the initial work.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2192" opendate="2015-12-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get rid of oro</summary>
      <description>Couple of classes still rely on oro, we should get rid of it.</description>
      <version>2.3.1,1.15</version>
      <fixedVersion>2.4,1.16</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-regex.sample.regex-normalize-scope1.xml</file>
      <file type="M">src.plugin.urlnormalizer-regex.sample.regex-normalize-default.xml</file>
      <file type="M">src.java.org.apache.nutch.parse.OutlinkExtractor.java</file>
      <file type="M">LICENSE.txt</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.regex-normalize.xml.template</file>
      <file type="M">conf.parse-plugins.xml</file>
      <file type="M">src.plugin.parse-js.src.test.org.apache.nutch.parse.js.TestJSParseFilter.java</file>
      <file type="M">src.plugin.parse-js.src.java.org.apache.nutch.parse.js.JSParseFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="220" opendate="2006-3-3 00:00:00" fixdate="2006-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PDF Box can&amp;#39;t parse document: java.lang.NullPointerException</summary>
      <description>This error was fixed in the ltest build of PDFBOx, which should be tested with nutch.&gt;&gt; 060228 160354 fetch okay, but can't parse&gt;&gt; http://www.mstc.state.ms.us/info/stats/transfer/tran0704.pdf, reason:&gt;&gt; failed(2,0): Can't be handled as pdf document. &gt;&gt; java.lang.NullPointerExceptionYes, the NPE should be fixed. BenRichard Braman wrote:&gt; Hi Bn,&gt;&gt; We actually got to the bottom of all of them except for 1... The &gt; content truncatetion was due to an inconsistancy bug in nutch config .&gt; The no permission to extract text is actually true, the author, the NC&gt; Department of revenue put this restriction on all of their files (I have&gt; asked them to remove it as it hampers public accessability). The Null&gt; pointer exception is the only one to deal with that may be due to the&gt; parsing bug . Is this one that you are referring to?&gt;&gt; ----Original Message----&gt; From: Ben Litchfield ben@csh.rit.edu&gt; Sent: Thursday, March 02, 2006 4:07 PM&gt; To: Richard Braman&gt; Cc: nutch-dev@lucene.apache.org; nutch-user@lucene.apache.org;&gt; pdfbox-user@lists.sourceforge.net&gt; Subject: Re: &amp;#91;PDFBox-user&amp;#93; PDF Parse Error&gt;&gt;&gt;&gt; I believe these errors are due to a parsing bug in PDFBox that has &gt; been fixed since the 0.7.2 release. Please give the nightly &gt; build(should be a drop in replacement) a try from &gt; http://www.pdfbox.org/dist and let me know if you are still having &gt; issues.&gt;&gt; Ben</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-pdf.plugin.xml</file>
      <file type="M">src.plugin.parse-pdf.lib.PDFBox-0.7.2-log4j.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="221" opendate="2006-3-4 00:00:00" fixdate="2006-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>prepare nutch for upcoming lucene 2.0</summary>
      <description>Remove all deprecated uses of lucene as they will vanish in 2.0</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.query-more.src.java.org.apache.nutch.searcher.more.DateQueryFilter.java</file>
      <file type="M">src.plugin.query-basic.src.java.org.apache.nutch.searcher.basic.BasicQueryFilter.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.LanguageIndexingFilter.java</file>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCIndexingFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCDeleteUnlicensedTool.java</file>
      <file type="M">src.java.org.apache.nutch.tools.PruneIndexTool.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.RawFieldQueryFilter.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.LuceneQueryOptimizer.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.FieldQueryFilter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexMerger.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
    </fixedFiles>
  </bug>
  <bug id="234" opendate="2006-3-17 00:00:00" fixdate="2006-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clustering extension code cleanups and a real JUnit test case for the current implementation.</summary>
      <description>I've cleaned up the code a bit and added a real test case for the clustering extension. This is in preparation for upgrading to the most recent Carrot2 codebase and I didn't want to mix these two patches together. I'd appreciate if somebody could review this patch so that I can integrate the newest C2 code this weekend. Thanks.</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.clustering-carrot2.src.test.org.apache.nutch.clustering.carrot2.ClustererTest.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClustererFactory.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClusterer.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.HitsCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="2397" opendate="2017-7-4 00:00:00" fixdate="2017-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parser to add paragraph line breaks</summary>
      <description>(initially reported with patch/pull-request by Vipul Behl, see #190)The parser (parse-tika and parse-html) could be improved to add line breaks between paragraphs, instead of writing the whole document into a single line.</description>
      <version>2.3.1,1.13</version>
      <fixedVersion>2.4,1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-zip.src.test.org.apache.nutch.parse.zip.TestZipParser.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2527" opendate="2018-3-7 00:00:00" fixdate="2018-4-7 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>URL filter: provide rules to exclude localhost and private address spaces</summary>
      <description>While checking the log files of a large web crawl, I've found hundreds of (luckily failed) requests of local or private content:2018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetching http://127.0.0.42/ ...018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetch of http://127.0.0.42/ failed with: java.net.ConnectException: Connection refused (Connection refused)URLs pointing to localhost, loop-back addresses, private address spaces should be blocked for a wider web crawl where links are not controlled to avoid that information is leaked by links or redirects pointing to web interfaces of services running on the crawling machines (e.g., HDFS, Hadoop YARN).Of course, this must be optional. For testing it's quite common to crawl your local machine.</description>
      <version>2.3.1,1.14</version>
      <fixedVersion>2.4,1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.regex-urlfilter.txt.template</file>
    </fixedFiles>
  </bug>
  <bug id="2581" opendate="2018-5-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Caching of redirected robots.txt may overwrite correct robots.txt rules</summary>
      <description>Redirected robots.txt rules are also cached for the target host. That may cause that the correct robots.txt rules are never fetched. E.g., http://wyomingtheband.com/robots.txt redirects to https://www.facebook.com/wyomingtheband/robots.txt. Because fetching fails with a 404 bots are allowed to crawl wyomingtheband.com. The rules is erroneously also cached for the redirect target host www.facebook.com which is clear regarding their robots.txt rules and does not allow crawling.Nutch may cache redirected robots.txt rules only if the path part (in doubt, including the query) of the redirect target URL is exactly /robots.txt.</description>
      <version>2.3.1,1.14</version>
      <fixedVersion>2.4,1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpRobotRulesParser.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
