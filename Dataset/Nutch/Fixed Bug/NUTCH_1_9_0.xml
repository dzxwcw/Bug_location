<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1825" opendate="2014-8-15 00:00:00" fixdate="2014-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-http may hang for certain web pages</summary>
      <description>There is a rare case where protocol-http will wait for data even when all the data has been sent.Patch is attached; please test and confirm.</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1832" opendate="2014-9-3 00:00:00" fixdate="2014-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Nutch work without an indexer</summary>
      <description>Nutch used to work out of the box, without requiring an indexing backend. As of 1.9, that's not the case anymore (it's possible even before that). Thanks to markus17 for pointing out that this is due to the indexing-solr plugin being enabled by default. We should disable it by default, so that the regression is removed.</description>
      <version>1.9</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1837" opendate="2014-9-8 00:00:00" fixdate="2014-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Tika 1.6</summary>
      <description>Tika 1.6 has just been released.Lets upgrade</description>
      <version>None</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1838" opendate="2014-9-9 00:00:00" fixdate="2014-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Host and domain based regex and automaton filtering</summary>
      <description>Both regex and automaton filter pass all URL's through all rules although this makes little sense if you have a lot of generated rules for many different hosts or domains. This patch allows the users to configure specific rules for a specific host or domain only, making filtering much more efficient.Each rule has an optional hostOrDomain field, the filter is applied for rules that have no hostOrDomain and for URL's that match the rule's host name and domain name.The following line enables hostOrDomain specific rules:&gt; www.example.orgThe following line disables/resets it again:&lt;full example:-some generic filter+another generic filter&gt; www.example.org-rule only applied to URL's of www.example.org+another rule only applied to URL's of www.example.org&gt; apache.org-rule only applied to URL's of apache.org+another rule only applied to URL's of apache.org&lt;-more generic rules+and another one</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-regex.src.test.org.apache.nutch.urlfilter.regex.TestRegexURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-regex.src.java.org.apache.nutch.urlfilter.regex.RegexURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-automaton.src.java.org.apache.nutch.urlfilter.automaton.AutomatonURLFilter.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexRule.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1854" opendate="2014-9-25 00:00:00" fixdate="2014-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>./bin/crawl fails with a parsing fetcher</summary>
      <description>If you run ./bin/crawl with a parsing fetcher e.g.&lt;property&gt;&gt; &lt;name&gt;fetcher.parse&lt;/name&gt;&gt; &lt;value&gt;false&lt;/value&gt;&gt; &lt;description&gt;If true, fetcher will parse content. Default is false,&gt; which means&gt; that a separate parsing step is required after fetching is&gt; finished.&lt;/description&gt;&gt; &lt;/property&gt;we get a horrible message as followsException in thread "main" java.io.IOException: Segment already parsed!We could improve this by making logging more complete and by adding a trigger to the crawl script which would check for crawl_parse for a given segment and then skip parsing if this is present.</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.segment.SegmentChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1857" opendate="2014-9-25 00:00:00" fixdate="2014-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>readb -dump -format csv should use comma</summary>
      <description>The -dump -format csv option currently uses ASCII character ';' %3B which is not a comma but instead a semi-colon.This is a pain in the back side as I always need to override this within the Solr update request.We should change the behavhiour to default to the common comma... as indicated herehttp://www.ietf.org/rfc/rfc4180.txt</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1866" opendate="2014-9-30 00:00:00" fixdate="2014-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ant eclipse target should not delete runtime</summary>
      <description>This is a pretty nasty bug which can really screw things up for you.The eclipse target should not delete runtime whenever invoked... this is terrible.</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1876" opendate="2014-10-16 00:00:00" fixdate="2014-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Crawler Commons 0.5</summary>
      <description>We just released crawler commons 0.5We should upgrade here in Nutch.https://code.google.com/p/crawler-commons/</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1877" opendate="2014-10-16 00:00:00" fixdate="2014-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suffix URL filter to ignore query string by default</summary>
      <description>Suffix URL filter entry: .mp3Does not filter out: http://www.example.org/file.mp3?a=b</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.suffix-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="188" opendate="2006-1-27 00:00:00" fixdate="2006-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add searchable mailing list links to http://lucene.apache.org/nutch/mailing_lists.html</summary>
      <description>Post links to searchable mail archives on nutch.org</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.include.footer.html</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1897" opendate="2014-12-11 00:00:00" fixdate="2014-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Easier debugging of plugin XML errors</summary>
      <description></description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.plugin.PluginManifestParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1913" opendate="2015-1-12 00:00:00" fixdate="2015-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LinkDB to implement db.ignore.external.links</summary>
      <description>LinkDB needs an option to ignore external links.</description>
      <version>1.9</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1919" opendate="2015-1-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting timeout when server returns Content-Length: 0</summary>
      <description>This has been investigated in fixed in the Storm-Crawler https://github.com/DigitalPebble/storm-crawler/issues/48.curl -I "http://www.dailynewslosangeles.com/"HTTP/1.1 301 Moved PermanentlyLocation: http://www.dailynews.comConnection: closeContent-Length: 0Content-Type: text/html; charset=UTF-8when fetching with Nutch we are getting a timeout exception :./nutch parsechecker -D http.agent.name="PebbleCrawler" "http://www.dailynewslosangeles.com/"fetching: http://www.dailynewslosangeles.com/Fetch failed with protocol status: exception(16), lastModified=0: java.net.SocketTimeoutException: Read timed outThe reason for this is that we are trying to read from the stream even though we know that the content length is 0.The patch attached fixes the issue.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1928" opendate="2015-1-30 00:00:00" fixdate="2015-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexing filter of documents by the MIME type</summary>
      <description>This allows to filter the indexed documents by the MIME type property of the crawled content. Basically this will allow you to restrict the MIME type of the contents that will be stored in Solr/Elasticsearch index without the need to restrict the crawling/parsing process, so no need to use URLFilter plugin family. Also this address one particular corner case when certain URLs doesn't have any format to filter such as some RSS feeds (http://www.awesomesite.com/feed) and it will end in your index mixed with all your HTML content.A configuration can file specified on the mimetype.filter.file property in the nutch-site.xml. This file use the same format as the urlfilter-suffix plugin. If no mimetype.filter.file key is found an allow all policy is used instead, so all your crawled documents will be indexed.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">default.properties</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1941" opendate="2015-2-11 00:00:00" fixdate="2015-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional rolling http.agent.name&amp;#39;s</summary>
      <description>In some scenarios, even whilst adhering to fetcher.crawl.delay, web admins can block your fetcher based merely on your crawler name. I propose the ability to implement rolling http.agent.name's which could be substituted every 5 seconds for example. This would mean that successive requests to the same domain would be sent with different http.agent.name. This behavior should be off by default.</description>
      <version>2.3,1.9</version>
      <fixedVersion>1.10,2.3.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2015-3-10 00:00:00" fixdate="2015-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Members to be public in URLCrawlDatum</summary>
      <description>URLCrawlDatum's datum member cannot be accessed from other unit tests.</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.crawl.CrawlDBTestUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1960" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for dump method of CommonCrawlDataDumper</summary>
      <description>Hi all,you can find in attachment the PATCH including an extremely simple JUnit test for dump method of CommonCrawlDataDumper class.Essentially, it checks if dump is able to create a given list of files from Butch segments (in testresources).Thanks a lot,Giuseppe</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1966" opendate="2015-3-17 00:00:00" fixdate="2015-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration endpoint for 1x REST API</summary>
      <description>The current work going on towards developing a REST service for the Nutch 1x branch includes many endpoints. This issue deals with the Configuration endpoint of the REST service. This is a sub issue for the major issue https://issues.apache.org/jira/browse/NUTCH-1931.</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1969" opendate="2015-3-18 00:00:00" fixdate="2015-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>URL Normalizer properly handling slashes</summary>
      <description>This is a URL normalizer we use that is simple to use and generate for dealing with hosts that mix up slash suffixed URL's with non-slash suffixed URL's.It is similar to the host nomalizer, reducing the number of duplicates while crawling. It takes the new line delimited rules, separated by either a tabulator or whitespace, followed by a + (PLUS) or - (MINUS) sign denoting whether or not a slash is to be added to the path.The normalizer ignores pages that look like files with extensions, see tests.Note: the normalizer must be enhanced to not take hosts as first argument of a rule, but host/path prefixes because some hosts need different rules depending on the root path. For example, example.org/cms/news/1/2/3/4 is a CMS that doesn't accept slashes, if they are suffixed, the user is redirected to a non-slash page; example.org/files/a/b/ wants to do it just the other way around.</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1976" opendate="2015-3-29 00:00:00" fixdate="2015-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Users to Set Hostname for Server</summary>
      <description>Users should be able to provide a hostname when starting the Nutch Server, rather than be stuck with localhost. Patch incoming.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1981" opendate="2015-4-2 00:00:00" fixdate="2015-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade icu4j</summary>
      <description>The icu4j version from 2009 is causing some compatibility issues with custom plugins we're developing. Please upgrade to a more recent version.I'm attaching a patch to this issue. Nutch builds and all tests pass without source code changes.</description>
      <version>2.3,1.9</version>
      <fixedVersion>1.10,2.3.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1986" opendate="2015-4-15 00:00:00" fixdate="2015-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify Elastic Search Indexer Plugin Settings</summary>
      <description>Was working on getting indexing into elastic search working and realized that the majority of my difficulties were simply me misunderstanding what the config needed. Patch incoming to hopefully clarify what is needed by default, what each option does, and add any helpful defaults.</description>
      <version>1.9</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1988" opendate="2015-4-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make nested output directory dump optional</summary>
      <description>NUTCH-1957 added nested directories to the bin/nutch dump output to help avoid naming conflicts in output files. It would be nice to be able to specify that you want the older flat directory output as an optional parameter.</description>
      <version>1.9</version>
      <fixedVersion>1.10,1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2004" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ParseChecker does not handle redirects</summary>
      <description>At the moment ParseChecker doesn't handle redirects. If it gets anything but a success status it errors out. It would be nice if it handled redirects a bit more gracefully based on the http.redirects config setting.</description>
      <version>1.9</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolStatus.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2141" opendate="2015-10-15 00:00:00" fixdate="2015-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the InteractiveSelenium plugin handler Interface to return page content</summary>
      <description>The handler interface in the protocol-interactiveselenium plugin currently provide methods to manipulate the page content and the HTTPResponse class read's the page content from the driver. This limits the amount of HTML content that could be returned to nutch.The processDriver method could return a String object instead. This is particularly helpful in cases such as handling pagination when multiple pages' content can be appended and returned from the handler.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.InteractiveSeleniumHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefaultHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefaultClickAllAjaxLinksHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefalultMultiInteractionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
