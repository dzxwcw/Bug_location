<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="102" opendate="2005-9-30 00:00:00" fixdate="2005-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>jobtracker does not start when webapps is in src</summary>
      <description>When starting the jobtracker from NUTCH_HOME by bin/nutch-daemon.sh start jobtrackerThe jobtracker search for the webapps folder in NUTCH_HOME, but it is under src/When manually copy the webapps folder into NUTCH_HOME jobtracker starts without any problems. Exception in thread "main" java.lang.NullPointerException at org.apache.nutch.mapred.JobTrackerInfoServer.&lt;init&gt;(JobTrackerInfoServer.java:67) at org.apache.nutch.mapred.JobTracker.&lt;init&gt;(JobTracker.java:232) at org.apache.nutch.mapred.JobTracker.startTracker(JobTracker.java:43) at org.apache.nutch.mapred.JobTracker.main(JobTracker.java:1043)</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="105" opendate="2005-10-7 00:00:00" fixdate="2005-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Network error during robots.txt fetch causes file to be ignored</summary>
      <description>Earlier we had a small network glitch which prevented us from retrievingthe robots.txt file for a site we were crawling at the time: nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193021 task_m_h02y5t Couldn't get robots.txt for http://www.japanesetranslator.co.uk/portfolio/: org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 10000 ms nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193031 task_m_h02y5t Couldn't get robots.txt for http://www.japanesetranslator.co.uk/translation/: org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 10000 msNutch then assumed that because we were unable to retrieve the file dueto network issues, that it didn't exist and we could crawl the entirewebsite. Nutch then successfully grabbed a few pages which were listedin the robots.txt as being disallowed.I think Nutch should continue attempting to retrieve the robots.txt fileuntil, at very least, we are able to establish a connection to the host,otherwise the host should be ignored until the next round of fetches.The webmaster of japanesetranslator.co.uk filed a complaint informing usof the issue.</description>
      <version>0.8,0.8.1,0.9.0</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1080" opendate="2011-8-11 00:00:00" fixdate="2011-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type safe members , arguments for better readability</summary>
      <description>Enable generics for some of the API, for better type safety and readability, in the process.</description>
      <version>None</version>
      <fixedVersion>1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-regex.src.test.org.apache.nutch.net.urlnormalizer.regex.TestRegexURLNormalizer.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.scoring.urlmeta.URLMetaScoringFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.Subcollection.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthenticationFactory.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthentication.java</file>
      <file type="M">src.plugin.feed.src.java.org.apache.nutch.parse.feed.FeedParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1180" opendate="2011-10-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateDB to backup previous CrawlDB</summary>
      <description>Nutch currently replaces an existing CrawlDB with the new CrawlDB. By optionally keeping a previous version on HDFS users can easily revert in case of a mistake without relying on external backup mechanims.This should be enabled by default.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="121" opendate="2005-10-21 00:00:00" fixdate="2005-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SegmentReader for mapred</summary>
      <description>The attached segment reader will dump segments approximately the same as 0.7 did with the -dump flag.Changes in the structure of the Data objects themselves causes slightly different output to be generated.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.util.StringUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.NutchConf.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="1210" opendate="2011-11-24 00:00:00" fixdate="2011-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DomainBlacklistFilter</summary>
      <description>The current DomainFilter acts as a white list. We also need a filter that acts as a black list so we can allow tld's and/or domains with DomainFilter but blacklist specific subdomains. If we would patch the current DomainFilter for this behaviour it would break current semantics such as it's precedence. Therefore i would propose a new filter instead.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1222" opendate="2011-12-14 00:00:00" fixdate="2011-4-14 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Upgrade to new Hadoop 0.22.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="131" opendate="2005-12-4 00:00:00" fixdate="2005-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Non-documented variable: mapred.child.heap.size</summary>
      <description>Got complaints about lack of heap space. Seems it was the children out of room for reduce of a updatedb.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="160" opendate="2006-1-1 00:00:00" fixdate="2006-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use standard Java Regex library rather than org.apache.oro.text.regex</summary>
      <description>org.apache.oro.text.regex is based on perl 5.003 which has some corner cases which perform poorly. The standard regular expression libraries for Java (1.4 and later) do not seen to contain these issues.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-regex.src.java.org.apache.nutch.net.RegexURLFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="173" opendate="2006-1-13 00:00:00" fixdate="2006-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PerHost Crawling Policy ( crawl.ignore.external.links )</summary>
      <description>There is two major way of crawl in Nutch.Intranet Crawl : forbidden all, allow somes few hostWhole-web crawl : allow all, forbidden few thinksI propose a third type of crawl.Directory Crawl : The purpose of this crawl is to manage few thousands of host wihtout managing rules pattern in UrlFilterRegexp.I made two patch for : 0.7, 0.7.1 and 0.8-devI propose a new boolean property in nutch-site.xml : crawl.ignore.external.links, with false value at default.By default this new feature don't modify the behavior of nutch crawler.When you setup this property to true, the crawler don't fetch external links of the host.So the crawl is limited to the host that you inject at the beginning at the crawl.I know there is some proposal of new crawl policy using the CrawlDatum in 0.8-dev branch. This feature colud be a easiest way to add quickly new crawl feature to nutch, waiting for a best way to improve crawl policy.I post two patch.Sorry for my very poor english &amp;#8211;Philippe</description>
      <version>0.7,0.7.1,0.8</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1970" opendate="2015-3-19 00:00:00" fixdate="2015-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pretty print JSON output in config resource</summary>
      <description>We can use Jackson to pretty print the JSON output from the Nutch server.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.resources.JobResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.resources.ConfigResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="201" opendate="2006-2-4 00:00:00" fixdate="2006-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for subcollections</summary>
      <description>Subcollection is a subset of an index. Subcollections are definedby urlpatterns in form of white/blacklist. So to get the page intosubcollection it must match the whitelist and not the blacklist.Subcollection definitions are read from a file subcollections.xmland the format is as follows (imagine here that you are crawling allthe virtualhosts from apache.org and you wan't to tag pages withurl pattern "http://lucene.apache.org/" to be part of subcollectionlucene.&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;subcollections&gt; &lt;subcollection&gt; &lt;name&gt;lucene&lt;/name&gt; &lt;id&gt;lucene&lt;/id&gt; &lt;whitelist&gt;http://lucene.apache.org/&lt;/whitelist&gt; &lt;blacklist /&gt; &lt;/subcollection&gt;&lt;/subcollections&gt;plugin contains indexingfilter, query filter and supporting classes</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2031" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Admin End point for Nutch 1.x REST service</summary>
      <description>This addresses the server administration endpoint for the REST service.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.resources.AbstractResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="207" opendate="2006-2-8 00:00:00" fixdate="2006-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bandwidth target for fetcher rather than a thread count</summary>
      <description>Increases or decreases the number of threads from the starting value (fetcher.threads.fetch) up to a maximum (fetcher.threads.maximum) to achieve a target bandwidth (fetcher.threads.bandwidth).It seems to be able to keep within 10% of the target bandwidth even when large numbers of errors are found or when a number of large pages is run across.To achieve more accurate tracking Nutch should keep track of protocol overhead as well as the volume of pages downloaded.</description>
      <version>0.8</version>
      <fixedVersion>1.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="209" opendate="2006-2-10 00:00:00" fixdate="2006-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>include nutch jar in mapred jobs</summary>
      <description>I just added a simple way in Hadoop to specify the job jar file. When constructing a JobConf one can specify a class whose containing jar is set to be the job's jar. To take advantage of this in Nutch, we could add a util class:public class NutchJob extends JobConf { public NutchJob(Configuration conf) { super(conf, NutchJob.class); }}Then change all of the places where we construct a JobConf to instead construct a NutchJob.Finally, we should add an ant target called 'job' that constructs a job jar, containing all of the classes and the plugins, and make this the default target. This way all Nutch code can be distributed with each job as it is submitted, and daemons would only need to be restarted when Hadoop code is updated.Does this sound reasonable?</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.NutchConfiguration.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">lib.hadoop-0.1-dev.jar</file>
      <file type="M">build.xml</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="242" opendate="2006-3-31 00:00:00" fixdate="2006-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add optional -urlFiltering to updatedb</summary>
      <description>Allow filtering the URLs completely out of the database during an updatedb run. This allows the regex-urlfilter.xml rules to be changed and the non-matching entries to be expunged from the database.Merging the functionality with updatedb was done for efficiency reasons since this eats up CPU time only where a separate job would use IO and CPU time.</description>
      <version>0.8</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.crawl.TestLinkDbMerger.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestCrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.LinkDbInlinks.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexSorter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexMerger.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
    </fixedFiles>
  </bug>
  <bug id="2420" opendate="2017-9-11 00:00:00" fixdate="2017-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in variable generate.max.count and fetcher.server.delay</summary>
      <description>Feature added by NUTCH-2368 does not work for multiple hosts. Once a HostDatum has been read by getHostDatum(), the next host cannot be read. Apparantly i need to open and close the SequenceFile.Readers for every HostDatum it needs. Reader has no reset() method or whatsoever.</description>
      <version>None</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
    </fixedFiles>
  </bug>
  <bug id="244" opendate="2006-4-5 00:00:00" fixdate="2006-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent handling of property values boundaries / unable to set db.max.outlinks.per.page to infinite</summary>
      <description>Some properties like file.content.limit support using negative numbers (-1) to 'disable' a limitation.Other properties do not support this. I tried disabling the limit set by db.max.outlinks.per.page, but this isn't possible.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.WritableTestUtils.java</file>
      <file type="M">src.test.org.apache.nutch.parse.TestParseData.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="246" opendate="2006-4-11 00:00:00" fixdate="2006-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>segment size is never as big as topN or crawlDB size in a distributed deployement</summary>
      <description>I didn't reopen NUTCH-136 since it is may related to the hadoop split.I tested this on two different deployement (with 10 ttrackers + 1 jobtracker and 9 ttracks and 1 jobtracker).Defining map and reduce task number in a mapred-default.xml does not solve the problem. (is in nutch/conf on all boxes)We verified that it is not a problem of maximum urls per hosts and also not a problem of the url filter.Looks like the first job of the Generator (Selector) already got to less entries to process. May be this is somehow releasted to split generation or configuration inside the distributed jobtracker since it runs in a different jvm as the jobclient.However we was not able to find the source for this problem.I think that should be fixed before publishing a nutch 0.8.</description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2490" opendate="2018-1-2 00:00:00" fixdate="2018-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sitemap processing: Sitemap index files not working</summary>
      <description>The sitemap processing feature does not properly handle sitemap index files due to a unnecessary conditional.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2491" opendate="2018-1-3 00:00:00" fixdate="2018-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate sitemap processing and HostDB into crawl script</summary>
      <description>Add three new steps to the crawl bash script:1. Generate HostDB from CrawlDB2. Inject URLs from sitemaps URLs found in hosts from HostDb3. If given, inject sitemap URLs specified in a configuration file / in configuration files</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug id="2492" opendate="2018-1-3 00:00:00" fixdate="2018-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more configuration parameters to crawl script</summary>
      <description>Instead of having to copy and adjust the crawl script in order to specify the following configuration options allow the user to pass them in using arguments: numSlaves numTasks sizeFetchlist timeLimitFetch numThreads</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug id="2493" opendate="2018-1-9 00:00:00" fixdate="2018-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add configuration parameter for sitemap processing to crawler script</summary>
      <description>While using the crawler script with the sitemap processing feature introduced in NUTCH-2491 I encountered some performance issues when working with large sitemaps.Therefore one should be able to specify if sitemap processing based on HostDB should take place and if so how frequently it should be done.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug id="261" opendate="2006-5-4 00:00:00" fixdate="2006-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi Language Support</summary>
      <description>Add multi-lingual support in Nutch, as described in http://wiki.apache.org/nutch/MultiLingualSupportThe document analysis part is actually implemented, and two analysis plugins (fr and de) are provided for testing (not deployed by default).The query analysis part is missing for a complete multi-lingual support.</description>
      <version>0.6,0.7,0.7.1,0.7.2,0.8</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.pages.sv.search.xml</file>
      <file type="M">src.web.pages.sr.search.xml</file>
      <file type="M">src.web.pages.sh.search.xml</file>
      <file type="M">src.web.pages.pt.search.xml</file>
      <file type="M">src.web.pages.pl.search.xml</file>
      <file type="M">src.web.pages.nl.search.xml</file>
      <file type="M">src.web.pages.ms.search.xml</file>
      <file type="M">src.web.pages.jp.search.xml</file>
      <file type="M">src.web.pages.it.search.xml</file>
      <file type="M">src.web.pages.hu.search.xml</file>
      <file type="M">src.web.pages.fr.search.xml</file>
      <file type="M">src.web.pages.fi.search.xml</file>
      <file type="M">src.web.pages.es.search.xml</file>
      <file type="M">src.web.pages.en.search.xml</file>
      <file type="M">src.web.pages.de.search.xml</file>
      <file type="M">src.web.pages.ca.search.xml</file>
      <file type="M">src.web.jsp.search.jsp</file>
      <file type="M">src.web.jsp.explain.jsp</file>
      <file type="M">src.java.org.apache.nutch.searcher.Query.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.OpenSearchServlet.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysisTokenManager.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysis.jj</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysis.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.AnalyzerFactory.java</file>
      <file type="M">docs.sv.search.html</file>
      <file type="M">docs.sr.search.html</file>
      <file type="M">docs.sh.search.html</file>
      <file type="M">docs.pt.search.html</file>
      <file type="M">docs.pl.search.html</file>
      <file type="M">docs.nl.search.html</file>
      <file type="M">docs.ms.search.html</file>
      <file type="M">docs.jp.search.html</file>
      <file type="M">docs.it.search.html</file>
      <file type="M">docs.hu.search.html</file>
      <file type="M">docs.fr.search.html</file>
      <file type="M">docs.fi.search.html</file>
      <file type="M">docs.es.search.html</file>
      <file type="M">docs.en.search.html</file>
      <file type="M">docs.de.search.html</file>
      <file type="M">docs.ca.search.html</file>
    </fixedFiles>
  </bug>
  <bug id="264" opendate="2006-5-4 00:00:00" fixdate="2006-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tools for merging and filtering CrawlDb-s and LinkDb-s</summary>
      <description>This patch contains implementations and unit tests for two new commands: mergedb: merges one or more CrawlDb-s, optionally filtering urls through the current URLFilters. mergelinkdb: as above, only for LinkDb-s. Optional filtering is applied both to toUrls and fromUrls in Inlinks.</description>
      <version>0.8</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.protocol.Content.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="275" opendate="2006-5-21 00:00:00" fixdate="2006-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetcher not parsing XHTML-pages at all</summary>
      <description>Server reports page as "text/html" - so I thought it would be processed as html.But something I guess evaluated the headers of the document and re-labeled it as "text/xml" (why not text/xhtml?).For some reason there is no plugin to be found for indexing text/xml (why does TextParser not feel responsible?).Links inside this document are NOT indexed at all - no digging this website actually stops here.Funny thing: For some magical reasons the dtd-files referenced in the header seem to be valid links for the fetcher and as such are indexed in the next round (if urlfilter allows).060521 025018 fetching http://www.secreturl.something/060521 025018 http.proxy.host = null060521 025018 http.proxy.port = 8080060521 025018 http.timeout = 10000060521 025018 http.content.limit = 65536060521 025018 http.agent = NutchCVS/0.8-dev (Nutch; http://lucene.apache.org/nutch/bot.html; nutch-agent@lucene.apache.org)060521 025018 fetcher.server.delay = 1000060521 025018 http.max.delays = 1000060521 025018 ParserFactory:Plugin: org.apache.nutch.parse.text.TextParser mapped to contentType text/xml via parse-plugins.xml, but its plugin.xml file does not claim to support contentType: text/xml060521 025018 ParserFactory: Plugin: org.apache.nutch.parse.rss.RSSParser mapped to contentType text/xml via parse-plugins.xml, but not enabled via plugin.includes in nutch-default.xml060521 025019 Using Signature impl: org.apache.nutch.crawl.MD5Signature060521 025019 map 0% reduce 0%060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s, 060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s,</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.mime-types.xml</file>
      <file type="M">src.test.org.apache.nutch.util.mime.test.xml</file>
      <file type="M">src.test.org.apache.nutch.util.mime.mime-types.txt</file>
    </fixedFiles>
  </bug>
  <bug id="279" opendate="2006-5-22 00:00:00" fixdate="2006-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additions for regex-normalize</summary>
      <description>Imho needed:1) Extend normalize-rules to commonly used session-id's etc.2) Ship a checker to check rules easily by hand</description>
      <version>0.8</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="292" opendate="2006-5-29 00:00:00" fixdate="2006-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OpenSearchServlet: OutOfMemoryError: Java heap space</summary>
      <description>java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space org.apache.nutch.searcher.FetchedSegments.getSummary(FetchedSegments.java:203) org.apache.nutch.searcher.NutchBean.getSummary(NutchBean.java:329) org.apache.nutch.searcher.OpenSearchServlet.doGet(OpenSearchServlet.java:155) javax.servlet.http.HttpServlet.service(HttpServlet.java:689) javax.servlet.http.HttpServlet.service(HttpServlet.java:802)The URL I use is:&amp;#91;...&amp;#93;something&amp;#91;...&amp;#93;/opensearch?query=mysearch&amp;start=0&amp;hitsPerSite=3&amp;hitsPerPage=20&amp;sort=urlIt seems to be a problem specific to the date I'm working with. Moving the start from 0 to 10 or changing the query works fine.Or maybe it doesn't have to do with sorting but it's just that I hit one "bad search-result" that has a broken summary?!! The problem is repeatable. So if anybody has an idea where to search / what to fix, I can easily try that out !!</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.summary-basic.src.java.org.apache.nutch.summary.basic.BasicSummarizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="293" opendate="2006-6-2 00:00:00" fixdate="2006-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>support for Crawl-delay in Robots.txt</summary>
      <description>Nutch need support for Crawl-delay defined in robots.txt, it is not a standard but a de-facto standard.See:http://help.yahoo.com/help/us/ysearch/slurp/slurp-03.htmlWebmasters start blocking nutch since we do not support it.</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="295" opendate="2006-6-2 00:00:00" fixdate="2006-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More description for fetcher.threads.fetch property</summary>
      <description>Added some description to the fetcher.threads.fetch property to explain the number of threads running in a cluster. Patch is attached.</description>
      <version>0.8</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3000" opendate="2023-9-13 00:00:00" fixdate="2023-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-selenium returns only the body,strips off the &lt;head/&gt; element</summary>
      <description>The selenium protocol returns only the body portion of the html, which means that neither the title nor the other page metadata in the &lt;head/&gt; section gets extracted.String innerHtml = driver.findElement(By.tagName("body")) .getAttribute("innerHTML");We should return the full html, no?</description>
      <version>None</version>
      <fixedVersion>1.20</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="3001" opendate="2023-9-13 00:00:00" fixdate="2023-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-selenium requires Content-Type header</summary>
      <description>It looks like the selenium protocol requires that there be a content-type header. The logic seems to be: If the content type is html or xhtml, use selenium, otherwise just grab the bytes. However, with the current logic, if the content-type is null, nothing is pulled. My guess is that the logic should be : if the content type is not null and equals html or xhtml use selenium, otherwise grab the bytes.Right? String contentType = getHeader(Response.CONTENT_TYPE); // handle with Selenium only if content type in HTML or XHTML if (contentType != null) { if (contentType.contains("text/html") || contentType.contains("application/xhtml")) { readPlainContent(url); } else {...</description>
      <version>None</version>
      <fixedVersion>1.20</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-selenium.src.java.org.apache.nutch.protocol.selenium.HttpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="303" opendate="2006-6-7 00:00:00" fixdate="2006-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>logging improvements</summary>
      <description>Switch to the apache commons logging facade.See HADOOP-211 and following thread http://www.mail-archive.com/nutch-developers%40lists.sourceforge.net/msg08706.html</description>
      <version>0.8</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypesReader.java</file>
      <file type="M">src.plugin.urlfilter-suffix.src.java.org.apache.nutch.urlfilter.suffix.SuffixURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-prefix.src.java.org.apache.nutch.urlfilter.prefix.PrefixURLFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.query-more.src.java.org.apache.nutch.searcher.more.DateQueryFilter.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.Http.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthenticationFactory.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.PrintCommandListener.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpResponse.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Ftp.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipTextExtractor.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipParser.java</file>
      <file type="M">src.plugin.parse-swf.src.java.org.apache.nutch.parse.swf.SWFParser.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.RSSParser.java</file>
      <file type="M">src.plugin.parse-pdf.src.java.org.apache.nutch.parse.pdf.PdfParser.java</file>
      <file type="M">src.plugin.parse-oo.src.java.org.apache.nutch.parse.oo.OOParser.java</file>
      <file type="M">src.plugin.parse-mspowerpoint.src.test.org.apache.nutch.parse.mspowerpoint.TestMSPowerPointParser.java</file>
      <file type="M">src.plugin.parse-mspowerpoint.src.java.org.apache.nutch.parse.mspowerpoint.PPTExtractor.java</file>
      <file type="M">src.plugin.parse-mspowerpoint.src.java.org.apache.nutch.parse.mspowerpoint.ContentReaderListener.java</file>
      <file type="M">src.plugin.parse-js.src.java.org.apache.nutch.parse.js.JSParseFilter.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.plugin.parse-ext.src.java.org.apache.nutch.parse.ext.ExtParser.java</file>
      <file type="M">src.plugin.ontology.src.java.org.apache.nutch.ontology.jena.OwlParser.java</file>
      <file type="M">src.plugin.ontology.src.java.org.apache.nutch.ontology.jena.OntologyImpl.java</file>
      <file type="M">src.plugin.microformats-reltag.src.java.org.apache.nutch.microformats.reltag.RelTagParser.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.test.org.apache.nutch.urlfilter.api.RegexURLFilterBaseTest.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.plugin.lib-parsems.src.java.org.apache.nutch.parse.ms.MSExtractor.java</file>
      <file type="M">src.plugin.lib-parsems.src.java.org.apache.nutch.parse.ms.MSBaseParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.NGramProfile.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.LanguageIdentifier.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCParseFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCIndexingFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCDeleteUnlicensedTool.java</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.Clusterer.java</file>
      <file type="M">src.plugin.analysis-fr.plugin.xml</file>
      <file type="M">src.java.org.apache.nutch.util.ThreadPool.java</file>
      <file type="M">bin.nutch</file>
      <file type="M">build.xml</file>
      <file type="M">src.java.org.apache.nutch.analysis.AnalyzerFactory.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.CommonGrams.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClustererFactory.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.MapWritable.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.SignatureFactory.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexMerger.java</file>
      <file type="M">src.java.org.apache.nutch.net.BasicUrlNormalizer.java</file>
      <file type="M">src.java.org.apache.nutch.net.RegexUrlNormalizer.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilterChecker.java</file>
      <file type="M">src.java.org.apache.nutch.net.UrlNormalizerFactory.java</file>
      <file type="M">src.java.org.apache.nutch.ontology.OntologyFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.OutlinkExtractor.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParsePluginsReader.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseUtil.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginDescriptor.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginManifestParser.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginRepository.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolFactory.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSearch.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.NutchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.OpenSearchServlet.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.Query.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.QueryFilters.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.SummarizerFactory.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.tools.DmozParser.java</file>
      <file type="M">src.java.org.apache.nutch.tools.PruneIndexTool.java</file>
      <file type="M">src.java.org.apache.nutch.util.DomUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.GZIPUtils.java</file>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypes.java</file>
    </fixedFiles>
  </bug>
  <bug id="305" opendate="2006-6-9 00:00:00" fixdate="2006-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update crawl and url filter lists to exclude jpeg|JPEG|bmp|BMP</summary>
      <description></description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.suffix-urlfilter.txt</file>
      <file type="M">conf.regex-urlfilter.txt.template</file>
      <file type="M">conf.crawl-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="309" opendate="2006-6-22 00:00:00" fixdate="2006-7-22 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Uses commons logging Code Guards</summary>
      <description>"Code guards are typically used to guard code that only needs to execute in support of logging, that otherwise introduces undesirable runtime overhead in the general case (logging disabled). Examples are multiple parameters, or expressions (e.g. string + " more") for parameters. Use the guard methods of the form log.is&lt;Priority&gt;() to verify that logging should be performed, before incurring the overhead of the logging method call. Yes, the logging methods will perform the same check, but only after resolving parameters."(description extracted from http://jakarta.apache.org/commons/logging/guide.html#Code_Guards)</description>
      <version>0.8</version>
      <fixedVersion>1.2,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
      <file type="M">src.plugin.urlfilter-suffix.src.java.org.apache.nutch.urlfilter.suffix.SuffixURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-prefix.src.java.org.apache.nutch.urlfilter.prefix.PrefixURLFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpAuthenticationFactory.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.DummySSLProtocolSocketFactory.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.PrintCommandListener.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipTextExtractor.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipParser.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.RSSParser.java</file>
      <file type="M">src.plugin.parse-pdf.src.java.org.apache.nutch.parse.pdf.PdfParser.java</file>
      <file type="M">src.plugin.parse-mspowerpoint.src.java.org.apache.nutch.parse.mspowerpoint.PPTExtractor.java</file>
      <file type="M">src.plugin.parse-mspowerpoint.src.java.org.apache.nutch.parse.mspowerpoint.ContentReaderListener.java</file>
      <file type="M">src.plugin.parse-js.src.java.org.apache.nutch.parse.js.JSParseFilter.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.plugin.ontology.src.java.org.apache.nutch.ontology.jena.OntologyImpl.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.NGramProfile.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.LanguageIdentifier.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.CommonGrams.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClustererFactory.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.MapWritable.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.SignatureFactory.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexMerger.java</file>
      <file type="M">src.java.org.apache.nutch.net.RegexUrlNormalizer.java</file>
      <file type="M">src.java.org.apache.nutch.net.UrlNormalizerFactory.java</file>
      <file type="M">src.java.org.apache.nutch.ontology.OntologyFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.OutlinkExtractor.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParsePluginsReader.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseUtil.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginDescriptor.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginManifestParser.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginRepository.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSearch.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.NutchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.OpenSearchServlet.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.QueryFilters.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.SummarizerFactory.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.servlet.Cached.java</file>
      <file type="M">src.java.org.apache.nutch.tools.DmozParser.java</file>
      <file type="M">src.java.org.apache.nutch.tools.PruneIndexTool.java</file>
      <file type="M">src.java.org.apache.nutch.util.LogUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypesReader.java</file>
      <file type="M">src.java.org.apache.nutch.util.ThreadPool.java</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.Clusterer.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCDeleteUnlicensedTool.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCIndexingFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.java.org.creativecommons.nutch.CCParseFilter.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="312" opendate="2006-6-28 00:00:00" fixdate="2006-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix for upcoming incompatibility with Hadoop-0.4</summary>
      <description>I have submitted a patch to Hadoop fixing tasktracker-latency issues. That patch introduces incompatibility with current nutch code, because the interface for OutputFormat will change. I will soon submit a patch for nutch that will fix this upcoming incompatibility with Hadoop.</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherOutputFormat.java</file>
      <file type="M">lib.hadoop-0.3.2.jar</file>
    </fixedFiles>
  </bug>
  <bug id="324" opendate="2006-7-19 00:00:00" fixdate="2006-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>db.score.link.internal and db.score.link.external are ignored</summary>
      <description>Configuration properties db.score.link.external and db.score.link.internal are ignored.In case of e.g. message board webpages or pages that have larger navigation menus on each page having a lower impact of internal links makes a lot of sense for scoring.Also for web spam this is a serious problem, since now spammers can setup just one domain with dynamically generated pages and this highly manipulate the nutch scores. So I also suggest that we give db.score.link.internal by default a value of something like 0.25.</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="332" opendate="2006-7-28 00:00:00" fixdate="2006-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>doubling score causes by page internal anchors.</summary>
      <description>When a page has no outlinks but several links to itself e.g. it has a set of anchors the scores of the page are distributed to its outlinks. But all this outlinks pointing to the page back. This causes that the page score is doubled. I'm not sure but may be this causes also a never ending fetching loop of this page, since outlinks with the status of CrawlDatum.STATUS_LINKED are set CrawlDatum.STATUS_DB_UNFETCHED in CrawlDBReducer line: 107. So may be the status fetched will be overwritten with unfetched. In such a case we fetch the page every-time again and also every-time double the score of this page what causes very high scores without any reasons.</description>
      <version>0.8</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="336" opendate="2006-8-1 00:00:00" fixdate="2006-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Harvested links shouldn&amp;#39;t get db.score.injected in addition to inbound contributions</summary>
      <description>Currently (even with Stefan's fix for NUTCH-324), harvested links have their initial scores set to db.score.injected + (sum of inbound contributions * db.score.link.&amp;#91;internal | external&amp;#93;), but this will place (at least external) harvested links even higher than injected URLs on the fetch list. Perhaps more importantly, this effect cascades.As a simple example, if each page in A-&gt;B-&gt;C-&gt;D has exactly one external link and only A is injected, then D will receive an initial score of at least (4*db.score.injected) with the default db.score.link.external of 1.0. Higher values of db.score.injected and db.score.link.external obviously exacerbate this problem.</description>
      <version>0.8</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilters.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
    </fixedFiles>
  </bug>
  <bug id="340" opendate="2006-8-4 00:00:00" fixdate="2006-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug(s) in 0.8 tutorial</summary>
      <description>There seems to be error(s) in whole web crawling section. This generates constantly (unneccessary) traffic to users list.</description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.src.documentation.content.xdocs.tutorial8.xml</file>
      <file type="M">site.tutorial8.pdf</file>
      <file type="M">site.tutorial8.html</file>
    </fixedFiles>
  </bug>
  <bug id="347" opendate="2006-8-12 00:00:00" fixdate="2006-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build: plugins&amp;#39; Jars not found</summary>
      <description>While building Nutch, I noticed several places where various Jars from plugins' lib directories could not be found, for example:$ ant package...deploy: &amp;#91;copy&amp;#93; Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-log4j/lib-log4j.jar to copy.init:init-plugin:compile:jar:deps-test:deploy: &amp;#91;copy&amp;#93; Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-nekohtml/lib-nekohtml.jar to copy....The problem is, these "lib-XXXX.jar" files do not exist. Instead, those Jars are typically named with a version in the name, like log4j-1.2.11.jar. I could not find where this "lib-" prefix comes from, nor where the version is dropped from the name. Anyone knows?In order to avoid these errors I had to make symbolic links and fake things:e.g. ln -s log4j-1.2.11.jar lib-log4j.jarBut this should really be fixed somewhere, I just can't see where... Note that this doesn't completely break the build, but missing Jars can't be a good thing.</description>
      <version>0.8</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build-plugin.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="375" opendate="2006-9-29 00:00:00" fixdate="2006-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Link to 0.8.x apidocs broken on website</summary>
      <description>&gt;It looks like the link from the Nutch Project Homepage to the API Docs&gt;for 0.8.x is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="384" opendate="2006-10-11 00:00:00" fixdate="2006-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protocol-file plugin does not allow the parse plugins framework to operate properly</summary>
      <description>When using the file protocol one can not map a parse plugin to a content type. The only way to get the plugin called is through the default plugin. The issue is that the content type never gets mapped. Currently the content type does not get set by the file protocol.</description>
      <version>0.8,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="385" opendate="2006-10-11 00:00:00" fixdate="2006-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve description of thread related configuration for Fetcher</summary>
      <description>For some time I've been puzzled by the interaction between two paramters that control how often the fetcher can access a particular host:1) The server delay, which comes back from the remote server during our processing of the robots.txt file, and which can be limited by fetcher.max.crawl.delay.2) The fetcher.threads.per.host value, particularly when this is greater than the default of 1.According to my (limited) understanding of the code in HttpBase.java:Suppose that fetcher.threads.per.host is 2, and that (by chance) the fetcher ends up keeping either 1 or 2 fetcher threads pointing at a particular host continuously. In other words, it never tries to point 3 at the host, and it always points a second thread at the host before the first thread finishes accessing it. Since HttpBase.unblockAddr never gets called with (((Integer)THREADS_PER_HOST_COUNT.get(host)).intValue() == 1), it never puts System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. Thus, the server delay will never be used at all. The fetcher will be continuously retrieving pages from the host, often with 2 fetchers accessing the host simultaneously.Suppose instead that the fetcher finally does allow the last thread to complete before it gets around to pointing another thread at the target host. When the last fetcher thread calls HttpBase.unblockAddr, it will now put System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. This, in turn, will prevent any threads from accessing this host until the delay is complete, even though zero threads are currently accessing the host.I see this behavior as inconsistent. More importantly, the current implementation certainly doesn't seem to answer my original question about appropriate definitions for what appear to be conflicting parameters. In a nutshell, how could we possibly honor the server delay if we allow more than one fetcher thread to simultaneously access the host?It would be one thing if whenever (fetcher.threads.per.host &gt; 1), this trumped the server delay, causing the latter to be ignored completely. That is certainly not the case in the current implementation, as it will wait for server delay whenever the number of threads accessing a given host drops to zero.</description>
      <version>None</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2007-10-17 00:00:00" fixdate="2007-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper (?) handling of URIs in TagSoup.</summary>
      <description>Doug Cook reported that TagSoup incorrectly handles some URI parameters. More discussion on the list and at TagSoup's mailing list.http://tech.groups.yahoo.com/group/tagsoup-friends/message/838I looked at the sources of TagSoup because I'm using it myself (although the URIs are not relevant for me). It seems like you can implement a naive workaround by remembering the parsing state and just avoiding entity resolution. Attached is the patch that does this.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.plugin.xml</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup.LICENSE.txt</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup-1.0rc3.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="621" opendate="2008-3-18 00:00:00" fixdate="2008-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch needs to declare it&amp;#39;s crypto usage</summary>
      <description>Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).See TIKA-118.</description>
      <version>0.7,0.7.1,0.7.2,0.8,0.8.1,0.9.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="651" opendate="2008-9-19 00:00:00" fixdate="2008-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bin/{start|stop}-balancer.sh from svn tracking</summary>
      <description>Files bin/{start|stop}-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version control.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-balancer.sh</file>
      <file type="M">bin.start-balancer.sh</file>
    </fixedFiles>
  </bug>
  <bug id="655" opendate="2008-10-1 00:00:00" fixdate="2008-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Injecting Crawl metadata</summary>
      <description>the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separated by '='. A input line might look like this:http://www.myurl.com \t categ=value1 \t categ2=value2This functionality can be useful to store external knowledge and index it with a custom plugin</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="656" opendate="2008-10-9 00:00:00" fixdate="2008-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DeleteDuplicates based on crawlDB only</summary>
      <description>The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.I was wondering whether we could use the information from the crawlDB instead to detect URLs to delete then do the deletions in an indexer-neutral way. As far as I understand the content of the crawlDB contains all the elements we need for dedup, namely : URL signature fetch time scoreIn map-reduce terms we would have two different jobs : read crawlDB and compare on URLs : keep only most recent element - oldest are stored in a file and will be deleted later read crawlDB and have a map function generating signatures as keys and URL + fetch time +score as value reduce function would depend on which parameter is set (i.e. use signature or score) and would output as list of URLs to deleteThis assumes that we can then use the URLs to identify documents in the indices.Any thoughts on this? Am I missing something?Julien</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.CleaningJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">src.bin.crawl</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
