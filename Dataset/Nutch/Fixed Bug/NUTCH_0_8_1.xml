<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="105" opendate="2005-10-7 00:00:00" fixdate="2005-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Network error during robots.txt fetch causes file to be ignored</summary>
      <description>Earlier we had a small network glitch which prevented us from retrievingthe robots.txt file for a site we were crawling at the time: nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193021 task_m_h02y5t Couldn't get robots.txt for http://www.japanesetranslator.co.uk/portfolio/: org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 10000 ms nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193031 task_m_h02y5t Couldn't get robots.txt for http://www.japanesetranslator.co.uk/translation/: org.apache.commons.httpclient.ConnectTimeoutException: The host did not accept the connection within timeout of 10000 msNutch then assumed that because we were unable to retrieve the file dueto network issues, that it didn't exist and we could crawl the entirewebsite. Nutch then successfully grabbed a few pages which were listedin the robots.txt as being disallowed.I think Nutch should continue attempting to retrieve the robots.txt fileuntil, at very least, we are able to establish a connection to the host,otherwise the host should be ignored until the next round of fetches.The webmaster of japanesetranslator.co.uk filed a complaint informing usof the issue.</description>
      <version>0.8,0.8.1,0.9.0</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="375" opendate="2006-9-29 00:00:00" fixdate="2006-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Link to 0.8.x apidocs broken on website</summary>
      <description>&gt;It looks like the link from the Nutch Project Homepage to the API Docs&gt;for 0.8.x is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="384" opendate="2006-10-11 00:00:00" fixdate="2006-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protocol-file plugin does not allow the parse plugins framework to operate properly</summary>
      <description>When using the file protocol one can not map a parse plugin to a content type. The only way to get the plugin called is through the default plugin. The issue is that the content type never gets mapped. Currently the content type does not get set by the file protocol.</description>
      <version>0.8,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="385" opendate="2006-10-11 00:00:00" fixdate="2006-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve description of thread related configuration for Fetcher</summary>
      <description>For some time I've been puzzled by the interaction between two paramters that control how often the fetcher can access a particular host:1) The server delay, which comes back from the remote server during our processing of the robots.txt file, and which can be limited by fetcher.max.crawl.delay.2) The fetcher.threads.per.host value, particularly when this is greater than the default of 1.According to my (limited) understanding of the code in HttpBase.java:Suppose that fetcher.threads.per.host is 2, and that (by chance) the fetcher ends up keeping either 1 or 2 fetcher threads pointing at a particular host continuously. In other words, it never tries to point 3 at the host, and it always points a second thread at the host before the first thread finishes accessing it. Since HttpBase.unblockAddr never gets called with (((Integer)THREADS_PER_HOST_COUNT.get(host)).intValue() == 1), it never puts System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. Thus, the server delay will never be used at all. The fetcher will be continuously retrieving pages from the host, often with 2 fetchers accessing the host simultaneously.Suppose instead that the fetcher finally does allow the last thread to complete before it gets around to pointing another thread at the target host. When the last fetcher thread calls HttpBase.unblockAddr, it will now put System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. This, in turn, will prevent any threads from accessing this host until the delay is complete, even though zero threads are currently accessing the host.I see this behavior as inconsistent. More importantly, the current implementation certainly doesn't seem to answer my original question about appropriate definitions for what appear to be conflicting parameters. In a nutshell, how could we possibly honor the server delay if we allow more than one fetcher thread to simultaneously access the host?It would be one thing if whenever (fetcher.threads.per.host &gt; 1), this trumped the server delay, causing the latter to be ignored completely. That is certainly not the case in the current implementation, as it will wait for server delay whenever the number of threads accessing a given host drops to zero.</description>
      <version>None</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="392" opendate="2006-10-25 00:00:00" fixdate="2006-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OutputFormat implementations should pass on Progressable</summary>
      <description>OutputFormat implementations should pass the Progressable they are passed to underlying SequenceFile implementations. This will keep reduce tasks from timing out when block writes are slow. This issue depends on http://issues.apache.org/jira/browse/HADOOP-636.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="419" opendate="2006-12-24 00:00:00" fixdate="2006-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>unavailable robots.txt kills fetch</summary>
      <description>I think there is another robots.txt-related problem which is notadressed by NUTCH-344,but also results in an aborted fetch.I am sure that in my last fetch all 17 fetcher threads diedwhile they were waiting for a robots.txt-file to be delivered by a notproperly responding web server.I looked at the squid access log, which is used by all fetch threads.It ends with many HTTP-504-errors ("gateway timeout") caused by acertain robots.txt url:&lt;....&gt;1166652253.332 899427 127.0.0.1 TCP_MISS/504 1450 GEThttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/html1166652343.350 899664 127.0.0.1 TCP_MISS/504 1450 GEThttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/html1166652353.560 899871 127.0.0.1 TCP_MISS/504 1450 GEThttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/htmlThese entries mean that it takes 15 minutes before the request endswith a timeout.This can be calculated from the squid log, the first column is therequest time (in UTC seconds), the second column is the duration ofthe request (in ms):900000/1000/60=15 minutes.As far as I understand it, every time a fetch thread tries to get thisrobots.txt-file the thread busy waits for the duration of the request(15 minutes).If this is right, then all 17 fetcher threads were caught in this trapat the time when fetching was aborted, as there are 17 requests inthe squid log which did not timeout before the message "aborting with17 threads" was written to the nutch-logfile.Setting fetcher.max.crawl.delay can not help here.I see 296 access attempts in total concerning this robots.txt-url inthe squid log of this crawl, but fetcher.max.crawl.delay is set to 30.</description>
      <version>0.8.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
    </fixedFiles>
  </bug>
  <bug id="428" opendate="2007-1-10 00:00:00" fixdate="2007-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException</summary>
      <description>I am using the NUTCH.Bat provided in one one of the thread. (i am not using CYGWIN) Whenever I try to fetch the Item, I am getting fetching failed "nullpointerexception" I have a URL Directory. which has urls.txt file. there is only one entry in the file which is http://www.winzip.com/land_about.htm. I have updated the crawl-urlfilter.txt with +^http://www.winzip.com/. Is there any other settings I am missing?? Any help is greatly appreciated. The command i used to start the crawl is nutch crawl urls -dir crawlResults -depth 1Here is my log crawl started in: crawlResultrootUrlDir = urlsthreads = 10depth = 1Injector: startingInjector: crawlDb: crawlResult/crawldbInjector: urlDir: urlsInjector: Converting injected urls to crawl db entries.Injector: Merging injected urls into crawl db.Injector: doneGenerator: startingGenerator: segment: crawlResult/segments/20070110085314Generator: Selecting best-scoring urls due for fetch.Generator: Partitioning selected urls by host, for politeness.Generator: done.Fetcher: startingFetcher: segment: crawlResult/segments/20070110085314Fetcher: threads: 10fetching http://www.winzip.com/land_about.htmfetch of http://www.winzip.com/land_about.htm failed with: java.lang.NullPointerExceptionFetcher: doneCrawlDb update: startingCrawlDb update: db: crawlResult/crawldbCrawlDb update: segment: crawlResult/segments/20070110085314CrawlDb update: Merging segment data into db.CrawlDb update: doneLinkDb: startingLinkDb: linkdb: crawlResult/linkdbLinkDb: adding segment: crawlResult/segments/20070110085314LinkDb: doneIndexer: startingIndexer: linkdb: crawlResult/linkdbIndexer: adding segment: crawlResult/segments/20070110085314Optimizing index.Indexer: doneDedup: startingDedup: adding indexes in: crawlResult/indexesDedup: doneAdding crawlResult/indexes/part-00000crawl finished: crawlResult</description>
      <version>0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="430" opendate="2007-1-13 00:00:00" fixdate="2007-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>integer overflow in HashComparator.compare</summary>
      <description>There's a integer overflow problem in HashComparator wich leads to fetchlist not to be sorted properly by hash of url. This leads to slower fetching speeds if there are many urls from same host as they are not evenly distributed.</description>
      <version>0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="449" opendate="2007-2-23 00:00:00" fixdate="2007-4-23 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Format of junit output should be configurable</summary>
      <description>Allow the junit output format to be set by a system property.</description>
      <version>0.8.1</version>
      <fixedVersion>1.7,2.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="497" opendate="2007-6-6 00:00:00" fixdate="2007-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extreme Nested Tags causes StackOverflowException in DomContentUtils...Spider Trap</summary>
      <description>Some webpages have a form of a spider trap that causes a StackOverflowException in DomContentUtils by having nested tags with thousands of layers deep. DomContentUtils when trying to get outlinks uses a recursive method to parse the html. With this type of nesting it errors out.</description>
      <version>0.8.1,0.9.0,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.fetcher.TestFetcher.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="567" opendate="2007-10-17 00:00:00" fixdate="2007-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper (?) handling of URIs in TagSoup.</summary>
      <description>Doug Cook reported that TagSoup incorrectly handles some URI parameters. More discussion on the list and at TagSoup's mailing list.http://tech.groups.yahoo.com/group/tagsoup-friends/message/838I looked at the sources of TagSoup because I'm using it myself (although the URIs are not relevant for me). It seems like you can implement a naive workaround by remembering the parsing state and just avoiding entity resolution. Attached is the patch that does this.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.plugin.xml</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup.LICENSE.txt</file>
      <file type="M">src.plugin.parse-html.lib.tagsoup-1.0rc3.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="621" opendate="2008-3-18 00:00:00" fixdate="2008-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch needs to declare it&amp;#39;s crypto usage</summary>
      <description>Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).See TIKA-118.</description>
      <version>0.7,0.7.1,0.7.2,0.8,0.8.1,0.9.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="658" opendate="2008-10-31 00:00:00" fixdate="2008-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Counter for # of doc fetched in Reporter</summary>
      <description>Having a Counter for the number of documents fetched duplicates the information in the status of each Map but should be summed across all the Map instances and displayed along with the standard Counters. The same mechanism could be used for the other possible status of a URL (redir / gone etc...)</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolStatus.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
