<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="2627" opendate="2018-7-27 00:00:00" fixdate="2018-2-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fetcher to optionally filter URLs</summary>
      <description>When running a large web crawl it happens that a webadmin requests to immediately stop crawling a certain domain. The default Nutch workflow applies URL filters only to seeds and outlinks. Applying filters during fetch list generation is expensive with a large CrawlDb (fetch lists are usually much shorter). Allowing the fetcher to optionally filter URLs would allow to apply changed filter rules to the next launched fetcher job even if the the segment has been already created (esp., if multiple segments are generated in one turn).</description>
      <version>1.16</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.QueueFeeder.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2628" opendate="2018-7-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fetcher: optionally generate signature of unparsed content</summary>
      <description>To generate a document signature (MD5 digest) of the binary content requires that documents are parsed during the parse or fetch step. The signature is required for deduplication and detection of unmodified content and should be always available, also in a workflow which does not require that documents are parsed, e.g., because HTML content is exported to WARC files.</description>
      <version>None</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2633" opendate="2018-8-9 00:00:00" fixdate="2018-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix deprecation warnings when building Nutch master branch under JDK 10.0.2+13</summary>
      <description>I just got around to making a dev upgrade to &gt;= JDK 10.When building master using environment JDKI get several compile time deprecations which are reflected in the attached build log. Additionally, I get some issues with Ivy... see belowWARNING: An illegal reflective access operation has occurredWARNING: Illegal reflective access by org.apache.ivy.util.url.IvyAuthenticator (file:/Users/lmcgibbn/.ant/lib/ivy-2.3.0.jar) to field java.net.Authenticator.theAuthenticatorWARNING: Please consider reporting this to the maintainers of org.apache.ivy.util.url.IvyAuthenticatorWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operationsWARNING: All illegal access operations will be denied in a future release[ivy:resolve] :: problems summary ::[ivy:resolve] :::: ERRORS[ivy:resolve] unknown resolver null[ivy:resolve] unknown resolver null[ivy:resolve] unknown resolver null</description>
      <version>1.16</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-cloudsearch.src.java.org.apache.nutch.indexwriter.cloudsearch.CloudSearchIndexWriter.java</file>
      <file type="M">src.test.org.apache.nutch.util.WritableTestUtils.java</file>
      <file type="M">src.test.org.apache.nutch.service.TestNutchServer.java</file>
      <file type="M">src.test.org.apache.nutch.segment.TestSegmentMergerCrawlDatums.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestCrawlDbMerger.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.CrawlDbUpdateUtil.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.CrawlDbUpdateTestDriver.java</file>
      <file type="M">src.plugin.urlnormalizer-slash.src.test.org.apache.nutch.net.urlnormalizer.slash.TestSlashURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-querystring.src.test.org.apache.nutch.net.urlnormalizer.querystring.TestQuerystringURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-querystring.src.java.org.apache.nutch.net.urlnormalizer.querystring.QuerystringURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-protocol.src.test.org.apache.nutch.net.urlnormalizer.protocol.TestProtocolURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-basic.src.java.org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-ajax.src.java.org.apache.nutch.net.urlnormalizer.ajax.AjaxURLNormalizer.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.scoring.urlmeta.URLMetaScoringFilter.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.indexer.urlmeta.URLMetaIndexingFilter.java</file>
      <file type="M">src.plugin.urlfilter-ignoreexempt.src.java.org.apache.nutch.urlfilter.ignoreexempt.ExemptionUrlFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.scoring-similarity.src.java.org.apache.nutch.scoring.similarity.cosine.Model.java</file>
      <file type="M">src.plugin.scoring-orphan.src.java.org.apache.nutch.scoring.orphan.OrphanScoringFilter.java</file>
      <file type="M">src.plugin.protocol-okhttp.src.java.org.apache.nutch.protocol.okhttp.OkHttp.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.Http.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.DummyX509TrustManager.java</file>
      <file type="M">src.plugin.protocol-htmlunit.src.java.org.apache.nutch.protocol.htmlunit.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpResponse.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpError.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Ftp.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileError.java</file>
      <file type="M">src.plugin.parsefilter-regex.src.test.org.apache.nutch.parsefilter.regex.TestRegexParseFilter.java</file>
      <file type="M">src.plugin.parsefilter-regex.src.java.org.apache.nutch.parsefilter.regex.RegexParseFilter.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestFeedParser.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.TikaParser.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.BoilerpipeExtractorRepository.java</file>
      <file type="M">src.plugin.parse-swf.src.java.org.apache.nutch.parse.swf.SWFParser.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.plugin.mimetype-filter.src.test.org.apache.nutch.indexer.filter.MimeTypeIndexingFilterTest.java</file>
      <file type="M">src.plugin.mimetype-filter.src.java.org.apache.nutch.indexer.filter.MimeTypeIndexingFilter.java</file>
      <file type="M">src.plugin.indexer-rabbit.src.java.org.apache.nutch.indexwriter.rabbit.RabbitDocument.java</file>
      <file type="M">src.plugin.indexer-elastic.src.test.org.apache.nutch.indexwriter.elastic.TestElasticIndexWriter.java</file>
      <file type="M">src.plugin.indexer-elastic.src.java.org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.java</file>
      <file type="M">src.plugin.indexer-elastic-rest.src.java.org.apache.nutch.indexwriter.elasticrest.ElasticRestIndexWriter.java</file>
      <file type="M">src.plugin.indexer-dummy.src.java.org.apache.nutch.indexwriter.dummy.DummyIndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AbstractFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AdaptiveFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DefaultFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchScheduleFactory.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.MimeAdaptiveFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.SignatureFactory.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ResolverThread.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.CleaningJob.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.Extension.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.Plugin.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.Content.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.Protocol.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolException.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolFactory.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolStatus.java</file>
      <file type="M">src.java.org.apache.nutch.segment.ContentAsTextInputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.service.impl.LinkReader.java</file>
      <file type="M">src.java.org.apache.nutch.service.impl.NodeReader.java</file>
      <file type="M">src.java.org.apache.nutch.service.impl.NutchServerPoolExecutor.java</file>
      <file type="M">src.java.org.apache.nutch.service.model.response.FetchNodeDbInfo.java</file>
      <file type="M">src.java.org.apache.nutch.service.resources.DbResource.java</file>
      <file type="M">src.java.org.apache.nutch.tools.arc.ArcSegmentCreator.java</file>
      <file type="M">src.java.org.apache.nutch.tools.Benchmark.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormatWARC.java</file>
      <file type="M">src.java.org.apache.nutch.tools.DmozParser.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.warc.WARCExporter.java</file>
      <file type="M">src.java.org.apache.nutch.util.AbstractChecker.java</file>
      <file type="M">src.java.org.apache.nutch.util.CrawlCompletionStats.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.EncodingDetector.java</file>
      <file type="M">src.java.org.apache.nutch.util.GenericWritableConfigurable.java</file>
      <file type="M">src.plugin.any23.src.test.org.apache.nutch.any23.TestAny23ParseFilter.java</file>
      <file type="M">src.plugin.creativecommons.src.test.org.creativecommons.nutch.TestCCParseFilter.java</file>
      <file type="M">src.plugin.feed.src.test.org.apache.nutch.parse.feed.TestFeedParser.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.index-geoip.src.java.org.apache.nutch.indexer.geoip.GeoIPDocumentCreator.java</file>
      <file type="M">src.plugin.index-jexl-filter.src.java.org.apache.nutch.indexer.jexl.JexlIndexingFilter.java</file>
      <file type="M">src.plugin.index-links.src.test.org.apache.nutch.indexer.links.TestLinksIndexingFilter.java</file>
      <file type="M">src.plugin.index-replace.src.java.org.apache.nutch.indexer.replace.ReplaceIndexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2663" opendate="2018-10-18 00:00:00" fixdate="2018-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve index-jexl-filter syntax for scripts</summary>
      <description>JEXL scripts need to be written using the array syntax to get the actual value (for instance, example extracted from the tests):doc.lang[0]=='en'Ideally, this would only be required if the actual value is really an array, and not for single value elements.</description>
      <version>1.16</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-jexl-filter.src.test.org.apache.nutch.indexer.jexl.TestJexlIndexingFilter.java</file>
      <file type="M">src.plugin.index-jexl-filter.src.java.org.apache.nutch.indexer.jexl.JexlIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.util.JexlUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2722" opendate="2019-6-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetch dependencies via https</summary>
      <description>Dependencies need to be fetched via https, see https://central.sonatype.org/articles/2019/Apr/30/http-access-to-repo1mavenorg-and-repomavenapacheorg-is-being-deprecated/</description>
      <version>2.4,2.5,1.16</version>
      <fixedVersion>2.4,1.16</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.mvn.template</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2726" opendate="2019-8-6 00:00:00" fixdate="2019-8-6 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade to Tika 1.22</summary>
      <description>Tika 1.22 has been released and we should upgrade master/1.x (from 1.20).</description>
      <version>1.16</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.TikaParser.java</file>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">src.plugin.parse-tika.howto.upgrade.tika.txt</file>
      <file type="M">src.plugin.parse-tika.build-ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.tika-config.xml.template</file>
    </fixedFiles>
  </bug>
  <bug id="2736" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Dockerfile to be based on recent Ubuntu LTS version</summary>
      <description></description>
      <version>1.16</version>
      <fixedVersion>1.16</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docker.README.md</file>
      <file type="M">docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="2745" opendate="2019-10-15 00:00:00" fixdate="2019-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Solr schema.xml not shipped in binary release</summary>
      <description>The binary release packages of Nutch 1.16 do not contain the Solr schema.xml - it should be shipped as part of the package.For now users of the binary package must take the schema.xml from the source package or download it from the source repositories: https://github.com/apache/nutch/blob/release-1.16/src/plugin/indexer-solr/schema.xml</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-solr.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2747" opendate="2019-10-17 00:00:00" fixdate="2019-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace remaining o.a.commons.logging by org.slf4j</summary>
      <description>A few classes still use logging classes of the package org.apache.commons.logging, they should rely on org.slf4j instead, see NUTCH-1078. The commons-logging lib is included as a transitive dependency. But Nutch classes should not use it. Five classes are found via git grep -F org.apache.commons.logging.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.scoring-depth.src.java.org.apache.nutch.scoring.depth.DepthScoringFilter.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.BoilerpipeExtractorRepository.java</file>
      <file type="M">src.plugin.parse-metatags.src.java.org.apache.nutch.parse.metatags.MetaTagsParser.java</file>
      <file type="M">src.plugin.index-replace.src.java.org.apache.nutch.indexer.replace.ReplaceIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.tools.Benchmark.java</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2019-10-18 00:00:00" fixdate="2019-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetch status gone (redirect exceeded) not to overwrite existing items in CrawlDb</summary>
      <description>If fetcher is following redirects and the max. number of redirects in a redirect chain (http.max.redirect) is reached, fetcher stores a CrawlDatum item with status "fetch_gone" and protocol status "redir_exceeded". During the next CrawlDb update the "gone" item will set the status of existing items (including "db_fetched") with "db_gone". It shouldn't as there has been no fetch of the final redirect target and indeed nothing is know about it's status. An wrong db_gone may then cause that a page gets deleted from the search index.There are two possible solutions:1. ignore protocol status "redir_exceeded" during CrawlDb update2. when http.redirect.max is hit the fetcher stores nothing or a redirect status instead of a fetch_goneSolution 2. seems easier to implement and it would be possible to make the behavior configurable: store the redirect target as outlink, i.e. same behavior as if http.redirect.max == 0 store "fetch_gone" (current behavior) store nothing, i.e. ignore those redirects - this should be the default as it's close to the current behavior without the risk to accidentally set successful fetches to db_gone</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2754" opendate="2019-11-13 00:00:00" fixdate="2019-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetcher.max.crawl.delay ignored if exceeding 5 min. / 300 sec.</summary>
      <description>Sites specifying a Crawl-Delay of more than 5 minutes (301 seconds or more) are always ignored, even if fetcher.max.crawl.delay is set to a higher value.We need to pass a higher value of fetcher.max.crawl.delay to crawler-commons' robots.txt parser otherwise it will use the internal default value of 300 sec. and disallow all sites specifying a longer Crawl-Delay in their robots.txt.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.protocol.RobotRulesParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="2758" opendate="2019-12-9 00:00:00" fixdate="2019-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add plugin READMEs to binary release packages</summary>
      <description>Almost 20 plugins have a README (.md or .txt) which explains how to use and configure the plugin. The READMEs should be included in the binary release packages.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2760" opendate="2019-12-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-okhttp: properly record HTTP version in request message header</summary>
      <description>The HTTP version in the request message tracked by the plugin protocol-okhttp (store.http.request=true) is not the version sent in the request but that received from the response.Note that the HTTP version sent in the request may differ from that sent back in the response. One example (tracked using wget):&gt; wget -d https://www.kp.ru/daily/27061/4129507/...---request begin---GET /daily/27061/4129507/ HTTP/1.1User-Agent: Wget/1.20.3 (linux-gnu)Accept: */*Accept-Encoding: identityHost: www.kp.ruConnection: Keep-Alive---request end---HTTP request sent, awaiting response... ---response begin---HTTP/1.0 200 OK...protocol-http uses the response version ("HTTP/1.0") also for the request:&gt; bin/nutch parsechecker -Dstore.http.headers=true -Dstore.http.request=true \ -Dplugin.includes='protocol-okhttp|parse-html' https://www.kp.ru/daily/27061/4129507/..._request_=GET /daily/27061/4129507/ HTTP/1.0..._response.headers_=HTTP/1.0 200 OK...The protocol-http tracks the versions correctly:&gt; bin/nutch parsechecker -Dstore.http.headers=true -Dstore.http.request=true \ -Dplugin.includes='protocol-http|parse-html' https://www.kp.ru/daily/27061/4129507/..._request_=GET /daily/27061/4129507/ HTTP/1.1..._response.headers_=HTTP/1.0 200 OK...</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-okhttp.src.java.org.apache.nutch.protocol.okhttp.OkHttp.java</file>
    </fixedFiles>
  </bug>
  <bug id="2761" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ivy jar fails to download</summary>
      <description>Building from scratch fails with [get] Can't get http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar to .../ivy/ivy-2.4.0.jarNeed switch to https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.scoring-similarity.build-ivy.xml</file>
      <file type="M">src.plugin.publish-rabbitmq.build-ivy.xml</file>
      <file type="M">src.plugin.protocol-selenium.build-ivy.xml</file>
      <file type="M">src.plugin.protocol-interactiveselenium.build-ivy.xml</file>
      <file type="M">src.plugin.parsefilter-naivebayes.build-ivy.xml</file>
      <file type="M">src.plugin.parse-tika.build-ivy.xml</file>
      <file type="M">src.plugin.lib-selenium.build-ivy.xml</file>
      <file type="M">src.plugin.lib-rabbitmq.build-ivy.xml</file>
      <file type="M">src.plugin.lib-htmlunit.build-ivy.xml</file>
      <file type="M">src.plugin.indexer-solr.build-ivy.xml</file>
      <file type="M">src.plugin.indexer-rabbit.build-ivy.xml</file>
      <file type="M">src.plugin.indexer-kafka.build-ivy.xml</file>
      <file type="M">src.plugin.indexer-elastic.build-ivy.xml</file>
      <file type="M">src.plugin.indexer-elastic-rest.build-ivy.xml</file>
      <file type="M">src.plugin.index-geoip.build-ivy.xml</file>
      <file type="M">src.plugin.exchange-jexl.build-ivy.xml</file>
      <file type="M">src.plugin.any23.build-ivy.xml</file>
      <file type="M">default.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2762" opendate="2020-1-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Replace http:// URLs by https:// (build files and documentation)</summary>
      <description>To avoid errors such as in NUTCH-2761, all http:// URLs in build files and documentation should be replaced by https:// if the https:// exists and there are no SSL errors.But not for license headers: the Apache license (see https://www.apache.org/licenses/LICENSE-2.0) still uses http:// to reference the full version of the license. Changing the URL could cause legal issues. XML namespaces unit tests examples historic references (eg. in CHANGES.txt)</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-slash.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-regex.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-querystring.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-protocol.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-pass.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-host.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-basic.src.java.org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-basic.ivy.xml</file>
      <file type="M">src.plugin.urlnormalizer-ajax.ivy.xml</file>
      <file type="M">src.plugin.urlmeta.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-validator.src.java.org.apache.nutch.urlfilter.validator.UrlValidator.java</file>
      <file type="M">src.plugin.urlfilter-validator.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-suffix.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-regex.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-prefix.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-ignoreexempt.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-fast.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-domain.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.ivy.xml</file>
      <file type="M">src.plugin.urlfilter-automaton.src.java.org.apache.nutch.urlfilter.automaton.package.html</file>
      <file type="M">src.plugin.urlfilter-automaton.src.java.org.apache.nutch.urlfilter.automaton.AutomatonURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-automaton.ivy.xml</file>
      <file type="M">src.plugin.tld.ivy.xml</file>
      <file type="M">src.plugin.subcollection.ivy.xml</file>
      <file type="M">src.plugin.scoring-similarity.ivy.xml</file>
      <file type="M">src.plugin.scoring-orphan.ivy.xml</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.scoring-opic.ivy.xml</file>
      <file type="M">src.plugin.scoring-link.ivy.xml</file>
      <file type="M">src.plugin.scoring-depth.ivy.xml</file>
      <file type="M">src.plugin.publish-rabbitmq.ivy.xml</file>
      <file type="M">src.plugin.protocol-selenium.README.md</file>
      <file type="M">src.plugin.protocol-selenium.ivy.xml</file>
      <file type="M">src.plugin.protocol-okhttp.jsp.redirect302.jsp</file>
      <file type="M">src.plugin.protocol-okhttp.jsp.redirect301.jsp</file>
      <file type="M">src.plugin.protocol-okhttp.ivy.xml</file>
      <file type="M">src.plugin.protocol-interactiveselenium.ivy.xml</file>
      <file type="M">src.plugin.protocol-http.jsp.redirect302.jsp</file>
      <file type="M">src.plugin.protocol-http.jsp.redirect301.jsp</file>
      <file type="M">src.plugin.protocol-http.ivy.xml</file>
      <file type="M">src.plugin.protocol-httpclient.ivy.xml</file>
      <file type="M">src.plugin.protocol-htmlunit.ivy.xml</file>
      <file type="M">src.plugin.protocol-ftp.ivy.xml</file>
      <file type="M">src.plugin.protocol-file.ivy.xml</file>
      <file type="M">src.plugin.parsefilter-regex.ivy.xml</file>
      <file type="M">src.plugin.parsefilter-naivebayes.ivy.xml</file>
      <file type="M">src.plugin.parse-zip.ivy.xml</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.package-info.java</file>
      <file type="M">.github.pull.request.template.md</file>
      <file type="M">default.properties</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">README.md</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
      <file type="M">src.plugin.any23.ivy.xml</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.Any23IndexingFilter.java</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.Any23ParseFilter.java</file>
      <file type="M">src.plugin.any23.src.java.org.apache.nutch.any23.package-info.java</file>
      <file type="M">src.plugin.creativecommons.ivy.xml</file>
      <file type="M">src.plugin.exchange-jexl.ivy.xml</file>
      <file type="M">src.plugin.feed.ivy.xml</file>
      <file type="M">src.plugin.headings.ivy.xml</file>
      <file type="M">src.plugin.index-anchor.ivy.xml</file>
      <file type="M">src.plugin.index-basic.ivy.xml</file>
      <file type="M">src.plugin.index-geoip.ivy.xml</file>
      <file type="M">src.plugin.index-geoip.src.java.org.apache.nutch.indexer.geoip.GeoIPIndexingFilter.java</file>
      <file type="M">src.plugin.index-geoip.src.java.org.apache.nutch.indexer.geoip.package-info.java</file>
      <file type="M">src.plugin.index-jexl-filter.ivy.xml</file>
      <file type="M">src.plugin.index-links.ivy.xml</file>
      <file type="M">src.plugin.index-metadata.ivy.xml</file>
      <file type="M">src.plugin.index-more.ivy.xml</file>
      <file type="M">src.plugin.index-replace.ivy.xml</file>
      <file type="M">src.plugin.index-static.ivy.xml</file>
      <file type="M">src.plugin.indexer-cloudsearch.ivy.xml</file>
      <file type="M">src.plugin.indexer-csv.ivy.xml</file>
      <file type="M">src.plugin.indexer-dummy.ivy.xml</file>
      <file type="M">src.plugin.indexer-elastic-rest.ivy.xml</file>
      <file type="M">src.plugin.indexer-elastic.ivy.xml</file>
      <file type="M">src.plugin.indexer-elastic.src.java.org.apache.nutch.indexwriter.elastic.package-info.java</file>
      <file type="M">src.plugin.indexer-kafka.ivy.xml</file>
      <file type="M">src.plugin.indexer-rabbit.ivy.xml</file>
      <file type="M">src.plugin.indexer-solr.ivy.xml</file>
      <file type="M">src.plugin.language-identifier.ivy.xml</file>
      <file type="M">src.plugin.lib-htmlunit.ivy.xml</file>
      <file type="M">src.plugin.lib-http.ivy.xml</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.plugin.lib-nekohtml.ivy.xml</file>
      <file type="M">src.plugin.lib-rabbitmq.ivy.xml</file>
      <file type="M">src.plugin.lib-regex-filter.ivy.xml</file>
      <file type="M">src.plugin.lib-selenium.ivy.xml</file>
      <file type="M">src.plugin.lib-xml.ivy.xml</file>
      <file type="M">src.plugin.microformats-reltag.ivy.xml</file>
      <file type="M">src.plugin.mimetype-filter.ivy.xml</file>
      <file type="M">src.plugin.nutch-extensionpoints.ivy.xml</file>
      <file type="M">src.plugin.parse-ext.ivy.xml</file>
      <file type="M">src.plugin.parse-html.ivy.xml</file>
      <file type="M">src.plugin.parse-js.ivy.xml</file>
      <file type="M">src.plugin.parse-metatags.ivy.xml</file>
      <file type="M">src.plugin.parse-swf.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2763" opendate="2020-1-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-okhttp (store.http.headers): add whitespace in status line after status code also when message is empty</summary>
      <description>RFC 7230 describes the HTTP response status line as:status-line = HTTP-version SP status-code SP reason-phrase CRLFThe "reason-phrase" is allowed to be empty, but the white space between status-code and reason-phrase is mandatory. The protocol-okhttp, when storing the HTTP response header (store.http.headers = true), does not add a white space when the message is empty, even if the original response header contained a white space after the status code.Note: protocol-http add the status line literally.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-okhttp.src.test.org.apache.nutch.protocol.okhttp.TestBadServerResponses.java</file>
      <file type="M">src.plugin.protocol-okhttp.src.java.org.apache.nutch.protocol.okhttp.OkHttp.java</file>
    </fixedFiles>
  </bug>
  <bug id="2772" opendate="2020-2-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Debugging parse filter to show serialized DOM tree</summary>
      <description>A tool to show the DOM tree (eg. serialized as XML/HTML) might be helpful for debugging, eg., see NUTCH-2769. The DOM tree is available in the parse plugins and is also passed to the HtmlParseFilter plugins. We could provide a parsefilter-debug plugin which logs the DOM tree and add the serialized string representation to the parse data.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.java.org.apache.nutch.util.DomUtil.java</file>
      <file type="M">default.properties</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2774" opendate="2020-2-28 00:00:00" fixdate="2020-3-28 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Annotate methods implementing the Hadoop API by @Override</summary>
      <description>Methods which implement or override the Hadoop API should always be annotated using @Override. This will help to avoid that methods are not called accidentally because the method name or signature do not match the current API.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-solr.src.java.org.apache.nutch.indexwriter.solr.SolrIndexWriter.java</file>
      <file type="M">src.plugin.indexer-dummy.src.java.org.apache.nutch.indexwriter.dummy.DummyIndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
      <file type="M">src.java.org.apache.nutch.util.ProtocolStatusStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.CrawlCompletionStats.java</file>
      <file type="M">src.java.org.apache.nutch.tools.warc.WARCExporter.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FreeGenerator.java</file>
      <file type="M">src.java.org.apache.nutch.tools.Benchmark.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.WebGraph.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.ScoreUpdater.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.NodeDumper.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LinkRank.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LinkDumper.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizerChecker.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilterChecker.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingJob.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFiltersChecker.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.CleaningJob.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDb.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.URLPartitioner.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DeduplicationJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
    </fixedFiles>
  </bug>
  <bug id="2775" opendate="2020-2-29 00:00:00" fixdate="2020-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetcher to guarantee minimum delay even if robots.txt defines shorter Crawl-delay</summary>
      <description>Fetcher uses the amount of seconds defined by "fetcher.server.delay" to delay between successive requests to the same server. Servers can request a longer delay using the Crawl-Delay directive in the robots.txt. This was thought to allow servers to set a longer delay. However, I've recently seen a server requesting "Crawl-Delay: 1". The delay is shorter than the default delay and Nutch may indeed now request one page per second. Later this server responds with "HTTP 429 Too Many Request". Stupid. What about ignoring Crawl-Delay values shorter than the configured default delay or a configurable minimum delay?I've already seen the same issue using a different crawler architecture.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2776" opendate="2020-3-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Fetcher to temporarily deduplicate followed redirects</summary>
      <description>If fetcher follows redirect (http.redirect.max &gt; 0), it may happen that many redirects of a site point to the same URL. In this situation, it might be good if fetcher could temporarily (for a configurable time period) deduplicate the redirect targets and skip all redirects except the first one. Typical examples of duplicated redirect targets are: instead of responding with HTTP status 404://resource-not-found/search//404/error/not-found/err/notfound.html a page to accept/decline cookies/cookie_usage.php</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetchItemQueues.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2777" opendate="2020-3-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 3.1</summary>
      <description>Hadoop 3.0.0 has been released in December 2017, the number of "legacy" clusters running on Hadoop 2.x should go down now a user reported that Nutch and Hadoop 3.1.3 work well on Windows</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2778" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>indexer-elastic to properly log errors</summary>
      <description>While verifying the solution of NUTCH-2757, it turned out that no errors are indicated if authentication fails because of wrong credentials. The indexing job succeeds and no errors are logged.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-elastic.src.java.org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2020-4-21 00:00:00" fixdate="2020-4-21 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Upgrade to Tika 1.24.1</summary>
      <description>Tika 1.24.1 should be released soon. I've upgraded Nutch to use the release candidate: all unit tests pass and processing PDFs, MP3s, etc. works. I'll open a PR but we need to wait for the final release of 1.24.1</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">src.plugin.parse-tika.howto.upgrade.tika.txt</file>
      <file type="M">src.plugin.parse-tika.build-ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2783" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Use (more) parametrized logging</summary>
      <description>Nutch uses slf4j's Logger (since NUTCH-851), but there are still many places where parametrized logging is not used or parameters are needless converted to strings before the call. This issue aims to improve the situation for commonly used tools and plugins in order to simplify code and improve the logging performance. A PR is on the way.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-solr.src.java.org.apache.nutch.indexwriter.solr.SolrIndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FreeGenerator.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbMapper.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.SignatureFactory.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DeduplicationJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
    </fixedFiles>
  </bug>
  <bug id="2784" opendate="2020-4-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add tool to list Nutch and Hadoop properties</summary>
      <description>Nutch properties are defined in nutch-default.xml but can be redefined (overridden) in nutch-site.xml or from command-line (-Dproperty=value). In addition, property definitions can include other properties (${property.name}) which makes it sometimes hard to figure out what the actual value of a property is.In short, a command-line tool which lists all properties and the configured values could be useful.</description>
      <version>None</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="2791" opendate="2020-6-9 00:00:00" fixdate="2020-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>domainstats, protocolstats and crawlcomplete do not handle GCS URLs</summary>
      <description>I am running Nutch in GCP Dataproc. The domainstats, protocolstats and crawlcomplete commands do not resolve crawldb paths correctly from GCS URLs.Also: protocolstats has an off-by-one error when resolving the numReducers argument crawlcomplete -inputDirs is inconsistent with other command: where domainstats expect directories containing a "current" path, crawlcomplete looks for directories containing a "crawldb/current" path.I will send a PR after creating the issue.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.ProtocolStatusStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.util.CrawlCompletionStats.java</file>
    </fixedFiles>
  </bug>
  <bug id="2794" opendate="2020-6-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional ciphers to HTTP base&amp;#39;s default cipher suite</summary>
      <description>More sites switch to stronger cipher suites to support and lib-http stays behind./This ticket adds some cipher suites and enables protocol-http to crawl affected sites.</description>
      <version>1.16</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2924" opendate="2021-12-30 00:00:00" fixdate="2021-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate maxCount expr evaluated only once</summary>
      <description>The generate.maxCount expression is evaluated only once in the generator's reducer, instead, it must be set once per host.</description>
      <version>1.16</version>
      <fixedVersion>1.20</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
