<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1771" opendate="2014-5-12 00:00:00" fixdate="2014-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Solrindex fails if a segment is corrupted or incomplete</summary>
      <description>When using solrindex to index multiple segments via -dir segment,the indexing fails if one or more segments are corrupted/incomplete (generated but not fetched for example)The failure is simply java.io exception.Deleting the segment fixes the issue.The expected behavior should be one of the following: skipping the segment and proceeding with others (while logging) stopping the indexing and logging the failed segment</description>
      <version>1.8,1.10</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingJob.java</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1863" opendate="2014-9-28 00:00:00" fixdate="2014-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add JSON format dump output to readdb command</summary>
      <description>Opening up the ability for third parties to consume Nutch crawldb data as JSON would be a poisitive thing IMHO.This issue should improve the readdb functionality of both 1.X to enable JSON dumps of crawldb data.</description>
      <version>2.3,1.10</version>
      <fixedVersion>1.17</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="1865" opendate="2014-9-30 00:00:00" fixdate="2014-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable use of SNAPSHOT&amp;#39;s with Nutch Ivy dependency management</summary>
      <description>Right now in 2.X we are able to tuse SNAPSHOT dependencies from http://respository.apache.org.We should port this to trunk as it is really helpful for example if you would like to use Tika SNAPSHOT which has loads of goodies.</description>
      <version>1.10</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1873" opendate="2014-10-10 00:00:00" fixdate="2014-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Solr IndexWriter/Job to report number of docs indexed.</summary>
      <description>It is annoying when reading logs to NOT know how many docs were indexed at a certain point in time. Of course i could go to the Solr server and see this... if I have access to the URL, but this is not always the case. I do however always have access to the logs.</description>
      <version>2.3,1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingJob.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1912" opendate="2015-1-11 00:00:00" fixdate="2015-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump tool -mimetype parameter needs to be optional to prevent NPE</summary>
      <description>There is a bug in NUTCH-1869 which, although the new feature works correctly, can result in a NPE if the new flag is not provided. This is hellish.We can possibly add an 'all' option which would dump everything.The purpose of NUTCH-1869 was so that you can have fine grained control over what content you wish to dump.</description>
      <version>1.10</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1920" opendate="2015-1-16 00:00:00" fixdate="2015-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Nutch to use Java 1.7</summary>
      <description>In order to build the Nutch Javadoc securely, we rely upon no less than Java version 7u25 or greater. See NUTCH-1590.indexer-elastic also requires a JDK 1.7 in order compile.We should make the upgrade and state support for Java 1.7 based on the following announcement from OracleEnd of Public Updates for Oracle JDK 7The April 2015 CPU release will be the last Oracle JDK 7 publicly available update. For more information, and details on how to receive longer term support for Oracle JDK 7, please see the Oracle Java SE Support Roadmap.</description>
      <version>2.3,1.10</version>
      <fixedVersion>1.10,2.3.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">default.properties</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1945" opendate="2015-2-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Test for XLSX parser</summary>
      <description>Add a test for Excel spreadsheets (xlsx) files: because the are formally also zip files (as well as other composite files) the MIME type detection is crucial also for parsing, cf. NUTCH-1605 and NUTCH-1925.</description>
      <version>1.10,2.3.1</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestRTFParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestPdfParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestOOParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestMSWordParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestImageMetadata.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestHtmlParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestFeedParser.java</file>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.parse.tika.TestEmbeddedDocuments.java</file>
      <file type="M">src.plugin.parse-tika.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1949" opendate="2015-2-24 00:00:00" fixdate="2015-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump out the Nutch data into the Common Crawl format</summary>
      <description>We are going to develop a CommonCrawlDataDumper.java class. The CommonCrawlDataDumper is a tool able to perfom the following steps: deserialize the crawled data from Nutch map serialized data on the proper JSON structure serialize the data into CBOR format optionally, compress the serialized data using gzipThis tool has to be able to work with either single Nutch segments or directory including segments as input data.Thanks lewismc and chrismattmann for your great suggestions, support and code.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.Benchmark.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1954" opendate="2015-3-7 00:00:00" fixdate="2015-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>FilenameTooLong error appears in CommonCrawlDumper</summary>
      <description>The issue from NUTCH-1950 is appearing in the CommonCrawlDumper tool as well (FilenameTooLong). I'm going to apply that fix here as well (based on MD5/message digest).</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1994" opendate="2015-4-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Apache Tika 1.8</summary>
      <description>Tika 1.8 was released this morning.Lets upgrade then release Nutch trunk.</description>
      <version>1.10,2.3.1</version>
      <fixedVersion>1.10,2.3.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1995" opendate="2015-4-22 00:00:00" fixdate="2015-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for wildcard to http.robot.rules.whitelist</summary>
      <description>The http.robot.rules.whitelist (NUTCH-1927) configuration parameter allows to specify a comma separated list of hostnames or IP addresses to ignore robot rules parsing for.Adding support for wildcard in http.robot.rules.whitelist could be very useful and simplify the configuration, for example, if we need to give many hostnames/addresses. Here is an example:&lt;name&gt;http.robot.rules.whitelist&lt;/name&gt; &lt;value&gt;*.sample.com&lt;/value&gt; &lt;description&gt;Comma separated list of hostnames or IP addresses to ignore robot rules parsing for. Use with care and only if you are explicitly allowed by the site owner to ignore the site's robots.txt! &lt;/description&gt;&lt;/property&gt;</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">src.java.org.apache.nutch.protocol.RobotRulesParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1996" opendate="2015-4-22 00:00:00" fixdate="2015-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make protocol-selenium README part of plugin</summary>
      <description>This is a simple issue which merely ports the documentation from the selenium plugin over to the source codehttps://github.com/momer/nutch-selenium/blob/master/README.md</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1998" opendate="2015-4-22 00:00:00" fixdate="2015-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for user-defined file extension to CommonCrawlDataDumper</summary>
      <description>CommonCrawlDataDumper tool is able to generate CBOR-encoded files, extracted from Nutch crawled data, using the Common Crawl format. By default, CommonCrawlDataDumper uses the original file extension.We are going to add support for a command-line option (e.g., -extension) that allows the user to provide a file extension to use in place of the original one.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.tools.TestCommonCrawlDataDumper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2011" opendate="2015-5-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Endpoint to support realtime JSON output from the fetcher</summary>
      <description>This fix will create an endpoint to query the Nutch REST service and get a real-time JSON response of the current/past Fetched URLs. This endpoint also includes pagination of the output to reduce data transfer bw in large crawls.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.resources.DbResource.java</file>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2021" opendate="2015-5-22 00:00:00" fixdate="2015-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use protocol-selenium to Capture Screenshots of the Page as it is Fetched</summary>
      <description>This should be a piece of cake. It can be done as followsWebDriver driver = new FirefoxDriver();driver.get("http://www.google.com/");File scrFile = ((TakesScreenshot)driver).getScreenshotAs(OutputType.FILE);// Now you can do whatever you need to do with it, for example copy somewhereFileUtils.copyFile(scrFile, new File("/usr/local/pics/screenshot.png"));</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-selenium.ivy.xml</file>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
      <file type="M">src.plugin.lib-selenium.plugin.xml</file>
      <file type="M">src.plugin.lib-selenium.ivy.xml</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2027" opendate="2015-5-31 00:00:00" fixdate="2015-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>seed list REST endpoint for Nutch 1.10</summary>
      <description>The endpoint for Nutch 1.10 that enables the user to set the seedlist for the REST api with a REST call.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2035" opendate="2015-6-3 00:00:00" fixdate="2015-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regex filter using case sensitive rules.</summary>
      <description>Regex expressions are computationally expensive and having “EXE|exe|JPG|jpg” etc etc..... adds up if we use complex rules.Regex filter should use case insensitive rules to make the rules more readable and improve performance.</description>
      <version>1.10</version>
      <fixedVersion>2.4,1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.regex-urlfilter.txt.template</file>
    </fixedFiles>
  </bug>
  <bug id="2037" opendate="2015-6-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job endpoint to support Indexing from the REST API</summary>
      <description>The job administration end point will now support indexing through this patch. The documentation of how to run this is on the Nutch REST API wiki. User defined indexer can be configured through the Configuration endpoint.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.impl.JobFactory.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingJob.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2039" opendate="2015-6-11 00:00:00" fixdate="2015-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relevance based scoring filter</summary>
      <description>A ScoringFilter plugin that uses a similarity measure to calculate the similarity between a given page(gold standard) and the currently parsed page. The score obtained from this similarity is then distributed to its outlinks. This filter aims to focus the crawler to crawl/explore relevant pages.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">default.properties</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2041" opendate="2015-6-17 00:00:00" fixdate="2015-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>indexer fails if linkdb is missing</summary>
      <description>If the linkdb is missing the indexer fails with2015-06-17 12:52:10,621 ERROR ...cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: .../linkdb/currentIf both db.ignore.internal.links and db.ignore.external.links there will be no LinkDb even if "invertlinks" is run (as consequence of NUTCH-1913). The script "bin/crawl" does not know about the values of these two properties and calls indexer with "-linkdb .../linkdb" which will then fail.Since "bin/crawl" is agnostic to properties defined in nutch-site.xml we solution similar to NUTCH-1854: make the tool/job more tolerant and log a warning instead of raising an error.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2042" opendate="2015-6-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parse-html increase chunk size used to detect charset</summary>
      <description>The chunk used to detect the encoding of a document is set to 2000 bytes. Although it is definitely best practice to "define" the character set on top, 2000 bytes are sometimes not enough: 20 longer &lt;link&gt; elements pointing to javascript and css libs may "hide" the &lt;meta&gt; element containing content type and encoding. Same problem has been observed in TIKA-357 and solved by increasing the buffer size to 8 kB.</description>
      <version>2.3,1.10</version>
      <fixedVersion>2.3.1,1.12</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2046" opendate="2015-6-24 00:00:00" fixdate="2015-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The crawl script should be able to skip an initial injection.</summary>
      <description>When our crawl gets really big a new injection takes considerable time as it updates crawldb, the crawl script should be able to skip the injection and go directly to the generate call.</description>
      <version>1.10</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.bin.crawl</file>
    </fixedFiles>
  </bug>
  <bug id="2049" opendate="2015-6-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Trunk to Hadoop &gt; 2.4 stable</summary>
      <description>Convo here - http://www.mail-archive.com/dev%40nutch.apache.org/msg18225.htmlI am +1 for taking trunk (or a branch of trunk) to explicit dependency on &gt; Hadoop 2.6.We can run our tests, we can validate, we can fix.I will be doing validation on 2.X in paralegal as this is what I use on my own projects.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.tools.proxy.SegmentHandler.java</file>
      <file type="M">src.test.org.apache.nutch.segment.TestSegmentMergerCrawlDatums.java</file>
      <file type="M">src.test.org.apache.nutch.segment.TestSegmentMerger.java</file>
      <file type="M">src.test.org.apache.nutch.net.TestURLNormalizers.java</file>
      <file type="M">src.test.org.apache.nutch.fetcher.TestFetcher.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestLinkDbMerger.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestInjector.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestGenerator.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestCrawlDbMerger.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestCrawlDbFilter.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.CrawlDBTestUtil.java</file>
      <file type="M">src.test.crawl-tests.xml</file>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.NaiveBayesParseFilter.java</file>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.NaiveBayesClassifier.java</file>
      <file type="M">src.java.org.apache.nutch.util.LockUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.HadoopFSUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">src.java.org.apache.nutch.service.JobManager.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.ContentAsTextInputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginManifestParser.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.Extension.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.parse.OutlinkExtractor.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFiltersChecker.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.MimeAdaptiveFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DeduplicationJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AdaptiveFetchSchedule.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">src.bin.crawl</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2052" opendate="2015-6-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance index-static to allow configurable delimiters</summary>
      <description>The index-static plugin has a set of fixed-value delimiters that control the parsing of the property index.static.comma is used to separate fieldscolon is used to separate field name from field valuespace is used to separate multiple values in the fieldThis set of choices makes it impossible to have a fixed field value containing a space, comma or colon.The proposed enhancement is to allow configuration properties to override any of these defaults.index.static.fieldsep (default comma)index.static.keysep (default colon)index.static.valuesep (default space)</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-static.src.test.org.apache.nutch.indexer.staticfield.TestStaticFieldIndexerTest.java</file>
      <file type="M">src.plugin.index-static.src.java.org.apache.nutch.indexer.staticfield.StaticFieldIndexer.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2062" opendate="2015-7-20 00:00:00" fixdate="2015-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Plugin for interacting with Selenium WebDriver</summary>
      <description>The protocol-selenium plugin is great for pulling webpages that dynamically load content. However, I've run into use cases where I need to actively interact with a page in Selenium before it becomes useful. For instance, I may need to paginate through a table to get all results that I'm interested in. This plugin will handle that use case.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2064" opendate="2015-7-21 00:00:00" fixdate="2015-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>URLNormalizer basic to encode reserved chars and decode non-reserved chars</summary>
      <description>NUTCH-1098 rewritten to work on trunk. Unit test is identical to 1098.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-basic.src.test.org.apache.nutch.net.urlnormalizer.basic.TestBasicURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-basic.src.java.org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2066" opendate="2015-7-23 00:00:00" fixdate="2015-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parameterize Generate REST endpoint</summary>
      <description>Allow user to specify crawldb and segment db in the Generate Job REST endpoint</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2068" opendate="2015-7-27 00:00:00" fixdate="2015-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow subcollection overrides via metadata</summary>
      <description>Similar to index-metdata but overrides subcollection. If both subcollection and index-metadata are active, you will get two values for the field possible causing multivalued field errors.</description>
      <version>1.10</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2069" opendate="2015-7-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore external links based on domain</summary>
      <description>We currently have `db.ignore.external.links` which is a nice way of restricting the crawl based on the hostname. This adds a new parameter 'db.ignore.external.links.domain' to do the same based on the domain.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2085" opendate="2015-8-25 00:00:00" fixdate="2015-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Guava</summary>
      <description>Upgrade Guava to 16.0.1. Higher will break tests.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2086" opendate="2015-8-25 00:00:00" fixdate="2015-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch 1.X Webui</summary>
      <description>To port the Apache Wicket based webui in Nutch 2.X to 1.X</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.model.request.SeedUrl.java</file>
      <file type="M">src.java.org.apache.nutch.service.model.request.SeedList.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2088" opendate="2015-8-28 00:00:00" fixdate="2015-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Optional Execution to Interactive Selenium Handlers</summary>
      <description>At the moment, all the Handlers run for every URL when using the interactive-selenium plugin. Often times when trying to do a deep crawl of a site you'll want to handle various subdomains and paths/files differently. You can effectively filter in the handlers at the moment, but only once you've loaded the WebDriver and incurred the associated overhead. It would be much nicer if the handler interface allowed for this check to occur prior to the request to retrieve page content.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.InteractiveSeleniumHandler.java</file>
      <file type="M">src.plugin.protocol-interactiveselenium.src.java.org.apache.nutch.protocol.interactiveselenium.handlers.DefaultHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21" opendate="2005-3-26 00:00:00" fixdate="2005-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parser plugin for MS PowerPoint slides</summary>
      <description>transfered from:http://sourceforge.net/tracker/index.php?func=detail&amp;aid=1109321&amp;group_id=59548&amp;atid=491356submitted by:Stephan Strittmatter</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2102" opendate="2015-9-16 00:00:00" fixdate="2015-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WARC Exporter</summary>
      <description>This patch adds a WARC exporter http://bibnum.bnf.fr/warc/WARC_ISO_28500_version1_latestdraft.pdf. Unlike the code submitted in https://github.com/apache/nutch/pull/55 which is based on the CommonCrawlDataDumper, this exporter is a MapReduce job and hence should be able to cope with large segments in a timely fashion and also is not limited to the local file system.Later on we could have a WARCImporter to generate segments from WARC files, which is outside the scope of the CCDD anyway. Also WARC is not specific to CommonCrawl, which is why the package name does not reflect it.I don't think it would be a problem to have both the modified CCDD and this class providing similar functionalities.This class is called in the following way ./nutch org.apache.nutch.tools.warc.WARCExporter /data/nutch-dipe/1kcrawl/warc -dir /data/nutch-dipe/1kcrawl/segments/</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2104" opendate="2015-9-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation to the protocol-selenium plugin Readme file re: selenium grid implementation</summary>
      <description>Adding some documentation to the protocol-selenium Readme file with regards to advice on using the selenium grid. Namely:(1) parameters to set for optimization of the grid (2) pitfalls to beware of when using the grid</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-selenium.README.md</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2108" opendate="2015-9-17 00:00:00" fixdate="2015-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a function to the selenium interactive plugin interface to do multiple manipulation of driver and then return the data</summary>
      <description>In the interactive selenium plugin we have to create handler classes for each manipulation of a page. Sometimes we need to manipulate a page in many ways and keep track of those manipulations. Like clicking on say each link in a table and then refreshing to get the original page back as even one click can make all other links go away. This can be done in a single loop. Which will be a little too much work and way complicated using multiple handlers. So, I am proposing a new function "String multiProcessDriver(WebDriver driver)" that takes the driver and returns a concatenated String along with the already present "void processDriver(WebDriver driver)".</description>
      <version>1.10</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2111" opendate="2015-9-19 00:00:00" fixdate="2015-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete temporary files location for selenium tmp files after driver quits</summary>
      <description>When using the selenium plug in (local mode or selenium grid), a large # tmp files can be generated for each webdriver executed. The default location for selenium is the /tmp library. Thus very quickly (and inadvertently) the nutch-selenium interaction can lead to filesystem issues. I propose to include a config in nutch-default.xml that allows users to specify where they want the selenium tmp files to be written.</description>
      <version>1.10</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2115" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add total counts to dump stats</summary>
      <description>It would be nice if the "dump" tool included total counts for the mimetype stats that it gives. Something along the lines of the following would be great when you have to deal with some larger crawls and don't want to bother doing the math yourself.Dumper File Stats: TOTAL Stats:[ {"mimeType":"application/xhtml+xml","count":"2"} {"mimeType":"application/octet-stream","count":"1"} {"mimeType":"text/html","count":"23"}]Total count: 26FILTERED Stats:[ {"mimeType":"text/html","count":"23"}]Total filtered count: 23</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.DumpFileUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2129" opendate="2015-10-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track Protocol Status in Crawl Datum</summary>
      <description>It's become necessary on a few crawls that I run to get protocol status code stats. After speaking with lewismc it seemed that there might not be a super convenient way of doing this as is, but it would be great to be able to add the functionality necessary to pull this information out.</description>
      <version>2.3,1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Ftp.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.java.org.apache.nutch.metadata.Nutch.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2139" opendate="2015-10-13 00:00:00" fixdate="2015-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Basic plugin to index inlinks and outlinks</summary>
      <description>Basic plugin that allows to index the inlinks and outlinks of the web pages, this could be very useful for analytic purposes, including neat visualizations using d3.js for instance.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2149" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST endpoint to read Nutch sequence files</summary>
      <description>This endpoint enables reading of the webgraph data like nodes, links and any other sequence file in the Nutch ecosystem via a RESTful interface. The current API documentation for this Reader endpoint is available at - http://docs.nutchpytonutchrestapi.apiary.io/Thanks to https://github.com/ContinuumIO/nutchpy for the initial work.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.service.NutchServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2150" opendate="2015-10-27 00:00:00" fixdate="2015-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ProtocolStatus Utility</summary>
      <description>It would be nice to have a utility for dumping protocol status code information for a crawl database. This will be a utility for getting a dump of the protocol status codes that builds off of NUTCH-2129</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.ProtocolStatusStatistics.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">src.bin.crawl</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2155" opendate="2015-10-28 00:00:00" fixdate="2015-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a "crawl completeness" utility</summary>
      <description>I've found it useful to have a tool for dumping some "completeness" information from a crawl similar to how domainstats does but including fetched and unfetched counts per domain/host. This is especially nice when doing vertical crawls over a few domains or just to see how much of a host/domain you've covered with your crawl so far.</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.CrawlCompletionStats.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2167" opendate="2015-11-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport TableUtil from 2.x for URL reversing</summary>
      <description>The TableUtil file provides a number of helpful utilities functions for URL reversing that would be useful to have in 1.x</description>
      <version>1.10</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2172" opendate="2015-11-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>index-more: document format of contenttype-mapping.txt</summary>
      <description>The index-more plugin uses the conf/contenttype-mapping.txt file to build up the mimeMap hash table (in the readConfiguration() method).The line splitting is performed around "\t", so it silently skip lines separated by simple spaces or more than one tab (see line 325).Changing the single-char string "\t" with the regex "s+" should do the magic.</description>
      <version>1.10</version>
      <fixedVersion>1.12</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2250" opendate="2016-4-14 00:00:00" fixdate="2016-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CommonCrawlDumper : Invalid format + skipped parts</summary>
      <description>The following issues are found with CommonCrawlDumper;1. Documents get duplicated in dump filesHow to reproduce bin/nutch commoncrawldump -segment .../segments -outputDir testdump -SimpleDateFormat -epochFilename -jsonArray -reverseKeyThe first ever written will contain 1 document.second file includes two documentsthird file includes first three documents and this grows linearly.2.If a segment has many parts (part-00000, part-00001,...) only the first part (part-00000 ) is being dumpedHow to reproduce ?Create segment with two parts (part-00000 and part-00001)</description>
      <version>1.10</version>
      <fixedVersion>1.12</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormat.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
