<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1605" opendate="2013-7-7 00:00:00" fixdate="2013-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mime type detector recognizes xlsx as zip file</summary>
      <description>With mime.type.magic as true (the default) Office Open XML spreadsheets (*.xlsx) are treated as zip files and not parsed correctly:% bin/nutch parsechecker http://localhost/test.xlsxfetching: http://localhost/test.xlsxparsing: http://localhost/test.xlsxcontentType: application/zip...Xlsx files are formally zip files. Nevertheless, both HTTP header and file name are clear:% wget -d http://localhost/test.xlsx...HTTP/1.1 200 OK...Content-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet...Tika 1.4 detects the type correctly:% java -jar tika-app-1.4.jar -d http://localhost/test/test.xlsxapplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet</description>
      <version>1.7,2.2.1</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1629" opendate="2013-8-20 00:00:00" fixdate="2013-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>there is no need to fail on empty lines in seed file when injecting.</summary>
      <description>right now, if there is an empty line in a seed file, TableUtil.reversUrl would throw an exception that would kill the inject job.</description>
      <version>1.7,2.2.1</version>
      <fixedVersion>2.3,1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1633" opendate="2013-8-26 00:00:00" fixdate="2013-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>slf4j is provided by hadoop and should not be included in the job file.</summary>
      <description>there are two issues with including slf4j in the job file. the minor of the two is that slf4j starts issuing warnings when it finds more than on instances in the classpath( GORA-272 ). the bigger issue happens when the versions of the slf4j in hadoop and nutch are not compatible (ex. hadoop 1.1.1 &amp; nutch 2.1) which results in all nutch jobs to crash.</description>
      <version>1.7,2.2.1</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1645" opendate="2013-9-30 00:00:00" fixdate="2013-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Junit Test Case for Adaptive Fetch Schedule class</summary>
      <description>Currently there is not Test Case for Adaptive Fetch Schedule. Junit test Writes for its.</description>
      <version>2.2.1</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1660" opendate="2013-11-2 00:00:00" fixdate="2013-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Index filter for Page&amp;#39;s latitude and longitude</summary>
      <description>I see some discuss about page's ip storing. I think If we have page's ip, we can index page's geo position as latitude and longitude. That use for location based searches. icebergx5 I know you have a patch about this in your secret patches Can you share us ?</description>
      <version>2.2.1</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-geoip.plugin.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFiltersChecker.java</file>
      <file type="M">default.properties</file>
      <file type="M">conf.solrindex-mapping.xml</file>
      <file type="M">conf.schema.xml</file>
      <file type="M">conf.schema-solr4.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1684" opendate="2013-12-11 00:00:00" fixdate="2013-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ParseMeta to be added before fetch schedulers are run</summary>
      <description>FetchSchedulers cannot operate on parseMeta in the CrawlDatum because it is added after the schedulers have run.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1721" opendate="2014-1-31 00:00:00" fixdate="2014-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Crawler commons 0.3</summary>
      <description></description>
      <version>1.7,2.2,2.2.1</version>
      <fixedVersion>2.3,1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1724" opendate="2014-2-10 00:00:00" fixdate="2014-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LinkDBReader to support regex output filtering</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1733" opendate="2014-3-11 00:00:00" fixdate="2014-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>parse-html to support HTML5 charset definitions</summary>
      <description>HTML 5 allows to specify the character encoding of a page per &lt;meta charset="..."&gt; Unicode Byte Order Mark (BOM)These are allowed in addition to previous HTTP/http-equiv Content-Type, see [1.Parse-html ignores both meta charset and BOM, falls back to the default encoding (cp1252). Parse-tika sets the encoding appropriately.</description>
      <version>1.8,2.2.1</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1759" opendate="2014-4-16 00:00:00" fixdate="2014-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Crawler Commons 0.4</summary>
      <description>We recently releaed CC 0.4. We should upgrade.</description>
      <version>1.8,2.2.1</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1785" opendate="2014-5-21 00:00:00" fixdate="2014-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to index raw content</summary>
      <description>Some use-cases require Nutch to actually write the raw content a configured indexing back-end. Since Content is never read, a plugin is out of the question and therefore we need to force IndexJob to process Content as well.</description>
      <version>None</version>
      <fixedVersion>1.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingJob.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.schema.xml</file>
      <file type="M">conf.schema-solr4.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1825" opendate="2014-8-15 00:00:00" fixdate="2014-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>protocol-http may hang for certain web pages</summary>
      <description>There is a rare case where protocol-http will wait for data even when all the data has been sent.Patch is attached; please test and confirm.</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1866" opendate="2014-9-30 00:00:00" fixdate="2014-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ant eclipse target should not delete runtime</summary>
      <description>This is a pretty nasty bug which can really screw things up for you.The eclipse target should not delete runtime whenever invoked... this is terrible.</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1876" opendate="2014-10-16 00:00:00" fixdate="2014-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Crawler Commons 0.5</summary>
      <description>We just released crawler commons 0.5We should upgrade here in Nutch.https://code.google.com/p/crawler-commons/</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1877" opendate="2014-10-16 00:00:00" fixdate="2014-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suffix URL filter to ignore query string by default</summary>
      <description>Suffix URL filter entry: .mp3Does not filter out: http://www.example.org/file.mp3?a=b</description>
      <version>1.9,2.2.1</version>
      <fixedVersion>2.3,1.10</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.suffix-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="188" opendate="2006-1-27 00:00:00" fixdate="2006-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add searchable mailing list links to http://lucene.apache.org/nutch/mailing_lists.html</summary>
      <description>Post links to searchable mail archives on nutch.org</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.include.footer.html</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2372" opendate="2017-4-10 00:00:00" fixdate="2017-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs build failing.</summary>
      <description>When we build javadocs of nutch using the command : "ant javadoc" we get a handful of errors and the build fails. This is because up to JDK 7, the Javadoc tool was pretty lenient. With JDK 8, a new part has been added to Javadoc called doclint and it changes that friendly behaviour. Warnings turned out into errors with JDK 8. The error log can be found here : https://paste.apache.org/sVQ5</description>
      <version>2.2.1,1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-domain.src.java.org.apache.nutch.urlfilter.domain.DomainURLFilter.java</file>
      <file type="M">src.plugin.urlnormalizer-querystring.src.java.org.apache.nutch.net.urlnormalizer.querystring.QuerystringURLNormalizer.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.scoring.urlmeta.URLMetaScoringFilter.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.indexer.urlmeta.URLMetaIndexingFilter.java</file>
      <file type="M">src.plugin.urlfilter-suffix.src.java.org.apache.nutch.urlfilter.suffix.SuffixURLFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbMapper.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizers.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseResult.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginRepository.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentPart.java</file>
      <file type="M">src.java.org.apache.nutch.service.impl.ConfManagerImpl.java</file>
      <file type="M">src.java.org.apache.nutch.tools.AbstractCommonCrawlFormat.java</file>
      <file type="M">src.java.org.apache.nutch.tools.arc.ArcRecordReader.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormat.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormatFactory.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormatSimple.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">src.java.org.apache.nutch.util.EncodingDetector.java</file>
      <file type="M">src.java.org.apache.nutch.util.LockUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.PrefixStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.SuffixStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.TableUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.TimingUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.TrieStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.URLUtil.java</file>
      <file type="M">src.plugin.feed.src.java.org.apache.nutch.indexer.feed.FeedIndexingFilter.java</file>
      <file type="M">src.plugin.index-anchor.src.java.org.apache.nutch.indexer.anchor.AnchorIndexingFilter.java</file>
      <file type="M">src.plugin.index-geoip.src.java.org.apache.nutch.indexer.geoip.GeoIPIndexingFilter.java</file>
      <file type="M">src.plugin.index-links.src.java.org.apache.nutch.indexer.links.LinksIndexingFilter.java</file>
      <file type="M">src.plugin.index-metadata.src.java.org.apache.nutch.indexer.metadata.MetadataIndexer.java</file>
      <file type="M">src.plugin.index-replace.src.java.org.apache.nutch.indexer.replace.ReplaceIndexer.java</file>
      <file type="M">src.plugin.indexer-dummy.src.java.org.apache.nutch.indexwriter.dummy.DummyIndexWriter.java</file>
      <file type="M">src.plugin.indexer-solr.src.java.org.apache.nutch.indexwriter.solr.SolrUtils.java</file>
      <file type="M">src.plugin.language-identifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
      <file type="M">src.plugin.lib-htmlunit.src.java.org.apache.nutch.protocol.htmlunit.HtmlUnitWebDriver.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
      <file type="M">src.plugin.mimetype-filter.src.java.org.apache.nutch.indexer.filter.MimeTypeIndexingFilter.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipTextExtractor.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Client.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.scoring-similarity.src.java.org.apache.nutch.scoring.similarity.util.LuceneTokenizer.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.src.java.org.apache.nutch.urlfilter.domainblacklist.DomainBlacklistURLFilter.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
