<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1932" opendate="2015-2-4 00:00:00" fixdate="2015-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically remove orphaned pages</summary>
      <description>Orphan scoring filter that determines whether a page has become orphaned, e.g. it has no more other pages linking to it. If a page hasn't been linked to after markGoneAfter seconds, the page is marked as gone and is then removed by an indexer. If a page hasn't been linked to after markOrphanAfter seconds, the page is removed from the CrawlDB.Note: if you have this plugin enabled you MUST make sure you visit 'almost' every URL at least once within the refetch interval. If you don't, non-orphans may be marked as orphan and get deleted! You can use NUTCH-2368 to make sure this is the case.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.crawl.TestCrawlDbStates.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilters.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">default.properties</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1933" opendate="2015-2-4 00:00:00" fixdate="2015-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nutch-selenium plugin</summary>
      <description>I updated the plugin nutch-selenium plugin to run against trunk.I feel that there is a good bit of work to be done here however early testing on my system are that it works.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1993" opendate="2015-4-21 00:00:00" fixdate="2015-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch does not use backup parsers</summary>
      <description>From reading the code it is clear that it is designed to allow using several parsers to parse a document in a sequence, until it is successfully parsed. In practice, this does not work because these lines if (parseResult != null &amp;&amp; !parseResult.isEmpty()) return parseResult;break the loop even if the parsing has failed because parseResult is not empty anyway, it contains a ParseData with ParseStatus.FAILED.A fix:if ( parseResult.isAnySuccess() ) return parseResult;Where parseResult.isAnySuccess() returns true if any of the parsing attempts were successful.This fix is important because it allows use of backup parsers as originally designed and thus increase index completeness.</description>
      <version>1.13</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseUtil.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseResult.java</file>
    </fixedFiles>
  </bug>
  <bug id="2136" opendate="2015-10-12 00:00:00" fixdate="2015-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a different version of Naive Bayes Parse Filter</summary>
      <description>There has been many dependency issues with the first implementation of Naive Bayes Parse Filter. The major dependencies were Mahout and Lucene. There was also the issue where the training process failed in the distributed mode due to the fact that a nested hadoop job was unable to run on the cluster.To remove all these issues and make the filter be able to run in a distributed environment I am going to implement my own version of Naive Bayes without any dependency on any machine learning libraries.</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.NaiveBayesParseFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2137" opendate="2015-10-12 00:00:00" fixdate="2015-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add changes.txt and ALV2 headers to the Naive Bayes Parse Filter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.Train.java</file>
      <file type="M">src.plugin.parsefilter-naivebayes.src.java.org.apache.nutch.parsefilter.naivebayes.Classify.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2372" opendate="2017-4-10 00:00:00" fixdate="2017-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadocs build failing.</summary>
      <description>When we build javadocs of nutch using the command : "ant javadoc" we get a handful of errors and the build fails. This is because up to JDK 7, the Javadoc tool was pretty lenient. With JDK 8, a new part has been added to Javadoc called doclint and it changes that friendly behaviour. Warnings turned out into errors with JDK 8. The error log can be found here : https://paste.apache.org/sVQ5</description>
      <version>2.2.1,1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-domain.src.java.org.apache.nutch.urlfilter.domain.DomainURLFilter.java</file>
      <file type="M">src.plugin.urlnormalizer-querystring.src.java.org.apache.nutch.net.urlnormalizer.querystring.QuerystringURLNormalizer.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.scoring.urlmeta.URLMetaScoringFilter.java</file>
      <file type="M">src.plugin.urlmeta.src.java.org.apache.nutch.indexer.urlmeta.URLMetaIndexingFilter.java</file>
      <file type="M">src.plugin.urlfilter-suffix.src.java.org.apache.nutch.urlfilter.suffix.SuffixURLFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbMapper.java</file>
      <file type="M">src.java.org.apache.nutch.hostdb.UpdateHostDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizers.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserChecker.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseResult.java</file>
      <file type="M">src.java.org.apache.nutch.plugin.PluginRepository.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentPart.java</file>
      <file type="M">src.java.org.apache.nutch.service.impl.ConfManagerImpl.java</file>
      <file type="M">src.java.org.apache.nutch.tools.AbstractCommonCrawlFormat.java</file>
      <file type="M">src.java.org.apache.nutch.tools.arc.ArcRecordReader.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlDataDumper.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormat.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormatFactory.java</file>
      <file type="M">src.java.org.apache.nutch.tools.CommonCrawlFormatSimple.java</file>
      <file type="M">src.java.org.apache.nutch.tools.FileDumper.java</file>
      <file type="M">src.java.org.apache.nutch.util.EncodingDetector.java</file>
      <file type="M">src.java.org.apache.nutch.util.LockUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.PrefixStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.SuffixStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.TableUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.TimingUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.TrieStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.URLUtil.java</file>
      <file type="M">src.plugin.feed.src.java.org.apache.nutch.indexer.feed.FeedIndexingFilter.java</file>
      <file type="M">src.plugin.index-anchor.src.java.org.apache.nutch.indexer.anchor.AnchorIndexingFilter.java</file>
      <file type="M">src.plugin.index-geoip.src.java.org.apache.nutch.indexer.geoip.GeoIPIndexingFilter.java</file>
      <file type="M">src.plugin.index-links.src.java.org.apache.nutch.indexer.links.LinksIndexingFilter.java</file>
      <file type="M">src.plugin.index-metadata.src.java.org.apache.nutch.indexer.metadata.MetadataIndexer.java</file>
      <file type="M">src.plugin.index-replace.src.java.org.apache.nutch.indexer.replace.ReplaceIndexer.java</file>
      <file type="M">src.plugin.indexer-dummy.src.java.org.apache.nutch.indexwriter.dummy.DummyIndexWriter.java</file>
      <file type="M">src.plugin.indexer-solr.src.java.org.apache.nutch.indexwriter.solr.SolrUtils.java</file>
      <file type="M">src.plugin.language-identifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
      <file type="M">src.plugin.lib-htmlunit.src.java.org.apache.nutch.protocol.htmlunit.HtmlUnitWebDriver.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.plugin.lib-selenium.src.java.org.apache.nutch.protocol.selenium.HttpWebClient.java</file>
      <file type="M">src.plugin.mimetype-filter.src.java.org.apache.nutch.indexer.filter.MimeTypeIndexingFilter.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipTextExtractor.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Client.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpBasicAuthentication.java</file>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">src.plugin.scoring-similarity.src.java.org.apache.nutch.scoring.similarity.util.LuceneTokenizer.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.src.java.org.apache.nutch.urlfilter.domainblacklist.DomainBlacklistURLFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2397" opendate="2017-7-4 00:00:00" fixdate="2017-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parser to add paragraph line breaks</summary>
      <description>(initially reported with patch/pull-request by Vipul Behl, see #190)The parser (parse-tika and parse-html) could be improved to add line breaks between paragraphs, instead of writing the whole document into a single line.</description>
      <version>2.3.1,1.13</version>
      <fixedVersion>2.4,1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-zip.src.test.org.apache.nutch.parse.zip.TestZipParser.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2398" opendate="2017-7-5 00:00:00" fixdate="2017-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetcher saving redirected robots.txt under redirect target URL</summary>
      <description>NUTCH-2300 lets the Fetcher store optionally the robots.txt response (content and HTTP status). If the '.../robots.txt' is redirected, the redirected content is also stored but with the redirect source URL as key. It should use the redirect target URL instead. Otherwise one of the responses is overwritten in the segments map file.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpRobotRulesParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="2399" opendate="2017-7-9 00:00:00" fixdate="2017-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>indexer-elastic does not index multi-value fields (only the first value is indexed)</summary>
      <description>Currently, if there is a NutchField with multiple values, only the first value is indexed (because this is what doc.getFieldValue returns). Pull request #200 checks if the NutchField has multiple values, and if so, they are added as an array (multivalue) field.https://github.com/apache/nutch/pull/200</description>
      <version>None</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-elastic.src.java.org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2403" opendate="2017-7-21 00:00:00" fixdate="2017-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch Selenium: Wrong documentation about PhantomJS</summary>
      <description>The Nutch Selenium documentation states that PhantomJS can be used as phantomJS for selenium.driver. The correct value would be phantomjs according to https://github.com/apache/nutch/blob/master/src/plugin/lib-selenium/src/java/org/apache/nutch/protocol/selenium/HttpWebClient.java#L124</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-selenium.README.md</file>
    </fixedFiles>
  </bug>
  <bug id="2411" opendate="2017-8-22 00:00:00" fixdate="2017-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Index-metadata to support indexing multiple values for a field</summary>
      <description>&lt;property&gt; &lt;name&gt;index.metadata.separator&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;description&gt; Separator to use if you want to index multiple values for a given field. Leave empty to treat each value as a single value. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;index.metadata.multivalued.fields&lt;/name&gt; &lt;value&gt;&lt;/value&gt; &lt;description&gt; Comma separated list of fields that are multi valued. &lt;/description&gt;&lt;/property&gt;</description>
      <version>1.13</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-metadata.src.java.org.apache.nutch.indexer.metadata.MetadataIndexer.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2413" opendate="2017-8-25 00:00:00" fixdate="2017-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parsing fetcher to respect property "parse.filter.urls"</summary>
      <description>In a situation when we want to:(1) Execute the fetch and parse together ("fetcher.parse" setting to "true")(2) Avoid applying the URL filters when executing this phase.Condition (2) can be configured when parsing is executed as a separate process by setting "parse.filter.urls" to "false".However, this setting ("parse.filter.urls") is ignored when we execute the fetch and parse phases together.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="2419" opendate="2017-9-6 00:00:00" fixdate="2017-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some URL filters and normalizers do not respect command-line override for rule file</summary>
      <description></description>
      <version>1.13</version>
      <fixedVersion>1.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-slash.src.test.org.apache.nutch.net.urlnormalizer.slash.TestSlashURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-slash.src.java.org.apache.nutch.net.urlnormalizer.slash.SlashURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-protocol.src.test.org.apache.nutch.net.urlnormalizer.protocol.TestProtocolURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-protocol.src.java.org.apache.nutch.net.urlnormalizer.protocol.ProtocolURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-host.src.test.org.apache.nutch.net.urlnormalizer.host.TestHostURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-host.src.java.org.apache.nutch.net.urlnormalizer.host.HostURLNormalizer.java</file>
      <file type="M">src.plugin.urlfilter-suffix.src.java.org.apache.nutch.urlfilter.suffix.SuffixURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-prefix.src.java.org.apache.nutch.urlfilter.prefix.PrefixURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domain.src.test.org.apache.nutch.urlfilter.domain.TestDomainURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domain.src.java.org.apache.nutch.urlfilter.domain.DomainURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.src.test.org.apache.nutch.urlfilter.domainblacklist.TestDomainBlacklistURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.src.java.org.apache.nutch.urlfilter.domainblacklist.DomainBlacklistURLFilter.java</file>
      <file type="M">src.plugin.parsefilter-regex.src.test.org.apache.nutch.parsefilter.regex.TestRegexParseFilter.java</file>
      <file type="M">src.plugin.parsefilter-regex.src.java.org.apache.nutch.parsefilter.regex.RegexParseFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2432" opendate="2017-9-25 00:00:00" fixdate="2017-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protocol httpclient to disable cookies if http.enable.cookie.header is false</summary>
      <description></description>
      <version>1.13</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="2433" opendate="2017-9-26 00:00:00" fixdate="2017-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Html Parser: keep htmltag where the outlinks are found</summary>
      <description>When parsing HTML pages, I need to know in which HTML tag the outlinks were found (for example, 'a', 'script', 'img', etc).I propose to add a new configuration value, "parser.html.outlinks.htmlnode_metadata_name".If this configuration property is not empty, all found outlinks will be assigned a metadata with the name indicated in this configuration property with the html tag name where the outlink was found.I will now send the pull request with my code implementation.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2435" opendate="2017-9-27 00:00:00" fixdate="2017-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New configuration allowing to choose whether to store &amp;#39;parse_text&amp;#39; directory or not.</summary>
      <description>Whenever a page is parsed, one of the outputs is the directory 'parse_text'.It is intended to be used at the indexing phase so the page can be searched from a search engine such as Solr.In my special crawling case, I don't need to index the page contents. Therefore, creating and filing the 'parse_text' is not required for me. To optimize performance, I don't want the crawler to store this information to the filesystem. I propose a new parameter "parser.store.text" allowing to choose whether to store 'parse_text' directory or not. Its default value, of course, is "true".</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2439" opendate="2017-10-11 00:00:00" fixdate="2017-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Apache Tika 1.17</summary>
      <description></description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2443" opendate="2017-10-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extract links from the video tag with the parse-html plugin</summary>
      <description>At the moment the parse-html extracts links from the tags a, area, form (configurable), frame, iframe, script, link, img. Since we allow extracting links to binary files (images) extracting links also from the video tag should be supported.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.tika.TestDOMContentUtils.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.test.org.apache.nutch.parse.html.TestDOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2444" opendate="2017-10-20 00:00:00" fixdate="2017-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HostDB CSV dumper to emit field header by default</summary>
      <description>Started to get annoyed by constantly having to look-u HostDatum for the field set.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.hostdb.ReadHostDb.java</file>
    </fixedFiles>
  </bug>
  <bug id="2445" opendate="2017-10-20 00:00:00" fixdate="2017-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetcher following outlinks to keep track of already fetched items</summary>
      <description>When fetcher.follow.outlinks.depth is non-zero, fetcher follows outlinks. This patch keeps track of already fetched URL's and thus avoid fetching the same URL twice.A Set is used to keep track of them, hashcodes to reduce memory usage. This is not used if fetcher doesn't follow outlinks.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.FetchItemQueue.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="2448" opendate="2017-10-23 00:00:00" fixdate="2017-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Sending an empty http.agent.version</summary>
      <description>http.agent.version defaults in nutch-default.xml to Nutch-1.14-SNAPSHOT (depending on the version of course).If I want to override it to not send a version as part of the user-agent, there is nothing I can do in nutch-site.xml, since putting an empty string there causes the default to be taken, and putting any value there causes a slash to be appended to the http.agent.name.As far as I can see, the only way to override it is to remove the value in nutch-default.xml, which is probably not the “correct” way, considering it contains a comment saying “Do not modify this file directly”.The suggested solution is to treat a white-space-only value as empty.</description>
      <version>1.13</version>
      <fixedVersion>2.4,1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="2449" opendate="2017-10-24 00:00:00" fixdate="2017-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Usage of Tika LanguageIdentifier in language-identifier plugin</summary>
      <description>The language-identifier plugin uses org.apache.tika.language.LanguageIdentifier for extracting the language from the document text. There are two issues with that: LanguageIdentifier is deprecated in Tika. It does not support CJK language (and I suspect a lot of other languages - https://wiki.apache.org/nutch/LanguageIdentifierPlugin#Implemented_Languages_and_their_ISO_636_Codes), and it doesn’t even fail gracefully with them - in my experience Chinese was recognized as Italian.</description>
      <version>1.13</version>
      <fixedVersion>1.19</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.language-identifier.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2452" opendate="2017-10-25 00:00:00" fixdate="2017-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem retrieving encoded URLs via FTP?</summary>
      <description>I tried running Nutch on my Synology NAS. As SMB protocol is not contained in Nutch, I turned on FTP service on the NAS and configured Nutch to crawl ftp://nas.The experience gives me varying results which seem to point to problems within Nutch. However this may need further evaluation.As some files could not be downloaded and I could not see a good error message I changed the method org.apache.nutch.protocol.ftp.FTP.getProtocolOutput(Text, CrawlDatum) to not only return protocol status but send the full exception and stack trace to the logs:{{ } catch (Exception e) {LOG.warn("Could not get {}", url, e);return new ProtocolOutput(null, new ProtocolStatus(e));}}}With this modification I suddenly see such messages in the logfile:{{2017-10-25 14:14:37,254 TRACE org.apache.nutch.protocol.ftp.Ftp - fetching ftp://nas/silver-sda2/home/vivi/Desktop/Pictures/Kenya%20Pics/2017-10-25 14:14:37,512 WARN org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/silver-sda2/home/vivi/Desktop/Pictures/Kenya%20Pics/org.apache.nutch.protocol.ftp.FtpError: Ftp Error: 404 at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:151) at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)}}Please mind the URL was not configured from me. Instead it was obtained by crawling my NAS. Also the URL looks perfectly fine to me. Even more, using Firefox and the same authentication data on the same URL displays the directory successfully. Therefore I suspect the FTP client is unable to decode the URL such that the FTP server would understand it.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpResponse.java</file>
    </fixedFiles>
  </bug>
  <bug id="2464" opendate="2017-11-20 00:00:00" fixdate="2017-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Plugin headings: Headers That Contain HTML Elements Are Not Parsed</summary>
      <description>Nutch does not appear to traverse the HTML elements that may be contained within header elements (e.g., H1, H2, H3, etc. tags). Many times there are anchors and/or &lt;span&gt; tags within these elements that contain the actual text nodes that should be picked up as the header value for indexing purposes.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.headings.src.java.org.apache.nutch.parse.headings.HeadingsParseFilter.java</file>
      <file type="M">src.plugin.headings.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2466" opendate="2017-11-28 00:00:00" fixdate="2017-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sitemap processor to follow redirects</summary>
      <description>It does follow http &gt; https, but not the following redirect, e.g. sitemap_index.xml that some websites have.</description>
      <version>1.13</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.SitemapProcessor.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2477" opendate="2017-12-12 00:00:00" fixdate="2017-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor *Checker classes to use base class for common code</summary>
      <description>The various Checker class implementations have quite a bit of duplicated code in them. This should be refactored for cleanliness and maintainability.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizerChecker.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilters.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilterChecker.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFiltersChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="2478" opendate="2017-12-12 00:00:00" fixdate="2017-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>// is not a valid base URL</summary>
      <description>This test fails: @Test public void testBadResolver() throws Exception { URL base = new URL("//www.example.org/"); String target = "index/produkt/kanaly/"; URL abs = URLUtil.resolveURL(base, target); Assert.assertEquals("http://www.example.org/index/produkt/kanaly/", abs.toString()); }and has to fail because of invalid base URL, so the current URL is used. If current URL is not /, its path will be prepended, resulting in 404 being crawled.This ticket must allow // as base, and resolve the protocol.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.TikaParser.java</file>
      <file type="M">src.plugin.parse-html.src.test.org.apache.nutch.parse.html.TestHtmlParser.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.java.org.apache.nutch.util.DomUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2480" opendate="2017-12-15 00:00:00" fixdate="2017-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade crawler-commons dependency to 0.9</summary>
      <description>Crawler-commons 0.9 is relased. We should upgrade the dependency: there are significant improvements in the sitemap parser, also crawler-commons 0.9 depends on Tika 1.16 which minimizes the gap to Tika 1.17 (NUTCH-2439).</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2483" opendate="2017-12-16 00:00:00" fixdate="2017-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove/replace indirect dependencies to org.json</summary>
      <description>As indirect transitive dependency we ship with Nutch 1.x binary packages a jar file of org.json which license is since one year among the category x licenses (see also license faq).We should check whether the library is mandatory and the exclude or replace it.</description>
      <version>1.13</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
