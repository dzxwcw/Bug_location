<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1420" opendate="2012-7-4 00:00:00" fixdate="2012-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get rid of the dreaded �</summary>
      <description>Some pages, especially PDF's, produce sequences with the dreaded � character. This patch removes them from the title and content field.</description>
      <version>None</version>
      <fixedVersion>1.7,2.2.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.util.StringUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1680" opendate="2013-12-10 00:00:00" fixdate="2013-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CrawldbReader to dump minRetry value</summary>
      <description>CrawlDBReader should be able to dump records based on minimum retry value.</description>
      <version>None</version>
      <fixedVersion>1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="173" opendate="2006-1-13 00:00:00" fixdate="2006-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PerHost Crawling Policy ( crawl.ignore.external.links )</summary>
      <description>There is two major way of crawl in Nutch.Intranet Crawl : forbidden all, allow somes few hostWhole-web crawl : allow all, forbidden few thinksI propose a third type of crawl.Directory Crawl : The purpose of this crawl is to manage few thousands of host wihtout managing rules pattern in UrlFilterRegexp.I made two patch for : 0.7, 0.7.1 and 0.8-devI propose a new boolean property in nutch-site.xml : crawl.ignore.external.links, with false value at default.By default this new feature don't modify the behavior of nutch crawler.When you setup this property to true, the crawler don't fetch external links of the host.So the crawl is limited to the host that you inject at the beginning at the crawl.I know there is some proposal of new crawl policy using the CrawlDatum in 0.8-dev branch. This feature colud be a easiest way to add quickly new crawl feature to nutch, waiting for a best way to improve crawl policy.I post two patch.Sorry for my very poor english &amp;#8211;Philippe</description>
      <version>0.7,0.7.1,0.8</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="261" opendate="2006-5-4 00:00:00" fixdate="2006-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi Language Support</summary>
      <description>Add multi-lingual support in Nutch, as described in http://wiki.apache.org/nutch/MultiLingualSupportThe document analysis part is actually implemented, and two analysis plugins (fr and de) are provided for testing (not deployed by default).The query analysis part is missing for a complete multi-lingual support.</description>
      <version>0.6,0.7,0.7.1,0.7.2,0.8</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.pages.sv.search.xml</file>
      <file type="M">src.web.pages.sr.search.xml</file>
      <file type="M">src.web.pages.sh.search.xml</file>
      <file type="M">src.web.pages.pt.search.xml</file>
      <file type="M">src.web.pages.pl.search.xml</file>
      <file type="M">src.web.pages.nl.search.xml</file>
      <file type="M">src.web.pages.ms.search.xml</file>
      <file type="M">src.web.pages.jp.search.xml</file>
      <file type="M">src.web.pages.it.search.xml</file>
      <file type="M">src.web.pages.hu.search.xml</file>
      <file type="M">src.web.pages.fr.search.xml</file>
      <file type="M">src.web.pages.fi.search.xml</file>
      <file type="M">src.web.pages.es.search.xml</file>
      <file type="M">src.web.pages.en.search.xml</file>
      <file type="M">src.web.pages.de.search.xml</file>
      <file type="M">src.web.pages.ca.search.xml</file>
      <file type="M">src.web.jsp.search.jsp</file>
      <file type="M">src.web.jsp.explain.jsp</file>
      <file type="M">src.java.org.apache.nutch.searcher.Query.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.OpenSearchServlet.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysisTokenManager.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysis.jj</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysis.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.AnalyzerFactory.java</file>
      <file type="M">docs.sv.search.html</file>
      <file type="M">docs.sr.search.html</file>
      <file type="M">docs.sh.search.html</file>
      <file type="M">docs.pt.search.html</file>
      <file type="M">docs.pl.search.html</file>
      <file type="M">docs.nl.search.html</file>
      <file type="M">docs.ms.search.html</file>
      <file type="M">docs.jp.search.html</file>
      <file type="M">docs.it.search.html</file>
      <file type="M">docs.hu.search.html</file>
      <file type="M">docs.fr.search.html</file>
      <file type="M">docs.fi.search.html</file>
      <file type="M">docs.es.search.html</file>
      <file type="M">docs.en.search.html</file>
      <file type="M">docs.de.search.html</file>
      <file type="M">docs.ca.search.html</file>
    </fixedFiles>
  </bug>
  <bug id="621" opendate="2008-3-18 00:00:00" fixdate="2008-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch needs to declare it&amp;#39;s crypto usage</summary>
      <description>Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).See TIKA-118.</description>
      <version>0.7,0.7.1,0.7.2,0.8,0.8.1,0.9.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="651" opendate="2008-9-19 00:00:00" fixdate="2008-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove bin/{start|stop}-balancer.sh from svn tracking</summary>
      <description>Files bin/{start|stop}-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version control.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-balancer.sh</file>
      <file type="M">bin.start-balancer.sh</file>
    </fixedFiles>
  </bug>
  <bug id="655" opendate="2008-10-1 00:00:00" fixdate="2008-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Injecting Crawl metadata</summary>
      <description>the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separated by '='. A input line might look like this:http://www.myurl.com \t categ=value1 \t categ2=value2This functionality can be useful to store external knowledge and index it with a custom plugin</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="656" opendate="2008-10-9 00:00:00" fixdate="2008-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DeleteDuplicates based on crawlDB only</summary>
      <description>The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.I was wondering whether we could use the information from the crawlDB instead to detect URLs to delete then do the deletions in an indexer-neutral way. As far as I understand the content of the crawlDB contains all the elements we need for dedup, namely : URL signature fetch time scoreIn map-reduce terms we would have two different jobs : read crawlDB and compare on URLs : keep only most recent element - oldest are stored in a file and will be deleted later read crawlDB and have a map function generating signatures as keys and URL + fetch time +score as value reduce function would depend on which parameter is set (i.e. use signature or score) and would output as list of URLs to deleteThis assumes that we can then use the URLs to identify documents in the indices.Any thoughts on this? Am I missing something?Julien</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.CleaningJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">src.bin.nutch</file>
      <file type="M">src.bin.crawl</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
