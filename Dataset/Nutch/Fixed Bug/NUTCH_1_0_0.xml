<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="497" opendate="2007-6-6 00:00:00" fixdate="2007-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extreme Nested Tags causes StackOverflowException in DomContentUtils...Spider Trap</summary>
      <description>Some webpages have a form of a spider trap that causes a StackOverflowException in DomContentUtils by having nested tags with thousands of layers deep. DomContentUtils when trying to get outlinks uses a recursive method to parse the html. With this type of nesting it errors out.</description>
      <version>0.8.1,0.9.0,1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.fetcher.TestFetcher.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">src.plugin.languageidentifier.src.java.org.apache.nutch.analysis.lang.HTMLLanguageParser.java</file>
    </fixedFiles>
  </bug>
  <bug id="501" opendate="2007-6-18 00:00:00" fixdate="2007-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a different caching mechanism for objects cached in configuration</summary>
      <description>As per HADOOP-1343, Configuration.setObject and Configuration.getObject (which are used by Nutch to cache arbitrary objects) are deprecated and will be removed soon. We have to implement an alternative caching mechanism and replace all usages of Configuration.{getObject,setObject} with the new mechanism.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.protocol.TestProtocolFactory.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.QueryFilters.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilters.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.HtmlParseFilters.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizers.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFilters.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.SignatureFactory.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchScheduleFactory.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.CommonGrams.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.AnalyzerFactory.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="504" opendate="2007-6-22 00:00:00" fixdate="2007-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NUTCH-443 broke parsing during fetching</summary>
      <description>After NUTCH-443, if one is parsing during fetching and parsing for a url fails, that url doesn't get segment name or similar properties in its metadata. Because of this, indexer fails (because, index expects to see segment name for all parses, even those that failed).</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.fetcher.TestFetcher.java</file>
      <file type="M">src.test.crawl-tests.xml</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher2.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="505" opendate="2007-6-23 00:00:00" fixdate="2007-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Outlink urls should be validated</summary>
      <description>See discussion here:http://www.nabble.com/fetching-http%3A--www.variety.com-%3C-div%3E%3C-a%3E-tf3961692.htmlParse plugins may extract garbage urls from pages. We need a url validation system that tests these urls and filters out garbage.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.parse.TestParseData.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipParser.java</file>
      <file type="M">src.plugin.parse-text.src.java.org.apache.nutch.parse.text.TextParser.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.RSSParser.java</file>
      <file type="M">src.plugin.parse-pdf.src.java.org.apache.nutch.parse.pdf.PdfParser.java</file>
      <file type="M">src.plugin.parse-js.src.java.org.apache.nutch.parse.js.JSParseFilter.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.plugin.parse-ext.src.java.org.apache.nutch.parse.ext.ExtParser.java</file>
      <file type="M">src.plugin.lib-parsems.src.java.org.apache.nutch.parse.ms.MSBaseParser.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseStatus.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseImpl.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherOutput.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.net.UrlValidator.java</file>
    </fixedFiles>
  </bug>
  <bug id="515" opendate="2007-7-16 00:00:00" fixdate="2007-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Next fetch time is set incorrectly</summary>
      <description>After NUTCH-61 , db.default.fetch.interval option is deprecated and superceded by db.fetch.interval.default. However, various parts in nutch still use the old option. Since old option is in days (with default being 30) and new option in seconds (default is ~250000), when nutch fetches a url, its next fetch time is set as **30 SECONDS** later. This means that nutch keeps refetching same urls over and over and over and over.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Injector.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="516" opendate="2007-7-17 00:00:00" fixdate="2007-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Next fetch time is not set when it is a CrawlDatum.STATUS_FETCH_GONE</summary>
      <description>We can not crawl some page due to a robots restriction. In this case we update the db with the Metada: pst:robots_denied(18) , we add the status code 3 and we change the fecth interval to 67.5 days.Unfortunetely the Fetch time is never change, so it keeps generating this page and fetching it every time.We should update the schedule fetch in crawldb to reflect to the fetch interval.We should add in crawldbreducer:case CrawlDatum.STATUS_FETCH_GONE: // permanent failure if (old != null) result.setSignature(old.getSignature()); // use old signature result.setStatus(CrawlDatum.STATUS_DB_GONE); result = schedule.setPageGoneSchedule((Text)key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime()); // set the schedule result = schedule.setFetchSchedule((Text)key, result, prevFetchTime, prevModifiedTime, fetch.getFetchTime(), fetch.getModifiedTime(), modified); break;</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.DefaultFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AbstractFetchSchedule.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="517" opendate="2007-7-18 00:00:00" fixdate="2007-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>build encoding should be UTF-8</summary>
      <description>build encoding send to javac should be UTF-8 so that non-ascii characters can be used in the source code. This issue has emerged from NUTCH-439</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">default.properties</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="518" opendate="2007-7-18 00:00:00" fixdate="2007-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix OpicScoringFilter to respect scoring filter chaining</summary>
      <description>Opic Scoring returns the score that it calculates, rather than returning previous_score * calculated_score. This prevents using another scoring filter along with Opic scoring.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.scoring-opic.src.java.org.apache.nutch.scoring.opic.OPICScoringFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="533" opendate="2007-7-30 00:00:00" fixdate="2007-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LinkDbMerger: url normalized is not updated in the key and inlinks list</summary>
      <description>The key url and inlinks url are passed through a normalizer. The url return are not updated and we keep collecting the old key url and inlinks related.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="536" opendate="2007-8-3 00:00:00" fixdate="2007-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce number of warnings in nutch core</summary>
      <description>Nutch core (code under src/java) gives around 600 warnings. Most of them are unused variables/imports and Java5 generics style warnings. This issue is to track changes to reduce number of warnings.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypeException.java</file>
      <file type="M">src.java.org.apache.nutch.util.TrieStringMatcher.java</file>
      <file type="M">src.java.org.apache.nutch.util.ThreadPool.java</file>
      <file type="M">src.java.org.apache.nutch.util.NodeWalker.java</file>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypesReader.java</file>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeTypes.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.analysis.CommonGrams.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysis.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchAnalysisTokenManager.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.NutchDocumentAnalyzer.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.ParseException.java</file>
      <file type="M">src.java.org.apache.nutch.analysis.TokenMgrError.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Inlinks.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDb.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbMerger.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.MapWritable.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.TextProfileSignature.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher2.java</file>
      <file type="M">src.java.org.apache.nutch.html.Entities.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingException.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexingFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexSorter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchSimilarity.java</file>
      <file type="M">src.java.org.apache.nutch.net.protocols.HttpDateFormat.java</file>
      <file type="M">src.java.org.apache.nutch.net.protocols.ProtocolException.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilterException.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLFilters.java</file>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizers.java</file>
      <file type="M">src.java.org.apache.nutch.parse.HtmlParseFilters.java</file>
      <file type="M">src.java.org.apache.nutch.parse.OutlinkExtractor.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseException.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParsePluginList.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParsePluginsReader.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserFactory.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseUtil.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolException.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolNotFound.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.ProtocolStatus.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilter.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilterException.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilters.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.servlet.Cached.java</file>
      <file type="M">src.java.org.apache.nutch.tools.DmozParser.java</file>
      <file type="M">src.java.org.apache.nutch.tools.PruneIndexTool.java</file>
      <file type="M">src.java.org.apache.nutch.util.FibonacciHeap.java</file>
      <file type="M">src.java.org.apache.nutch.util.mime.MimeType.java</file>
    </fixedFiles>
  </bug>
  <bug id="547" opendate="2007-9-3 00:00:00" fixdate="2007-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redirection handling: YahooSlurp&amp;#39;s algorithm</summary>
      <description>After reading Yahoo's algorithm (then one Andrzej linked to:http://help.yahoo.com/l/nz/yahooxtra/search/webcrawler/slurp-11.html )in the redirect/alias handling discussion, I had a bit of a sparetime, so I implemented it.Note that the patch I am attaching is for the 'choosing' algorithm described inYahoo's help page. It makes no attempt to handle aliases in any way. (See http://www.nabble.com/Redirects-and-alias-handling-%28LONG%29-tf4270371.html#a12154362 for the discussion about alias handling).E.g,generate "http://www.milliyet.com.tr/"fetch "http:/www.milliyet.com.tr/" which redirects to"http://www.milliyet.com.tr/2007/08/29/index.html?ver=39".Update second page's datum's metadata to indicate that"http://www.milliyet.com.tr/" is the representative form.Updatedb, invertlinks, etc...While indexing second page, change its "url" field to"http://www.milliyet.com.tr/".</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.HtmlParser.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.util.URLUtil.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseStatus.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.metadata.Nutch.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher2.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="56" opendate="2005-5-3 00:00:00" fixdate="2005-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Crawling sites with 403 Forbidden robots.txt</summary>
      <description>If a 403 error is encountered when trying to access the robots.txt file, Nutch does not crawl any pages from that site. This behavior is consistent with the RFC recommendation for the robot exclusion protocol. However, Google does crawl sites that exhibit this type of behavior, because most webmasters of these sites are unaware of robots.txt conventions and do want their site to be crawled.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.RobotRulesParser.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.RobotRulesParser.java</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="578" opendate="2007-11-20 00:00:00" fixdate="2007-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>URL fetched with 403 is generated over and over again</summary>
      <description>I have not changed the following parameter in the nutch-default.xml:&lt;property&gt; &lt;name&gt;db.fetch.retry.max&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;description&gt;The maximum number of times a url that has encountered recoverable errors is generated for fetch.&lt;/description&gt;&lt;/property&gt;However, there is a URL which is on the site that I'm crawling, www.teachertube.com, which keeps being generated over and over again for almost every segment (many more times than 3):fetch of http://www.teachertube.com/images/ failed with: Http code=403, url=http://www.teachertube.com/images/This is a bug, right?Thanks.</description>
      <version>1.0.0</version>
      <fixedVersion>1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="579" opendate="2007-11-21 00:00:00" fixdate="2007-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Feed plugin only indexes one post per feed due to identical digest</summary>
      <description>When parsing an rss feed, only one post will be indexed per feed. The reason for this is that the digest, which is calculated for based on the content (or the url if the content is null) is always the same for each post in a feed.I noticed this when I was examining my lucene indexes using Luke. All of the individual feed entries were being indexed properly but then when the dedup step ran, my merged index ended up with only one document.As a quick fix, I simply overrode the digest in the FeedIndexingFilter.java, by adding the following code to the filter function:byte[] signature = MD5Hash.digest(url.toString()).getDigest();doc.removeField("digest");doc.add(new Field("digest", StringUtil.toHexString(signature), Field.Store.YES, Field.Index.NO));This seems to fix the issue as the index now contains the proper number of documents.Anyone have any comments on whether this is a good solution or if there is a better solution?</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.MD5Signature.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="603" opendate="2008-2-5 00:00:00" fixdate="2008-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more default url normalizations</summary>
      <description>By default the regex-urlnormalizers only remove PHPSESSID strings. I propose adding in more default url normalizers including expressions for removing different types of session ids, removing default pages, remvoing interpage links, and cleaning up url strings. The point of these expressions is to decrease the number of duplicate urls that are being stored and scored in the crawl database and being fetched.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-regex.src.test.org.apache.nutch.net.urlnormalizer.regex.TestRegexURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-regex.sample.regex-normalize-default.xml</file>
      <file type="M">src.plugin.urlnormalizer-regex.sample.regex-normalize-default.test</file>
      <file type="M">conf.regex-normalize.xml.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="605" opendate="2008-2-8 00:00:00" fixdate="2008-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change deprecated configuration methods for Hadoop</summary>
      <description>Changes use of the now deprecated addFinalResource and addDefaultResource methods to just use addResouce</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.crawl.CrawlDBTestUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.NutchConfiguration.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="608" opendate="2008-2-9 00:00:00" fixdate="2008-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade nutch to use released apache-tika-0.1-incubating</summary>
      <description>This patch will upgrade Nutch to use the released tika-0.1-incubating jar containing stable APIs and code, as opposed to the -dev version of the jar file that's currently in place in SVN.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.parse-zip.src.java.org.apache.nutch.parse.zip.ZipTextExtractor.java</file>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.Content.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserFactory.java</file>
      <file type="M">lib.tika-0.1-dev.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="61" opendate="2005-6-6 00:00:00" fixdate="2005-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adaptive re-fetch interval. Detecting umodified content</summary>
      <description>Currently Nutch doesn't adjust automatically its re-fetch period, no matter if individual pages change seldom or frequently. The goal of these changes is to extend the current codebase to support various possible adjustments to re-fetch times and intervals, and specifically a re-fetch schedule which tries to adapt the period between consecutive fetches to the period of content changes.Also, these patches implement checking if the content has changed since last fetching; protocol plugins are also changed to make use of this information, so that if content is unmodified it doesn't have to be fetched and processed.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.FtpResponse.java</file>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Ftp.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.Indexer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher2.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="616" opendate="2008-2-26 00:00:00" fixdate="2008-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reset Fetch Retry counter when fetch is successful</summary>
      <description>We manage a counter to check how many time the URL has been consecutively in state Retry following some trouble to get the page.Here is a sample of the code:case ProtocolStatus.RETRY: // retry fit.datum.setRetriesSinceFetch(fit.datum.getRetriesSinceFetch()+1); However i notice that we don't reinitialize this counter at 0 in the case of successful fetch.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher2.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DefaultFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AdaptiveFetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AbstractFetchSchedule.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="627" opendate="2008-4-10 00:00:00" fixdate="2008-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimize host address lookup</summary>
      <description>The simple patch that I'm about to attach keeps track of hosts whose "max URLs per host" limit we already reached, as well as hosts whose hostname-&gt;IP lookup already failed. For such hosts, further DNS lookups are skipped: there is no point in looking up a hostname yet again if we already have the max number of URLs for that host there is little point in attempting to look up a hostname yet again if the previous lookup already failedIn a simple test, this saved a few hundred thousand lookups for the first case and a few hundred lookups for the second case.If nobody complains, I'll commit by the end of the week.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="63" opendate="2005-6-18 00:00:00" fixdate="2005-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>the distributed search client generate too much logging statements</summary>
      <description>For each query (depending on the number of segments) tooo many logging statements are generated.After a short timethis generates gigs of log files. This logging should change to debug: LOG.info("Client: segment "segments&amp;#91;j&amp;#93;" at "+addr);</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSearch.java</file>
    </fixedFiles>
  </bug>
  <bug id="635" opendate="2008-6-12 00:00:00" fixdate="2008-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LinkAnalysis Tool for Nutch</summary>
      <description>This is a basic pagerank type link analysis tool for nutch which simulates a sparse matrix using inlinks and outlinks and converges after a given number of iterations. This tool is mean to replace the current scoring system in nutch with a system that converges instead of exponentially increasing scores. Also includes a tool to create an outlinkdb.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.TestURLUtil.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.java.org.apache.nutch.util.URLUtil.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.ScoringFilters.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="642" opendate="2008-8-6 00:00:00" fixdate="2008-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit tests fail when run in non-local mode</summary>
      <description>Unit tests work correctly only when run in Hadoop "local" mode. In distributed mode the classpath that JUnit uses doesn't contain the job jar, so Hadoop doesn't know where to find the implementing classes, and consequently all map-reduce jobs fail.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="645" opendate="2008-8-19 00:00:00" fixdate="2008-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse-swf unit test failing</summary>
      <description>Parse-swf unit tests fail under Java 1.6, but run successfuly under Java 1.5</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-swf.src.java.org.apache.nutch.parse.swf.SWFParser.java</file>
      <file type="M">src.plugin.parse-swf.sample.test2.txt</file>
      <file type="M">src.plugin.parse-swf.sample.test1.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="662" opendate="2008-11-21 00:00:00" fixdate="2008-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Nutch to use Lucene 2.4</summary>
      <description>Upgrade nutch to use Lucene 2.4. This release changes the lucene file format. New indexes created by this lucene version will NOT be readable by older versions. Lucene 2.4 can read and update older index formats although updating an older format will convert it to the new format. There are also some performance and functionality improvments.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-lucene-analyzers.plugin.xml</file>
      <file type="M">src.plugin.lib-lucene-analyzers.lib.lucene-analyzers-2.3.0.jar</file>
      <file type="M">src.java.org.apache.nutch.indexer.FsDirectory.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.DeleteDuplicates.java</file>
      <file type="M">lib.lucene-misc-2.3.0.jar</file>
      <file type="M">lib.lucene-core-2.3.0.jar</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.indexer.TestDeleteDuplicates.java</file>
    </fixedFiles>
  </bug>
  <bug id="665" opendate="2008-11-26 00:00:00" fixdate="2008-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Search Load Testing Tool</summary>
      <description>A tool which spawn a number of threads and executes searches against configured search servers. This is used for light load testing of search servers.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="667" opendate="2008-11-26 00:00:00" fixdate="2008-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Input Format for working with Content in Hadoop Streaming</summary>
      <description>This is a ContextAsText input format that removes line endings with spaces that allow Nutch content to be used more effectively inside of Hadoop streaming jobs that allow MapReduce jobs to be written in any language that can communicate with stdin and stdout.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="668" opendate="2008-12-3 00:00:00" fixdate="2008-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Domain URL Filter</summary>
      <description>A URLFilter that adds the ability to filter out URLs by top level domain or by hostname. A configuration file with a listing of URLs is used to denote accepted urls.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="68" opendate="2005-7-5 00:00:00" fixdate="2005-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A tool to generate arbitrary fetchlists</summary>
      <description>This is a tool to generate arbitrary fetchlists out of plain-text URL lists. I found it useful quite often, e.g. when I had to fetch certain specific pages without adding them to DB, or for testing purposes.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="684" opendate="2009-1-30 00:00:00" fixdate="2009-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dedup support for Solr</summary>
      <description>After NUTCH-442, nutch now can index to both solr and lucene. However, duplicate deletion feature (based on digests) is only available in lucene. It should also be available for solr.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrConstants.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.nutch</file>
    </fixedFiles>
  </bug>
  <bug id="687" opendate="2009-2-17 00:00:00" fixdate="2009-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RAT</summary>
      <description>Add apache rat so we can easily see the situation with required headers</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="688" opendate="2009-2-17 00:00:00" fixdate="2009-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing/wrong headers in source files</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.searcher.SearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.util.FSUtils.java</file>
      <file type="M">src.java.org.apache.nutch.util.EncodingDetector.java</file>
      <file type="M">src.java.org.apache.nutch.tools.SearchLoadTester.java</file>
      <file type="M">src.java.org.apache.nutch.tools.ResolveUrls.java</file>
      <file type="M">src.java.org.apache.nutch.tools.compat.ReprUrlFixer.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentReader.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentPart.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">src.java.org.apache.nutch.segment.ContentAsTextInputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.Summary.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.SolrSearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.SegmentBean.java</file>
      <file type="M">src.java.org.apache.nutch.util.GenericWritableConfigurable.java</file>
      <file type="M">src.java.org.apache.nutch.util.NodeWalker.java</file>
      <file type="M">src.plugin.field-basic.src.java.org.apache.nutch.indexer.field.basic.BasicFieldFilter.java</file>
      <file type="M">src.plugin.field-boost.src.java.org.apache.nutch.indexer.field.boost.BoostFieldFilter.java</file>
      <file type="M">src.plugin.response-json.src.java.org.apache.nutch.searcher.response.json.JSONResponseWriter.java</file>
      <file type="M">src.plugin.response-xml.src.java.org.apache.nutch.searcher.response.xml.XMLResponseWriter.java</file>
      <file type="M">src.plugin.scoring-link.src.java.org.apache.nutch.scoring.link.LinkAnalysisScoringFilter.java</file>
      <file type="M">src.plugin.urlfilter-domain.src.java.org.apache.nutch.urlfilter.domain.DomainURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domain.src.test.org.apache.nutch.urlfilter.domain.TestDomainURLFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.NutchWritable.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.AnchorFields.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.BasicFields.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.CustomFields.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldFilter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldFilters.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.Fields.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldsWritable.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldType.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.field.FieldWritable.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.lucene.LuceneConstants.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.lucene.LuceneWriter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchDocument.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchIndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchIndexWriterFactory.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchSimilarity.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrConstants.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LinkDatum.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LinkDumper.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LinkRank.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.LoopReader.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.Loops.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.Node.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.NodeDumper.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.NodeReader.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.ScoreUpdater.java</file>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.WebGraph.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSegmentBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.response.RequestUtils.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.response.ResponseWriter.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.response.ResponseWriters.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.response.SearchResults.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.response.SearchServlet.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.RPCSearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.RPCSegmentBean.java</file>
    </fixedFiles>
  </bug>
  <bug id="691" opendate="2009-2-18 00:00:00" fixdate="2009-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jakarta poi jars to the most relevant version</summary>
      <description>Update jakarta poi jars to the most relevant version closes bug NUTCH-591.</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-msword.src.test.org.apache.nutch.parse.msword.TestMSWordParser.java</file>
      <file type="M">src.plugin.parse-msword.src.java.org.apache.nutch.parse.msword.WordExtractor.java</file>
      <file type="M">src.plugin.parse-msword.src.java.org.apache.nutch.parse.msword.Word6Extractor.java</file>
      <file type="M">src.plugin.parse-msword.src.java.org.apache.nutch.parse.msword.chp.Word6CHPBinTable.java</file>
      <file type="M">src.plugin.parse-msword.build.xml</file>
      <file type="M">src.plugin.lib-jakarta-poi.plugin.xml</file>
      <file type="M">src.plugin.lib-jakarta-poi.lib.poi-scratchpad-3.0-alpha1-20050704.jar</file>
      <file type="M">src.plugin.lib-jakarta-poi.lib.poi-3.0-alpha1-20050704.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="697" opendate="2009-2-20 00:00:00" fixdate="2009-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate log output for solr indexer and dedup</summary>
      <description></description>
      <version>1.0.0</version>
      <fixedVersion>1.2,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="698" opendate="2009-2-20 00:00:00" fixdate="2009-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CrawlDb is corrupted after a few crawl cycles</summary>
      <description>After change to hadoop's MapWritable, crawldb becomes corrupted after some fetch cycles. For more details see this discussion thread:http://www.nabble.com/Fetcher2-crashes-with-current-trunk-td21978049.html</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="699" opendate="2009-2-20 00:00:00" fixdate="2009-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an "official" solr schema for solr integration</summary>
      <description>See Andrzej's comments on NUTCH-684 for more info.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="703" opendate="2009-2-25 00:00:00" fixdate="2009-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 0.19.1</summary>
      <description>From release notes: "Release 0.19.1 fixes many critical bugs in 0.19.0, including **some data loss issues**.".</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.native.Linux-i386-32.libhadoop.a</file>
      <file type="M">lib.native.Linux-amd64-64.libhadoop.a</file>
      <file type="M">lib.hadoop-0.19.0-core.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="714" opendate="2009-3-10 00:00:00" fixdate="2009-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a SFTP and SCP Protocol Handler</summary>
      <description>An SFTP and SCP Protocol handler is needed to fetch intranet content on an SFTP or SCP server.</description>
      <version>1.0.0</version>
      <fixedVersion>nutchgora</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="716" opendate="2009-3-10 00:00:00" fixdate="2009-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make subcollection index filed multivalued</summary>
      <description>Looks like a reasonable thing to do. Marking as 1.2 and will commit if no one objects</description>
      <version>1.0.0</version>
      <fixedVersion>1.2,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.schema.xml</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="727" opendate="2009-3-19 00:00:00" fixdate="2009-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add KEYS file to release artifact</summary>
      <description>comment from Grant:&gt;&gt; Where's the KEYS file for Nutch?&gt;&gt; hi,&gt;&gt; the keys file is at the top level nutch directory (eg: http://www.nic.funet.fi/pub/mirrors/apache.org/lucene/nutch/KEYS)OK, I think it should be in the tarball, too., at the top</description>
      <version>1.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="732" opendate="2009-4-7 00:00:00" fixdate="2009-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Subcollection plugin not working on Nutch-1.0</summary>
      <description>I am trying to get subcollections working, using Nutch-1.0 !I configured subcolections.xml then I added the plugin on nutch-site.xml.When the index finishes, I opened lucene luke to check if the database was working properly.The field subcollection is populated as it should, but searching for any subcollection, on the search tab of luke, returns no results.If I do a search on the url field, I can see that every record has a subcollection associated, yet i can't search for using the subcollection field.search examples on luke:subcollection:sub1 -&gt; no resultsurl:sub1 -&gt; results with field subcollection populated -&gt; sub1Same results using:./bin/nutch org.apache.nutch.searcher.NutchBean "subcollection:sub1 sub"If i use the "explain", subcollection field is there with the correct word.It makes no sense so i beleive it's a bug.</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.java</file>
      <file type="M">src.plugin.subcollection.src.java.org.apache.nutch.collection.CollectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="739" opendate="2009-5-28 00:00:00" fixdate="2009-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SolrDeleteDuplications too slow when using hadoop</summary>
      <description>in my environment i always have many warnings like this on the dedup stepTask attempt_200905270022_0212_r_000003_0 failed to report status for 600 seconds. Killing!solr logs:INFO: [] webapp=/solr path=/update params={wt=javabin&amp;waitFlush=true&amp;optimize=true&amp;waitSearcher=true&amp;maxSegments=1&amp;version=2.2} status=0 QTime=173741May 27, 2009 10:29:27 AM org.apache.solr.update.processor.LogUpdateProcessor finishINFO: {optimize=} 0 173599May 27, 2009 10:29:27 AM org.apache.solr.core.SolrCore executeINFO: [] webapp=/solr path=/update params={wt=javabin&amp;waitFlush=true&amp;optimize=true&amp;waitSearcher=true&amp;maxSegments=1&amp;version=2.2} status=0 QTime=173599May 27, 2009 10:29:27 AM org.apache.solr.search.SolrIndexSearcher closeINFO: Closing Searcher@2ad9ac58 mainMay 27, 2009 10:29:27 AM org.apache.solr.core.JmxMonitoredMap$SolrDynamicMBean getMBeanInfoWARNING: Could not getStatistics on info bean org.apache.solr.search.SolrIndexSearcherorg.apache.lucene.store.AlreadyClosedException: this IndexReader is closed....So I think the problem in the piece of code on line 301 of SolrDeleteDuplications ( solr.optimize() ). Because we have few job tasks each of ones tries to optimize solr indexes before closing.The simplest way to avoid this bug - removing this line and sending "&lt;optimize/&gt;" message directly to solr server after dedup step</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrDeleteDuplicates.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="740" opendate="2009-5-28 00:00:00" fixdate="2009-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration option to override default language for fetched pages.</summary>
      <description>By default "Accept-Language" HTTP request header is set to English. Unfortunately this value is hard coded and seems there is no way to override it. As a result you may index English version of pages even though you would prefer it in different language.</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.Http.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="741" opendate="2009-5-29 00:00:00" fixdate="2009-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Job file includes multiple copies of nutch config files.</summary>
      <description>From a clean checkout, running "ant tar" will create a .job file. The .job file includes two copies of the nutch-site.xml and nutch-default.xml file.</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="743" opendate="2009-6-23 00:00:00" fixdate="2009-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Site search powered by Lucene/Solr</summary>
      <description>Replace current Nutch site search with Lucene/Solr powered search hosted by Lucid Imagination (http://www.lucidimagination.com/search). It allows one to search all of the Nutch (content from other parts of the Lucene ecosystem is also available) content from a single place, including web, wiki, JIRA and mail archives. Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. A preview of the site with the new search enabled is available at http://people.apache.org/~siren/site/</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.src.documentation.skinconf.xml</file>
      <file type="M">src.site.forrest.properties</file>
      <file type="M">site.version.control.html</file>
      <file type="M">site.tutorial8.html</file>
      <file type="M">site.tutorial.html</file>
      <file type="M">site.nightly.html</file>
      <file type="M">site.mailing.lists.html</file>
      <file type="M">site.linkmap.html</file>
      <file type="M">site.issue.tracking.html</file>
      <file type="M">site.index.html</file>
      <file type="M">site.i18n.html</file>
      <file type="M">site.credits.html</file>
      <file type="M">site.bot.html</file>
      <file type="M">site.about.html</file>
    </fixedFiles>
  </bug>
  <bug id="753" opendate="2009-9-7 00:00:00" fixdate="2009-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent new Fetcher to retrieve the robots twice</summary>
      <description>The new Fetcher which is now used by default handles the robots file directly instead of relying on the protocol. The options Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS are set to false to prevent fetching the robots.txt twice (in Fetcher + in protocol), which avoids calling robots.isAllowed. However in practice the robots file is still fetched as there is a call to robots.getCrawlDelay() a bit further which is not covered by the if (Protocol.CHECK_ROBOTS).</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="754" opendate="2009-9-16 00:00:00" fixdate="2009-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use GenericOptionsParser instead of FileSystem.parseArgs()</summary>
      <description>FileSystem.parseArgs() should be replaced with GenericOptionsParser. Doing this allows to compile Nutch with the 0.20.* branch of Hadoop</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.TestNodeWalker.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.Content.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseText.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParseData.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="758" opendate="2009-9-30 00:00:00" fixdate="2009-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set subversion eol-style to "native"</summary>
      <description>It would be really nice to set the subversion eol-style (end-of-line style) to "native" - makes it much easier for different contributors on different OS's to contribute patches.</description>
      <version>1.0.0</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.pages.th.search.xml</file>
      <file type="M">src.web.pages.sr.search.xml</file>
      <file type="M">src.web.pages.sh.search.xml</file>
      <file type="M">src.web.pages.pl.search.xml</file>
      <file type="M">src.web.pages.nl.search.xml</file>
      <file type="M">src.web.pages.es.search.xml</file>
      <file type="M">src.web.pages.en.search.xml</file>
      <file type="M">src.web.pages.ca.search.xml</file>
      <file type="M">src.web.pages.ca.help.xml</file>
      <file type="M">src.web.pages.ca.about.xml</file>
      <file type="M">src.web.include.ca.header.xml</file>
      <file type="M">src.plugin.urlfilter-prefix.src.java.org.apache.nutch.urlfilter.prefix.PrefixURLFilter.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.XMLCharacterRecognizer.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMBuilder.java</file>
      <file type="M">src.plugin.ontology.lib.jena-2.1.LICENSE.txt</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.NutchInputComponent.java</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.NutchDocument.java</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.HitsClusterAdapter.java</file>
      <file type="M">src.plugin.clustering-carrot2.src.java.org.apache.nutch.clustering.carrot2.Clusterer.java</file>
      <file type="M">src.plugin.clustering-carrot2.readme.txt</file>
      <file type="M">src.plugin.clustering-carrot2.plugin.xml</file>
      <file type="M">src.java.org.apache.nutch.searcher.NutchBean.java</file>
      <file type="M">src.java.org.apache.nutch.ontology.OntologyFactory.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClustererFactory.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.OnlineClusterer.java</file>
      <file type="M">src.java.org.apache.nutch.clustering.HitsCluster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="760" opendate="2009-10-15 00:00:00" fixdate="2009-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow field mapping from nutch to solr index</summary>
      <description>I am using nutch to crawl sites and have combined itwith solr pushing the nutch index using the solrindex command. I haveset it up as specified on the wiki using the copyField url to id in theschema. Whilst this works fine it is stuff's up my inputs from othersources in solr (e.g. using the solr data import handler) as they haveboth id's and url's. I have patch that implements a nutch xml schemadefining what basic nutch fields map to in your solr push.</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.searcher.SolrSearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="767" opendate="2009-11-18 00:00:00" fixdate="2009-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Tika to v0.5 for the MimeType detection</summary>
      <description>The version 0.5 of TIka requires a few changes to the MimeType implementation. Tika is now split in several jars, we need to place the tika-core.jar in the main nutch lib.</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.tika-0.1-incubating.jar</file>
      <file type="M">conf.tika-mimetypes.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.protocol.TestContent.java</file>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="779" opendate="2010-1-18 00:00:00" fixdate="2010-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mechanism for passing metadata from parse to crawldb</summary>
      <description>The patch attached allows to pass parse metadata to the corresponding entry of the crawldb. Comments are welcome</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDatum.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="78" opendate="2005-8-6 00:00:00" fixdate="2005-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>German texts on website</summary>
      <description>The German properties-files with the texts to present on the websites were incomplete, or with wrong spellings.Please find attached the corrected files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.locale.org.nutch.jsp.search.de.properties</file>
    </fixedFiles>
  </bug>
  <bug id="782" opendate="2010-2-1 00:00:00" fixdate="2010-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to order htmlparsefilters</summary>
      <description>Patch which adds a new parameter 'htmlparsefilter.order' which specifies the order in which HTMLParse filters are applied. HTMLParse filter ordering MAY have an impact on end result, as some filters could rely on the metadata generated by a previous filter.</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.HtmlParseFilters.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="796" opendate="2010-2-20 00:00:00" fixdate="2010-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Zero results problems difficult to troubleshoot due to lack of logging</summary>
      <description>There are a few places where search can fail in a distributed environment, but when configuration is not quite right, there are no indications of errors or logging.Increased logging of failures would help troubleshoot such problems, as well as lower the "I get 0 results, why?" questions that come across the mailing lists. Areas where logging would be helpful:search app cannot locate search-servers.txtsearch app cannot find searcher node listed in search-server.txtsearch app cannot connect to port on searcher specified in search-server.txtsearcher (bin/nutch server...) cannot find indexsearcher cannot find segmentsAccess denied in any of the above scenarios.There are probably more that would be helpful, but I am not yet familiar to know all the points of possible failure between the webpage and a search node.</description>
      <version>1.0.0,1.1</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.searcher.LuceneSearchBean.java</file>
      <file type="M">src.java.org.apache.nutch.searcher.DistributedSearchBean.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="824" opendate="2010-5-20 00:00:00" fixdate="2010-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.</summary>
      <description>Hello,I am performing a local file system crawling.My problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.For example, I will see the following error:fetching file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.htmlorg.apache.nutch.protocol.file.FileError: File Error: 404 at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:92) at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:535)fetch of file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html failed with: org.apache.nutch.protocol.file.FileError: File Error: 404I am using nutch-1.0.Among other standard settings, I configured nutch-site.conf as follows:&lt;property&gt; &lt;name&gt;plugin.includes&lt;/name&gt; &lt;value&gt;protocol-file|protocol-http|urlfilter-regex|parse-(text|html|js|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt; &lt;description&gt;Regular expression naming plugin directory names to include. Any plugin not matching this expression is excluded. In any case you need at least include the nutch-extensionpoints plugin. By default Nutch includes crawling just HTML and plain text via HTTP, and basic indexing and search plugins. In order to use HTTPS please enable protocol-httpclient, but be aware of possible intermittent problems with the underlying commons-httpclient library. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;file.content.limit&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;Moreover, crawl-urlfilter.txt looks like: skip http:, ftp:, &amp; mailto: urls-^(http|ftp|mailto): skip image and other suffixes we can't yet parse-\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$ skip URLs containing certain characters as probable queries, etc.-&amp;#91;?*!@=&amp;#93; skip URLs with slash-delimited segment that repeats 3+ times, to break loops-.*(/&amp;#91;^/&amp;#93;)/&amp;#91;^/&amp;#93;\1/&amp;#91;^/&amp;#93;+\1/ accept hosts in MY.DOMAIN.NAME#+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/ accept everything else+.*~ &amp;#8212;Thanks,Michela</description>
      <version>1.0.0,1.2,1.3,nutchgora</version>
      <fixedVersion>1.0.0,1.3,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.test.org.apache.nutch.protocol.file.testprotocolfile.txt</file>
      <file type="M">src.plugin.protocol-file.build.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.plugin.protocol-file.src.test.org.apache.nutch.protocol.file.TestProtocolFile.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.sample.testprotocolfile.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="825" opendate="2010-5-21 00:00:00" fixdate="2010-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish nutch artifacts to central maven repository</summary>
      <description>As per the discussion at NUTCH-821, publishing nutch artifacts to maven will be nice. NUTCH-821 already introduces dependency management with ivy. As for the remaining, ant task for generating pom files should be developed, and artifacts should be published to maven repo by a committer after a release.</description>
      <version>None</version>
      <fixedVersion>nutchgora</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="826" opendate="2010-5-24 00:00:00" fixdate="2010-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mailing list is broken.</summary>
      <description>All of the following addresses are failing:nutch-user@nutch.apache.orgnutch-user-subscribe@nutch.apache.orgnutch-user-subscribe@lucene.apache.orgFor the last one, the mailer daemon said "This mailing list has moved to user at nutch.apache.org."Below is the message I tried to send:Hi people,I've been banging my head against this problem for two days now.Simply, I want to add a field with the value of a given meta tag.I've been trying the parse-xml plugin, but that seems that it doesn'twork with version 1.0. I've tried the code athttp://sujitpal.blogspot.com/2009/07/nutch-getting-my-feet-wet.htmland it hasn't worked. I don't even know why. I don't even know if myplugin is being used... or even looked for! Nutch seems to have ainfuriating "Fail silently" policy for plugins. I put aSystem.exit(1) in my filters just to see if my code is even beingencountered. It has not in spite of my config telling it to.Here's my config:nutch-site.xml...&lt;property&gt; &lt;name&gt;plugin.includes&lt;/name&gt; &lt;value&gt;protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|metadata&lt;/value&gt;&lt;/property&gt;...parse-plugins.xml...&lt;mimeType name="application/xhtml+xml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/html"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/sgml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/xml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="parse-rss" /&gt; &lt;plugin id="metadata" /&gt; &lt;plugin id="feed" /&gt;&lt;/mimeType&gt;...&lt;alias name="metadata"extension-id="com.example.website.nutch.parsing.MetaTagExtractorParseFilter"/&gt;...I've also copied the plugin.xml and jar from my build/metadata to theplugins root dir.Nonetheless, Nutch runs and puts data in solr for me. Afaik, Nutch iscompletely unaware of my plugin despite my config options. Is thesome other place I need to tell Nutch to use my plugin? Is there someother approach to do this without having to write a plugin? This doesseem like a lot of work to simply get a meta tag into a field. Anyhelp would be appreciated.Sincerely,John Sherwood</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.src.documentation.content.xdocs.version.control.xml</file>
      <file type="M">src.site.src.documentation.content.xdocs.mailing.lists.xml</file>
      <file type="M">site.version.control.pdf</file>
      <file type="M">site.version.control.html</file>
      <file type="M">site.mailing.lists.pdf</file>
      <file type="M">site.mailing.lists.html</file>
    </fixedFiles>
  </bug>
  <bug id="905" opendate="2010-9-11 00:00:00" fixdate="2010-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable file protocol parent directory crawling</summary>
      <description>See the discussion on NUTCH-407: apply the patch and backport to 1.2 and port to 2.0.</description>
      <version>1.0.0,1.1</version>
      <fixedVersion>1.2,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
