<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1010" opendate="2011-6-22 00:00:00" fixdate="2011-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ContentLength not trimmed</summary>
      <description>Somewhere in some component the ContentLength field is not trimmed. This allows a seemingly numeric field to be treated as a string by the indexer in cases one or more leading or trailing whitespace is added. The result is a hard to debug exception with no way to identify the bad document (amongst thousands) or the bad field.Jun 22, 2011 1:03:42 PM org.apache.solr.common.SolrException logSEVERE: java.lang.NumberFormatException: For input string: "32717 " at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48) at java.lang.Long.parseLong(Long.java:419) at java.lang.Long.parseLong(Long.java:468)This can be quickly fixed in the index-more plugin by simply using the trim() when adding the field.</description>
      <version>1.3,1.4</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1011" opendate="2011-6-23 00:00:00" fixdate="2011-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize duplicate slashes in URL&amp;#39;s</summary>
      <description>Many websites produce faulty URL's with multiple slashes e.g. http://cocoon.apache.org///////////////////////1.x/dynamic.htmlThis can be really nasty if the number of slashes varies, resulting in many URL's actually pointing to the same page and generating new (unique) URL's to the same or other duplicate pages.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.net.TestURLNormalizers.java</file>
      <file type="M">conf.regex-normalize.xml.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1022" opendate="2011-6-28 00:00:00" fixdate="2011-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version number of Nutch agent in conf</summary>
      <description></description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1031" opendate="2011-7-6 00:00:00" fixdate="2011-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delegate parsing of robots.txt to crawler-commons</summary>
      <description>We're about to release the first version of Crawler-Commons http://code.google.com/p/crawler-commons/ which contains a parser for robots.txt files. This parser should also be better than the one we currently have in Nutch. I will delegate this functionality to CC as soon as it is available publicly</description>
      <version>None</version>
      <fixedVersion>1.7,2.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-ftp.src.java.org.apache.nutch.protocol.ftp.Ftp.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.File.java</file>
      <file type="M">src.plugin.lib-http.src.test.org.apache.nutch.protocol.http.api.TestRobotRulesParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.RobotRulesParser.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.Protocol.java</file>
      <file type="M">src.java.org.apache.nutch.protocol.EmptyRobotRules.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1037" opendate="2011-7-9 00:00:00" fixdate="2011-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deduplicate anchors before indexing</summary>
      <description>Anchors are not deduplicated before indexing. This can result in a very high number of similar and identical anchors being indexed. Before indexing, anchors must be deduplicated at least on case.Use anchorIndexingFilter.deduplicate=true to deduplicate anchors case-insensitive.</description>
      <version>None</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-anchor.src.java.org.apache.nutch.indexer.anchor.AnchorIndexingFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1043" opendate="2011-7-12 00:00:00" fixdate="2011-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pattern for filtering .js in default url filters</summary>
      <description>The Javascript parser is not used by default as it is extremely noisy, however the default URL filters do not filter out URLs ending in .js and the default parser (Tika) can't parse them. In a nutshell we are fetching URLS that we know can't be parsed.I suggest that we add a regex to the default URL filters. If people are interested in fetching and parsing .js files they can activate the plugin in their conf and remove the regex in the URL filters.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.regex-urlfilter.txt.template</file>
      <file type="M">conf.automaton-urlfilter.txt.template</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1053" opendate="2011-7-15 00:00:00" fixdate="2011-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parsing of RSS feeds fails</summary>
      <description>See discussion on http://lucene.472066.n3.nabble.com/RSS-feed-parsing-on-Nutch-1-3-td3166487.htmlHave been able to reproduce the problem and will look into it</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.feed.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1054" opendate="2011-7-15 00:00:00" fixdate="2011-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make linkDB optional during indexing</summary>
      <description>Having a linkDB is currently mandatory for indexing, however not all users are interested in using the anchors. The linkDB should be optional while indexing</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1057" opendate="2011-7-16 00:00:00" fixdate="2011-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make fetcher thread time out configurable</summary>
      <description>The fetcher sets a time out value based of half the mapred.task.timeout value. This is not a proper value for all cases. Add an option (fetcher.thread.timeout.divisor) to configure the divisor used and default it to two.</description>
      <version>None</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1067" opendate="2011-7-22 00:00:00" fixdate="2011-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure minimum throughput for fetcher</summary>
      <description>Large fetches can contain a lot of url's for the same domain. These can be very slow to crawl due to politeness from robots.txt, e.g. 10s per url. If all other url's have been fetched, these queue's can stall the entire fetcher, 60 url's can then take 10 minutes or even more. This can usually be dealt with using the time bomb but the time bomb value is hard to determine.This patch adds a fetcher.throughput.threshold setting meaning the minimum number of pages per second before the fetcher gives up. It doesn't use the global number of pages / running time but records the actual pages processed in the previous second. This value is compared with the configured threshold.Besides the check the fetcher's status is also updated with the actual number of pages per second and bytes per second.</description>
      <version>None</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">src.test.org.apache.nutch.fetcher.TestFetcher.java</file>
      <file type="M">src.java.org.apache.nutch.tools.Benchmark.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1069" opendate="2011-7-25 00:00:00" fixdate="2011-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Readlinkdb broken on Hadoop &gt; 0.20</summary>
      <description>reading the linkdb doesn't work on Hadoop 0.20+. It believes data is to be read from the _SUCCESS file that is written by newer Hadoop version.Quick fix is to remove the _SUCCESS file</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.LinkDbMerger.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1071" opendate="2011-7-28 00:00:00" fixdate="2011-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Crawldb update to total counts per status</summary>
      <description>The reduce phase of the crawldb update outputs all the entries that will be found in the updated crawldb. We can use the counters to summarise the number of URLs per status, which is a bit like the readdb -stats functionality except that it does not require an additional step. This is a useful way of monitoring the progress of a crawl using the Hadoop JobTracker UI.</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReducer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1087" opendate="2011-8-23 00:00:00" fixdate="2011-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate crawl command and replace with example script</summary>
      <description>remove the crawl command add basic crawl shell scriptSee thread:http://www.mail-archive.com/dev@nutch.apache.org/msg03848.html</description>
      <version>1.4</version>
      <fixedVersion>1.6,2.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.bin.nutch</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1101" opendate="2011-8-31 00:00:00" fixdate="2011-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Options to purge db_gone records in updatedb</summary>
      <description>Add option to updatedb to filter out records with status db_gone (http 404). This is especially useful in cases where a crawl db is targetted at only a specific site. If the site, for some reason, suddenly changes a lot of url's we'll get a crawl db filled with garbage. Since the targetted site is known (or controlled) it is safe to get rid of all these url's: reduce db size, reduce useless http requests.</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbFilter.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1105" opendate="2011-9-7 00:00:00" fixdate="2011-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MaxContentLength option for index-basic</summary>
      <description>Like the limit for title, the basic indexing filter should have an optional setting to limit and truncate the content length.</description>
      <version>1.3,1.4,nutchgora</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1117" opendate="2011-9-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for index-anchor</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-anchor.src.java.org.apache.nutch.indexer.anchor.AnchorIndexingFilter.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1118" opendate="2011-9-24 00:00:00" fixdate="2011-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for index-basic</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1119" opendate="2011-9-24 00:00:00" fixdate="2011-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for index-static</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-static.src.java.org.apache.nutch.indexer.staticfield.StaticFieldIndexer.java</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1125" opendate="2011-9-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for tld</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>2.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.tools.CrawlDBScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1126" opendate="2011-9-24 00:00:00" fixdate="2011-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for urlfilter-prefix</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.8,2.2.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1127" opendate="2011-9-24 00:00:00" fixdate="2011-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JUnit test for urlfilter-validator</summary>
      <description>This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1139" opendate="2011-9-30 00:00:00" fixdate="2011-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexer to delete documents</summary>
      <description>Add an option -delete to the solrindex command. With this feature enabled documents of the currently processing segment with status FETCH_GONE or FETCH_REDIR_PERM are deleted, a following SolrClean is not required anymore.This issue is a follow up of NUTCH-1052.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.NutchIndexWriter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1154" opendate="2011-10-7 00:00:00" fixdate="2011-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Tika 0.10</summary>
      <description>There have been significant improvements in Tika 0.10 and it would be nice to use the latest Tika in 1.4.</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.test.org.apache.nutch.tika.TestRTFParser.java</file>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1176" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Abandoned">
    <buginformation>
      <summary>Fix all javadoc warnings from nightly builds</summary>
      <description>The warnings can clearly be seen from the javadoc target (near bottom) of any successful nightly build. An example is provided below.https://builds.apache.org/job/nutch-trunk/1638/console</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseResult.java</file>
      <file type="M">src.plugin.lib-regex-filter.src.java.org.apache.nutch.urlfilter.api.RegexURLFilterBase.java</file>
      <file type="M">src.java.org.apache.nutch.util.NodeWalker.java</file>
      <file type="M">src.java.org.apache.nutch.util.MimeUtil.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.TopLevelDomain.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainSuffixes.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainSuffix.java</file>
      <file type="M">src.java.org.apache.nutch.tools.arc.ArcRecordReader.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrDeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.FetchSchedule.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AbstractFetchSchedule.java</file>
    </fixedFiles>
  </bug>
  <bug id="1182" opendate="2011-10-26 00:00:00" fixdate="2011-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetcher to log hung threads</summary>
      <description>While crawling a slow server with a couple of very large PDF documents (30 MB) on itafter some time and a bulk of successfully fetched documents the fetcher stopswith the message: Aborting with 10 hung threads.From now on every cycle ends with hung threads, almost no documents are fetchedsuccessfully. In addition, strange hadoop errors are logged: fetch of http://.../xyz.pdf failed with: java.lang.NullPointerException at java.lang.System.arraycopy(Native Method) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1108) ...or Exception in thread "QueueFeeder" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)I've run the debugger and found: after the "hung threads" are reported the fetcher stops but the threads are still alive and continue fetching a document. In consequence, this will limit the small bandwidth of network/server even more after the document is fetched the thread tries to write the content via output.collect() which must fail because the fetcher map job is already finished and the associated temporary mapred directory is deleted. The error message may get mixed with the progress output of the next fetch cycle causing additional confusion. documents/URLs causing the hung thread are never reported nor stored. That is, it's hard to track them down, and they will cause a hung thread again and again.The problem is reproducible when fetching bigger documents and setting mapred.task.timeout to a low value (this will definitely cause hung threads).</description>
      <version>1.3,1.4</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1192" opendate="2011-11-2 00:00:00" fixdate="2011-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add &amp;#39;/runtime&amp;#39; to svn ignore</summary>
      <description>Add '/runtime' to svn ignore. The .gitignore file already has the fix.</description>
      <version>None</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1208" opendate="2011-11-22 00:00:00" fixdate="2011-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t include KEYS file in bin distribution</summary>
      <description>We should get rid of the KEYS file in the bin packaging (zip/tar) in 1.5.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1214" opendate="2011-11-29 00:00:00" fixdate="2011-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DomainStats tool should be named for what it&amp;#39;s doing</summary>
      <description>DomainStats tool can calculate on host, domain and suffix. The job name should reflect these types.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1215" opendate="2011-11-29 00:00:00" fixdate="2011-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UpdateDB should not require segment as input</summary>
      <description>UpdateDB requires an input segment. This causes the metrics for the records of the segment to change, e.g. from fetched to not_modified and changes an adaptive fetch schedule accordingly. This should not happen when one needs to update for filtering of normalizing or other maintenance.</description>
      <version>1.4</version>
      <fixedVersion>1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1237" opendate="2011-12-27 00:00:00" fixdate="2011-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve javac arguements for more verbose output</summary>
      <description>When trying to fix another problem I stumbled across this one. I think it is important to ensure that the javac outputs info regarding deprecation and unchecked operations.</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>nutchgora,1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">default.properties</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1239" opendate="2011-12-29 00:00:00" fixdate="2011-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webgraph should remove deleted pages from segment input</summary>
      <description>Webgraph's outlink job is currently unable to remove links. It should expand it's segment input and be able to remove nodes for pages that no longer exist.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.WebGraph.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.NutchWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1259" opendate="2012-1-25 00:00:00" fixdate="2012-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store detected content type in crawldatum metadata</summary>
      <description>The MIME-type detected by Tika's Detect() API is never added to a Parse's ContentMetaData or ParseMetaData. Because of this bad Content-Types will end up in the documents.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="126" opendate="2005-11-18 00:00:00" fixdate="2005-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetching via https does not work with a proxy (patch)</summary>
      <description>Trying to fetch content from an SSL-Server using a proxy does not work due to a bug in the protocol-httpclient plugin.The attached patch fixes this problem.Ciao -Fritz</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.DummySSLProtocolSocketFactory.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1262" opendate="2012-1-31 00:00:00" fixdate="2012-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map `duplicating` content-types to a single type</summary>
      <description>Similar or duplicating content-types can end-up differently in an index. With, for example, both application/xhtml+xml and text/html it is impossible to use a single filter to select `web pages`.See also: http://lucene.472066.n3.nabble.com/application-xhtml-xml-gt-text-html-td3699942.htmlContent-Type mapping is disabled by default and is enabled via moreIndexingFilter.mapMimeTypes. Example mapping file is provided in conf/.# target MIME-type &lt;TAB&gt; type1 [&lt;TAB&gt; type2 ...]# Map XHTML to HTMLtext/html application/xhtml+xml# Map XHTML and HTML to something elseWeb page text/html application/xhtml+xml# Map some office documents to each otherOffice document application/vnd.oasis.opendocument.text application/x-tika-msoffice</description>
      <version>1.4</version>
      <fixedVersion>1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1300" opendate="2012-3-6 00:00:00" fixdate="2012-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexer to filter and normalize URL&amp;#39;s</summary>
      <description>Indexers should be able to normalize URL's. This is useful when a new normalizer is applied to the entire CrawlDB. Without it, some or all records in a segment cannot be indexed at all.</description>
      <version>None</version>
      <fixedVersion>1.6</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.net.URLNormalizers.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1305" opendate="2012-3-8 00:00:00" fixdate="2012-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Domain(blacklist)URLFilter to trim entries</summary>
      <description>Both filters should handle entries with trailing whitespace.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlfilter-domain.src.java.org.apache.nutch.urlfilter.domain.DomainURLFilter.java</file>
      <file type="M">src.plugin.urlfilter-domainblacklist.src.java.org.apache.nutch.urlfilter.domainblacklist.DomainBlacklistURLFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1310" opendate="2012-3-14 00:00:00" fixdate="2012-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch to send HTTP-accept header</summary>
      <description>Nutch does not send a HTTP-accept header with its requests. This is usually not a problem but some firewall do not like it and will reject the request.</description>
      <version>1.4</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.lib-http.src.java.org.apache.nutch.protocol.http.api.HttpBase.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1319" opendate="2012-3-21 00:00:00" fixdate="2012-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HostNormalizer</summary>
      <description>Nutch would benefit from having a host normalizer. A host normalizer maps a given host to the desired host. A basic example is to map www.apache.org to apache.org. The Apache website is one of many on the internet that has a duplicate website on the same domain just because it allows both www and non-www to return HTTP 200 and proper content.It is also able to handle wildcards such as *.example.org to example.org if there are multiple sub domains that actually point to the same website.Large internet crawls tend to get polluted very quickly due to these problems. It also leads to skewed scores in the webgraph as different websites link to different versions of the same duplicate website. An example:# Force all sub domains to non-www.*.example.com example.com# Force www sub domain to non-www.www.example.net example.net# Force non-www. sub domain to wwwexample.org www.example.org</description>
      <version>None</version>
      <fixedVersion>1.6</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.net.TestURLNormalizers.java</file>
      <file type="M">src.plugin.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1323" opendate="2012-4-2 00:00:00" fixdate="2012-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AjaxNormalizer</summary>
      <description>A two-way normalizer for Nutch able to deal with AJAX URL's, converting them to escaped_fragment URL's and back to an AJAX URL.https://developers.google.com/webmasters/ajax-crawling/</description>
      <version>None</version>
      <fixedVersion>1.10</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1327" opendate="2012-4-2 00:00:00" fixdate="2012-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryStringNormalizer</summary>
      <description>A normalizer for dealing with query strings. Sorting query strings is helpful in preventing duplicates for some (bad) websites.</description>
      <version>None</version>
      <fixedVersion>1.9</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1330" opendate="2012-4-6 00:00:00" fixdate="2012-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OutlinkDB to preserve back up</summary>
      <description>The webgraph's outlinkDB is the single source for all scoring jobs and GB's that eventually come out. In case of disaster, that didn't happen yet, it should be able to preserve back up just like other DB's. This means users with an existing outlinkdb must move it from a crawl/webgraphdb/outlinks/ to crawl/webgraphdb/outlinks/current/.</description>
      <version>1.4</version>
      <fixedVersion>1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.scoring.webgraph.WebGraph.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1331" opendate="2012-4-11 00:00:00" fixdate="2012-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>limit crawler to defined depth</summary>
      <description>there is a need to limit crawler to some defined depth, and importance of this option is to avoid crawling of infinite loops, with dynamic generated urls, that occur in some sites, and to optimize crawler to select important urls.an option is define a iteration limit on generate,fetch,parse,updatedb cycle, but it works only if in each cycle, all of unfetched urls become fetched, (without recrawling them and with some other considerations)we can define a new parameter in CrawlDatum, named depth, and like score-opic algorithm, compute depth of a link after parse, and in generate, only select urls with valid depth.</description>
      <version>1.4</version>
      <fixedVersion>1.7</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="783" opendate="2010-2-1 00:00:00" fixdate="2010-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IndexingFiltersChecker Utility</summary>
      <description>This patch contains a new utility which allows to check the configuration of the indexing filters. The IndexingFiltersChecker reads and parses a URL and run the indexers on it. Displays the fields obtained and the first 100 characters of their value.Can be used e.g. ./nutch org.apache.nutch.indexer.IndexingFiltersChecker http://www.lemonde.fr/</description>
      <version>1.4</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="784" opendate="2010-2-1 00:00:00" fixdate="2010-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CrawlDBScanner</summary>
      <description>The patch file contains a utility which dumps all the entries matching a regular expression on their URL. The dump mechanism of the crawldb reader is not very useful on large crawldbs as the ouput can be extremely large and the -url function can't help if we don't know what url we want to have a look at.The CrawlDBScanner can either generate a text representation of the CrawlDatum-s or binary objects which can then be used as a new CrawlDB. Usage: CrawlDBScanner &lt;crawldb&gt; &lt;output&gt; &lt;regex&gt; &amp;#91;-s &lt;status&gt;&amp;#93; &lt;-text&gt;regex: regular expression on the crawldb key-s status : constraint on the status of the crawldb entries e.g. db_fetched, db_unfetched-text : if this parameter is used, the output will be of TextOutputFormat; otherwise it generates a 'normal' crawldb with the MapFileOutputFormatfor instance the command below : ./nutch com.ant.CrawlDBScanner crawl/crawldb /tmp/amazon-dump .+amazon.com.* -s db_fetched -textwill generate a text file /tmp/amazon-dump containing all the entries of the crawldb matching the regexp .+amazon.com.* and having a status of db_fetched</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="809" opendate="2010-4-2 00:00:00" fixdate="2010-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse-metatags plugin</summary>
      <description>Parse-metatags pluginThe parse-metatags plugin consists of a HTMLParserFilter which takes as parameter a list of metatag names with '*' as default value. The values are separated by ';'.In order to extract the values of the metatags description and keywords, you must specify in nutch-site.xml&lt;property&gt; &lt;name&gt;metatags.names&lt;/name&gt; &lt;value&gt;description;keywords&lt;/value&gt;&lt;/property&gt;The MetatagIndexer uses the output of the parsing above to create two fields 'keywords' and 'description'. Note that keywords is multivalued.The query-basic plugin is used to include these fields in the search e.g. in nutch-site.xml&lt;property&gt; &lt;name&gt;query.basic.description.boost&lt;/name&gt; &lt;value&gt;2.0&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;query.basic.keywords.boost&lt;/name&gt; &lt;value&gt;2.0&lt;/value&gt;&lt;/property&gt;This code has been developed by DigitalPebble Ltd and offered to the community by ANT.com</description>
      <version>1.4,nutchgora</version>
      <fixedVersion>1.5</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="81" opendate="2005-8-15 00:00:00" fixdate="2005-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Webapp only works when deployed in root</summary>
      <description>Index.jsp does a redirect (not forward) to the language folder.The links in the html however are relative to the language folder, not the application root.Not sure what the desired behavoir is, change the html (where is it generated?) or the redirect.</description>
      <version>None</version>
      <fixedVersion>0.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.web.jsp.index.jsp</file>
      <file type="M">src.web.jsp.search.jsp</file>
    </fixedFiles>
  </bug>
</bugrepository>
