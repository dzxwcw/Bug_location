<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="1004" opendate="2011-6-7 00:00:00" fixdate="2011-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not index empty values for title field</summary>
      <description>Tika can generate multiple values for the title field for some files such as certain PDF's. It seems parse-tika's DOMContentUtils.getTitle() and helper methods are responsible for this behaviour. We should add a check on this to prevent empty values for the title field.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1005" opendate="2011-6-7 00:00:00" fixdate="2011-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parse headings plugin</summary>
      <description>Very simple plugin for extracting and indexing a comma separated list of headings via the headings configuration directive.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1010" opendate="2011-6-22 00:00:00" fixdate="2011-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ContentLength not trimmed</summary>
      <description>Somewhere in some component the ContentLength field is not trimmed. This allows a seemingly numeric field to be treated as a string by the indexer in cases one or more leading or trailing whitespace is added. The result is a hard to debug exception with no way to identify the bad document (amongst thousands) or the bad field.Jun 22, 2011 1:03:42 PM org.apache.solr.common.SolrException logSEVERE: java.lang.NumberFormatException: For input string: "32717 " at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48) at java.lang.Long.parseLong(Long.java:419) at java.lang.Long.parseLong(Long.java:468)This can be quickly fixed in the index-more plugin by simply using the trim() when adding the field.</description>
      <version>1.3,1.4</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1024" opendate="2011-6-29 00:00:00" fixdate="2011-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamically set fetchInterval by MIME-type</summary>
      <description>Add facility to configure default or fixed fetchInterval values by MIME-type. This is useful for conserving resources for files that are known to change frequently or never and everything in between. simple key\tvalue\n configuration file only set fetchInterval for new documents keep max fetchInterval fixed by current config</description>
      <version>None</version>
      <fixedVersion>1.6</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.metadata.Nutch.java</file>
      <file type="M">src.java.org.apache.nutch.metadata.HttpHeaders.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.AdaptiveFetchSchedule.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1037" opendate="2011-7-9 00:00:00" fixdate="2011-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deduplicate anchors before indexing</summary>
      <description>Anchors are not deduplicated before indexing. This can result in a very high number of similar and identical anchors being indexed. Before indexing, anchors must be deduplicated at least on case.Use anchorIndexingFilter.deduplicate=true to deduplicate anchors case-insensitive.</description>
      <version>None</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-anchor.src.java.org.apache.nutch.indexer.anchor.AnchorIndexingFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1074" opendate="2011-8-1 00:00:00" fixdate="2011-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>topN is ignored with maxNumSegments</summary>
      <description>When generating segments with topN and maxNumSegments, topN is not respected. It looks like the first generated segment contains topN * maxNumSegments of URLs's, at least the number of map input records roughly matches.</description>
      <version>1.3</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Generator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1096" opendate="2011-8-25 00:00:00" fixdate="2011-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Empty (not null) ContentLength results in failure of fetch</summary>
      <description>In rare occasions, servers return an empty string ContentLength, which results in a fetch failure. One could argue whether the fetch is allowed to proceed in these cases. I for one believe it is. (Just like the cases where the header is null or not properly trimmed).Patch will be right up.</description>
      <version>1.3</version>
      <fixedVersion>1.4,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1105" opendate="2011-9-7 00:00:00" fixdate="2011-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MaxContentLength option for index-basic</summary>
      <description>Like the limit for title, the basic indexing filter should have an optional setting to limit and truncate the content length.</description>
      <version>1.3,1.4,nutchgora</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1106" opendate="2011-9-9 00:00:00" fixdate="2011-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Options to skip url&amp;#39;s based on length</summary>
      <description>Adds option to skip URL's exceeding a certain length. At first we used regex to impose this limit but having this options configurable is more convenient. Comments?</description>
      <version>1.3</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseOutputFormat.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherThread.java</file>
      <file type="M">conf.regex-urlfilter.txt.template</file>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1110" opendate="2011-9-13 00:00:00" fixdate="2011-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Updatedb must not write _SUCCESS file</summary>
      <description>Readdb fails with the presence of a _SUCCESS file written by Hadoop mapred after update job.</description>
      <version>1.3</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDb.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1113" opendate="2011-9-15 00:00:00" fixdate="2011-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging segments causes URLs to vanish from crawldb/index?</summary>
      <description>When I run Nutch, I use the following steps:nutch inject crawldb/ url.txtrepeated 3 times:nutch generate crawldb/ segments/ -normalizenutch fetch `ls -d segments/* | tail -1`nutch parse `ls -d segments/* | tail -1`nutch update crawldb `ls -d segments/* | tail -1`nutch mergesegs merged/ -dir segments/nutch invertlinks linkdb/ -dir merged/nutch index index/ crawldb/ linkdb/ -dir merged/ (I forward ported the lucene indexing code from Nutch 1.1).When I crawl with merging segments, I lose about 20% of the URLs that wind up in the index vs. when I crawl without merging the segments. Somehow the segment merger causes me to lose ~20% of my crawl database!</description>
      <version>1.3</version>
      <fixedVersion>1.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.segment.TestSegmentMergerCrawlDatums.java</file>
      <file type="M">src.java.org.apache.nutch.segment.SegmentMerger.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1115" opendate="2011-9-19 00:00:00" fixdate="2011-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Option to disable fixing of embedded params in DomContentUtils</summary>
      <description>Add option to disable fixing of embedded params:http://lucene.472066.n3.nabble.com/Outlinks-with-embedded-params-td3332396.htmlWhen enabled, millions of crap url's are output as outlink. This results in many 404 in the DB and many very long URL's that actually lead to the same page.</description>
      <version>1.3</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.src.java.org.apache.nutch.parse.tika.DOMContentUtils.java</file>
      <file type="M">src.plugin.parse-html.src.java.org.apache.nutch.parse.html.DOMContentUtils.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1146" opendate="2011-10-4 00:00:00" fixdate="2011-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get rid of _success files in webgraph code</summary>
      <description>WebGraph tools here and there also suffer from reading a _SUCCESS file. All jobs there should disable this setting.</description>
      <version>None</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1173" opendate="2011-10-17 00:00:00" fixdate="2011-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DomainStats doesn&amp;#39;t count db_not_modified</summary>
      <description>The DomainStats tool does not count records with a not_modified status. Domains therefore can disappear from the stats output while they shouldn't.</description>
      <version>1.3</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1178" opendate="2011-10-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect CSV header CrawlDatumCsvOutputFormat</summary>
      <description>The CSV header doesn't mention both retry interval fields (seconds + days). We should either add another field to the header to get rid of one retry interval field. I prefer the former as people may already rely on the current format.</description>
      <version>1.3</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1182" opendate="2011-10-26 00:00:00" fixdate="2011-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetcher to log hung threads</summary>
      <description>While crawling a slow server with a couple of very large PDF documents (30 MB) on itafter some time and a bulk of successfully fetched documents the fetcher stopswith the message: Aborting with 10 hung threads.From now on every cycle ends with hung threads, almost no documents are fetchedsuccessfully. In addition, strange hadoop errors are logged: fetch of http://.../xyz.pdf failed with: java.lang.NullPointerException at java.lang.System.arraycopy(Native Method) at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1108) ...or Exception in thread "QueueFeeder" java.lang.NullPointerException at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48) at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)I've run the debugger and found: after the "hung threads" are reported the fetcher stops but the threads are still alive and continue fetching a document. In consequence, this will limit the small bandwidth of network/server even more after the document is fetched the thread tries to write the content via output.collect() which must fail because the fetcher map job is already finished and the associated temporary mapred directory is deleted. The error message may get mixed with the progress output of the next fetch cycle causing additional confusion. documents/URLs causing the hung thread are never reported nor stored. That is, it's hard to track them down, and they will cause a hung thread again and again.The problem is reproducible when fetching bigger documents and setting mapred.task.timeout to a low value (this will definitely cause hung threads).</description>
      <version>1.3,1.4</version>
      <fixedVersion>2.3,1.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1185" opendate="2011-10-31 00:00:00" fixdate="2011-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decrease solr.commit.size</summary>
      <description>Default document batch size should be decreased to prevent OOMEs.</description>
      <version>1.3</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1193" opendate="2011-11-2 00:00:00" fixdate="2011-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect url transform to lowercase: parameter solr</summary>
      <description></description>
      <version>1.3</version>
      <fixedVersion>1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.Crawl.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1195" opendate="2011-11-2 00:00:00" fixdate="2011-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Solr 4x (trunk) example schema</summary>
      <description>The conf/schema.xml that we ship works ok for Solr 3.x, but in Solr trunk some of the class names have been changed, and some field types have been redefined, so if you simply drop this schema into Solr it will cause severe errors and indexing won't work.I propose to add a version of the schema.xml file that is tailored to Solr 4.x so that users can deploy this schema when indexing to Solr trunk.</description>
      <version>None</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="824" opendate="2010-5-20 00:00:00" fixdate="2010-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.</summary>
      <description>Hello,I am performing a local file system crawling.My problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.For example, I will see the following error:fetching file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.htmlorg.apache.nutch.protocol.file.FileError: File Error: 404 at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:92) at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:535)fetch of file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html failed with: org.apache.nutch.protocol.file.FileError: File Error: 404I am using nutch-1.0.Among other standard settings, I configured nutch-site.conf as follows:&lt;property&gt; &lt;name&gt;plugin.includes&lt;/name&gt; &lt;value&gt;protocol-file|protocol-http|urlfilter-regex|parse-(text|html|js|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)&lt;/value&gt; &lt;description&gt;Regular expression naming plugin directory names to include. Any plugin not matching this expression is excluded. In any case you need at least include the nutch-extensionpoints plugin. By default Nutch includes crawling just HTML and plain text via HTTP, and basic indexing and search plugins. In order to use HTTPS please enable protocol-httpclient, but be aware of possible intermittent problems with the underlying commons-httpclient library. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;file.content.limit&lt;/name&gt; &lt;value&gt;-1&lt;/value&gt;&lt;/property&gt;Moreover, crawl-urlfilter.txt looks like: skip http:, ftp:, &amp; mailto: urls-^(http|ftp|mailto): skip image and other suffixes we can't yet parse-\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$ skip URLs containing certain characters as probable queries, etc.-&amp;#91;?*!@=&amp;#93; skip URLs with slash-delimited segment that repeats 3+ times, to break loops-.*(/&amp;#91;^/&amp;#93;)/&amp;#91;^/&amp;#93;\1/&amp;#91;^/&amp;#93;+\1/ accept hosts in MY.DOMAIN.NAME#+^http://([a-z0-9]*\.)*MY.DOMAIN.NAME/ accept everything else+.*~ &amp;#8212;Thanks,Michela</description>
      <version>1.0.0,1.2,1.3,nutchgora</version>
      <fixedVersion>1.0.0,1.3,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-file.src.test.org.apache.nutch.protocol.file.testprotocolfile.txt</file>
      <file type="M">src.plugin.protocol-file.build.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">src.plugin.protocol-file.src.test.org.apache.nutch.protocol.file.TestProtocolFile.java</file>
      <file type="M">src.plugin.protocol-file.src.java.org.apache.nutch.protocol.file.FileResponse.java</file>
      <file type="M">src.plugin.protocol-file.sample.testprotocolfile.txt</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="825" opendate="2010-5-21 00:00:00" fixdate="2010-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publish nutch artifacts to central maven repository</summary>
      <description>As per the discussion at NUTCH-821, publishing nutch artifacts to maven will be nice. NUTCH-821 already introduces dependency management with ivy. As for the remaining, ant task for generating pom files should be developed, and artifacts should be published to maven repo by a committer after a release.</description>
      <version>None</version>
      <fixedVersion>nutchgora</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="826" opendate="2010-5-24 00:00:00" fixdate="2010-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mailing list is broken.</summary>
      <description>All of the following addresses are failing:nutch-user@nutch.apache.orgnutch-user-subscribe@nutch.apache.orgnutch-user-subscribe@lucene.apache.orgFor the last one, the mailer daemon said "This mailing list has moved to user at nutch.apache.org."Below is the message I tried to send:Hi people,I've been banging my head against this problem for two days now.Simply, I want to add a field with the value of a given meta tag.I've been trying the parse-xml plugin, but that seems that it doesn'twork with version 1.0. I've tried the code athttp://sujitpal.blogspot.com/2009/07/nutch-getting-my-feet-wet.htmland it hasn't worked. I don't even know why. I don't even know if myplugin is being used... or even looked for! Nutch seems to have ainfuriating "Fail silently" policy for plugins. I put aSystem.exit(1) in my filters just to see if my code is even beingencountered. It has not in spite of my config telling it to.Here's my config:nutch-site.xml...&lt;property&gt; &lt;name&gt;plugin.includes&lt;/name&gt; &lt;value&gt;protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|metadata&lt;/value&gt;&lt;/property&gt;...parse-plugins.xml...&lt;mimeType name="application/xhtml+xml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/html"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/sgml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="metadata" /&gt;&lt;/mimeType&gt;&lt;mimeType name="text/xml"&gt; &lt;plugin id="parse-html" /&gt; &lt;plugin id="parse-rss" /&gt; &lt;plugin id="metadata" /&gt; &lt;plugin id="feed" /&gt;&lt;/mimeType&gt;...&lt;alias name="metadata"extension-id="com.example.website.nutch.parsing.MetaTagExtractorParseFilter"/&gt;...I've also copied the plugin.xml and jar from my build/metadata to theplugins root dir.Nonetheless, Nutch runs and puts data in solr for me. Afaik, Nutch iscompletely unaware of my plugin despite my config options. Is thesome other place I need to tell Nutch to use my plugin? Is there someother approach to do this without having to write a plugin? This doesseem like a lot of work to simply get a meta tag into a field. Anyhelp would be appreciated.Sincerely,John Sherwood</description>
      <version>None</version>
      <fixedVersion>1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.src.documentation.content.xdocs.version.control.xml</file>
      <file type="M">src.site.src.documentation.content.xdocs.mailing.lists.xml</file>
      <file type="M">site.version.control.pdf</file>
      <file type="M">site.version.control.html</file>
      <file type="M">site.mailing.lists.pdf</file>
      <file type="M">site.mailing.lists.html</file>
    </fixedFiles>
  </bug>
  <bug id="873" opendate="2010-8-7 00:00:00" fixdate="2010-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ivy configuration settings don&amp;#39;t include Gora</summary>
      <description>The Nutch 2.0 trunk now requires Gora, and even though it's not available in any repository, we should still configure Ivy to depend on it so that the build will work provided you follow the Gora instructions here:http://github.com/enis/goraI've fixed it locally and will commit an update shortly that takes care of it. In order to compile Nutch trunk now (before we get Gora into a repo), here are the steps (copied from http://github.com/enis/gora):$ git clone git://github.com/enis/gora.git$ cd gora $ antThis will install Gora into your local Ivy repo. Then from there on out, just update your Ivy resolver (or alternatively just the Nutch build post this issue being resolved) and you're good.</description>
      <version>None</version>
      <fixedVersion>nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ivy.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="888" opendate="2010-8-16 00:00:00" fixdate="2010-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove parse-rss</summary>
      <description>See https://issues.apache.org/jira/browse/NUTCH-887CM : I wrote parse-rss back in 2005, and used commons-feedparser from Kevin Burton and his crew. At the time it was well developed, and a little more flexible and easier for me to pick up than Rome. Since then however, its development has really become stagnant and it is no longer maintained.In terms of real differences in terms of functionality, they are roughly equivalent so there isn't much difference.Already +1 from Andrzej and Chris. Will remove it tomorrow if there aren't any objections in the meantime</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
      <file type="M">default.properties</file>
      <file type="M">build.xml</file>
      <file type="M">src.plugin.parse-tika.build.xml</file>
      <file type="M">src.plugin.parse-rss.src.test.org.apache.nutch.parse.rss.TestRSSParser.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.structs.RSSItem.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.structs.RSSChannel.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.RSSParser.java</file>
      <file type="M">src.plugin.parse-rss.src.java.org.apache.nutch.parse.rss.FeedParserListenerImpl.java</file>
      <file type="M">src.plugin.parse-rss.sample.rsstest.rss</file>
      <file type="M">src.plugin.parse-rss.plugin.xml</file>
      <file type="M">src.plugin.parse-rss.lib.commons-feedparser-0.6-fork.jar</file>
      <file type="M">src.plugin.parse-rss.ivy.xml</file>
      <file type="M">src.plugin.parse-rss.build.xml</file>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.parse-plugins.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.nutch.parse.TestParserFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="913" opendate="2010-10-13 00:00:00" fixdate="2010-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nutch should use new namespace for Gora</summary>
      <description>Gora is in Apache Incubator now (Yey!). We recently changed Gora's namespace from org.gora to org.apache.gora. This means nutch should use the new namespace otherwise it won't compile with newer builds of Gora.</description>
      <version>None</version>
      <fixedVersion>nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.nutch.util.CrawlTestUtil.java</file>
      <file type="M">src.test.org.apache.nutch.util.AbstractNutchTest.java</file>
      <file type="M">src.test.org.apache.nutch.storage.TestGoraStorage.java</file>
      <file type="M">src.test.org.apache.nutch.crawl.TestInjector.java</file>
      <file type="M">src.test.nutch-site.xml</file>
      <file type="M">src.java.org.apache.nutch.util.WebPageWritable.java</file>
      <file type="M">src.java.org.apache.nutch.util.IdentityPageReducer.java</file>
      <file type="M">src.java.org.apache.nutch.util.domain.DomainStatistics.java</file>
      <file type="M">src.java.org.apache.nutch.storage.WebTableCreator.java</file>
      <file type="M">src.java.org.apache.nutch.storage.WebPage.java</file>
      <file type="M">src.java.org.apache.nutch.storage.StorageUtils.java</file>
      <file type="M">src.java.org.apache.nutch.storage.ProtocolStatus.java</file>
      <file type="M">src.java.org.apache.nutch.storage.ParseStatus.java</file>
      <file type="M">src.java.org.apache.nutch.parse.ParserJob.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerReducer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerJob.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherReducer.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetcherJob.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.FetchEntry.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.WebTableReader.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.InjectorJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.GeneratorReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.GeneratorMapper.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DbUpdaterJob.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DbUpdateReducer.java</file>
      <file type="M">src.java.org.apache.nutch.crawl.DbUpdateMapper.java</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="935" opendate="2010-11-17 00:00:00" fixdate="2010-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unnecessary /./ in basic urlnormalizer</summary>
      <description>remove unnecessary /./ in basic urlnormalizer, because this is a rather a sign of bad webserver configuration than of a wanted link.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.urlnormalizer-basic.src.test.org.apache.nutch.net.urlnormalizer.basic.TestBasicURLNormalizer.java</file>
      <file type="M">src.plugin.urlnormalizer-basic.src.java.org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="940" opendate="2010-11-26 00:00:00" fixdate="2010-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>static field plugin</summary>
      <description>A simple plugin called at indexing that adds fields with static data. You can specify a list of &lt;fieldname&gt;:&lt;fieldcontent&gt; per nutch job.It can be useful when collections can't be created by urlpatterns, like in subcollection, but on a job-basis.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.4</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.build.xml</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.plugin.index-static.src.java.org.apache.nutch.indexer.staticfield.StaticFieldIndexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="948" opendate="2010-12-21 00:00:00" fixdate="2010-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Lucene dependencies</summary>
      <description>Branch-1.3 still has Lucene libs, but uses Lucene only in one place, namely it uses DateTools in index-basic. DateTools should be replaced with Solr's DateUtil, as we did in trunk, and then we can remove Lucene libs as a dependency.</description>
      <version>1.3</version>
      <fixedVersion>1.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="951" opendate="2011-1-5 00:00:00" fixdate="2011-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport changes from 2.0 into 1.3</summary>
      <description>I've compared the changes from 2.0 with 1.3 and found the following differences (excluding anything specific to 2.0/GORA) NUTCH-564 External parser supports encoding attribute (Antony Bowesman, mattmann) NUTCH-714 Need a SFTP and SCP Protocol Handler (Sanjoy Ghosh, mattmann) NUTCH-825 Publish nutch artifacts to central maven repository (mattmann) NUTCH-851 Port logging to slf4j (jnioche) NUTCH-861 Renamed HTMLParseFilter into ParseFilter NUTCH-872 Change the default fetcher.parse to FALSE (ab). NUTCH-876 Remove remaining robots/IP blocking code in lib-http (ab) NUTCH-880 REST API for Nutch (ab) NUTCH-883 Remove unused parameters from nutch-default.xml (jnioche) NUTCH-884 FetcherJob should run more reduce tasks than default (ab) NUTCH-886 A .gitignore file for Nutch (dogacan) NUTCH-894 Move statistical language identification from indexing to parsing step NUTCH-921 Reduce dependency of Nutch on config files (ab) NUTCH-930 Remove remaining dependencies on Lucene API (ab) NUTCH-931 Simple admin API to fetch status and stop the service (ab) NUTCH-932 Bulk REST API to retrieve crawl results as JSON (ab)Let's go through this and decide what to port to 1.3</description>
      <version>1.3</version>
      <fixedVersion>1.3</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-ext.src.java.org.apache.nutch.parse.ext.ExtParser.java</file>
      <file type="M">src.plugin.parse-ext.plugin.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="954" opendate="2011-1-6 00:00:00" fixdate="2011-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bugfix for Content-Length limit in http protocols</summary>
      <description>3. Content-Length limit (nutch3.patch)This is related to NUTCH-899.The patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few bytes. Both protocol-http and protocol-httpclient plugins were problematic.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.protocol-http.src.java.org.apache.nutch.protocol.http.HttpResponse.java</file>
      <file type="M">src.plugin.protocol-httpclient.src.java.org.apache.nutch.protocol.httpclient.HttpResponse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="965" opendate="2011-2-8 00:00:00" fixdate="2011-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip parsing for truncated documents</summary>
      <description>The issue you're likely to run into when parsing truncated FLV files is described here:http://www.mail-archive.com/user@nutch.apache.org/msg01880.htmlThe parser library gets stuck in infinite loop as it encounters corrupted data due to for example truncating big binary files at fetch time.</description>
      <version>None</version>
      <fixedVersion>nutchgora,1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.parse.ParseSegment.java</file>
      <file type="M">src.java.org.apache.nutch.fetcher.Fetcher.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="967" opendate="2011-2-17 00:00:00" fixdate="2011-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Tika 0.9</summary>
      <description></description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.parse-tika.plugin.xml</file>
      <file type="M">src.plugin.parse-tika.ivy.xml</file>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">conf.tika-mimetypes.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="980" opendate="2011-4-12 00:00:00" fixdate="2011-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix IllegalAccessError with slf4j used in Solrj.</summary>
      <description>Currently Solr commands fail because of: Exception in thread "main" java.lang.IllegalAccessError: tried to access field org.slf4j.impl.StaticLoggerBinder.SINGLETON from class org.slf4j.LoggerFactory at org.slf4j.LoggerFactory.staticInitialize(LoggerFactory.java:83) at org.slf4j.LoggerFactory.&lt;clinit&gt;(LoggerFactory.java:73) at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.&lt;clinit&gt;(CommonsHttpSolrServer.java:78)Julien looked it up http://www.slf4j.org/faq.html#IllegalAccessError , we need to change the versions in Ivy. I haven't yet come around to test it with trunk so we need to look for it there as well.</description>
      <version>1.3</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="983" opendate="2011-4-13 00:00:00" fixdate="2011-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade SolrJ</summary>
      <description>Solr 3.1 has been released a while ago. The Javabin format between 1.4.1 and 3.1 has been changed so our SolrJ 1.4.1 cannot send documents to 3.1. Since Nutch 2.0 won't be released within a short period i believe it would be a good idea to upgrade our SolrJ to 3.1. New Solr users are encouraged to use Solr 3.1 or upgrade so i expect more users wanting to use 3.1 as well. Any thoughts?</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="987" opendate="2011-4-26 00:00:00" fixdate="2011-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support HTTP auth for Solr communication</summary>
      <description>At the moment we cannot send data directly to a public HTTP auth protected Solr instance. I've a WIP that passes a configured HTTPClient object to CommonsHttpSolrServer, it works. This issue should add this ability to indexing, dedup and clean and be configured from some configuration file.Enable Solr HTTP auth communication by setting the following parameters in your nutch-site config: solr.auth=true solr.auth.username=USERNAME solr.auth.password=PASSWORD</description>
      <version>None</version>
      <fixedVersion>1.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrIndexer.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrDeleteDuplicates.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrClean.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.IndexerMapReduce.java</file>
      <file type="M">conf.nutch-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="991" opendate="2011-4-27 00:00:00" fixdate="2011-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SolrDedup must issue a commit</summary>
      <description>Title says it all. SolrDedup job doesn't commit but it should.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrDeleteDuplicates.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="996" opendate="2011-5-8 00:00:00" fixdate="2011-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Indexer adds solr.commit.size+1 docs</summary>
      <description>SolrIndexer adds one additional document. This issue can be spotted easily with Solr 3.1 which accurately reports the number of added docs in the log.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3,nutchgora</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="997" opendate="2011-5-17 00:00:00" fixdate="2011-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IndexingFitlers to store Date objects instead of Strings</summary>
      <description>See Nutch-985.Several IndexingFilters generate fields containing Dates with String values. This patch changes this so that Date objects are stored then converted into whatever type and format are required during the indexing.</description>
      <version>1.3,nutchgora</version>
      <fixedVersion>1.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.index-more.src.java.org.apache.nutch.indexer.more.MoreIndexingFilter.java</file>
      <file type="M">src.plugin.index-basic.src.java.org.apache.nutch.indexer.basic.BasicIndexingFilter.java</file>
      <file type="M">src.plugin.feed.src.java.org.apache.nutch.indexer.feed.FeedIndexingFilter.java</file>
      <file type="M">src.java.org.apache.nutch.indexer.solr.SolrWriter.java</file>
      <file type="M">conf.schema.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
