<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="NUTCH">
  <bug id="2463" opendate="2017-11-20 00:00:00" fixdate="2017-11-20 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Enable sampling CrawlDB</summary>
      <description>CrawlDB can grow to contain billions of records. When that happens readdb -dump is pretty useless, and readdb -topN can run for ages (and does not provide a statistically correct sample).We should add a parameter -sample to readdb -dump which is followed by a number between 0 and 1, and only that fraction of records from the CrawlDB will be processed.The sample should be statistically random, and all the other filters should be applied on the sampled records.</description>
      <version>None</version>
      <fixedVersion>1.14</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.nutch.crawl.CrawlDbReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="2499" opendate="2018-1-16 00:00:00" fixdate="2018-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Elastic REST Indexer: Duplicate values</summary>
      <description>Due to a change in https://github.com/apache/nutch/commit/160758023e3de83894ae4fe654c17fde62aba50e#diff-408fd2f17bc9791dcbf531ffe6574a6a the Elastic REST indexer does not work with HashSets for values anymore but instead saves duplicated values as arrays.</description>
      <version>None</version>
      <fixedVersion>1.15</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.plugin.indexer-elastic-rest.src.java.org.apache.nutch.indexwriter.elasticrest.ElasticRestIndexWriter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2722" opendate="2019-6-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetch dependencies via https</summary>
      <description>Dependencies need to be fetched via https, see https://central.sonatype.org/articles/2019/Apr/30/http-access-to-repo1mavenorg-and-repomavenapacheorg-is-being-deprecated/</description>
      <version>2.4,2.5,1.16</version>
      <fixedVersion>2.4,1.16</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.mvn.template</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2808" opendate="2020-7-11 00:00:00" fixdate="2020-12-11 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Document side effects of ignoring robots.txt</summary>
      <description>(see NUTCH-1927 and NUTCH-2803)The aim of NUTCH-1927 was to make it possible to ignore the robots.txt for a defined set of hosts/domains. Ignoring the robots.txt entirely has some site effects which should be documented: undesired content (duplicates, private pages, etc.) may get indexed the Crawl-Delay is ignored no sitemaps are detected (cf. NUTCH-2807)</description>
      <version>None</version>
      <fixedVersion>1.19</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.nutch-default.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
