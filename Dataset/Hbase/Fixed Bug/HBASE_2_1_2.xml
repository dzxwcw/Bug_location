<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="21299" opendate="2018-10-12 00:00:00" fixdate="2018-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>List counts of actual region states in master UI tables section</summary>
      <description>It the tables panel, we list Open Regions, Offline Regions, Failed Regions, Split Region, and Other. This is not very useful. It is from a time before region states were edited down. Better to list OPEN, CLOSED, OPENING, CLOSING...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="21518" opendate="2018-11-27 00:00:00" fixdate="2018-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMasterFailoverWithProcedures is flaky</summary>
      <description>TestMasterFailoverWithProcedures test is failing frequently, times out. I faced this failure on 2.0.3RC0 vote and it also appears on multiple flaky dashboards.branch-2: https://builds.apache.org/view/H-L/view/HBase/job/HBase-Flaky-Tests/job/branch-2/2007/branch-2.1: https://builds.apache.org/view/H-L/view/HBase/job/HBase-Flaky-Tests/job/branch-2.1/2002/branch-2.0: https://builds.apache.org/view/H-L/view/HBase/job/HBase-Flaky-Tests/job/branch-2.0/1988/  [INFO] Running org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures[ERROR] Tests run: 4, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 780.648 s &lt;&lt;&lt; FAILURE! - in org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures[ERROR] org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures Time elapsed: 749.024 s &lt;&lt;&lt; ERROR!org.junit.runners.model.TestTimedOutException: test timed out after 780 seconds at org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.tearDown(TestMasterFailoverWithProcedures.java:86)[ERROR] org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures Time elapsed: 749.051 s &lt;&lt;&lt; ERROR!java.lang.Exception: Appears to be stuck in thread RS-EventLoopGroup-3-2</description>
      <version>2.2.0,2.0.3,2.1.2</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.2,2.0.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21620" opendate="2018-12-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem in scan query when using more than one column prefix filter in some cases.</summary>
      <description>In some cases, unable to get the scan results when using more than one column prefix filter.Attached a java file to import the data which we used and a text file containing the values..While executing the following query (hbase shell as well as java program) it is waiting indefinitely and after RPC timeout we got the following error.. Also we noticed high cpu, high load average and very frequent young gc  in the region server containing this row...scan 'namespace:tablename',{STARTROW =&gt; 'test',ENDROW =&gt; 'test', FILTER =&gt; "ColumnPrefixFilter('1544770422942010001_') OR ColumnPrefixFilter('1544769883529010001_')"}ROW                                                  COLUMN+CELL                                                                   ERROR: Call id=18, waitTime=60005, rpcTimetout=60000 Note: Table scan operation and scan with a single column prefix filter works fine in this case.When we check the same query in hbase-1.2.5 it is working fine.Can you please help me on this..</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,1.4.8,2.1.2,2.0.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.2,2.0.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterListOnMini.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterList.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterListWithOR.java</file>
    </fixedFiles>
  </bug>
  <bug id="2163" opendate="2010-1-23 00:00:00" fixdate="2010-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZK dependencies - explicitly add them until ZK artifacts are published to mvn repository</summary>
      <description>Currently we include the binary of zookeeper but we need to add the dependencies explicitly as well ( similar to a recent issue , related to thrift ). zk depends on log4j / jline . log4j is already in. This patch adds jline to the dependencies explicitly.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21630" opendate="2018-12-21 00:00:00" fixdate="2018-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[shell] Define ENDKEY == STOPROW (we have ENDROW)</summary>
      <description>Small fix... In HTableDescriptor, the end row is labelled ENDKEY. In the shell, I should be able to scan to the ENDKEY... but I can't. Scan takes STOPROW or ENDROW.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.table.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.constants.rb</file>
    </fixedFiles>
  </bug>
  <bug id="21631" opendate="2018-12-21 00:00:00" fixdate="2018-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>list_quotas should print human readable values for LIMIT</summary>
      <description>The list_quotas command should print human readable values for LIMIT. For e.g. In the below case, printing LIMIT =&gt; 3G is better than printing LIMIT =&gt; 3221225472hbase(main):006:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't2', LIMIT =&gt; '3G', POLICY =&gt; NO_WRITESTook 0.0132 secondshbase(main):007:0&gt; list_quotasOWNER QUOTAS TABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, LIMIT =&gt; 3221225472, VIOLATION_POLICY =&gt; NO_WRITES1 row(s)Took 0.0281 seconds </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.2,2.0.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.SpaceLimitSettings.java</file>
      <file type="M">hbase-shell.src.test.ruby.hbase.quotas.test.rb</file>
    </fixedFiles>
  </bug>
  <bug id="21635" opendate="2018-12-23 00:00:00" fixdate="2018-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use maven enforcer to ban imports from illegal packages</summary>
      <description>Now we use checkstyle to report the illegal imports, but it will be easy to be ignored by developpers.https://github.com/skuzzle/restrict-imports-enforcer-ruleThis is an extension for the maven enforcer plugin, which is used to ban imports, and the advantage is that it will cause a compile error, which is not likely to be ignored. The extension is not perfect, but I think it is worth a try.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.2,2.0.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestBalancerStatusTagInJMXMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.DeadServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21644" opendate="2018-12-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify table procedure runs infinitely for a table having region replication &gt; 1</summary>
      <description>Steps to reproduce Create a table with region replication set to a value greater than 1 Modify any of the table properties, say max file sizeExpected Result The modify table should succeed and run to completion.Actual Result The modify table keep running infinitelyAnalysis/Issue The problem occurs due to inifinitely looping between states REOPEN_TABLE_REGIONS_REOPEN_REGIONS and REOPEN_TABLE_REGIONS_CONFIRM_REOPENED of ReopenTableRegionsProcedure, called as part of ModifyTableProcedure.Consequences For a table having region replicas: Any modify table operation fails to complete Also, enable table replication fails to complete as it is unable to change the replication scope of the table in source cluster</description>
      <version>3.0.0-alpha-1,2.1.1,2.1.2</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.replication.TestReplicationAdminWithClusters.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="21645" opendate="2018-12-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perform sanity check and disallow table creation/modification with region replication &lt; 1</summary>
      <description>We should perform sanity check and disallow table creation with region replication &lt; 1 or modification of an existing table with new region replication value &lt; 1.</description>
      <version>3.0.0-alpha-1,1.5.0,2.1.1,2.1.2</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="21705" opendate="2019-1-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should treat meta table specially for some methods in AsyncAdmin</summary>
      <description>For example, tableExists, isTableEnabled, isTableDisabled...For now, we will go to the meta table directly but obviously, meta table does not contain the record for itself...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi3.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="21753" opendate="2019-1-22 00:00:00" fixdate="2019-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support getting the locations for all the replicas of a region</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HRegionLocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="21791" opendate="2019-1-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade thrift dependency to 0.12.0</summary>
      <description>As somebody have already known, that there is a CVE for thrift from 0.5.0 to 0.11.0.https://nvd.nist.gov/vuln/detail/CVE-2018-1320As the CVE is already public, let's upgrade our thrift dependency and release new versions ASAP.</description>
      <version>3.0.0-alpha-1,1.5.0,1.3.3,2.2.0,1.4.9,2.1.2,1.2.10,2.0.4</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TReadType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDeleteType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TConsistency.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompareOp.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21794" opendate="2019-1-27 00:00:00" fixdate="2019-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Coprocessor observer example given in section 111.1 of the ref guide.</summary>
      <description>The given example should be changed after the CP changes (HBASE-17732)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21795" opendate="2019-1-28 00:00:00" fixdate="2019-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client application may get stuck (time bound) if a table modify op is called immediately after split op</summary>
      <description>Steps: Create a table Split the table Modify the table immediately after splittingExpected: The modify table procedure completes and control returns back to clientActual: The modify table procedure completes and control does not return back to client, until catalog janitor runs and deletes parent or future timeout occurs</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21806" opendate="2019-1-29 00:00:00" fixdate="2019-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to roll WAL on very slow syncs</summary>
      <description>In large heterogeneous clusters sometimes a slow datanode can cause WAL syncs to be very slow. In this case, before the bad datanode recovers, or is discovered and repaired, it would be helpful to roll WAL on a very slow sync to get a new pipeline.Otherwise the slow WAL will impact write latency for a long time (slow writes result in less writes result in the WAL not being rolled for longer)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="21883" opendate="2019-2-12 00:00:00" fixdate="2019-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhancements to Major Compaction tool</summary>
      <description>I would like to add new compaction tools based on churromorales's tool at HBASE-19528.We internally have tools that pick and compact regions based on multiple criteria. Since Rahul already has a version in community, we would like to build on top of it instead of pushing yet another tool.With this jira, I would like to add a tool which looks at regions beyond TTL and compacts them in a rsgroup. We have time series data and those regions will become dead after a while, so we compact those regions to save disk space. We also merge those empty regions to reduce load, but that tool comes later.Will prep a patch for 2.x once 1.5 gets in.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.compaction.TestMajorCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.compaction.TestMajorCompactionRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.compaction.MajorCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.compaction.MajorCompactionRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="21884" opendate="2019-2-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix box/unbox findbugs warning in secure bulk load</summary>
      <description>Reason TestsFindBugs module:hbase-serverBoxed value is unboxed and then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:[line 268]Looking at branch-2 and master I suspect we're doing the same wasteful operation but findbugs can't see it through the lambda definition.</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,2.1.1,2.0.3,2.1.2,2.0.4,1.4.10,1.3.4,1.2.11,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.0.5,1.3.4,1.2.11,2.3.0,2.1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21889" opendate="2019-2-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use thrift 0.12.0 when build thrift by compile-thrift profile</summary>
      <description>Build command.mvn compile -Pcompile-thrift</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21976" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deal with RetryImmediatelyException for batching request</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  <bug id="21977" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip replay WAL and update seqid when open regions restored from snapshot</summary>
      <description>TableSnapshotScanner restore a snapshot and then open the restored regions. When open these regions, we can skip replay WAL and update seqid.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreSnapshotHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ClientSideRegionScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="21978" opendate="2019-3-2 00:00:00" fixdate="2019-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should close AsyncRegistry if we fail to get cluster id when creating AsyncConnection</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22077" opendate="2019-3-21 00:00:00" fixdate="2019-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose sleep time as a command line argument of IntergationTestBackupRestore</summary>
      <description>Extend command line arguments of IntergationTestBackupRestore with a sleep time of chaos monkey options to be able to setup policy of region server restarts more granularly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBackupRestore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2208" opendate="2010-2-10 00:00:00" fixdate="2010-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableServers # processBatchOfRows - converts from List to [ ] - Expensive copy</summary>
      <description>With autoFlush to false and a large write buffer on HTable, when we write bulk puts - TableServer # processBatchOfRows , convert the input (List) to an [ ] , before sending down the wire. With a write buffer as large as 20 MB , that becomes an expensive copy when we do - list.toArray(new T[ ] ). May be - should we change the wire protocol to support List as well , and then revisit this to prevent the bulk copy ?Batch b = new Batch(this) { @Override int doCall(final List&lt;Row&gt; currentList, final byte [] row, final byte [] tableName) throws IOException, RuntimeException { *final Put [] puts = currentList.toArray(PUT_ARRAY_TYPE);* return getRegionServerWithRetries(new ServerCallable&lt;Integer&gt;(this.c, tableName, row) { public Integer call() throws IOException { return server.put(location.getRegionInfo().getRegionName(), puts); } }); }</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22312" opendate="2019-4-25 00:00:00" fixdate="2019-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop 3 profile for hbase-shaded-mapreduce should like mapreduce as a provided dependency</summary>
      <description>the hadoop 3 profile currently misses declaring a provided dependency on the core mapreduce client module. that means we pick it up as a compile dependency from the hbase-mapreduce module, which means we include things in the shaded jar that we don't need to. (and expressly aren't supposed to include because they're supposed to come from Hadoop at runtime).</description>
      <version>2.1.0,2.2.0,2.1.1,2.1.2,2.1.3,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22341" opendate="2019-4-30 00:00:00" fixdate="2019-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add explicit guidelines for removing deprecations in book</summary>
      <description>Based on the discussion on the mailing list about the removal of deprecated versions, the client API compatibility should be extended to make it clear when a deprecated API will be removed.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22344" opendate="2019-4-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document deprecated public APIs</summary>
      <description>Currently some public APIs don't document when their deprecation was introduced and when they are expected to be removed. The documentation should be extended for APIs marked as public and limited private.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.LoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RpcSchedulerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RowTooBigException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeer.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.CellCreator.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Counter.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.TableName.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ImmutableBytesWritable.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyValueMatchingQualifiersFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.TableDescriptorBuilder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.TableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.SnapshotDescription.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="22456" opendate="2019-5-22 00:00:00" fixdate="2019-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Polish TestSplitTransitionOnCluster</summary>
      <description>Remove the compaction state check in TestSplitTransitionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor, as region.compact is a synchronous call, we will only return when the compaction is finished. And not sure why we do not wait for the split to finish in this test, before getting the daughter regions...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="22617" opendate="2019-6-22 00:00:00" fixdate="2019-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recovered WAL directories not getting cleaned up</summary>
      <description>While colocating the recovered edits directory with hbase.wal.dir, BASE_NAMESPACE_DIR got missed. This results in recovered edits being put in a separate directory rather than the default region directory even if the hbase.wal.dir is not overridden. Eg. if data is stored in /hbase/data/namespace/table1, recovered edits are put in  /hbase/namespace/table1. This also messes up the regular cleaner chores which never operate on this new directory and these directories will never be deleted, even for split parents or dropped tables. We should change the default back to have the base namespace directory in path.</description>
      <version>1.3.3,2.2.0,1.4.8,2.1.1,1.4.9,2.1.2,1.4.10,2.1.3,1.3.4,2.1.4,2.1.5,1.3.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRecoveredEdits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.GCRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CommonFSUtils.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.util.BackupUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22685" opendate="2019-7-12 00:00:00" fixdate="2019-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add to migration doc that meta should be healthy before upgrade</summary>
      <description>Add a note to doc that operator ensures healthy meta before upgrade.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22929" opendate="2019-8-27 00:00:00" fixdate="2019-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MemStoreLAB ChunkCreator may memory leak</summary>
      <description>We use hbase 2.1.2 with memstorelab enableRegionServer crashed case of oom I dump the heap ,found the ChunkCreator may be memory leakThe heap is 32GB, hbase.regionserver.global.memstore.size=0.4,hbase.hregion.memstore.mslab.enabled=truehbase.hregion.memstore.chunkpool.initialsize=0.5,hbase.hregion.memstore.chunkpool.maxsize=1.0BucketCache with offheap</description>
      <version>2.1.2</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScannerClosure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreLABImpl.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
