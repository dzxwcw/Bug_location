<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="1470" opendate="2009-6-1 00:00:00" fixdate="2009-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase and HADOOP-4379, dhruba&amp;#39;s flush/sync</summary>
      <description>This covers work with HADOOP-4379</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14700" opendate="2015-10-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a "permissive" mode for secure clusters to allow "simple" auth clients</summary>
      <description>When implementing HBase security for an existing cluster, it can be useful to support mixed secure and insecure clients while all client configurations are migrated over to secure authentication. We currently have an option to allow secure clients to fallback to simple auth against insecure clusters. By providing an analogous setting for servers, we would allow a phased rollout of security: First, security can be enabled on the cluster servers, with the "permissive" mode enabled Clients can be converting to using secure authentication incrementally The server audit logs allow identification of clients still using simple auth to connect Finally, when sufficient clients have been converted to secure operation, the server-side "permissive" mode can be removed, allowing completely secure operation.Obviously with this enabled, there is no effective access control, but this would still be a useful tool to enable a smooth operational rollout of security. Permissive mode would of course be disabled by default. Enabling it should provide a big scary warning in the logs on startup, and possibly be flagged on relevant UIs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,0.98.16,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestSecureRPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15720" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print row locks at the debug dump page</summary>
      <description>We had to debug cases where some handlers are holding row locks for an extended time (and maybe leak) and other handlers are getting timeouts for obtaining row locks. We should add row lock information at the debug page at the RS UI to be able to live-debug such cases.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,1.0.4,1.1.5,1.2.2,0.98.20,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSDumpServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="15801" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle for all branches</summary>
      <description>We should use the same checkstyle for all branches.</description>
      <version>1.3.0,1.2.1,1.0.3,0.98.19,1.4.0,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.0.4,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15811" opendate="2016-5-10 00:00:00" fixdate="2016-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Get after batch Put does not fetch all Cells</summary>
      <description>A big batch put followed by a batch get does not always return all Cells put. See attached test program by Robert Farr that reproduces the issue. It seems to be an issue to do with a cluster of more than one machine. Running against a single machine does not have the problem (though the single machine may have many regions). Robert was unable to make his program fail with a single machine only.I reproduced what Robert was seeing running his program. I was also unable to make a single machine fail. In a batch of 1000 puts, I see one to three Gets fail. I noticed too that if I wait a second after a fail and then re-get, the Get succeeds.</description>
      <version>1.0.0,1.3.0,1.2.1,0.98.19</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="15844" opendate="2016-5-17 00:00:00" fixdate="2016-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We should respect hfile.block.index.cacheonwrite when write intermediate index Block</summary>
      <description>BlockIndexWriter#writeIntermediateBlock if (cacheConf != null) { HFileBlock blockForCaching = blockWriter.getBlockForCaching(cacheConf); cacheConf.getBlockCache().cacheBlock(new BlockCacheKey(nameForCaching, beginOffset, true, blockForCaching.getBlockType()), blockForCaching); }The if condition should be ?if (cacheConf != null &amp;&amp; cacheConf.shouldCacheIndexesOnWrite())</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="15854" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log the cause of SASL connection failures</summary>
      <description>This is the same fix as for HADOOP-11291 to add more info during logging of SASL connection failures.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AsyncRpcChannelImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15856" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cached Connection instances can wind up with addresses never resolved</summary>
      <description>During periods where DNS is not working properly, we can wind up caching connections to master or regionservers where the initial hostname resolution and the resolution is never re-attempted. This means that clients will forever get UnknownHostException for any calls.When constructing a BlockingRpcChannelImplementation, we instantiate the InetSocketAddress to use for the connection. This instance is then used in the rpc client connection, where we check isUnresolved() and throw an UnknownHostException if that returns true. However, at this point the rpc channel is already cached in the HConnectionImplementation map of stubs. So at this point it will never be resolved.Setting the config for hbase.resolve.hostnames.on.failure masks this issue, since the stub key used is modified to contain the address. However, even in that case, if DNS fails, an rpc channel instance with unresolved ISA will still be cached in the stubs under the hostname only key.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.MetaTableLocator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientTimeouts.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AbstractRpcClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="15864" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse the testing helper to wait regions in transition</summary>
      <description>There are a bunch of unit test that do the same loop to wait for region in transitions. get rid of the duplicate code and call the helpers.</description>
      <version>1.3.0,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildOverlap.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildHole.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.handler.TestTableDeleteFamilyHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
    </fixedFiles>
  </bug>
  <bug id="15882" opendate="2016-5-23 00:00:00" fixdate="2016-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to yetus precommit 0.3.0</summary>
      <description>Now that Yetus 0.3.0 is out, we should update our precommit builds so we can use docker again.Most of the changes to our personality should be covered in the updated hbase example that ships with 0.3.0. we'll have to forward port the flakey test work.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15883" opendate="2016-5-23 00:00:00" fixdate="2016-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding WAL files and tracking offsets in HBase.</summary>
      <description>Implemented simple offset tracking for WAL files inside of an HBase table along with Unit Testing. Have not implemented queue claiming yet</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestReplicationHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15925" opendate="2016-5-31 00:00:00" fixdate="2016-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compat-module maven variable not evaluated</summary>
      <description>Looks like we've regressed on HBASE-8488. Have a look at the dependency artifacts list on http://mvnrepository.com/artifact/org.apache.hbase/hbase-testing-util/1.2.1. Notice the direct dependency's artifactId is ${compat.module}.</description>
      <version>1.0.0,1.1.0,1.2.0,1.2.1,1.0.3,1.1.5</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15948" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port "HADOOP-9956 RPC listener inefficiently assigns connections to readers"</summary>
      <description>Esteban noticed we were missing this upstream issue. Seems to make no difference in profiling but here is the patch anyways.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.AbstractTestIPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15950" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix memstore size estimates to be more tighter</summary>
      <description>While testing something else, I was loading a region with a lot of data. Writing 30M cells in 1M rows, with 1 byte values. The memstore size turned out to be estimated as 4.5GB, while with the JFR profiling I can see that we are using 2.8GB for all the objects in the memstore (KV + KV byte[] + CSLM.Node + CSLM.Index). This obviously means that there is room in the write cache that we are not effectively using.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWalAndCompactingMemStoreFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="15977" opendate="2016-6-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed variable substitution on home page</summary>
      <description>Check out the top-left of hbase.apache.org, there's an unevaluated variable $banner.name leaking through.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16031" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documents about "hbase.replication" default value seems wrong</summary>
      <description>public static final String REPLICATION_ENABLE_KEY = "hbase.replication"; public static final boolean REPLICATION_ENABLE_DEFAULT = true;The code shows that default value is true, but documents shows the default value is false.| hbase.replication| Whether replication is enabled or disabled on a given cluster| false</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16035" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested AutoCloseables might not all get closed</summary>
      <description>Subtle problem in HBASE-15891:try (A myA = new A(new B()))An exception thrown between B starting to open an A finishing initialization may not result in B being closed. A safer syntax would be:try(B myB = new B(); A myA = newA(myB))</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.log.LogLevel.java</file>
    </fixedFiles>
  </bug>
  <bug id="16056" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - fix master crash for FileNotFound</summary>
      <description>syuanjiang and tedyu reported a backup master not able to start with FileNotFound during proc-v2 lease recovery. (another restart should have solved the problem)FileNotFoundException: File does not exist: /hbase/MasterProcWALs/state-000001.lognamenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease(FSNamesystem.java:2877) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.recoverLease(NameNodeRpcServer.java:753) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.recoverLease(ClientNamenodeProtocolServerSideTranslatorPB.java:671) this may happen when the other master is still active (e.g. GC) and tries to remove files while the other master tries to become active. This operation is retryable so the code should able to handle that.</description>
      <version>1.3.0,1.2.1,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16061" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow logging to a buffered console</summary>
      <description>We've seen some stalls when logging to the ConsoleLogger that's getting redirected to a file. We should allow people to use a buffered console appender to keep from having stalls when the logging disk is busy.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="16068" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - use consts for conf properties in tests</summary>
      <description>replace the hardcoded properties string conf.set("foo.key", v) in the tests with the use of the configuration property constants that we already have</description>
      <version>1.3.0,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureReplayOrder.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16069" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo "trapsparently" in item 3 of chapter 87.2 of Reference Guide</summary>
      <description>In Chapter 87.2. Coprocessor Implementation Overview...3. Call the coprocessor from your client-side code. HBase handles the coprocessor trapsparently....Correct "trapsparently" into "transparently"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16083" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix table based replication related configs</summary>
      <description>Small style changes to make the new Table Based Replication configs match the other HBase configs</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationTableBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateHBaseImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestTableBasedReplicationSourceManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16085" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add on metric for failed compactions</summary>
      <description>Failing compactions doesn't stop the server so things can go un-noticed for quite a while if there are no metrics.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16095" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add priority to TableDescriptor and priority region open thread pool</summary>
      <description>This is in the similar area with HBASE-15816, and also required with the current secondary indexing for Phoenix. The problem with P secondary indexes is that data table regions depend on index regions to be able to make progress. Possible distributed deadlocks can be prevented via custom RpcScheduler + RpcController configuration via HBASE-11048 and PHOENIX-938. However, region opening also has the same deadlock situation, because data region open has to replay the WAL edits to the index regions. There is only 1 thread pool to open regions with 3 workers by default. So if the cluster is recovering / restarting from scratch, the deadlock happens because some index regions cannot be opened due to them being in the same queue waiting for data regions to open (which waits for RPC'ing to index regions which is not open). This is reproduced in almost all Phoenix secondary index clusters (mutable table w/o transactions) that we see. The proposal is to have a "high priority" region opening thread pool, and have the HTD carry the relative priority of a table. This maybe useful for other "framework" level tables from Phoenix, Tephra, Trafodian, etc if they want some specific tables to become online faster. As a follow up patch, we can also take a look at how this priority information can be used by the rpc scheduler on the server side or rpc controller on the client side, so that we do not have to set priorities manually per-operation.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.ExecutorType.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.EventType.java</file>
    </fixedFiles>
  </bug>
  <bug id="16101" opendate="2016-6-24 00:00:00" fixdate="2016-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Perf Tool for WAL</summary>
      <description>Add upstream tools to measure procedure perf There are 2 main thing to measure: Procedure Loading Procedure Writing</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractHBaseTool.java</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALPerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug id="16114" opendate="2016-6-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get regionLocation of required regions only for MR jobs</summary>
      <description>We should only get the location of regions required during the MR job. This will help for jobs with large regions but the job itself scans only a small portion of it. Similar changes can be seen in MultiInputFormatBase.java.</description>
      <version>1.2.1</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16177" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In dev mode thrift server can&amp;#39;t be run</summary>
      <description>Error: Could not find or load main class org.apache.hadoop.hbase.thrift2.ThriftServer</description>
      <version>1.3.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16194" opendate="2016-7-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should count in MSLAB chunk allocation into heap size change when adding duplicate cells</summary>
      <description>See more details about problem description and analysis in HBASE-16193</description>
      <version>1.2.1,1.1.5,0.98.20</version>
      <fixedVersion>1.3.0,1.1.6,0.98.21,1.2.3,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HeapMemStoreLAB.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Segment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MutableSegment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AbstractMemStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1621" opendate="2009-7-7 00:00:00" fixdate="2009-1-7 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>merge tool should work on online cluster</summary>
      <description>taking down the entire cluster to merge 2 regions is a pain, i dont see why the table or regions specifically couldnt be taken offline, then merged then brought back up.this might need a new API to the regionservers so they can take direction from not just the master.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16211" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMXCacheBuster restarting the metrics system might cause tests to hang</summary>
      <description>JMXCacheBuster restarts the metrics system. In Phoenix we are manually injecting a sink to the metric system which gets lost when we restart the metric system. See PHOENIX-3062</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.impl.JmxCacheBuster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16678" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce jobs do not update counters from ScanMetrics</summary>
      <description>Was inspecting a perf issue, where we needed the scanner metrics as counters for a MR job. Turns out that the HBase scan counters are no longer working in 1.0+. I think it got broken via HBASE-13030. These are the counters: HBase Counters BYTES_IN_REMOTE_RESULTS=0 BYTES_IN_RESULTS=280 MILLIS_BETWEEN_NEXTS=11 NOT_SERVING_REGION_EXCEPTION=0 NUM_SCANNER_RESTARTS=0 NUM_SCAN_RESULTS_STALE=0 REGIONS_SCANNED=1 REMOTE_RPC_CALLS=0 REMOTE_RPC_RETRIES=0 RPC_CALLS=3 RPC_RETRIES=0</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16682" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Shell tests failure. NoClassDefFoundError for MiniKdc</summary>
      <description>Stacktracejava.lang.NoClassDefFoundError: org/apache/hadoop/minikdc/MiniKdc at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.minikdc.MiniKdc at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1768" opendate="2009-8-14 00:00:00" fixdate="2009-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST server has upper limit of 5k PUT</summary>
      <description>This is getting in way of our uploading images to hbase.Below is what we see when big img to put.$ curl -v -T /tmp/y.row http://localhost:12041/api/jimk/row/x?column=misc:stack_testing:* About to connect() to localhost port 12041* Trying 127.0.0.1... connected* Connected to localhost (127.0.0.1) port 12041&gt; PUT /api/jimk/row/x?column=misc:stack_testing: HTTP/1.1&gt; User-Agent: curl/7.15.5 (x86_64-redhat-linux-gnu) libcurl/7.15.5 OpenSSL/0.9.8b zlib/1.2.3 libidn/0.6.5&gt; Host: localhost:12041&gt; Accept: */*&gt; Content-Length: 229886&gt; Expect: 100-continue&gt;&lt; HTTP/1.1 100 ContinueHTTP/1.1 500 Internal Server Error&lt; Content-Type: text/xml; charset=iso-8859-1&lt; Transfer-Encoding: chunked&lt; Server: Jetty(6.1.14)* Connection #0 to host localhost left intact* Closing connection #0&lt;status&gt;&lt;code&gt;500&lt;/code&gt;&lt;message&gt;org.apache.hadoop.hbase.rest.exception.HBaseRestException: XML document structures must start and end within the same entity.&lt;/message&gt;&lt;error&gt;true&lt;/error&gt;&lt;/status&gt;[stack@aa0-007-2 tmp]$</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="19075" opendate="2017-10-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task tabs on master UI cause page scroll</summary>
      <description>On the master info page, the clicking the tabs under Tasks causes the page to scroll back to the top of the page.TasksShow All Monitored Tasks Show non-RPC Tasks Show All RPC Handler Tasks Show Active RPC Calls Show Client Operations View as JSON^^ Any of thoseThe other tab-like links on the page keep the scroll in the same location.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.jamon</file>
    </fixedFiles>
  </bug>
</bugrepository>
