<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="1236" opendate="2009-3-3 00:00:00" fixdate="2009-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve readability of table descriptions in the UI</summary>
      <description>The current ruby hash style dump displayed in the UI makes it hard for a human to quickly understand the details of a given table. Improve the print out to have more layout. Due to the fact that there could be many tables and even more column families, probably use a light Javascript based open and collapse layout. I would look for example at how the webdeveloper toolbar in Firefox does that for the Javascript tab it opens.</description>
      <version>0.20.0,0.20.1,0.90.0</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.master.jsp</file>
    </fixedFiles>
  </bug>
  <bug id="12362" opendate="2014-10-28 00:00:00" fixdate="2014-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interim documentation of important master and regionserver metrics</summary>
      <description>Currently we have a section of the manual titled "Most Important RegionServer Metrics" but all it says is:Previously, this section contained a list of the most important RegionServer metrics. However, the list was extremely out of date. In some cases, the name of a given metric has changed. In other cases, the metric seems to no longer be exposed. An effort is underway to create automatic documentation for each metric based upon information pulled from its implementationIn the meantime, let's continue to maintain a list of operationally useful metrics.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12363" opendate="2014-10-28 00:00:00" fixdate="2014-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve how KEEP_DELETED_CELLS works with MIN_VERSIONS</summary>
      <description>Brainstorming...This morning in the train (of all places) I realized a fundamental issue in how KEEP_DELETED_CELLS is implemented.The problem is around knowing when it is safe to remove a delete marker (we cannot remove it unless all cells affected by it are remove otherwise).This was particularly hard for family marker, since they sort before all cells of a row, and hence scanning forward through an HFile you cannot know whether the family markers are still needed until at least the entire row is scanned.My solution was to keep the TS of the oldest put in any given HFile, and only remove delete markers older than that TS.That sounds good on the face of it... But now imagine you wrote a version of ROW 1 and then never update it again. Then later you write a billion other rows and delete them all. Since the TS of the cells in ROW 1 is older than all the delete markers for the other billion rows, these will never be collected... At least for the region that hosts ROW 1 after a major compaction.Note, in a sense that is what HBase is supposed to do when keeping deleted cells: Keep them until they would be removed by some other means (for example TTL, or MAX_VERSION when new versions are inserted).The specific problem here is that even as all KVs affected by a delete marker are expired this way the marker would not be removed if there just one older KV in the HStore.I don't see a good way out of this. In parent I outlined these four solutions:So there are three options I think: Only allow the new flag set on CFs with TTL set. MIN_VERSIONS would not apply to deleted rows or delete marker rows (wouldn't know how long to keep family deletes in that case). (MAX)VERSIONS would still be enforced on all rows types except for family delete markers. Translate family delete markers to column delete marker at (major) compaction time. Change HFileWriterV* to keep track of the earliest put TS in a store and write it to the file metadata. Use that use expire delete marker that are older and hence can't affect any puts in the file. Have Store.java keep track of the earliest put in internalFlushCache and compactStore and then append it to the file metadata. That way HFileWriterV* would not need to know about KVs.And I implemented #4.I'd love to get input on ideas.</description>
      <version>None</version>
      <fixedVersion>0.98.8,0.99.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestReversibleScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeepDeletes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanInfo.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestHColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="1360" opendate="2009-4-30 00:00:00" fixdate="2009-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>move up to Thrift 0.2.0</summary>
      <description>Move HBase thrift bits up to Thrift 0.2.0 when it is released.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.package.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">lib.libthrift-r771587.jar</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1494" opendate="2009-6-6 00:00:00" fixdate="2009-6-6 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>mapred classes is Deprecated need to use new hadoop.mapreduce package</summary>
      <description></description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hirb.rb</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="17554" opendate="2017-1-27 00:00:00" fixdate="2017-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Figure 2.0.0 Hadoop Version Support; update refguide</summary>
      <description>Refguide has hbase-2.0.0 working with 2.6.1+ and 2.7.1+ but I just tried tip of master against hadoop-2.7.3 and it fails with a netty version complaint (same as up in HADOOP-13866 which is trying to update netty for hadoop3 and 2.9?). This issue is about determining proper hadoop versions we work with when hbase2 ships.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17555" opendate="2017-1-27 00:00:00" fixdate="2017-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change calls to deprecated getHBaseAdmin to getAdmin</summary>
      <description>HBaseTestingUtil.getHBaseAdmin is deprecated and was replaced with getAdmin. Change the calls to getHBaseAdmin to getAdmin where possible.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestScannersWithLabels.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandlerWithLabels.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestRegionMover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckEncryption.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorScanPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNamespace.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestMultiVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestMetaTableAccessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestHColumnDescriptorDefaultVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestAcidGuarantees.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotClientRetries.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.MobSnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityWithCheckAuths.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDeletes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabels.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelReplicationWithExpAsString.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestWithDisabledAuthorization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestTablePermissions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestScanEarlyTermination.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCoprocessorWhitelistMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCellACLs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestSerialReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSyncUpTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSmallTests.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationChangingPeerRegionservers.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestRegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestGlobalThrottler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestLogRolling.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.throttle.TestCompactionWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestTags.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitWalDataLoss.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSCVFWithMiniCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerWithBulkload.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRemoveRegionMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPerColumnFamilyFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMobStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestJoinedScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionRandomKeying.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDeleteMobTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactionState.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestFIFOCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaThrottle.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestProcedureManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.TestMobDataBlockEncoding.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.TestExpiredMobFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.compactions.TestMobCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestWarmupRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRollingRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterRestartAfterDisablingTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestGetLastFlushedSequenceId.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTableDescriptorModificationFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTableDDLProcedureBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestEnableTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedureFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestAddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizerOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableSnapshotInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableSnapshotInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultiTableSnapshotInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestLoadAndSwitchEncodeOnDisk.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestChangingEncoding.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.fs.TestBlockReorder.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWrapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.FilterTestingCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverScannerOpenHook.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestOpenTableInCoprocessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithRemove.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.constraint.TestConstraint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestTableSnapshotScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotMetadata.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotCloneIndependence.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSizeFailures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicaWithCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMobCloneSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMetaWithReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestLeaseRenewal.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestIllegalTableDescriptor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTableMultiplexerFlushCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFastFail.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestEnableTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientOperationInterrupt.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestBlockEvictionFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBase.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroups.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestTableScan.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestTableResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestSchemaResource.java</file>
      <file type="M">hbase-archetypes.hbase-client-project.src.test.java.org.apache.hbase.archetypes.exemplars.client.TestHelloHBase.java</file>
      <file type="M">hbase-archetypes.hbase-shaded-client-project.src.test.java.org.apache.hbase.archetypes.exemplars.shaded.client.TestHelloHBase.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestBatchCoprocessorEndpoint.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestClassLoading.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorTableEndpoint.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorEndpoint.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRowProcessorEndpoint.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.Action.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.AddColumnAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeBloomFilterAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeCompressionAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeEncodingAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeSplitPolicyAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeVersionsAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.CompactMobAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.CompactRandomRegionOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.CompactTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.DecreaseMaxHFileSizeAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.FlushRandomRegionOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.FlushTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.MergeRandomAdjacentRegionsOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.MoveRandomRegionOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.MoveRegionsOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.RemoveColumnAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.SnapshotTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.SplitAllRegionOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.SplitRandomRegionOfTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.TruncateTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestIngest.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestIngestWithEncryption.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestingUtility.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestLazyCfLoading.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestManyRegions.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestRegionReplicaPerf.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mttr.IntegrationTestMTTR.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.rsgroup.IntegrationTestRSGroup.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.StripeCompactionsPerformanceEvaluation.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestTimeBoundedRequestsWithRegionReplicas.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.trace.IntegrationTestSendTraceRequests.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.client.TestRemoteTable.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.RowResourceBase.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestGzipFilter.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestMultiRowResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestNamespacesInstanceResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestNamespacesResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestScannersWithFilters.java</file>
    </fixedFiles>
  </bug>
  <bug id="18801" opendate="2017-9-12 00:00:00" fixdate="2017-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bulk load cleanup may falsely deem file deletion successful</summary>
      <description>Toward the cleanupBulkLoad() method: fs.delete(new Path(request.getBulkToken()), true);The return value from delete() call is ignore, potentially leading to file lying around after the cleanup.This applies to all branches.Discovered when investigating bulk load test failure.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1923" opendate="2009-10-20 00:00:00" fixdate="2009-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bulk incremental load into an existing table</summary>
      <description>hbase-48 is about bulk load of a new table,maybe it's more practicable to bulk load aganist a existing table.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionInfo.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.Driver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.ImmutableBytesWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.docs.src.documentation.content.xdocs.site.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19230" opendate="2017-11-9 00:00:00" fixdate="2017-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write up fixVersion policy from dev discussion in refguide</summary>
      <description>Useful discussion up on dev list "Fix version maintenance for 1.4.0" where we state fixVersion in JIRA policy we all seem to be following but have never written down anywhere. This issue is about synopsizing what came of the discussion in the refguide dev/RM section.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="19940" opendate="2018-2-5 00:00:00" fixdate="2018-2-5 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>TestMetaShutdownHandler flakey</summary>
      <description>Fails 13% of the time. One of the RS won't go down. It has an errant thread running. Not sure what.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2057" opendate="2009-12-18 00:00:00" fixdate="2009-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cluster won&amp;#39;t stop</summary>
      <description>It seems that clusters on trunk have some trouble stopping. Even manually deleting the shutdown file in ZK doesn't always help. Investigate.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ZKMasterAddressWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="20571" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMXJsonServlet generates invalid JSON if it has NaN in metrics</summary>
      <description>/jmx servlet responses invalid JSON, if some metrics are NaN: "l1CacheHitCount" : 0, "l1CacheMissCount" : 0, "l1CacheHitRatio" : NaN, "l1CacheMissRatio" : NaN, "l2CacheHitCount" : 0, "l2CacheMissCount" : 0, "l2CacheHitRatio" : 0.0, "l2CacheMissRatio" : 0.0,NaN is an invalid character sequence in JSON. We should not response NaN in metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,2.0.1,1.4.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JSONBean.java</file>
    </fixedFiles>
  </bug>
  <bug id="2109" opendate="2010-1-11 00:00:00" fixdate="2010-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>status &amp;#39;simple&amp;#39; should show total requests per second, also the requests/sec is wrong as is</summary>
      <description>status 'simple' doesnt give us aggregate load, leaving the user to add up numbers by hand. Futhermore, the per-server requests numbers are off, too high by a factor of 3 - they are using the default toString() which assumes a 1 second report rate, when the shipping default is 3 seconds.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="21091" opendate="2018-8-21 00:00:00" fixdate="2018-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Hadoop compatibility table</summary>
      <description>https://lists.apache.org/thread.html/7016d322a07e96dccdb071041c37238e43d3df4f93e9515d52ccfafc@%3Cdev.hbase.apache.org%3E covers some discussion around our Hadoop Version Compatibility table. A "leading" suggestion to make this more clear is to use a green/yellow/red (traffic-signal) style marking, instead of using specifics words/phrases (as they're often dependent on the interpretation of the reader).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21095" opendate="2018-8-22 00:00:00" fixdate="2018-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The timeout retry logic for several procedures are broken after master restarts</summary>
      <description>For TRSP, and also RTP in branch-2.0 and branch-2.1, if we fail to assign or unassign a region, we will set the procedure to WAITING_TIMEOUT state, and rely on the ProcedureEvent in RegionStateNode to wake us up later. But after restarting, we do not suspend the ProcedureEvent in RSN, and also do not add the procedure to the ProcedureEvent's suspending queue, so we will hang there forever as no one will wake us up.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21280" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add anchors for each heading in UI</summary>
      <description>On larger clusters, its annoying having to scroll down on each refresh. Anchors would help pin page to a section in UI (until our UI gets redone...)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="21281" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update bouncycastle dependency.</summary>
      <description>Looks like we still depend on bcprov-jdk16 for some x509 certificate generation in our tests. Bouncycastle has moved beyond this in 1.47, changing the artifact names.http://www.bouncycastle.org/wiki/display/JA1/Porting+from+earlier+BC+releases+to+1.47+and+laterThere are some API changes too, but it looks like we don't use any of these.It seems like we also have vestiges in the POMs from when we were depending on a specific BC version that came in from Hadoop. We now have a KeyStoreTestUtil class in HBase, which makes me think we can also clean up some dependencies.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21282" opendate="2018-10-9 00:00:00" fixdate="2018-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to latest jetty 9.2 and 9.3 versions</summary>
      <description>Looks like we have dependencies on both jetty 9.2 and 9.3, but we're lagging pretty far behind in both. We can upgrade both of these to the latest (august 2018). I'll also have to take a look at why we're using two separate versions (maybe we didn't want to switch from jetty-jsp to apache-jsp on 9.2-&gt;9.3?). Not sure if there's a good reason for this.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21284" opendate="2018-10-10 00:00:00" fixdate="2018-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forward port HBASE-21000 to branch-2</summary>
      <description>See parent for details.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.throttle.PressureAwareCompactionThroughputController.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2150" opendate="2010-1-21 00:00:00" fixdate="2010-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecated HBC(Configuration) constructor doesn&amp;#39;t call this()</summary>
      <description>While trying to port some 0.20 code, I found that HBC(Configuration) doesn't call the default constructor and thus never leads HBase ressources. This breaks compatibility.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21650" opendate="2018-12-27 00:00:00" fixdate="2018-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DDL operation and some other miscellaneous to thrift2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21682" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support getting from specific replica</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableRegionReplicasGet.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21684" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw DNRIOE when connection or rpc client is closed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.StoppedRpcClientException.java</file>
    </fixedFiles>
  </bug>
  <bug id="21685" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change repository urls to Gitbox</summary>
      <description>Moving to Gitbox is approaching and references to git-wip-us need to be changed to gitbox.Some of the Jenkins jobs are referring to git-wip-us which if going to be locked after the migration. We could move them to github so the build flow will remain intact.Previous discussion on dev@: https://lists.apache.org/thread.html/3496568d6cc002f74f5c3bcce46ed44b7ee9e90d7d53af2c65b6f785@%3Cdev.hbase.apache.org%3E After this notify INFRA to make the change</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5,1.3.4,1.2.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.main.asciidoc..chapters.rpc.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21840" opendate="2019-2-4 00:00:00" fixdate="2019-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHRegionWithInMemoryFlush fails with NPE</summary>
      <description>Found this one when testing 2.1.3.Exception in thread "PutThread" java.lang.NullPointerException at org.apache.hadoop.hbase.regionserver.CompactingMemStore.checkActiveSize(CompactingMemStore.java:392) at org.apache.hadoop.hbase.regionserver.AbstractMemStore.internalAdd(AbstractMemStore.java:307) at org.apache.hadoop.hbase.regionserver.AbstractMemStore.add(AbstractMemStore.java:132) at org.apache.hadoop.hbase.regionserver.AbstractMemStore.add(AbstractMemStore.java:112) at org.apache.hadoop.hbase.regionserver.HStore.add(HStore.java:750) at org.apache.hadoop.hbase.regionserver.HRegion.applyToMemStore(HRegion.java:4420) at org.apache.hadoop.hbase.regionserver.HRegion.access$500(HRegion.java:226) at org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation.applyFamilyMapToMemStore(HRegion.java:3479) at org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation.lambda$writeMiniBatchOperationsToMemStore$0(HRegion.java:3170) at org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation.visitBatchOperations(HRegion.java:3103) at org.apache.hadoop.hbase.regionserver.HRegion$BatchOperation.writeMiniBatchOperationsToMemStore(HRegion.java:3162) at org.apache.hadoop.hbase.regionserver.HRegion$MutationBatchOperation.writeMiniBatchOperationsToMemStore(HRegion.java:3644) at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchMutate(HRegion.java:4058) at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:3991) at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:3922) at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:3913) at org.apache.hadoop.hbase.regionserver.HRegion.batchMutate(HRegion.java:3927) at org.apache.hadoop.hbase.regionserver.HRegion.doBatchMutate(HRegion.java:4254) at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:3046)And later the test is stuck, since the MVCC can not be advanced any more.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServicesForStores.java</file>
    </fixedFiles>
  </bug>
  <bug id="2264" opendate="2010-2-24 00:00:00" fixdate="2010-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust the contrib apps to the Maven project layout</summary>
      <description>This is a follow-up patch for HBASE-2254.This patch aims only at the contrib apps to change their layout to the standard Maven project layout.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.VersionMessage.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.regionserver.transactional.TestTHLogRecovery.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.regionserver.transactional.TestTHLog.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.regionserver.transactional.DisabledTestTransactionalHLogManager.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.regionserver.transactional.DisabledTestHLogRecovery.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.client.transactional.TestTransactions.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.client.transactional.StressTestTransactions.java</file>
      <file type="M">contrib.transactional.src.test.org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.THLogRecoveryManager.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.THLogKey.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.THLog.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.CleanOldTransactionsChore.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexMaintenanceUtils.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.ipc.IndexedRegionInterface.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.UnknownTransactionException.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionState.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionScannerCallable.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionLogger.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.package.html</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.LocalTransactionLogger.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.JtaXAResource.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.HBaseBackedTransactionLogger.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.CommitUnsuccessfulException.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.UniqueIndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.SimpleIndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.package.html</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecificationArray.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecification.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexNotFoundException.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableDescriptor.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.java</file>
      <file type="M">contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTable.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestVersionResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestTableResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestStatusResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestSchemaResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestScannerResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.TestRowResource.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.Test00MiniCluster.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestVersionModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestTableSchemaModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestTableRegionModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestTableListModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestTableInfoModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestStorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestStorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestScannerModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestRowModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestCellSetModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.model.TestCellModel.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.MiniClusterTestCase.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.auth.TestJDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.auth.TestHTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.org.apache.hadoop.hbase.stargate.auth.TestHBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.VersionResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.TableResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.StorageClusterVersionResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.StorageClusterStatusResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.SchemaResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ScannerResultGenerator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ScannerInstanceResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RowSpec.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RowResultGenerator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RootResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ResourceConfig.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.RegionsResource.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.provider.producer.ProtobufMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.provider.producer.PlainTextMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.provider.JAXBContextResolver.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.VersionMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.TableSchemaMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.TableListMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.TableInfoMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.StorageClusterStatusMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.ScannerMessage.proto</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.ipc.ReplicationRegionInterface.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.package.html</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.replication.ReplicationRegion.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.replication.ReplicationRegionServer.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.replication.ReplicationSink.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.replication.ReplicationSource.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.wal.replication.ReplicationHLog.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.replication.ReplicationConnectionManager.java</file>
      <file type="M">contrib.mdc.replication.src.java.org.apache.hadoop.hbase.replication.ReplicationZookeeperHelper.java</file>
      <file type="M">contrib.mdc.replication.src.test.org.apache.hadoop.hbase.regionserver.replication.TestReplicationSink.java</file>
      <file type="M">contrib.mdc.replication.src.test.org.apache.hadoop.hbase.replication.TestReplication.java</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.auth.Authenticator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.auth.HBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.auth.HTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.auth.JDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.auth.User.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.client.Client.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.client.Cluster.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.client.Response.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.Constants.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.Main.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.CellModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.CellSetModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.ColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.ModelSchema.xsd</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.RowModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.ScannerModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.StorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.StorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.TableInfoModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.TableListModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.TableModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.TableRegionModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.TableSchemaModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.model.VersionModel.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.package.html</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.ProtobufMessageHandler.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.CellMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.CellSetMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.ColumnSchemaMessage.proto</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellSetMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ScannerMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableListMessage.java</file>
      <file type="M">contrib.stargate.src.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableSchemaMessage.java</file>
    </fixedFiles>
  </bug>
  <bug id="22642" opendate="2019-6-28 00:00:00" fixdate="2019-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make move operations of RSGroup idempotent</summary>
      <description>Currently, when moving tables or servers to a group, only groupInfo is checked. And in RSGroup implementation, groupinfo is written to disk before regions movements are done. If there are some problems caused move regions abort, some regions will be on wrong regionservers. What's the worse, retry the move operation will be rejected because of the correct groupinfo.We think when moving, not only groupInfo should be checked, but also relevant region assignments should be checked and corrected.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin2.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2276" opendate="2010-2-28 00:00:00" fixdate="2010-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hbase Shell hcd() method is broken by the replication scope parameter</summary>
      <description>Since an additional HColumnDescriptor constructor parameter (scope) was introduced, hbase shell hcd() method fails to create a HColumnDescriptor object:hbase(main):007:0&gt; alter 'doc_total_stats', {NAME =&gt; 'country_views', VERSIONS =&gt; 1}ArgumentError: wrong # of arguments(8 for 9)</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22820" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not need to persist default rs group now</summary>
      <description>As now the rs group info for a table is stored in the table metadata, we only need to store the servers of a rs group to the rs group table, so we do not need to store the default rs group anymore. The servers in default group will be refreshed automatically.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.VerifyingRSGroupAdminClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="23121" opendate="2019-10-4 00:00:00" fixdate="2019-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create-release is not pushing artifacts to repository.a.o</summary>
      <description>A commenting of a variable made it so we failed finding items just-published to local repo. Let me fix. This bug killed RC2 of hbase-thirdparty.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.release-util.sh</file>
      <file type="M">dev-support.create-release.release-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23130" opendate="2019-10-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.7 to download page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2316" opendate="2010-3-12 00:00:00" fixdate="2010-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need an ability to run shell tests w/o invoking junit</summary>
      <description>It would be nice to have an ability to run shell tests in console and see the results w/o going through junit and its test logs.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.ruby.tests.runner.rb</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="23165" opendate="2019-10-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbtop] Some modifications from HBASE-22988</summary>
      <description>Some modifications happened in the review of HBASE-22988. We can forward port them to the master branch.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.9,2.2.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.TestUtils.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.terminal.TerminalPrinterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.terminal.KeyPressTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.terminal.CursorTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.TopScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.TopScreenModelTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.PagingTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.MessageModeScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.InputModeScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.top.FilterDisplayModeScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.mode.ModeScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.help.HelpScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.screen.field.FieldScreenPresenterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.RecordTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.RecordFilterTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.UserModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.TableModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.RequestCountPerSecondTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.RegionServerModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.RegionModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.NamespaceModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.ModeTestBase.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.mode.ClientModeTest.java</file>
      <file type="M">hbase-hbtop.src.test.java.org.apache.hadoop.hbase.hbtop.field.FieldValueTest.java</file>
      <file type="M">hbase-hbtop.src.main.java.org.apache.hadoop.hbase.hbtop.screen.top.TopScreenPresenter.java</file>
      <file type="M">hbase-hbtop.src.main.java.org.apache.hadoop.hbase.hbtop.screen.field.FieldScreenView.java</file>
      <file type="M">hbase-hbtop.src.main.java.org.apache.hadoop.hbase.hbtop.RecordFilter.java</file>
      <file type="M">hbase-hbtop.src.main.java.org.apache.hadoop.hbase.hbtop.mode.RequestCountPerSecond.java</file>
      <file type="M">conf.log4j-hbtop.properties</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="2336" opendate="2010-3-17 00:00:00" fixdate="2010-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix build broken with HBASE-2334</summary>
      <description>HBASE-2334 was a bit to eager to put SLF4J in the "test" scope. Thrift needs SLF4J.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2348" opendate="2010-3-20 00:00:00" fixdate="2010-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stargate needs both JAR and WAR artifacts</summary>
      <description>Since the move to Maven we have lost the ability to have Stargate bundled as both a JAR and a WAR file (currently just the latter).I'm proposing to split the current stargate module further into 2 sub-modules 'stargate-war' and 'stargate-core' (totally up for discussion on naming). Basically moving existing sources down to 'stargate-core' and relocating the conf section into the war sub-module.I'll be doing this via a github fork to make the review and merge process easier. However I suspect I'll do the Maven side ok but totally hose the Git part, so strap yourselves in..</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResultGenerator.java</file>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.util.TestHTableTokenBucket.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestVersionResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestTableResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestStatusResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestSchemaResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestScannersWithFilters.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestScannerResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.TestRowResource.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.Test00MiniCluster.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestVersionModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableSchemaModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableRegionModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableListModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableInfoModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestStorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestStorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestScannerModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestRowModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestCellSetModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.model.TestCellModel.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.MiniClusterTestBase.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestJDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.VersionMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableSchemaMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableListMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableInfoMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.StorageClusterStatusMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.ScannerMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.ColumnSchemaMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.CellSetMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.CellMessage.proto</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.model.ModelSchema.xsd</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.VersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.util.UserData.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.util.TokenBucket.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.util.SoftUserData.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.util.HTableTokenBucket.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.User.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.TableResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterVersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterStatusResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.SchemaResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerInstanceResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowSpec.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.stargate.conf.jetty.xml</file>
      <file type="M">contrib.stargate.conf.web.xml</file>
      <file type="M">contrib.stargate.pom.xml</file>
      <file type="M">contrib.stargate.src.main.javadoc.org.apache.hadoop.hbase.stargate.package.html</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.Authenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.HBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.HTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.JDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.ZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.client.Client.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.client.Cluster.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.client.Response.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.Constants.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.Main.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.metrics.StargateMetrics.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.metrics.StargateStatistics.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.CellModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.CellSetModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.ColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.RowModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.ScannerModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.StorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.StorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.TableInfoModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.TableListModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.TableModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.TableRegionModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.TableSchemaModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.VersionModel.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ProtobufMessageHandler.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellSetMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ScannerMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableListMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableSchemaMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.VersionMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.provider.JAXBContextResolver.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.provider.producer.PlainTextMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.provider.producer.ProtobufMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RegionsResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ResourceConfig.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RootResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="2353" opendate="2010-3-22 00:00:00" fixdate="2010-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-2283 removed bulk sync optimization for multi-row puts</summary>
      <description>previously to HBASE-2283 we used to call flush/sync once per put(Put[]) call (ie: batch of commits). Now we do for every row. This makes bulk uploads slower if you are using WAL. Is there an acceptable solution to achieve both safety and performance by bulk-sync'ing puts? Or would this not work in face of atomic guarantees?discuss!</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestAcidGuarantees.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MultithreadedTestUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23549" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document steps to disable MOB for a column family</summary>
      <description>Ref guide has steps for enabling MOB on a column family but has no corresponding instructions on safely turning the feature off.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3,2.1.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase.mob.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="2364" opendate="2010-3-23 00:00:00" fixdate="2010-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ignore Deprecations during build</summary>
      <description>I'd like to propose tweaking the Maven compiler plugin definition to ignore deprecations. firstly, during a Maven build, the deprecation output is totally ignored by everyone I'm sure, and IDE's do a much better job of tracking these anyway, so the value to the output is just not there.Secondly when one has this many deprecation warnings it's actually hiding any compiler errors. I was in a conversation with 'adragomir' on IRC last night (my time) and he was bitten by this, because the ERROR level stuff is done first, but quickly scrolls off the screen and becomes hidden.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23930" opendate="2020-3-4 00:00:00" fixdate="2020-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should attempt to format `timestamp` attributes as ISO-8601</summary>
      <description>Most of the time, the timestamp: long attribute of a cell is a timestamp. The shell should make an attempt to interpret these values as timestamps and print them out as such. Current practice is to copy the value out and pass it through an external tool.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.table.rb</file>
    </fixedFiles>
  </bug>
  <bug id="24370" opendate="2020-5-14 00:00:00" fixdate="2020-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid aggressive MergeRegion and GCMultipleMergedRegionsProcedure</summary>
      <description>In https://github.com/apache/hbase/blob/a40a0322a73add68d9cb0579abacdd6a2e41e8fb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java#L478,  prepareMergeRegion, it checks if one of merged parent regions is a merged child region and has not been GCed. If it is ready to GC, it will kick off a GCMultipleMergedRegionsProcedure and also start the MergeRegionProcedure. There is a race condition here. If MergeRegionProcedure finishes first, it will delete meta row for the merged child region. Then GCMultipleMergedRegionsProcedure runs, and because the newly added check, it thinks GC has been done and wont schedule GCRegionProcedure to clean up those merged parent regions. The end result is that these merged parent regions are left as orphans on Filesystem. https://github.com/apache/hbase/blob/a40a0322a73add68d9cb0579abacdd6a2e41e8fb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/GCMultipleMergedRegionsProcedure.java#L105 The proposed solution is to avoid being so aggressive, if it needs to kick off GCMultipleMergedRegionsProcedure, then abort MergeRegionProcedure and user can try MergeRegionProcedure later.https://github.com/apache/hbase/blob/a40a0322a73add68d9cb0579abacdd6a2e41e8fb/hbase-server/src/main/java/org/apache/hadoop/hbase/master/assignment/MergeTableRegionsProcedure.java#L478  </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMetaFixer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2452" opendate="2010-4-15 00:00:00" fixdate="2010-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix our Maven dependencies</summary>
      <description>There are quite a few implicit dependencies or dependencies which could be moved to our parent's dependencyManagement section. This patch cleans all that up.There are a few dependencies where newer versions are available and trunk might be the correct place and time to upgrade if needed. Here is a list:commons-logging 1.0.4 (2004) -&gt; 1.1.1 (2007)commons-lang 2.4 (2008) -&gt; 2.5 (2010)commons-math 2.0 (2009) -&gt; 2.1 (2010)jasper-runtime 5.5.12 (2007) -&gt; 5.5.23 (2008)jetty 6.1.14 (2008) -&gt; 6.1.23 (2010) | 7.0.2 (2010)log4j 1.2.15 (2007) -&gt; 1.2.16 (2010)Stargate:commons-httpclient 3.0.1 (2006) -&gt; 3.1 (2007)hsqldb 1.8.0.10 (2008) -&gt; 1.8.1.2 (2010)</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">core.pom.xml</file>
      <file type="M">contrib.transactional.pom.xml</file>
      <file type="M">contrib.stargate.war.pom.xml</file>
      <file type="M">contrib.stargate.pom.xml</file>
      <file type="M">contrib.stargate.core.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2474" opendate="2010-4-20 00:00:00" fixdate="2010-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in 2248 - mixed version reads (not allowed by spec)</summary>
      <description>While doing a concurrent read/write test, the reader eventually gets a situation where the first column in the result set has the wrong 'value' than the rest of the result set (of 50 columns or so). The test (included) does puts of 50 columns with all the same (Random) value. The reader validates that all values are equal, and fails.</description>
      <version>0.20.4,0.20.5,0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2483" opendate="2010-4-23 00:00:00" fixdate="2010-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some tests do not use ephemeral ports</summary>
      <description>For example, seems like most of the tests bind the master to port 60000. This doesn't work on shared build machines where multiple hbase test targets might run concurrently.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2541" opendate="2010-5-13 00:00:00" fixdate="2010-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transactional contrib</summary>
      <description>It has been decided to remove the transactional/tableindexed contrib from hbase. It will live in github instead.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.pom.xml</file>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.regionserver.transactional.TestTHLogRecovery.java</file>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.regionserver.transactional.TestTHLog.java</file>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.client.transactional.TestTransactions.java</file>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.client.transactional.StressTestTransactions.java</file>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.THLogRecoveryManager.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.THLogKey.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.THLog.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.CleanOldTransactionsChore.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexMaintenanceUtils.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegionServer.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.ipc.IndexedRegionInterface.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.UnknownTransactionException.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.TransactionState.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.TransactionScannerCallable.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.TransactionLogger.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.LocalTransactionLogger.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.JtaXAResource.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.HBaseBackedTransactionLogger.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.transactional.CommitUnsuccessfulException.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.UniqueIndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.SimpleIndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecificationArray.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecification.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexNotFoundException.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexKeyGenerator.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableDescriptor.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTable.java</file>
      <file type="M">contrib.transactional.src.main.javadoc.org.apache.hadoop.hbase.client.transactional.package.html</file>
      <file type="M">contrib.transactional.src.main.javadoc.org.apache.hadoop.hbase.client.tableindexed.package.html</file>
      <file type="M">contrib.transactional.README.txt</file>
      <file type="M">contrib.transactional.pom.xml</file>
      <file type="M">contrib.transactional.bin.TableIndexed.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2542" opendate="2010-5-13 00:00:00" fixdate="2010-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fold Stargate into core</summary>
      <description>Freeze o.a.h.h.stargate and move to github for some transitional period. Replace o.a.h.h.rest with Stargate: Remove o.a.h.h.rest and related artifacts Remove Stargate WAR target (leaving only daemonized/embedded Jetty launch option) Move o.a.h.h.stargate into place as o.a.h.h.rest and pull related dependencies into core</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RootResource.java</file>
      <file type="M">core.pom.xml</file>
      <file type="M">contrib.stargate.war.src.main.webapp.WEB-INF.web.xml</file>
      <file type="M">contrib.stargate.war.src.main.webapp.WEB-INF.jetty.xml</file>
      <file type="M">contrib.stargate.war.pom.xml</file>
      <file type="M">contrib.stargate.pom.xml</file>
      <file type="M">contrib.stargate.NOTICE.txt</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.util.TestHTableTokenBucket.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestVersionResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestTableResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestStatusResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestSchemaResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestScannersWithFilters.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestScannerResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.TestRowResource.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.Test00MiniCluster.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.PerformanceEvaluation.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestVersionModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableSchemaModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableRegionModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableListModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestTableInfoModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestStorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestStorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestScannerModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestRowModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestCellSetModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.model.TestCellModel.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.MiniClusterTestBase.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.client.TestRemoteTable.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.client.TestRemoteAdmin.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestJDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHTableAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHBCAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.VersionMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableSchemaMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableListMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.TableInfoMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.StorageClusterStatusMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.ScannerMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.ColumnSchemaMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.CellSetMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.CellMessage.proto</file>
      <file type="M">contrib.stargate.core.src.main.resources.org.apache.hadoop.hbase.stargate.model.ModelSchema.xsd</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.VersionResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.util.UserData.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.util.TokenBucket.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.util.SoftUserData.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.util.HTableTokenBucket.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.User.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.TableResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterVersionResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterStatusResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.SchemaResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResultGenerator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ScannerInstanceResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RowSpec.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RowResultGenerator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">contrib.stargate.core.pom.xml</file>
      <file type="M">contrib.stargate.core.src.main.javadoc.org.apache.hadoop.hbase.stargate.package.html</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.Authenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.HBCAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.HTableAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.JDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.ZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.client.Client.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.client.Cluster.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.client.RemoteAdmin.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.client.RemoteHTable.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.client.Response.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.Constants.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ExistsResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.Main.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.metrics.StargateMetrics.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.metrics.StargateStatistics.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.CellModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.CellSetModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.ColumnSchemaModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.RowModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.ScannerModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.StorageClusterStatusModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.StorageClusterVersionModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.TableInfoModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.TableListModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.TableModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.TableRegionModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.TableSchemaModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.VersionModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ProtobufMessageHandler.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellSetMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ScannerMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableListMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableSchemaMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.VersionMessage.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.provider.JAXBContextResolver.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.provider.producer.PlainTextMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.provider.producer.ProtobufMessageBodyProducer.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RegionsResource.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ResourceConfig.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2561" opendate="2010-5-18 00:00:00" fixdate="2010-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanning .META. while split in progress yields IllegalArgumentException</summary>
      <description>Running scan '.META.' from the shell throws IllegalArgumentException if a split is running at the same time:hbase(main):004:0&gt; scan '.META.'ROW COLUMN+CELL VerifiableEditor,,127414503 column=info:regioninfo, timestamp=1274145178356, value=REGION =&gt; {NAME =&gt; 'Verifi 3318 ableEditor,,1274145033318', STARTKEY =&gt; '', ENDKEY =&gt; '-1942612687&lt;1274143362177&gt; ', ENCODED =&gt; 1741581486, OFFLINE =&gt; true, SPLIT =&gt; true, TABLE =&gt; {{NAME =&gt; 'Ver ifiableEditor', FAMILIES =&gt; [{NAME =&gt; 'info', REPLICATION_SCOPE =&gt; '0', COMPRESSI ON =&gt; 'NONE', VERSIONS =&gt; '3', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMO RY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} VerifiableEditor,,127414503 column=info:server, timestamp=1274145178356, value= 3318 ERROR: java.lang.IllegalArgumentException: offset (0) + length (8) exceed the capacity of the array: 0</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.ruby.hbase.table.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2581" opendate="2010-5-20 00:00:00" fixdate="2010-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bloom commit broke TestShell</summary>
      <description>TestShell is not passing on hudson after bloom commit.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2588" opendate="2010-5-21 00:00:00" fixdate="2010-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add easier way to ship hbase dependencies to MR cluster with job</summary>
      <description>It's a bit of a pain to ship the hbase, ZK, etc jars onto the cluster. This JIRA is to provide a nice programattic way to get these jars into the classpath of the MR child.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2625" opendate="2010-5-28 00:00:00" fixdate="2010-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make testDynamicBloom()&amp;#39;s "randomness" deterministic</summary>
      <description>Had a failure with testDynamicBloom on Hudson today. Will investigate, however it would be nice to reproduce the problem to make sure it's not the fault of my test assumptions. I plan to seed the Random number generator with the current time and print that out for post-mortem analysis.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestByteBloomFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2632" opendate="2010-5-30 00:00:00" fixdate="2010-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should autodetect terminal width</summary>
      <description>Right now the width is hardcoded, which is annoying when trying to scan wide tables, etc.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.ruby.test.helper.rb</file>
      <file type="M">src.test.ruby.shell.shell.test.rb</file>
      <file type="M">src.test.ruby.hbase.hbase.test.rb</file>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="2636" opendate="2010-6-1 00:00:00" fixdate="2010-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Jetty to 6.1.24</summary>
      <description>Jetty is the servlet container used to host the REST interface and the InfoServers. We are currently pulling version 6.1.14 but the latest version of the server component is 6.1.24. On the Solr list Yonik was suggesting they upgrade. I'll try it out and see what happens.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2650" opendate="2010-6-1 00:00:00" fixdate="2010-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Consolidate user guide style documentation</summary>
      <description>It would be great to clean up our documentation prior to the next major release. We have various bits of docs strewn throughout the JavaDoc, but it's a lot of "hidden gems" (eg the mapreduce package docs) whereas a separate "programmers guide" would be a lot better.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2661" opendate="2010-6-3 00:00:00" fixdate="2010-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test case for row atomicity</summary>
      <description>Here's a functional test case that verifies that rows are seen to be modified atomically.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2683" opendate="2010-6-7 00:00:00" fixdate="2010-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it obvious in the documentation that ZooKeeper needs permanent storage</summary>
      <description>If our users let HBase manage ZK, they probably won't bother combing through hbase-default.xml to figure that they need to set hbase.zookeeper.property.dataDir to something else than /tmp. It probably happened to deinspanjer in prod today and that's a show stopper.The fix would be, at least, to improve the Getting Started documentation to include that configuration in the "Fully-Distributed Operation" section.</description>
      <version>None</version>
      <fixedVersion>0.20.5,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.javadoc.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2694" opendate="2010-6-8 00:00:00" fixdate="2010-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RS to Master region open/close messaging into ZooKeeper</summary>
      <description>As a first step towards HBASE-2485, this issue is about changing the message flow of opening and closing of regions without actually changing the implementation of what happens on both the Master and RegionServer sides. This way we can debug the messaging changes before the introduction of more significant changes to the master architecture and handling of regions in transition.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ZKMasterAddressWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ProcessRegionStatusChange.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ProcessRegionClose.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2724" opendate="2010-6-14 00:00:00" fixdate="2010-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade guava dependency to r05</summary>
      <description>gauva r05 is out and it's in the proper maven repos. So, we don't need to point to my mvn repo anymore, and we should use the new version.http://smallwig.blogspot.com/2010/06/guava-release-05.html</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2725" opendate="2010-6-14 00:00:00" fixdate="2010-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutdown hook management is gone in trunk; restore</summary>
      <description>Shutdown hook handling is gone from trunk. Investigate (I think I did this cleaning up tests but was over-enthusiastic pruning).</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2727" opendate="2010-6-14 00:00:00" fixdate="2010-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Splits writing one file only is untenable; need dir of recovered edits ordered by sequenceid.</summary>
      <description>This issue comes of tlipcon doing a bit of human unit testing. His speculation is:Let a region X deploy to server A. Server A opens the region, then closes it.Let region X now deploy to server B. Server B now crashes.Both server A and server B now have edits for region X in their WALs.The processing of server crashes is currently sequential. If server A crashes before server B, server A will write out a file of recovered edits for region X but region X was not deployed on server A so, the file will just sit there unused. The processing of server B crash will overwrite the recovered edits file written by the split of server A wal. This is ok.But if somehow, server B processing is done before server A's, then interesting issues will likely arise; in the main, there is danger that the server B's recovered edits could be overwritten.Another issue comes up in the review of hbase-1025. During the replay of edits on region deploy, if the hosting regionserver crashes before we have processed all of the recovered edits, we could lose some (the recovery of the regionserver that is replaying the edits could overwrite the log of edits only partially replayed).Discussing up on IRC, whats needed is a directory of edits to replay ordered by sequenceid. On recovery, we play the oldest through to the newest removing the edits only on successfully replay.Making blocker on 0.21 since this is a correctness issue.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2729" opendate="2010-6-15 00:00:00" fixdate="2010-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>flushCache should write to a tmp directory and then move into the store directory</summary>
      <description>Currently it appears that internalFlushCache writes directly to the target spot of the flushed data. The finally() block appends the metadata and closes the file as if nothing bad went wrong in case of an exception. This is really bad, since it means that an IOE in the middle of flushing cache could easily write a valid looking file with only half the data, which would then prevent us from recovering those edits during log replay.Instead, it should flush to a tmp location and move it into the region dir only after it's successfully written.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2730" opendate="2010-6-15 00:00:00" fixdate="2010-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose RS work queue contents on web UI</summary>
      <description>Would be nice to be able to see the contents of the various work queues - eg to know what regions are pending compaction/split/flush/etc. This is handy for debugging why a region might be blocked, etc.</description>
      <version>None</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSDumpServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="2733" opendate="2010-6-15 00:00:00" fixdate="2010-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-2353 broke timestamp replacement on Puts when writeToWAL disabled</summary>
      <description>In refactoring for HBASE-2353, it ended up that updateKeys() was only called on KVs if writeToWAL was set to true. This caused failure of TestGetClosestRowBefore (though the real bug was in put)</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2735" opendate="2010-6-16 00:00:00" fixdate="2010-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make HBASE-2694 replication-friendly</summary>
      <description>HBASE-2694 did a good bit of reworking around ZK, and removed/changed some pieces that were needed for replication to work correctly. Mainly 2 things: listZnodes needs to offer a version that takes a Watcher, else registering yourself as a listener is too much of a pain since you then filter a lot of stuff based on the path of the event/ A lot more important, the new Multiton implemented in ZKW prevents from starting multiple clusters inside the same JVM since both masters would use the same "name".I will provide a patch for both issues.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RSZookeeperUpdater.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ZKUnassignedWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2763" opendate="2010-6-22 00:00:00" fixdate="2010-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cross-port HADOOP-6833 IPC parameter leak bug</summary>
      <description>There's a bug where any RPC call that throws an exception ends up leaking the parameter objects of that call. This was introduced by HBASE-2360</description>
      <version>0.20.5,0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2779" opendate="2010-6-23 00:00:00" fixdate="2010-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build a -src tgz to sit beside our -bin tgz when you call maven assembly:assembly</summary>
      <description>Reinstitute a patch of Paul Smiths w/ some amendements.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="28" opendate="2008-1-24 00:00:00" fixdate="2008-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] thrift put/mutateRow methods need to throw IllegalArgument exceptions</summary>
      <description>Some inputs to these methods can trigger an underlying IllegalArgumentException which needs to bubble up to a thrift IllegalArgument exception</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2808" opendate="2010-6-30 00:00:00" fixdate="2010-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the implementation of replication</summary>
      <description>From HBASE-2223, we need to provide an overview of how replication was implemented. For example: How ZK is used What are the general flows How failover works</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.package.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2010-7-6 00:00:00" fixdate="2010-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>not able to run the test suite in background because TestShell gets suspended on tty output</summary>
      <description>Can't run the test suite in background. Problem seems to be due to TestShell.This works fine:% mvn test -Dtest=TestShell -Dtest.output=trueBut:% mvn test -Dtest=TestShell -Dtest.output=true &amp; or,% mvn test -Dtest=TestShell -Dtest.output=true &gt;&amp; test.log &amp;causes test to hang, and eventually timeout after 3600 seconds.The process is reported as being suspended on tty output.&amp;#91;3&amp;#93; + Suspended (tty output) mvn test -Dtest=TestShell -Dtest.output=true</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2820" opendate="2010-7-7 00:00:00" fixdate="2010-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbck throws an error if hbase root dir isn&amp;#39;t on default FS</summary>
      <description>"Wrong FS" exception gets thrown since we construct the default FS instead of the one from rootdir</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseFsck.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2823" opendate="2010-7-8 00:00:00" fixdate="2010-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Entire Row Deletes not stored in Row+Col Bloom</summary>
      <description>If the user issues a Row Delete on an family with Row+Col blooms, that information is not currently detected by shouldSeek(). Possible known solutions are:1. adding Row as Bloom Filter Key on Row Delete, shouldSeek() should do both a Row &amp; Row+Col query for Row+Col filters.2. keep delete information in a separate storage element.#1 seems like the best solution, but need to investigate further and fix this problem.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2824" opendate="2010-7-9 00:00:00" fixdate="2010-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a filter that randomly included rows based on a configured chance.</summary>
      <description>We needed a filter that allows us to randomly include rows. I will attach a patch with a simple Filter that is able to do this.For example, if you wish to do a scan but only want 1/4 of all values (randomly distributed), you can use: new RandomRowFilter(0.25f)I just wanted to share this Filter with you. It's up to you if you want to include it in the codebase.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.filter.TestRandomRowFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2825" opendate="2010-7-9 00:00:00" fixdate="2010-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scans respect row locks</summary>
      <description>In the javadoc Package org.apache.hadoop.hbase.client Description it states that "Scans (currently) operate without respect for row locks." I think that since 0.20.4 and HBASE-2248 this is no longer the case.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.package-info.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2843" opendate="2010-7-17 00:00:00" fixdate="2010-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-add bloomfilter test over-zealously removed by HBASE-2625</summary>
      <description>I removed TestByteBloomFilter when I shouldn't have when I removed all related to unused dynamic bloomfilters. Readd.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2844" opendate="2010-7-17 00:00:00" fixdate="2010-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Capping the number of regions</summary>
      <description>It may sometimes be advantageous to prevent the number of regions from growing very large. This may happen if the values are large in size even though the number of keyvalues are not large. If the number of regions becomes too large, then it is difficult to accommodate the memstore for each region in memory. In such cases, we either have to flush out memstore to disk or decrease size of each memstore.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2846" opendate="2010-7-18 00:00:00" fixdate="2010-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make rest server be same as thrift and avro servers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="2890" opendate="2010-7-30 00:00:00" fixdate="2010-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initialize RPC JMX metrics on startup</summary>
      <description>Currently RPC call metrics are created dynamically based on cluster activity. So when monitoring via JMX, not all RPC statistics may be present in the exported MBean, depending on what the past cluster activity has been.HBASE-2146 has previously added code to initialize the MBean attributes on startup for all RPC methods, but in a way that depended on the defunct code -&gt; method name mappings.This issue is to initialize the exported MBean attributes in a cleaner way, by introspecting the RPC protocol interfaces (HMasterInterface, HMasterRegionInterface, HRegionInterface).</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2948" opendate="2010-9-1 00:00:00" fixdate="2010-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bin/hbase shell broken</summary>
      <description>hbase shell is broken after master rewrite merge:hbase(main):001:0&gt; statusERROR: undefined method `getZooKeeperWrapper' for #&lt;#&lt;Class:01x17eda64e&gt;:0x73415727&gt;Here is some help for this command: Show cluster status. Can be 'summary', 'simple', or 'detailed'. The default is 'summary'. Examples: hbase&gt; status hbase&gt; status 'simple' hbase&gt; status 'summary' hbase&gt; status 'detailed'hbase(main):001:0&gt; listTABLE ERROR: undefined method `getZooKeeperWrapper' for #&lt;#&lt;Class:01x63220fd1&gt;:0x513c952f&gt;Here is some help for this command: List all tables in hbase. Optional regular expression parameter could be used to filter the output. Examples: hbase&gt; list hbase&gt; list 'abc.*'</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="2967" opendate="2010-9-8 00:00:00" fixdate="2010-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed split: IOE &amp;#39;File is Corrupt!&amp;#39; -- sync length not being written out to SequenceFile</summary>
      <description>We saw this on one of our clusters:2010-09-07 18:07:16,229 WARN org.apache.hadoop.hbase.master.RegionServerOperationQueue: Failed processing: ProcessServerShutdown of sv4borg18,60020,1283516293515; putting onto delayed todo queuejava.io.IOException: File is corrupt! at org.apache.hadoop.io.SequenceFile$Reader.readRecordLength(SequenceFile.java:1907) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1932) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:121) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:113) at org.apache.hadoop.hbase.regionserver.wal.HLog.parseHLog(HLog.java:1493) at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1256) at org.apache.hadoop.hbase.regionserver.wal.HLog.splitLog(HLog.java:1143) at org.apache.hadoop.hbase.master.ProcessServerShutdown.process(ProcessServerShutdown.java:299) at org.apache.hadoop.hbase.master.RegionServerOperationQueue.process(RegionServerOperationQueue.java:147) at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:532)Because it was an IOE, it got requeued. Each time around we failed on it again.A few things:+ This exception needs to add filename and the position in file at which problem found.+ Need to commit little patch over in HBASE-2889 that outputs position and ordinal of wal edit because it helps diagnose these kinds of issues.+ We should be able to skip the bad edit; just postion ourselves at byte past the bad sync and start reading again+ There must be something about our setup that makes it so we fail write of the sync 16 random bytes that make up the SF 'sync' marker though oddly for one of the files, the sync failure happens at 1/3rd of the way into a 64MB wal, edit #2000 out of 130k odd edits.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2975" opendate="2010-9-9 00:00:00" fixdate="2010-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DFSClient names in master and RS should be unique</summary>
      <description>In the post-newmaster trunk, there's some code which gives the regionserver and masters fancy names based on hostname and port. This breaks log recovery, though, if the master starts recovering a log (ie has an append lease on a log file), then crashes and comes back on the same port. The NN doesn't see this as a new client, since the client name is the same, so it thinks it still holds a lease on the file. The new master, though, can't call append() because the NN thinks it's appending, so it loops forever.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2979" opendate="2010-9-10 00:00:00" fixdate="2010-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failing TestMultParrallel in hudson build</summary>
      <description>Its failing w/ a while now.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestMultiParallel.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RootRegionTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MetaNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.rolling-restart.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2984" opendate="2010-9-10 00:00:00" fixdate="2010-10-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[shell] Altering a family shouldn&amp;#39;t reset to default unchanged attributes</summary>
      <description>I changed the replication on a family that was also VERSIONS =&gt; 1 and COMPRESSION =&gt; LZO. I forgot that you have to respecify everything everytime you alter a family, so both were reset to 3 and NONE. Then the regions were compacted... and it has been splitting for about 20 minutes now. Fortunately this is our MR environment so our web site isn't affected, but it's still a major pain. Oh and also the table cannot be disabled to be re-altered since split parents are always present (I hope it'll stop splitting before midnight).The shell should use the old values for attributes that aren't changed.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3000" opendate="2010-9-15 00:00:00" fixdate="2010-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "hbase classpath" command to dump classpath</summary>
      <description>For apps that want to depend on hbase, it would be handy to have an "hbase classpath" command that dumps a string suitable for including in the classpath of the dependent application.</description>
      <version>0.90.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="3001" opendate="2010-9-15 00:00:00" fixdate="2010-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ship dependency jars to the cluster for all jobs</summary>
      <description>It would be handy if we automatically shipped dependency jars to the cluster with jobs by default. This makes it easier to run HBase without changing hadoop-env.sh on the cluster. We already have some utilities here from a previous JIRA, but it didn't get fully integrated.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.package-info.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3038" opendate="2010-9-25 00:00:00" fixdate="2010-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALReaderFSDataInputStream.getPos() fails if Filesize &gt; MAX_INT</summary>
      <description>WALReaderFSDataInputStream.getPos() uses this.in.available() to determine the actual length of the file. Except that available() returns an int instead of a long. Therefore, our current logic is broke when trying to read a split log &gt; 2GB.</description>
      <version>0.89.20100621,0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="3043" opendate="2010-9-28 00:00:00" fixdate="2010-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;hbase-daemon.sh stop regionserver&amp;#39; should kill compactions that are in progress</summary>
      <description>During rolling restarts, we'll occasionally get into a situation with our 100-node cluster where a RS stop takes 5-10 minutes. The problem is that the RS is undergoing a compaction and won't stop until it is complete. In a stop situation, it would be preferable to preempt the compaction, delete the newly-created compaction file, and try again once the cluster is restarted.</description>
      <version>0.89.20100621,0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="3058" opendate="2010-9-30 00:00:00" fixdate="2010-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix REST tests on trunk</summary>
      <description>Most of the REST tests do not pass on trunk. Most likely because configuration is being generated internally within REST classes rather than being passed in, so when tests override configs they are not getting picked up.There was a similar issue already fixed with thrift and avro.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3065" opendate="2010-10-1 00:00:00" fixdate="2010-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry all &amp;#39;retryable&amp;#39; zk operations; e.g. connection loss</summary>
      <description>The 'new' master refactored our zk code tidying up all zk accesses and coralling them behind nice zk utility classes. One improvement was letting out all KeeperExceptions letting the client deal. Thats good generally because in old days, we'd suppress important state zk changes in state. But there is at least one case the new zk utility could handle for the application and thats the class of retryable KeeperExceptions. The one that comes to mind is conection loss. On connection loss we should retry the just-failed operation. Usually the retry will just work. At worse, on reconnect, we'll pick up the expired session event. Adding in this change shouldn't be too bad given the refactor of zk corralled all zk access into one or two classes only.One thing to consider though is how much we should retry. We could retry on a timer or we could retry for ever as long as the Stoppable interface is passed so if another thread has stopped or aborted the hosting service, we'll notice and give up trying. Doing the latter is probably better than some kinda timeout.HBASE-3062 adds a timed retry on the first zk operation. This issue is about generalizing what is over there across all zk access.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestMultiParallel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitLogWorker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKSplitLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
    </fixedFiles>
  </bug>
  <bug id="3066" opendate="2010-10-1 00:00:00" fixdate="2010-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We don&amp;#39;t put the port for hregionserver up into znode since new master</summary>
      <description>Found by jd</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3070" opendate="2010-10-1 00:00:00" fixdate="2010-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add to hbaseadmin means of shutting down a regionserver</summary>
      <description>The HRegionInterface exposes stop. A couple of lines would make it so can shutdown a regionserver remotely.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3071" opendate="2010-10-1 00:00:00" fixdate="2010-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Graceful decommissioning of a regionserver</summary>
      <description>Currently if you stop a regionserver nicely, it'll put up its stopping flag and then close all hosted regions. While the stopping flag is in place all region requests are rejected. If this server was under load, closing could take a while. Only after all is closed is the master informed and it'll restart assigning (in old master, master woud get a report with list of all regions closed, in new master the zk expired is triggered and we'll run shutdown handler).At least in new master, we have means of disabling balancer, and then moving the regions off the server one by one via HBaseAdmin methods &amp;#8211; we shoud write a script to do this at least for rolling restarts &amp;#8211; but we need something better.</description>
      <version>None</version>
      <fixedVersion>0.90.3,0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="3081" opendate="2010-10-5 00:00:00" fixdate="2010-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log Splitting &amp; Replay: Distinguish between Network IOE and Parsing IOE</summary>
      <description>Originally, if HBase got an IOE from HDFS while splitting or opening a region, it would abort the operation. The assumption being that this is a network failure that will likely disappear at a later time or different partition of the network. However, if HBase gets parsing exceptions, we want to log the problem and continue opening/splitting the region anyways, because parsing is an idempotent problem and retries won't fix this issue.</description>
      <version>0.89.20100924,0.90.0</version>
      <fixedVersion>0.89.20100924,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3082" opendate="2010-10-5 00:00:00" fixdate="2010-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For ICV gets, first look in MemStore before reading StoreFiles</summary>
      <description>For incrementColumnValue operations, it is possible to check MemStore for the column being incremented without sacrificing correctness. If the column is not found in MemStore, we would then have to do a normal Get that opens/checks all StoreFiles for the given Store.In practice, this makes increment operations significantly faster for recently/frequently incremented columns.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3085" opendate="2010-10-6 00:00:00" fixdate="2010-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaResource broken on TRUNK up on HUDSON</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3090" opendate="2010-10-8 00:00:00" fixdate="2010-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t include hbase-default in conf/ assembly</summary>
      <description>We currently pack hbase-default.xml into the distribution in the conf/ dir. This is really error prone for long running installations at customer sites, since it usually ends up in a conf/ directory that survives an upgrade. Thus we carry over old defaults from previous versions of HBase or miss out on defaults for new configuration parameters. hbase-default.xml is already packed into the jar, so we should not need it in a conf dir.We should generate a documentation page from hbase-default.xml, though.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3092" opendate="2010-10-8 00:00:00" fixdate="2010-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace deprecated "new HBaseConfiguration(...)" calls</summary>
      <description>This is just a patch to replace all uses of the HBaseConfiguration with the proper HBaseConfiguration.create methods.HBASE-2996 introduced thousands of warnings about this deprecation and it was easier to replace them all in code than to find the problem in Maven</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MapFilePerformanceEvaluation.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3094" opendate="2010-10-8 00:00:00" fixdate="2010-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes for miscellaneous broken tests</summary>
      <description>So, tests have been failing so long with in particular, some tests running w/o finishing hiding behind them tests that have been broke for ages. Broken tests has let a raft of brokenness to creep in. This issue is a grab back of fixes for all those failing up on hudson.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestLoadBalancer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDeadServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseClusterTestCase.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestGetRowVersions.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="3095" opendate="2010-10-8 00:00:00" fixdate="2010-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client needs to reconnect if it expires its zk session</summary>
      <description>Clients use an HConnection down in their guts to connect to the hbase cluster. Master-is-running and root-region-location are up in zk. Setup of a new HConnection sets up a connection to ZooKeeper. If the session with ZK expires for whatever reason &amp;#8211; in tests they would expire because zk ensemble was restarted across tests or we might expire because of a long GC, well, it'll be frustrating to users if we do not just try and resetup the zk connection.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3102" opendate="2010-10-11 00:00:00" fixdate="2010-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance HBase rMetrics for Long-running Stats</summary>
      <description>Example: a useful metric to observe track is compaction count + duration. Since compactions are long running and only happen infrequently, the avg/opcount stats are should be reset longer than the default polling period (5 sec). In addition to 'hbase.period', we should allow a different duration after which long-running metrics should expire. This would also fix our existing metrics problem where min/max stats are never reset until the process is restarted/upgraded.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.metrics.MasterMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">conf.hadoop-metrics.properties</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3139" opendate="2010-10-21 00:00:00" fixdate="2010-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Server shutdown processor stuck because meta not online</summary>
      <description>Playing with rolling restart I see that the server hosting root and meta can go down close to each other. In below, note how we are processing server hosting ROOT and part of its processing involves reading .META. content to see what servers it was carrying. Well, note that .META. is offline at time (our verification attempt failed because server had just been shutdown and verification got ConnectException). So we pause the server shutdown processing till .META. comes back online &amp;#8211; only it never does.2010-10-21 07:32:23,931 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper2010-10-21 07:32:23,953 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for sv2borg182,60020,1287645693959 2010-10-21 07:32:23,994 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper2010-10-21 07:32:24,020 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Creating (or updating) unassigned node for 70236052 with OFFLINE state 2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan for -ROOT-,,0.70236052 so generated a random one; hri=-ROOT-,,0.70236052, src=, dest=sv2borg181,60020,1287646329081; 8 (online=8, exclude=null) available servers 2010-10-21 07:32:24,045 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region -ROOT-,,0.70236052 to sv2borg181,60020,1287646329081 2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1; java.net.ConnectException: Connection refused2010-10-21 07:32:24,048 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Current cached META location is not valid, resetting2010-10-21 07:32:24,079 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT- 2010-10-21 07:32:24,162 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT-2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv2borg181,60020,1287646329081, region=70236052/-ROOT- 2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 70236052; deleting unassigned node 2010-10-21 07:32:24,212 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x12bcd5b344b0115 Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED 2010-10-21 07:32:24,238 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 2010-10-21 07:32:27,902 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg183,60020,1287646347597, regionCount=0, userLoad=false 2010-10-21 07:32:30,523 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg184,60020,1287645693960] 2010-10-21 07:32:30,523 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg184,60020,1287645693960 to dead servers, submitted shutdown handler to be executed 2010-10-21 07:32:36,254 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg184,60020,1287646355951, regionCount=0, userLoad=false 2010-10-21 07:32:39,567 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg185,60020,1287645693959] 2010-10-21 07:32:39,567 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg185,60020,1287645693959 to dead servers, submitted shutdown handler to be executed 2010-10-21 07:32:45,614 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=sv2borg185,60020,1287646365304, regionCount=0, userLoad=false 2010-10-21 07:32:48,652 INFO org.apache.hadoop.hbase.zookeeper.RegionServerTracker: RegionServer ephemeral node deleted, processing expiration [sv2borg186,60020,1287645693962] 2010-10-21 07:32:48,652 DEBUG org.apache.hadoop.hbase.master.ServerManager: Added=sv2borg186,60020,1287645693962 to dead servers, submitted shutdown handler to be executed 2010-10-21 07:32:50,097 INFO org.apache.hadoop.hbase.master.ServerManager: regionservers=8, averageload=93.38, deadservers=[sv2borg185,60020,1287645693959, sv2borg183,60020,1287645693959, sv2borg182,60020,1287645693959, sv2borg184,60020,1287645693960, sv2borg186,60020,1287645693962]....We're supposed to have a thread of 5 executors to handle server shutdowns. I see an executor stuck waiting on .META. but I dont see any others running. Odd. Trying to figure why executors are 1 only."MASTER_SERVER_OPERATIONS-sv2borg180:60000-1" daemon prio=10 tid=0x0000000041dc7000 nid=0x50a4 in Object.wait() [0x00007f285d537000] java.lang.Thread.State: TIMED_WAITING (on object monitor) at java.lang.Object.wait(Native Method) at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:324) - locked &lt;0x00007f286d150ce8&gt; (a java.util.concurrent.atomic.AtomicBoolean) at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:359) at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:487) at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:115) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:150) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:619)</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.executor.TestExecutorService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3141" opendate="2010-10-21 00:00:00" fixdate="2010-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master RPC server needs to be started before an RS can check in</summary>
      <description>Starting up an RPC server is done in two steps. In the constructor, we instantiate the RPC server. Then in startServiceThreads() we start() it.If someone RPCs in between the instantiation and the start(), it seems that bad things can happen. We need to make sure this can't happen and there aren't any races here.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3163" opendate="2010-10-28 00:00:00" fixdate="2010-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If we timeout PENDING_CLOSE and send another closeRegion RPC, need to handle NSRE from RS (comes as a RemoteException)</summary>
      <description>When we send a closeRegion RPC to an RS, we are catching NSRE but when the RS is the one throwing the NSRE, then it comes back as a RemoteException (then an NSRE) and we aren't unwrapping it properly.We need to catch this and then deal with it appropriately.Still tracking how this happened in the first place.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3170" opendate="2010-10-29 00:00:00" fixdate="2010-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionServer confused about empty row keys</summary>
      <description>I'm no longer sure about the expected behavior when using an empty row key (e.g. a 0-byte long byte array). I assumed that this was a legitimate row key, just like having an empty column qualifier is allowed. But it seems that the RegionServer considers the empty row key to be whatever the first row key is.Version: 0.89.20100830, r0da2890b242584a8a5648d83532742ca7243346b, Sat Sep 18 15:30:09 PDT 2010hbase(main):001:0&gt; scan 'tsdb-uid', {LIMIT =&gt; 1}ROW COLUMN+CELL \x00 column=id:metrics, timestamp=1288375187699, value=foo \x00 column=id:tagk, timestamp=1287522021046, value=bar \x00 column=id:tagv, timestamp=1288111387685, value=qux 1 row(s) in 0.4610 secondshbase(main):002:0&gt; get 'tsdb-uid', ''COLUMN CELL id:metrics timestamp=1288375187699, value=foo id:tagk timestamp=1287522021046, value=bar id:tagv timestamp=1288111387685, value=qux 3 row(s) in 0.0910 secondshbase(main):003:0&gt; get 'tsdb-uid', "\000"COLUMN CELL id:metrics timestamp=1288375187699, value=foo id:tagk timestamp=1287522021046, value=bar id:tagv timestamp=1288111387685, value=qux 3 row(s) in 0.0550 secondsThis isn't a parsing problem with the command-line of the shell. I can reproduce this behavior both with plain Java code and with my asynchbase client.Since I don't actually have a row with an empty row key, I expected that the first get would return nothing.</description>
      <version>0.89.20100621,0.89.20100924,0.90.0,0.90.1,0.90.2,0.90.3,0.90.4,0.90.5,0.90.6,0.92.0,0.92.1</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
    </fixedFiles>
  </bug>
  <bug id="3173" opendate="2010-10-29 00:00:00" fixdate="2010-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell</summary>
      <description>HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3181" opendate="2010-10-31 00:00:00" fixdate="2010-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Review, document, and fix up Regions-in-Transition timeout logic</summary>
      <description>In some of the testing Stack and I have been doing, we've uncovered some issues with concurrent RS failure and when the Master is under heavy load. It's led to situations where we handle ZK events far after they actually occur and have uncovered some issues in our timeout logic.This jira is about reviewing the timeout semantics, especially around ZK usage, and ensuring that we handle things appropriately.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestRollingRestart.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKAssign.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3185" opendate="2010-11-1 00:00:00" fixdate="2010-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>User-triggered compactions are triggering splits!</summary>
      <description>Doh... This came in with original commit of master rewrite, not sure why it's in there.compactRegion is calling region.shouldSplit(true);</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3194" opendate="2010-11-3 00:00:00" fixdate="2010-11-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase should run on both secure and vanilla versions of Hadoop 0.20</summary>
      <description>There have been a couple cases recently of folks trying to run HBase trunk (or 0.89 DRs) on CDH3b3 or secure Hadoop. While HBase security is in the works, it currently only runs on secure Hadoop versions. Meanwhile HBase trunk won't compile on secure Hadoop due to backward incompatible changes in org.apache.hadoop.security.UserGroupInformation.This issue is to work out the minimal set of changes necessary to allow HBase to build and run on both secure and non-secure versions of Hadoop. Though, with secure Hadoop, I don't even think it's important to target running with HDFS security enabled (and krb authentication). Just allow HBase to build and run in both versions.I think mainly this amounts to abstracting usage of UserGroupInformation and UnixUserGroupInformation.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3209" opendate="2010-11-9 00:00:00" fixdate="2010-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Compaction Heuristic</summary>
      <description>We have a whole bunch of compaction awesome in our internal 0.89 branch. Porting this to 0.90:1) don't unconditionally compact 4 files. have a min threshold2) intelligently upgrade minors to majors3) new compaction algo (derived in HBASE-2462 )</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
    </fixedFiles>
  </bug>
  <bug id="3211" opendate="2010-11-9 00:00:00" fixdate="2010-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Key (Index) Only Fetches</summary>
      <description>When you retrieve data from HBase you get Key (Row+Column+Timestamp) + Values. It would be nice to have a mode where we only fetch the keys (i.e. the index) but not the values.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestKeyValue.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.filter.TestFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3212" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More testing of enable/disable uncovered base condition not in place; i.e. that only one enable/disable runs at a time</summary>
      <description>Testing, uncovered fact that master has 3 handlers currently for enable/disable/delete and modify when hbase-3112 was built on supposition that there was only one.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3213" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If do abort of backup master will get NPE instead of graceful abort</summary>
      <description>Doesn't really matter because it's aborting the server anyways, but I've seen it on TestMasterFailover. Simple fix.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3214" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS is failing</summary>
      <description>Failing on hudson and locally</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3219" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split parents are reassigned on restart and on disable/enable</summary>
      <description>J-D found this nice bug testing.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3221" opendate="2010-11-10 00:00:00" fixdate="2010-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race between splitting and disabling</summary>
      <description>There's a race when you disable a table, if one of the region servers started the split before the call and it reports after the master scanned the .META. regions, then you're not disabling the daughter regions. I see this in the master log:2010-11-10 15:29:35,990 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Attemping to disable table TestTable2010-11-10 15:29:35,996 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Offlining 2 regions....2010-11-10 15:29:39,014 INFO org.apache.hadoop.hbase.master.handler.DisableTableHandler: Disabled table is done=true2010-11-10 15:29:39,105 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,,1289431761746.9de9305168be83098273e083f24ea5d8.: Daughters; TestTable,,1289431775593.a08b127a61b89e268129aa022fd18ce1., TestTable,0001037720,1289431775593.4a5f831723ffbdb859d45510742d9926. from hbasedev,60020,1289431673756...</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3222" opendate="2010-11-11 00:00:00" fixdate="2010-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver region listing in UI is no longer ordered.</summary>
      <description>J-D spotted this one.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.regionserver.regionserver.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3223" opendate="2010-11-11 00:00:00" fixdate="2010-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get VersionInfo for Running HBase Process</summary>
      <description>bin/hbase VersionInfo is a great existing utility to provide version info about Hbase jar files. Unfortunately, there is no way to currently get this information for the running process. For this jira, add an easy/quick way to see verify the rev of the running jar.We got recently bit internally because our running jar was a different version from the jar that we had recently pushed and caused havoc on our cluster. This problem is more important to fix now that we have rolling upgrades and will regularly have cluster scenarios with mixed-version RSs.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.metrics.MasterMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.MetricsMBeanBase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3224" opendate="2010-11-11 00:00:00" fixdate="2010-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in KeyValue$KVComparator.compare when compacting</summary>
      <description>While testing normal insertion via PE, I got this recurrent NPE coming out of KeyValue$KVComparator.compare while it's compacting. So far I saw 2 different stack traces:java.lang.NullPointerException at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1356) at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:250) at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385) at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291) at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324) at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926) at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)java.lang.NullPointerException at org.apache.hadoop.hbase.KeyValue$KVComparator.compare(KeyValue.java:1375) at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:180) at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:156) at org.apache.hadoop.hbase.regionserver.KeyValueHeap$KVScannerComparator.compare(KeyValueHeap.java:146) at java.util.PriorityQueue.siftUpUsingComparator(PriorityQueue.java:594) at java.util.PriorityQueue.siftUp(PriorityQueue.java:572) at java.util.PriorityQueue.offer(PriorityQueue.java:274) at java.util.PriorityQueue.add(PriorityQueue.java:251) at org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:258) at org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:385) at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:291) at org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:324) at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:926) at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:734)</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">src.docbkx.book.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3230" opendate="2010-11-12 00:00:00" fixdate="2010-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refresh our hadoop jar and update zookeeper to just-released 3.3.2.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3231" opendate="2010-11-12 00:00:00" fixdate="2010-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to zookeeper 3.3.2.</summary>
      <description>I tried updating zk over in HBASE-3230 but TestHQuorum.... is failing.</description>
      <version>None</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.zookeeper.TestHQuorumPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3233" opendate="2010-11-13 00:00:00" fixdate="2010-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Long Running Stats</summary>
      <description>HBASE-3102 has a small bug. Once long running stats are reset, the reset flag is never cleared. This is a one-line fix for this issue. Verified on our test clusters.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.PersistentMetricsTimeVaryingRate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3234" opendate="2010-11-14 00:00:00" fixdate="2010-11-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hdfs-724 "breaks" TestHBaseTestingUtility multiClusters</summary>
      <description>We upgraded our hadoop jar in TRUNK to latest on 0.20-append branch. TestHBaseTestingUtility started failing reliably. If I back out hdfs-724, the test passes again. This issue is about figuring whats up here.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3237" opendate="2010-11-16 00:00:00" fixdate="2010-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split request accepted -- BUT CURRENTLY A NOOP</summary>
      <description>The "split" button from the web UI displays this message and indeed seems to do nothing.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3243" opendate="2010-11-17 00:00:00" fixdate="2010-1-17 01:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Disable Table closed region on wrong host</summary>
      <description>I ran some YCSB benchmarks which resulted in about 150 regions worth of data overnight. Then I disabled the table, and the master for some reason closed one region on the wrong server. The server ignored this, but the region remained open on a different server, which later flipped out when it tried to flush due to hlog accumulation.</description>
      <version>0.90.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="3269" opendate="2010-11-23 00:00:00" fixdate="2010-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase table truncate semantics seems broken as "disable" table is now async by default.</summary>
      <description>The new async design for disable table seems to have caused a side effect on the truncate command. (IRC chat with jdcryans)Apparent Cause: "Disable" is now async by default. When truncate is called, the disable operation returns immediately and when the drop is called, the disable operation is still not completed. This results in HMaster.checkTableModifiable() throwing a TableNotDisabledException.With earlier versions, disable returned only after Table was disabled.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="3270" opendate="2010-11-23 00:00:00" fixdate="2010-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When we create the .version file, we should create it in a tmp location and then move it into place</summary>
      <description>Todd suggests over in HBASE-3258 that writing hbase.version, we should write it off in a /tmp location and then move it into place after writing it to protect against case where file writer crashes between creation and write.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="3277" opendate="2010-11-24 00:00:00" fixdate="2010-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Shell zk_dump command broken</summary>
      <description>HBase shell zk_dump command is broken. Typing "zk_dump" in the shell gives: hbase(main):002:0&gt; zk_dumpERROR: undefined method `dump' for #&lt;Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0xd6ee28&gt;Here is some help for this command:Dump status of HBase cluster as seen by ZooKeeper.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3279" opendate="2010-11-25 00:00:00" fixdate="2010-11-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[rest] Filter for gzip/deflate content encoding that wraps both input and output side</summary>
      <description>After HBASE-3275 the REST gateway will return gzip or deflate encoded content to the client if the client requested it using the appropriate Accept-Encoding header. However Jetty's GzipFilter only wraps output side processing. A client can submit gzip or deflate encoded requests (i.e. Content-Encoding: gzip ; Content-Type: ...) but the data is not decoded, it is simply passed through. Implement a filter that also wraps input side processing, so clients can submit compressed PUT or POST bodies.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.HBaseRESTTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.Response.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3280" opendate="2010-11-29 00:00:00" fixdate="2010-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>YouAreDeadException being swallowed in HRS getMaster()</summary>
      <description>In the HRS, when we lose our connection to the master, we enter into a loop where we keep trying to get the new master location in ZK and attempt to send our heartbeat. Within tryRegionServerReport() we could get a YouAreDeadException, but we won't let it out. This leads to the RS continuously heartbeating in to the master although the master keeps telling it to kill itself.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3285" opendate="2010-11-29 00:00:00" fixdate="2010-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hlog recovery takes too much time</summary>
      <description>Currently HBase uses append to trigger the close of HLog during Hlog split. Append is a very expensive operation, which involves not only NameNode operations but creating a writing pipeline. If one of datanodes on the pipeline has a problem, this recovery may takes minutes. I'd like implement a lightweight NameNode operation to trigger lease recovery and make HBase to use this instead.</description>
      <version>None</version>
      <fixedVersion>0.90.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3286" opendate="2010-11-30 00:00:00" fixdate="2010-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master passes IP and not hostname back to region server</summary>
      <description>Starting my little test cluster on the latest from 0.90, I see:2010-11-29 23:21:34,131 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true2010-11-29 23:21:34,134 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 22 region(s) to sv2borg181,61020,12910728862822010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 24 region(s) to sv2borg182,61020,12910728854732010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 37 region(s) to sv2borg183,61020,12910728856462010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 25 region(s) to sv2borg184,61020,12910728867342010-11-29 23:21:34,135 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 26 region(s) to sv2borg185,61020,12910728866062010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 70 region(s) to sv2borg186,61020,12910728854862010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 30 region(s) to sv2borg187,61020,12910728863552010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 89 region(s) to sv2borg188,61020,12910728859262010-11-29 23:21:34,136 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 701 region(s) to sv2borg189,61020,1291072886739After another restart:2010-11-30 00:03:38,100 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 1024 region(s) across 9 server(s), retainAssignment=true2010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg181,61020,12910754099842010-11-30 00:03:38,103 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 82 region(s) to sv2borg182,61020,12910754099562010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 61 region(s) to sv2borg183,61020,12910754099522010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 122 region(s) to sv2borg184,61020,12910754099572010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 59 region(s) to sv2borg185,61020,12910754099552010-11-30 00:03:38,104 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 71 region(s) to sv2borg186,61020,12910754099632010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 52 region(s) to sv2borg187,61020,12910754110492010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 254 region(s) to sv2borg188,61020,12910754103602010-11-30 00:03:38,105 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 252 region(s) to sv2borg189,61020,1291075409959I also saw one time where everything was assigned to 189.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerAddress.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3287" opendate="2010-11-30 00:00:00" fixdate="2010-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to cache blocks on hfile write and evict blocks on hfile close</summary>
      <description>This issue is about adding configuration options to add/remove from the block cache when creating/closing files. For use cases with lots of flushing and compacting, this might be desirable to prevent cache misses and maximize the effective utilization of total block cache capacity.The first option, hbase.rs.cacheblocksonwrite, will make it so we pre-cache blocks as we are writing out new files.The second option, hbase.rs.evictblocksonclose, will make it so we evict blocks when files are closed.</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.TestHalfStoreFileReader.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekTo.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestReseekTo.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileSeek.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFilePerformance.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.RandomSeek.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HFilePerformanceEvaluation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.SimpleBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3290" opendate="2010-11-30 00:00:00" fixdate="2010-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Max Compaction Size</summary>
      <description>Add ability to specify a maximum storefile size for compaction. After this limit, we will not include this file in compactions. This is useful for large object stores and clusters that pre-split regions.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSelection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3291" opendate="2010-12-1 00:00:00" fixdate="2010-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If split happens while regionserver is going down, we can stick open.</summary>
      <description>J-D found this one testing. He found that if a split comes in during shutdown of a regionserver, then the regionserver can stick open... and won't go down.We fixed a similar problem in the past where if balancer cut in during shutdown and assigned a regionserver an region during shutdown, we'd open it and it'd cause us again to stick open. We fixed that by introducing the 'closing' state.Fix for the issue j-d found is to do closing check when onlining daughters.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3292" opendate="2010-12-1 00:00:00" fixdate="2010-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose block cache hit/miss/evict counts into region server metrics</summary>
      <description>Right now only the hit ratio is exposed into the rs metrics. This value tends to change very slowly and hardly at all once the cluster has been up for some time.We should expose the aggregate hit/miss/evict counts so you can more effectively see how things change over time.</description>
      <version>None</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3295" opendate="2010-12-1 00:00:00" fixdate="2010-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dropping a 1k+ regions table likely ends in a client socket timeout and it&amp;#39;s very confusing</summary>
      <description>I tried truncating a 1.6k regions table from the shell and, after the usual disabling timeout, I then got a socket timeout on the second invocation while it was dropping. It looked like this:ERROR: java.net.SocketTimeoutException: Call to sv2borg180/10.20.20.180:61000 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.20.20.180:59153 remote=sv2borg180/10.20.20.180:61000]At first I thought that was coming from the master because HDFS was somehow slow, but then understood that it was my socket that timed out meaning that the master was still dropping the table. Calling truncate again, I got:ERROR: Unknown table TestTable!Which means that the table would be deleted... I learned later that it wasn't totally deleted after I shut down the cluster. So it leaves me in a situation where I have to manually delete the files on the FS and the remaining .META. entries.Since I expect a few people will hit this issue rather soon, for 0.90.0, I propose we just set the socket timeout really high in the shell. For 0.90.1, or 0.92, we should do for drop what we do for disabling.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3296" opendate="2010-12-1 00:00:00" fixdate="2010-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Newly created table ends up disabled instead of assigned</summary>
      <description>Something that was seen by someone on the channel yesterday and by me this morning, it's possible to create a table that ends up disabled and the 'create' calls times out. The master log looks like:2010-12-01 19:32:52,350 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: HLog configuration: blocksize=64 MB, rollsize=60.8 MB, enabled=true, flushlogentries=1, optionallogflushinternal=1000ms2010-12-01 19:32:52,450 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-2002010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: New hlog /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs/hlog.12912319723502010-12-01 19:32:52,451 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Using getNumCurrentReplicas--HDFS-8262010-12-01 19:32:52,452 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.2010-12-01 19:32:52,645 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.; next sequenceid=12010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Added region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368. to META2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.: disabling compactions &amp; flushes2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.Store: closed info2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer interrupted while waiting for sync requests2010-12-01 19:32:52,784 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: IPC Server handler 4 on 61000.logSyncer exiting2010-12-01 19:32:52,784 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: closing hlog writer in hdfs://sv2borg180:9100/hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.logs2010-12-01 19:32:52,908 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Moved 1 log files to /hbase/TestTable/2e2099cd5fce907e670ce8596d9b2368/.oldlogs2010-12-01 19:32:52,966 INFO org.apache.hadoop.hbase.master.AssignmentManager: Table TestTable disabled; skipping assign of TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Table being disabled so deleting ZK node and removing from regions in transition, skipping assignment of region TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.2010-12-01 19:32:52,967 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:61000-0x12c84725b4b00b6 Deleting existing unassigned node for 2e2099cd5fce907e670ce8596d9b2368 that is in expected state RS_ZK_REGION_CLOSED2010-12-01 19:32:52,968 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Tried to delete closed node for REGION =&gt; {NAME =&gt; 'TestTable,,1291231972140.2e2099cd5fce907e670ce8596d9b2368.', STARTKEY =&gt; '', ENDKEY =&gt; '', ENCODED =&gt; 2e2099cd5fce907e670ce8596d9b2368, TABLE =&gt; {{NAME =&gt; 'TestTable', FAMILIES =&gt; [{NAME =&gt; 'info', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} but it does not exist so just offlining</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3301" opendate="2010-12-2 00:00:00" fixdate="2010-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Treat java.net.SocketTimeoutException same as ConnectException assigning/unassigning regions.</summary>
      <description>I saw java.net.SocketTimeoutException on my cluster this evening.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3303" opendate="2010-12-2 00:00:00" fixdate="2010-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower hbase.regionserver.handler.count from 25 back to 10</summary>
      <description>With HBASE-2506 in mind, I tested a low-memory environment (2GB of heap) with a lot of concurrent writers using the default write buffer to verify if a lower number of handlers actually helps reducing the occurrence full GCs. Very unscientifically, at this moment I think it's safe to say that yes, it helps.With the defaults, I saw a region server struggling more and more because the random inserters at some point started filling up all the handlers and were all BLOCKED trying to sync the WAL. It's safe to say that each of those clients carried a payload that the GC cannot get rid of and it's one that we don't account for (as opposed to MemStore and the block cache).With a much lower setting of 5, I didn't see the situation.It kind of confirms my hypothesis but I need to do more proper testing. In the mean time, in order to lower the onslaught of users that write to the ML complaining about either GCs or OOMEs, I think we should set the handlers back to what it was originally (10) for 0.90.0 and add some documentation about configuring hbase.regionserver.handler.countI'd like to hear others' thoughts.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3332" opendate="2010-12-10 00:00:00" fixdate="2010-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regions stuck in transition after RS failure</summary>
      <description>Testing 0.90rc2 I ran into this issue. The test scenario was to kill -9 the server hosting ROOT and META, and before it had been detected, run "balancer" from the shell. After logs were split and regions were reassigned, I ended up with some regions stuck in transition.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3334" opendate="2010-12-10 00:00:00" fixdate="2010-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refresh our hadoop jar because of HDFS-1520</summary>
      <description>HDFS-1520 adds a new lightweight lease recovery mechanism, but also bumped the protocol's version to 42 which means that currently 0.90 doesn't work on a clean checkout of 0.20-append.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3335" opendate="2010-12-10 00:00:00" fixdate="2010-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add BitComparator for filtering</summary>
      <description>There are many uses cases for setting bits flags and being able filter on those seems like a good idea.The BitComparator would take a byte[] and a bitwise operator for its constructor. It would perform the bitwise operation on the column value bytes with the specified byte[]. Then return whether the result was non-zero. Only the CompareOp.EQUAL and CompareOp.NOT_EQUAL would make sense with this comparator.The binary bitwise operators that apply are AND, OR, and XOR.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3343" opendate="2010-12-13 00:00:00" fixdate="2010-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Server not shutting down after losing log lease</summary>
      <description>Ran into this bug testing 0.90rc2. I kill -STOPed a server, and then -CONT it after its logs had been split. It correctly decided it should abort, but got stuck during the shutdown process.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3348" opendate="2010-12-13 00:00:00" fixdate="2010-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Observers to completely override base function</summary>
      <description>Currently an observer can act as a filter or translator but cannot stop a subsequent call down to the base method for get, put, delete, etc. This means an observer cannot completely override the base function. To deal with this we can: Change the preXXX methods to return the same type as the postXXX methods, the same return type of the base method. Extend Coprocessor.Environment with methods that get/set a "should continue" flag.The framework should check the "should continue" flag before calling the base method. If not, just return what was returned by the preXXX method.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverStacking.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorEnvironment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserverCoprocessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3349" opendate="2010-12-13 00:00:00" fixdate="2010-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass HBase configuration to HttpServer</summary>
      <description>When we construct Hadoop's HttpServer, we don't pass an HBaseConfiguration to it. So, the new ConfServlet in Hadoop trunk (and CDH) doesn't show HBase's configuration parameters.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3352" opendate="2010-12-14 00:00:00" fixdate="2010-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enabling a non-existent table from shell prints no error</summary>
      <description>hbase(main):001:0&gt; enable 'testtable'0 row(s) in 0.3120 secondsOnly thing is that I don't have a table called 'testtable'</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3362" opendate="2010-12-15 00:00:00" fixdate="2010-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If .META. offline between OPENING and OPENED, then wrong server location in .META. is possible</summary>
      <description>This is a good one. It happened to me testing OOME in split logging. Balancer moves region to new location, regionservrer X. New location regionserver X successfully opens the region and then goes to update .META. At this point, the server carrying .META. crashes. Regionserver X is stuck waiting on .META. to come back online. It takes so long master times out the region-in-transition Master assigns the region elsewhere to regionserver Y It opens successfully on regionserver Y and then it also parks waiting on .META. coming online .META. comes online The two servers X and Y race to update .META.I saw case where server X edit went in after server Ys edit which means that lookups in .META. get the wrong server. HBCK can detect this situation.RegionServer X when it wakes up coreeclty notices that its lost control of the region but the damage is done &amp;#8211; where damage is .META. edit.Chatting with Jon, he suggested that regionserver X should 'rollback' the .META. edit &amp;#8211; do explicit delete of what it added. This would work I think but chatting more, I'll make a fix that keeps updating the zookeeper OPENING state while edit goes on in a separate thread. Our continuous setting of OPENING will make it so region-in-transition does not timeout.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3363" opendate="2010-12-15 00:00:00" fixdate="2010-5-15 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>ReplicationSink should batch delete</summary>
      <description>Now that it is possible to multi delete, it should be integrated in ReplicationSink.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.replication.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3368" opendate="2010-12-16 00:00:00" fixdate="2010-3-16 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Split message can come in before region opened message; results in &amp;#39;Region has been PENDING_CLOSE for too long&amp;#39; cycle</summary>
      <description>Another good one. Look at these excerpts from master log:2010-12-16 00:49:45,749 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: TestTable,0078922610,1292373363753.490b382bae33642d12cd717b5785698b.: Daughters; TestTable,0078922610,1292460584999.c8b95dfc9a671083bafdaa0341279777., TestTable,0078933586, 1292460584999.7cc636c9a7274eec4e784df2efebbca3. from XXX185,60020,1292460570976....2010-12-16 00:49:46,132 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region TestTable,0078922610,1292373363753.490b382bae33642d12cd717b5785698b. on XXX185,60020,1292460570976... so the split will have cleared the parent from in-memory data structures and then the open handler will add them back (though region is offlined, split).Then the balancer runs....... only no one is holding the region thats being balanced.Over on XXX185 I see the open and then split at these times:2010-12-16 00:49:43,740 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened TestTable,0078922610,1292373363753.490b382bae33642d12cd717b5785698b......2010-12-16 00:49:45,003 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region TestTable,0078922610,1292373363753.490b382bae33642d12cd717b5785698b.So, the fact that it takes the Master a while to get around to the zk watcher processing messes us up.Root problem is that we're using two different message buses, zk and then heartbeat. Intent is to do all over zk and remove hearbeat but looking at what to do for 0.90.0.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3393" opendate="2010-12-26 00:00:00" fixdate="2010-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Avro gateway to use Avro 1.4.1 and the new server.join() method</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.package.html</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.hbase.genavro</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.HBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ATimeRange.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ATableExists.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ATableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AServerLoad.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AServerInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AServerAddress.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AScan.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AResultEntry.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AResult.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ARegionLoad.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.APut.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AMasterNotRunning.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AIOError.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AIllegalArgument.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AGet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AFamilyDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ADelete.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.ACompressionAlgorithm.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AColumnValue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AColumn.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.generated.AClusterStatus.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.AvroUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.AvroServer.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3402" opendate="2010-12-30 00:00:00" fixdate="2010-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web UI shows two META regions</summary>
      <description>Running 0.90@r1052112 I see two regions for META on the same server. Both have start key '' and end key ''.Things seem to work OK, but it's very strange.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3403" opendate="2010-12-30 00:00:00" fixdate="2010-1-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region orphaned after failure during split</summary>
      <description>ERROR: Region hdfs://haus01.sf.cloudera.com:11020/hbase-normal/usertable/2ad8df700eea55f70e02ea89178a65a2 on HDFS, but not listed in META or deployed on any region server.ERROR: Found inconsistency in table usertableNot sure how I got into this state, will look through logs.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3418" opendate="2011-1-5 00:00:00" fixdate="2011-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment operations can break when qualifiers are split between memstore/snapshot and storefiles</summary>
      <description>Doing investigation around some observed resetting counter behavior.An optimization was added to check memstore/snapshots first and then check storefiles if not all counters were found. However it looks like this introduced a bug when columns for a given row/family in a single increment operation are spread across memstores and storefiles.The results from get operations on both memstores and storefiles are appended together but when processed are expected to be fully sorted. This can lead to invalid results.Need to sort the combined result of memstores + storefiles.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3419" opendate="2011-1-5 00:00:00" fixdate="2011-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If re-transition to OPENING during log replay fails, server aborts. Instead, should just cancel region open.</summary>
      <description>The Progressable used on region open to tickle the ZK OPENING node to prevent the master from timing out a region open operation will currently abort the RegionServer if this fails for some reason. However it could be "normal" for an RS to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open.We had a cluster trip over some other issue (for some reason, the tickle was not happening in &lt; 30 seconds, so master was timing out every time). Because of the abort on BadVersion, this eventually led to every single RS aborting itself eventually taking down the cluster.</description>
      <version>0.90.0,0.92.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3421" opendate="2011-1-5 00:00:00" fixdate="2011-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Very wide rows -- 30M plus -- cause us OOME</summary>
      <description>From the list, see 'jvm oom' in http://mail-archives.apache.org/mod_mbox/hbase-user/201101.mbox/browser, it looks like wide rows &amp;#8211; 30M or so &amp;#8211; causes OOME during compaction. We should check it out. Can the scanner used during compactions use the 'limit' when nexting? If so, this should save our OOME'ing (or, we need to add to the next a max size rather than count of KVs).</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3422" opendate="2011-1-5 00:00:00" fixdate="2011-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Balancer will try to rebalance thousands of regions in one go; needs an upper bound added.</summary>
      <description>See HBASE-3420. Therein, a wonky cluster had 5k regions on one server and &lt; 1k on others. Balancer ran and wanted to redistribute 3k+ all in one go. Madness.If a load of rebalancing to be done, should be done somewhat piecemeal. We need maximum regions to rebalance at a time upper bound at a minimum.</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3431" opendate="2011-1-8 00:00:00" fixdate="2011-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver is not using the name given it by the master; double entry in master listing of servers</summary>
      <description>Our man Ted Dunning found the following where RS checks in with one name, the master tells it use another name but we seem to go ahead and continue with our original name.In RS logs I see:2011-01-07 15:45:50,757 INFO org.apache.hadoop.hbase.regionserver.HRegionServer [regionserver60020]: Master passed us address to use. Was=perfnode11:60020, Now=10.10.30.11:60020On master I see2011-01-07 15:45:38,613 INFO org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 0 on 60000]: Registering server=10.10.30.11,60020,1294443935414, regionCount=0, userLoad=false....then later2011-01-07 15:45:44,247 INFO org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 2 on 60000]: Registering server=perfnode11,60020,1294443935414, regionCount=0, userLoad=trueThis might be since we started letting servers register in other than with the reportStartup.</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3440" opendate="2011-1-13 00:00:00" fixdate="2011-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean out load_table.rb and make sure all roads lead to completebulkload tool</summary>
      <description>Up on list Vidhya tried using load_table.rb with 0.90 and new master and it don't work any more now we assign differently. Clean out this script. Make sure all doc points at completebulkload tool instead.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.loadtable.rb</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3443" opendate="2011-1-13 00:00:00" fixdate="2011-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ICV optimization to look in memstore first and then store files (HBASE-3082) does not work when deletes are in the mix</summary>
      <description>For incrementColumnValue() HBASE-3082 adds an optimization to check memstores first, and only if not present in the memstore then check the store files. In the presence of deletes, the above optimization is not reliable.If the column is marked as deleted in the memstore, one should not look further into the store files. But currently, the code does so.Sample test code outline:admin.createTable(desc)table = HTable.new(conf, tableName)table.incrementColumnValue(Bytes.toBytes("row"), cf1name, Bytes.toBytes("column"), 5);admin.flush(tableName)sleep(2)del = Delete.new(Bytes.toBytes("row"))table.delete(del)table.incrementColumnValue(Bytes.toBytes("row"), cf1name, Bytes.toBytes("column"), 5);get = Get.new(Bytes.toBytes("row"))keyValues = table.get(get).raw()keyValues.each do |keyValue| puts "Expect 5; Got Value=#{Bytes.toLong(keyValue.getValue())}";endThe above prints:Expect 5; Got Value=10</description>
      <version>0.90.0,0.90.1,0.90.2,0.90.3,0.90.4,0.90.5,0.90.6,0.92.0,0.92.1</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="3444" opendate="2011-1-14 00:00:00" fixdate="2011-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to prove Bytes.toBytesBinary and Bytes.toStringBinary() is reversible</summary>
      <description>Bytes.toStringBinary() doesn't escape \.Otherwise the transformation isn't reversiblebyte[] a = {'\', 'x' , '0', '0'}Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
    </fixedFiles>
  </bug>
  <bug id="3446" opendate="2011-1-17 00:00:00" fixdate="2011-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ProcessServerShutdown fails if META moves, orphaning lots of regions</summary>
      <description>I ran a rolling restart on a 5 node cluster with lots of regions, and afterwards had LOTS of regions left orphaned. The issue appears to be that ProcessServerShutdown failed because the server hosting META was restarted around the same time as another server was being processed</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.ruby.shell.shell.test.rb</file>
      <file type="M">src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestMergeTable.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestMetaMigration.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.catalog.TestMetaReaderEditor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.catalog.TestCatalogTracker.java</file>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MetaNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerServices.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3449" opendate="2011-1-17 00:00:00" fixdate="2011-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Server shutdown handlers deadlocked waiting for META</summary>
      <description>I have a situation where both of my MASTER_META_SERVER_OPERATIONS handlers are handling server shutdowns, and both of them are waiting on ROOT, which isn't coming up. Unclear exactly how this happened, but I triggered it by doing a rolling restart.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3465" opendate="2011-1-23 00:00:00" fixdate="2011-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hbase should use a HADOOP_HOME environment variable if available.</summary>
      <description>I have been burned a few times lately while developing code by having the make sure that the hadoop jar in hbase/lib is exactly correct. In my own deployment, there are actually 3 jars and a native library to keep in sync that hbase shouldn't have to know about explicitly. A similar problem arises when using stock hbase with CDH3 because of the security patches changing the wire protocol.All of these problems could be avoided by not assuming that the hadoop library is in the local directory. Moreover, I think it might be possible to assemble the distribution such that the compile time hadoop dependency is in a cognate directory to lib and is referenced using a default value for HADOOP_HOME.Does anybody have any violent antipathies to such a change?</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="3483" opendate="2011-1-26 00:00:00" fixdate="2011-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No soft flush trigger on global memstore limit</summary>
      <description>I think this is the reason people see long blocking periods under write load.Currently when we hit the global memstore limit, we call reclaimMemStoreMemory() which is synchronized - thus everyone has to wait until the memory has flushed down to the low water mark. This causes every writer to block for 10-15 seconds on a large heap.Instead we should start triggering flushes (in another thread) whenever we're above the low water mark. Then only block writers when we're above the high water mark.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3488" opendate="2011-1-29 00:00:00" fixdate="2011-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CellCounter to count multiple versions of rows</summary>
      <description>Currently RowCounter only retrieves latest version for each row.Some applications would store multiple versions for the same row.RowCounter should accept a new parameter for the number of versions to return.Scan object would be configured with version parameter (for scan.maxVersions).Then the following API should be called: public KeyValue[] raw() {</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3494" opendate="2011-1-31 00:00:00" fixdate="2011-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>checkAndPut implementation doesnt verify row param and writable row are the same</summary>
      <description>the API checkAndPut, and on the server side checkAndMutate doesn't enforce that the row in the API call and the row in the passed writable that should be executed if the check passes, are the same row! Looking at the code, if someone were to 'fool' us, we'd probably end up with rows in the wrong region in the worst case. Or we'd end up with non-locked puts/deletes to different rows since the checkAndMutate grabs the row lock and calls put/delete methods that do not grab row locks.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3495" opendate="2011-2-1 00:00:00" fixdate="2011-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell is failing on subsequent split calls</summary>
      <description>While working on HBASE-3492 I came across another oddity with manual splits:hbase(main):003:0&gt; split 'testtable' 0 row(s) in 3.0590 secondshbase(main):004:0&gt; scan '.META.', { COLUMNS =&gt; ['info:regioninfo'] } ROW COLUMN+CELL testtable,,1296545855212.5e4ef9631cacb6b column=info:regioninfo, timestamp=1296545855770, value=REGION =&gt; {NAME =&gt; 'testtable,,1296545855212.5e4ef9631cacb6b2c6c 2c6c338140c53cad4. 338140c53cad4.', STARTKEY =&gt; '', ENDKEY =&gt; 'row-mdc', ENCODED =&gt; 5e4ef9631cacb6b2c6c338140c53cad4, TABLE =&gt; {{NAME =&gt; ' testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOO MFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-mdc,1296545855212.46e57f0c column=info:regioninfo, timestamp=1296545855774, value=REGION =&gt; {NAME =&gt; 'testtable,row-mdc,1296545855212.46e57f0ca4eb a4eba8d3e5bef6365159a660. a8d3e5bef6365159a660.', STARTKEY =&gt; 'row-mdc', ENDKEY =&gt; '', ENCODED =&gt; 46e57f0ca4eba8d3e5bef6365159a660, TABLE =&gt; {{NA ME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPR ESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2 ', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKS IZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} 2 row(s) in 0.6690 secondshbase(main):005:0&gt; split 'testtable' 0 row(s) in 0.4030 secondshbase(main):006:0&gt; split 'testtable'ERROR: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2376) at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:2196) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:309) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1057)Here is some help for this command:Split entire table or pass a region to split individual region. With the second parameter, you can specify an explicit split key for the region. Examples: split 'tableName' split 'regionName' # format: 'tableName,startKey,id' split 'tableName', 'splitKey' split 'regionName', 'splitKey'It takes minutes for this to clear out eventually. Why is this not retried or flushed out right away?A few minutes later I see this in the logs:2011-02-01 08:42:42,062 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,,1296545879295.dfcc24e02e27e60160612dd5398cbd1e., qualifier=splitA, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 12011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 02011-02-01 08:42:42,064 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-dau,1296545879295.4073eb6c82755aab57778af2dba39e22., qualifier=splitB, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. because daughter splits no longer hold references2011-02-01 08:42:42,065 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/5e4ef9631cacb6b2c6c338140c53cad42011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 12011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 02011-02-01 08:42:42,067 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. from META2011-02-01 08:42:42,069 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 02011-02-01 08:42:42,070 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 12011-02-01 08:42:42,071 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-mdc,1296545879558.94cb351e5dd36c269247dd8a1a79373c., qualifier=splitA, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.2011-02-01 08:42:42,073 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 12011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 12011-02-01 08:42:42,074 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-seq,1296545879558.43c5ffe1ca7dd6d1374b7b7430a7d261., qualifier=splitB, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660. because daughter splits no longer hold references2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/46e57f0ca4eba8d3e5bef6365159a660The the next split call works while the subsequent ones fail again. In other words the split is dropped somewhere and picked up by the catalog classes later while the shell does not see the new daughter regions?Even .META. is offhbase(main):011:0&gt; scan '.META.', { COLUMNS =&gt; ['info:regioninfo'] }ROW COLUMN+CELL testtable,,1296545879295.dfcc24e02e27e60 column=info:regioninfo, timestamp=1296546225693, value=REGION =&gt; {NAME =&gt; 'testtable,,1296545879295.dfcc24e02e27e601606 160612dd5398cbd1e. 12dd5398cbd1e.', STARTKEY =&gt; '', ENDKEY =&gt; 'row-dau', ENCODED =&gt; dfcc24e02e27e60160612dd5398cbd1e, OFFLINE =&gt; true, SPL IT =&gt; true, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0 ', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TT L =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,,1296546225506.f3a53bfa1bfd5ae column=info:regioninfo, timestamp=1296546225763, value=REGION =&gt; {NAME =&gt; 'testtable,,1296546225506.f3a53bfa1bfd5ae6cbb 6cbb0641d43f8a242. 0641d43f8a242.', STARTKEY =&gt; '', ENDKEY =&gt; 'row-aaa', ENCODED =&gt; f3a53bfa1bfd5ae6cbb0641d43f8a242, TABLE =&gt; {{NAME =&gt; ' testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOO MFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-aaa,1296546225506.4253ecd9 column=info:regioninfo, timestamp=1296546225761, value=REGION =&gt; {NAME =&gt; 'testtable,row-aaa,1296546225506.4253ecd9c94c c94c38b66bdf8cd17b07efcb. 38b66bdf8cd17b07efcb.', STARTKEY =&gt; 'row-aaa', ENDKEY =&gt; 'row-dau', ENCODED =&gt; 4253ecd9c94c38b66bdf8cd17b07efcb, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3' , COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-dau,1296545879295.4073eb6c column=info:regioninfo, timestamp=1296546225913, value=REGION =&gt; {NAME =&gt; 'testtable,row-dau,1296545879295.4073eb6c8275 82755aab57778af2dba39e22. 5aab57778af2dba39e22.', STARTKEY =&gt; 'row-dau', ENDKEY =&gt; 'row-mdc', ENCODED =&gt; 4073eb6c82755aab57778af2dba39e22, OFFLIN E =&gt; true, SPLIT =&gt; true, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATI ON_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false ', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-dau,1296546225769.529fdb6b column=info:regioninfo, timestamp=1296546225971, value=REGION =&gt; {NAME =&gt; 'testtable,row-dau,1296546225769.529fdb6bcca8 cca8459349c81b518a24436b. 459349c81b518a24436b.', STARTKEY =&gt; 'row-dau', ENDKEY =&gt; 'row-gbo', ENCODED =&gt; 529fdb6bcca8459349c81b518a24436b, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3' , COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-gbo,1296546225769.374d4364 column=info:regioninfo, timestamp=1296546225968, value=REGION =&gt; {NAME =&gt; 'testtable,row-gbo,1296546225769.374d4364574a 574ad1c5f522aa55b3d81586. d1c5f522aa55b3d81586.', STARTKEY =&gt; 'row-gbo', ENDKEY =&gt; 'row-mdc', ENCODED =&gt; 374d4364574ad1c5f522aa55b3d81586, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3' , COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-mdc,1296545879558.94cb351e column=info:regioninfo, timestamp=1296545879815, value=REGION =&gt; {NAME =&gt; 'testtable,row-mdc,1296545879558.94cb351e5dd3 5dd36c269247dd8a1a79373c. 6c269247dd8a1a79373c.', STARTKEY =&gt; 'row-mdc', ENDKEY =&gt; 'row-seq', ENCODED =&gt; 94cb351e5dd36c269247dd8a1a79373c, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3' , COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-seq,1296545879558.43c5ffe1 column=info:regioninfo, timestamp=1296546226107, value=REGION =&gt; {NAME =&gt; 'testtable,row-seq,1296545879558.43c5ffe1ca7d ca7dd6d1374b7b7430a7d261. d6d1374b7b7430a7d261.', STARTKEY =&gt; 'row-seq', ENDKEY =&gt; '', ENCODED =&gt; 43c5ffe1ca7dd6d1374b7b7430a7d261, OFFLINE =&gt; tr ue, SPLIT =&gt; true, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOP E =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOC KCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NO NE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-seq,1296546225975.c9188f86 column=info:regioninfo, timestamp=1296546226161, value=REGION =&gt; {NAME =&gt; 'testtable,row-seq,1296546225975.c9188f869822 9822da3ff21215a98a99ff5a. da3ff21215a98a99ff5a.', STARTKEY =&gt; 'row-seq', ENDKEY =&gt; 'row-vfk', ENCODED =&gt; c9188f869822da3ff21215a98a99ff5a, TABLE =&gt; {{NAME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3' , COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} testtable,row-vfk,1296546225975.682a4dbf column=info:regioninfo, timestamp=1296546226156, value=REGION =&gt; {NAME =&gt; 'testtable,row-vfk,1296546225975.682a4dbf9800 980035dc379c6ccd7418cb08. 35dc379c6ccd7418cb08.', STARTKEY =&gt; 'row-vfk', ENDKEY =&gt; '', ENCODED =&gt; 682a4dbf980035dc379c6ccd7418cb08, TABLE =&gt; {{NA ME =&gt; 'testtable', FAMILIES =&gt; [{NAME =&gt; 'cf1', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPR ESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}, {NAME =&gt; 'cf2 ', BLOOMFILTER =&gt; 'NONE', REPLICATION_SCOPE =&gt; '0', VERSIONS =&gt; '3', COMPRESSION =&gt; 'NONE', TTL =&gt; '2147483647', BLOCKS IZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]}} 10 row(s) in 0.2610 secondsLook at the ENKDEYs.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.1,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3499" opendate="2011-2-2 00:00:00" fixdate="2011-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Users upgrading to 0.90.0 need to have their .META. table updated with the right MEMSTORE_SIZE</summary>
      <description>With Jack Levin, we were able to figure that users that are upgrading from a 0.20.x era cluster have their .META. schema set with a 16KB MEMSTORE_SIZE. This was done in order to minimize lost meta rows when append wasn't available but even if we changed it in HTD, we also have to make sure all users upgrading to 0.90 have it changed too.In Jack's case, he ended up with 2143 storefiles in .META. during a cold start, slowing everything down. He reported a few times in the past that his .META. was always extremely busy.We should be able to do it as a one-off thing in HMaster when opening .META. (an update in place).</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3506" opendate="2011-2-5 00:00:00" fixdate="2011-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow table name expressed in regex in drop, disable, enable operations</summary>
      <description>Ability to disable, drop and enable tables using regex expression is desirable.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.ruby.shell.rb</file>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3509" opendate="2011-2-7 00:00:00" fixdate="2011-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metric for flush queue length</summary>
      <description>We have a metric for compaction queue length. Would be nice to have one for flush queue length as well.</description>
      <version>None</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3510" opendate="2011-2-7 00:00:00" fixdate="2011-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thread name for IPC reader threads</summary>
      <description>The IPC readers come out of a thread pool but have no name, which is annoying.</description>
      <version>None</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3511" opendate="2011-2-7 00:00:00" fixdate="2011-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow rolling restart to apply to only RS or only masters</summary>
      <description>Rolling restart currently does both masters and RSs. Would like to be able to specify one or the other.</description>
      <version>None</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.rolling-restart.sh</file>
    </fixedFiles>
  </bug>
  <bug id="3512" opendate="2011-2-7 00:00:00" fixdate="2011-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Coprocessors: Shell support for listing currently loaded coprocessor set</summary>
      <description>Add support to the shell for listing the coprocessors loaded globally on the regionserver and those loaded on a per-table basis.Perhaps by extending the 'status' command.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3513" opendate="2011-2-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade thrift to 0.5.0 and use mvn version</summary>
      <description>We should upgrade our thrift to 0.5.0, it is the latest and greatest and is in apache maven repo.Doing some testing with a thrift 0.5.0 server, and an older pre-release php client shows the two are on-wire compatible.Given that the upgrade is entirely on the server side, and has no wire-impact this should be a relatively low-impact change.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3516" opendate="2011-2-8 00:00:00" fixdate="2011-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Coprocessors: add test cases for loading coprocessor jars from hdfs and local fs.</summary>
      <description>Loading coprocessors classes from jar files (at hdfs or local fs) is supported from CP framework right now. We used to have a test case to cover this scenario which uses an base-64 encoded string at the test case to represent a compiled jar file. This hardcoded way was not acceptable as a valid test case, so we removed it eventually. We need to have a better way to redo this case. Option 1) modify maven file in order to compile a test cp class into jar, and put it to hdfs and local fs, and run the cp class loading test; option 2) use Java 6.0 Compiler API to compile the test case at runtime and create the jar file?Need more time to investigate which one is better.</description>
      <version>0.90.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3520" opendate="2011-2-10 00:00:00" fixdate="2011-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update our bundled hadoop from branch-0.20-append to latest (rpc version 43)</summary>
      <description>Our 0.90.1RC0 won't run against head of branch-0.20-append; it has bundled an hadoop with an rpc version of 42 whereas head is at version 43. Here's the commit that changed the version:------------------------------------------------------------------------r1057313 | hairong | 2011-01-10 11:01:36 -0800 (Mon, 10 Jan 2011) | 2 linesHDFS-1554. New semantics for recoverLease. Contributed by Hairong Kuang.</description>
      <version>None</version>
      <fixedVersion>0.90.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3522" opendate="2011-2-10 00:00:00" fixdate="2011-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unbundle our RPC versioning; rather than a global for all 4 Interfaces -- region, master, region to master, and coprocesssors -- instead version each individually</summary>
      <description>We'd undo the global RPC version so a change in CP Interface or a change in the 'private' regionserver to master Interface would not break clients who do not use CPs or who don't care about the private regionserver to master protocol.Benoît suggested this. I want it because I want to get rid of heartbeating so will want to change the regionserver to master Interface.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestHMasterRPCException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion.java</file>
    </fixedFiles>
  </bug>
  <bug id="3525" opendate="2011-2-11 00:00:00" fixdate="2011-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn assembly is over-filling the hbase lib dir</summary>
      <description>Here is what our lib dir looks this in 0.90.1:-rwxr-xr-x 1 Stack staff 62983 Mar 16 2009 activation-1.1.jar-rwxr-xr-x 1 Stack staff 1034049 May 21 2009 ant-1.6.5.jar-rwxr-xr-x 1 Stack staff 1323005 Jul 20 2009 ant-1.7.1.jar-rwxr-xr-x 1 Stack staff 12143 Jul 20 2009 ant-launcher-1.7.1.jar-rwxr-xr-x 1 Stack staff 43033 May 5 2009 asm-3.1.jar-rwxr-xr-x 1 Stack staff 339831 Oct 18 10:05 avro-1.3.3.jar-rwxr-xr-x 1 Stack staff 41123 Dec 8 2009 commons-cli-1.2.jar-rwxr-xr-x 1 Stack staff 58160 Oct 18 10:05 commons-codec-1.4.jar-rwxr-xr-x 1 Stack staff 112341 Mar 16 2009 commons-el-1.0.jar-rwxr-xr-x 1 Stack staff 305001 Mar 16 2009 commons-httpclient-3.1.jar-rwxr-xr-x 1 Stack staff 279193 May 17 2010 commons-lang-2.5.jar-rwxr-xr-x 1 Stack staff 60686 Mar 13 2009 commons-logging-1.1.1.jar-rwxr-xr-x 1 Stack staff 180792 Mar 4 2010 commons-net-1.4.1.jar-rwxr-xr-x 1 Stack staff 3566844 Jun 5 2009 core-3.1.1.jar-rwxr-xr-x 1 Stack staff 936397 Oct 18 10:05 guava-r06.jar-rwxr-xr-x 1 Stack staff 2707856 Jan 11 13:26 hadoop-core-0.20-append-r1056497.jar-rwxr-xr-x 1 Stack staff 2241521 Feb 9 15:57 hbase-0.90.1.jar-rwxr-xr-x 1 Stack staff 706710 Mar 4 2010 hsqldb-1.8.0.10.jar-rwxr-xr-x 1 Stack staff 171958 Oct 18 10:05 jackson-core-asl-1.5.5.jar-rwxr-xr-x 1 Stack staff 17065 Oct 18 10:05 jackson-jaxrs-1.5.5.jar-rwxr-xr-x 1 Stack staff 386509 Oct 18 10:05 jackson-mapper-asl-1.4.2.jar-rwxr-xr-x 1 Stack staff 24745 Oct 18 10:05 jackson-xc-1.5.5.jar-rwxr-xr-x 1 Stack staff 408133 May 21 2010 jasper-compiler-5.5.23.jar-rwxr-xr-x 1 Stack staff 76844 May 17 2010 jasper-runtime-5.5.23.jar-rwxr-xr-x 1 Stack staff 103515 May 6 2009 jaxb-api-2.1.jar-rwxr-xr-x 1 Stack staff 867801 Mar 4 2010 jaxb-impl-2.1.12.jar-rwxr-xr-x 1 Stack staff 455517 Oct 18 10:05 jersey-core-1.4.jar-rwxr-xr-x 1 Stack staff 142827 Oct 18 10:05 jersey-json-1.4.jar-rwxr-xr-x 1 Stack staff 677600 Oct 18 10:05 jersey-server-1.4.jar-rwxr-xr-x 1 Stack staff 377780 Mar 4 2010 jets3t-0.7.1.jar-rwxr-xr-x 1 Stack staff 67758 May 6 2009 jettison-1.1.jar-rwxr-xr-x 1 Stack staff 539912 Jan 3 16:51 jetty-6.1.26.jar-rwxr-xr-x 1 Stack staff 177131 Jan 3 16:51 jetty-util-6.1.26.jar-rwxr-xr-x 1 Stack staff 87325 Jul 20 2009 jline-0.9.94.jar-rwxr-xr-x 1 Stack staff 4477138 Jan 3 16:51 jruby-complete-1.0.3.jar-rwxr-xr-x 1 Stack staff 1024680 May 17 2010 jsp-2.1-6.1.14.jar-rwxr-xr-x 1 Stack staff 134910 May 17 2010 jsp-api-2.1-6.1.14.jar-rwxr-xr-x 1 Stack staff 46367 Mar 4 2010 jsr311-api-1.1.1.jar-rwxr-xr-x 1 Stack staff 121070 Mar 13 2009 junit-3.8.1.jar-rwxr-xr-x 1 Stack staff 11981 Mar 4 2010 kfs-0.3.jar-rwxr-xr-x 1 Stack staff 481535 Oct 18 10:05 log4j-1.2.16.jar-rwxr-xr-x 1 Stack staff 65261 Apr 14 2009 oro-2.0.8.jar-rwxr-xr-x 1 Stack staff 29392 Jun 14 2010 paranamer-2.2.jar-rwxr-xr-x 1 Stack staff 5420 Jun 14 2010 paranamer-ant-2.2.jar-rwxr-xr-x 1 Stack staff 6931 Jun 14 2010 paranamer-generator-2.2.jar-rwxr-xr-x 1 Stack staff 328635 Mar 4 2010 protobuf-java-2.3.0.jar-rwxr-xr-x 1 Stack staff 173236 Jun 14 2010 qdox-1.10.1.jardrwxr-xr-x 7 Stack staff 238 Feb 8 16:23 ruby-rwxr-xr-x 1 Stack staff 132368 May 17 2010 servlet-api-2.5-6.1.14.jar-rwxr-xr-x 1 Stack staff 23445 Mar 4 2010 slf4j-api-1.5.8.jar-rwxr-xr-x 1 Stack staff 9679 Mar 4 2010 slf4j-log4j12-1.5.8.jar-rwxr-xr-x 1 Stack staff 26514 May 6 2009 stax-api-1.0.1.jar-rwxr-xr-x 1 Stack staff 187530 Mar 4 2010 thrift-0.2.0.jar-rwxr-xr-x 1 Stack staff 15010 Mar 4 2010 xmlenc-0.52.jar-rwxr-xr-x 1 Stack staff 598364 Dec 10 15:13 zookeeper-3.3.2.jarWe are picking up bunch of hadoop dependencies. I'd think it harmless other than the bulk.</description>
      <version>None</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.all.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3542" opendate="2011-2-17 00:00:00" fixdate="2011-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultiGet methods in Thrift</summary>
      <description>The Thrift API does not expose multi-get operations. This patch adds the meyhods getRows, getRowsWithColumns, getRowsTs and getRowsWithColumnsTs.</description>
      <version>0.90.0</version>
      <fixedVersion>0.90.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3563" opendate="2011-2-24 00:00:00" fixdate="2011-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[site] Add one-page-only version of hbase doc</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3581" opendate="2011-2-28 00:00:00" fixdate="2011-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase rpc should send size of response</summary>
      <description>The RPC reply from Server-&gt;Client does not include the size of the payload, it is framed like so:&lt;i32&gt; callId&lt;byte&gt; errorFlag&lt;byte[]&gt; dataThe data segment would contain enough info about how big the response is so that it could be decoded by a writable reader.This makes it difficult to write buffering clients, who might read the entire 'data' then pass it to a decoder. While less memory efficient, if you want to easily write block read clients (eg: nio) it would be necessary to send the size along so that the client could snarf into a local buf.The new proposal is:&lt;i32&gt; callId&lt;i32&gt; size&lt;byte&gt; errorFlag&lt;byte[]&gt; datathe size being sizeof(data) + sizeof(errorFlag).</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3600" opendate="2011-3-3 00:00:00" fixdate="2011-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update our jruby to 1.6.0</summary>
      <description>We reverted our jruby jar to 1.0.3 because of licensing issues in 1.5.x. The jruby crew fixed the licensing issues in 1.6.0RC2 (which is released but not yet in a mvn repo). This issue is about updating our ruby. The old ruby 'works' but is bad in many ways; bad parse errors, missing language support that made us redo a bunch of our script to not use import (and removal of TestShell, our unit test that ran jruby tests).This issue is about updating our jruby and in particular, reenabling the TestShell unit test removed by HBASE-3374.</description>
      <version>None</version>
      <fixedVersion>0.90.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3601" opendate="2011-3-4 00:00:00" fixdate="2011-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMasterFailover broken in TRUNK</summary>
      <description>After HBASE-3573, went in, TestMasterFailover broke. The change in shutdown technique revealed an issue with our in-memory accounting when a master joins an already cluster; we don't add .META. and ROOT to our set of online regions in the new master so could make for some interesting issues as the new master progressed (Previous shutdown did a count of remaining servers, new shutdown process looks at in-memory state to see if only catalog carrying regionservers online... this is what was going out of whack in new master).</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3603" opendate="2011-3-4 00:00:00" fixdate="2011-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove -XX:+HeapDumpOnOutOfMemoryError autodump of heap option on OOME</summary>
      <description>By default the -XX:+HeapDumpOnOutOfMemoryError option is set for HBase. Means we'll dump heap when we OOME. In the heaps we run with 8G, 16G, etc., these can make for hefty files. No one really looks at these things other than a few weirdos and even then, the interesting ones are too big to ship easily. Meantime, they can cause headache. E.g. you are on EC2, root is but a small partition, and you set up hbase on root partition... a heap dump could cause your root partition to fill and make the machine unapproachable.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3605" opendate="2011-3-4 00:00:00" fixdate="2011-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix balancer log message</summary>
      <description>From Gaojinchao up on user list:In balanceCluster function , It should be "leastloaded=" + serversByLoad.firstKey ().getLoad().getNumberOfRegions())"if(serversByLoad.lastKey().getLoad().getNumberOfRegions() &lt;= max &amp;&amp; serversByLoad.firstKey().getLoad().getNumberOfRegions() &gt;= min) { // Skipped because no server outside (min,max) range LOG.info("Skipping load balancing. servers=" + numServers + " " + "regions=" + numRegions + " average=" + average + " " + "mostloaded=" + serversByLoad.lastKey().getLoad().getNumberOfRegions() + " leastloaded=" + serversByLoad.lastKey().getLoad().getNumberOfRegions()); return null; }</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3609" opendate="2011-3-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the selection of regions to balance; part 2</summary>
      <description>See 'HBASE-3586 Improve the selection of regions to balance' for discussion of algorithms that improve on current random assignment.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestLoadBalancer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3764" opendate="2011-4-11 00:00:00" fixdate="2011-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Book.xml - adding 2 FAQs (SQL and arch question)</summary>
      <description>Adding 2 general FAQs.1) does HBase support SQL? (Hive, but not really for most cases)... 2) how does HBase work on HDFS? (if HDFS is for large files without fast lookup, how does HBase work?) Doesn't answer the question inline but refers to DataModel and Arch.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3897" opendate="2011-5-19 00:00:00" fixdate="2011-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Docs (notsoquick guide) suggest invalid XML</summary>
      <description>If you follow http://hbase.apache.org/notsoquick.html, you'll put the following in your hbase-site.xml:&lt;configuration&gt; ... &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000/hbase&lt;/value&gt; &lt;description&gt;The directory shared by region servers. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;description&gt;The replication count for HLog &amp; HFile storage. Should not be greater than HDFS datanode count. &lt;/description&gt; &lt;/property&gt; ...&lt;/configuration&gt;Except, oops, that's invalid XML:[Fatal Error] hbase-site.xml:34:50: The entity name must immediately follow the '&amp;' in the entity reference.Exception in thread "main" java.lang.RuntimeException: org.xml.sax.SAXParseException: The entity name must immediately follow the '&amp;' in the entity reference. at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1393) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1261) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1192) at org.apache.hadoop.conf.Configuration.get(Configuration.java:415) at org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:63) at org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:76) at org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:86) at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2737)Trivial patch to follow.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.getting.started.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3988" opendate="2011-6-14 00:00:00" fixdate="2011-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Infinite loop for secondary master</summary>
      <description>There seems be a bug that the secondary master didn't come out when the primary master dead. Because the secondary master will be in a loop forever to watch a local variable before setting a zk watcher.However this local variable is changed by the zk call back function.So the secondary master will be in the infinite loop forever.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4072" opendate="2011-7-7 00:00:00" fixdate="2011-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate/disable and remove support for reading ZooKeeper zoo.cfg files from the classpath</summary>
      <description>This issue was found by Lars: http://search-hadoop.com/m/n04sthNcji2/zoo.cfg+vs+hbase-site.xml&amp;subj=Re+zoo+cfg+vs+hbase+site+xmlLets fix the inconsistency found and fix the places where we use non-zk attribute name for a zk attribute in hbase (There's only a few places that I remember &amp;#8211; maximum client connections is one IIRC)</description>
      <version>0.90.0</version>
      <fixedVersion>0.95.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKConfig.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="4930" opendate="2011-12-1 00:00:00" fixdate="2011-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reformat HBase home page</summary>
      <description>Reformat the HBase home page. It's not a radical redesign but breaks the information that is there into clearer sections: Welcome to HBase! (it's always good to be polite to the reader) When Should I Use HBase? Features How Can I Get More Information? NewsI also updated the feature-list in this page (added some things, clarified a few things, removed one or two things)Note: I just overhauled the FAQ in the book today as well.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.index.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5072" opendate="2011-12-20 00:00:00" fixdate="2011-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Max Value for Per-Store Metrics</summary>
      <description>We were bit in our multi-tenant cluster because one of our Stores encountered a bug and grew its StoreFile count. We didn't notice this because the StoreFile count currently reported by the RegionServer is an average of all Stores in the region. For the per-Store metrics, we should also record the max so we can notice outliers.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="8754" opendate="2013-6-17 00:00:00" fixdate="2013-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log the client IP/port of the balancer invoker</summary>
      <description>There's no way for any ops to answer today: "Who turned off/on the balancer?". All the logs print is the state when the RPC call is invoked, nothing else.Given this is a critical piece of admin functionality, we should log the IP for it at least.</description>
      <version>0.90.0</version>
      <fixedVersion>0.98.0,0.96.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="8755" opendate="2013-6-17 00:00:00" fixdate="2013-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A new write thread model for HLog to improve the overall HBase write throughput</summary>
      <description>In current write model, each write handler thread (executing put()) will individually go through a full 'append (hlog local buffer) =&gt; HLog writer append (write to hdfs) =&gt; HLog writer sync (sync hdfs)' cycle for each write, which incurs heavy race condition on updateLock and flushLock.The only optimization where checking if current syncTillHere &gt; txid in expectation for other thread help write/sync its own txid to hdfs and omitting the write/sync actually help much less than expectation.Three of my colleagues(Ye Hangjun / Wu Zesheng / Zhang Peng) at Xiaomi proposed a new write thread model for writing hdfs sequence file and the prototype implementation shows a 4X improvement for throughput (from 17000 to 70000+). I apply this new write thread model in HLog and the performance test in our test cluster shows about 3X throughput improvement (from 12150 to 31520 for 1 RS, from 22000 to 70000 for 5 RS), the 1 RS write throughput (1K row-size) even beats the one of BigTable (Precolator published in 2011 says Bigtable's write throughput then is 31002). I can provide the detailed performance test results if anyone is interested.The change for new write thread model is as below: 1&gt; All put handler threads append the edits to HLog's local pending buffer; (it notifies AsyncWriter thread that there is new edits in local buffer) 2&gt; All put handler threads wait in HLog.syncer() function for underlying threads to finish the sync that contains its txid; 3&gt; An single AsyncWriter thread is responsible for retrieve all the buffered edits in HLog's local pending buffer and write to the hdfs (hlog.writer.append); (it notifies AsyncFlusher thread that there is new writes to hdfs that needs a sync) 4&gt; An single AsyncFlusher thread is responsible for issuing a sync to hdfs to persist the writes by AsyncWriter; (it notifies the AsyncNotifier thread that sync watermark increases) 5&gt; An single AsyncNotifier thread is responsible for notifying all pending put handler threads which are waiting in the HLog.syncer() function 6&gt; No LogSyncer thread any more (since there is always AsyncWriter/AsyncFlusher threads do the same job it does)</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestDurability.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
