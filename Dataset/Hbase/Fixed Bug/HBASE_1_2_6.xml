<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="18122" opendate="2017-5-26 00:00:00" fixdate="2017-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanner id should include ServerName of region server</summary>
      <description>Now the scanner id is a long number from 1 to max in a region server. Each new scanner will have a scanner id.If a client has a scanner whose id is x, when the RS restart and the scanner id is also incremented to x or a little larger, there will be a scanner id collision.So the scanner id should now be same during each time the RS restart. We can add the start timestamp as the highest several bits in scanner id uint64.And because HBASE-18121 is not easy to fix and there are many clients with old version. We can also encode server host:port into the scanner id.So we can use ServerName.</description>
      <version>1.4.0,1.3.1,1.1.10,1.2.6,2.0.0</version>
      <fixedVersion>1.4.0,1.3.2,1.1.11,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCallerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="18269" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jython docs out of date</summary>
      <description>The documentation describing how to launch Jython + HBase is out of date. - https://hbase.apache.org/book.html#jythonFirst, we would set the classpath differently:HBASE_CLASSPATH=/home/hbase/jython.jar bin/hbase org.python.util.jythonThen, the actual code example is out of date too:&gt;&gt;&gt; desc = HTableDescriptor(tablename)&gt;&gt;&gt; desc.addFamily(HColumnDescriptor("content:"))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; at org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(HColumnDescriptor.java:566) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:470) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:425) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:390) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:338) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:327) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.python.core.PyReflectedConstructor.constructProxy(PyReflectedConstructor.java:211)We should make sure that the examples we claim are runnable actually are.</description>
      <version>1.3.1,1.2.6,1.5.0,1.4.2,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18276" opendate="2017-6-27 00:00:00" fixdate="2017-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Release 1.2.7</summary>
      <description>about time to get rolling on 1.2.7 for ~monthly</description>
      <version>None</version>
      <fixedVersion>1.2.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18303" opendate="2017-6-30 00:00:00" fixdate="2017-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up some parameterized test declarations</summary>
      <description>While debugging something unrelated, I noticed that we use the constructor form of junit parameterized tests, instead of the annotated members form.I personally find using the @Parameter annotation more clear.Also, we can move the parameter generator to hbase-common so that it is accessible in more modules.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorScanPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileWriterV3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockIndex.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestSeekToBlockWithEncoders.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestMultiRowResource.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.prefixtree.row.TestRowEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.keyvalue.TestKeyValueTool.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestByteBufferUtils.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.types.TestStruct.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.HBaseCommonTestingUtility.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.filter.TestKeyOnlyFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="18311" opendate="2017-7-3 00:00:00" fixdate="2017-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up the quickstart guide</summary>
      <description>extra commas, period, etc.</description>
      <version>1.3.1,1.2.6,1.1.11,2.0.0-alpha-1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18314" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate the findbugs warnings for hbase-examples</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.client.example.AsyncClientExample.java</file>
    </fixedFiles>
  </bug>
  <bug id="18315" opendate="2017-7-4 00:00:00" fixdate="2017-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate the findbugs warnings for hbase-rest</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-2,2.0.0,1.2.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.NamespacesInstanceModel.java</file>
    </fixedFiles>
  </bug>
  <bug id="18316" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement async admin operations for draining region servers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="18317" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement async admin operations for Normalizer/CleanerChore/CatalogJanitor</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncBalancerAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="18318" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement updateConfiguration/stopMaster/stopRegionServer/shutdown methods</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcedureAdminApi.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncClusterAdminApi.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdminBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="18319" opendate="2017-7-4 00:00:00" fixdate="2017-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement getClusterStatus/getRegionLoad/getCompactionState/getLastMajorCompactionTimestamp methods</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="1832" opendate="2009-9-12 00:00:00" fixdate="2009-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Faster enable/disable/delete</summary>
      <description>The enable/disable/delete is slow. Looking at code, its heavyweight. It doesn't do bulk scanning nor bulk writing. Try doing some code client side that does bulk scan and bulk puts. It might run faster.</description>
      <version>None</version>
      <fixedVersion>0.20.1,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RetryableMetaOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ChangeTableState.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18329" opendate="2017-7-6 00:00:00" fixdate="2017-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update links in config guide to point to java 8 references</summary>
      <description></description>
      <version>1.3.1,1.2.6,1.1.11,2.0.0-alpha-1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="1833" opendate="2009-9-12 00:00:00" fixdate="2009-9-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hfile.main fixes</summary>
      <description>hfile.main has a bunch of nice new facility added by lars. This issue is small fixes.</description>
      <version>None</version>
      <fixedVersion>0.20.1,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1848" opendate="2009-9-17 00:00:00" fixdate="2009-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixup shell for HBASE-1822</summary>
      <description>Shell was broken by HBASE-1822. Fix.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.contrib.transactional.src.test.org.apache.hadoop.hbase.regionserver.transactional.TestTHLog.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.THLogRecoveryManager.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionLogger.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.package.html</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.HBaseBackedTransactionLogger.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="18481" opendate="2017-7-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The autoFlush flag was not used in PE tool</summary>
      <description>After HBASE-12728, PE used the BufferedMutator for random/sequential write test and the autoFlush flag was not used. So all write test will buffered the write request and send as a batch request when the buffer has filled.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug id="18522" opendate="2017-8-4 00:00:00" fixdate="2017-8-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RowMutations support to Batch</summary>
      <description>RowMutations is multiple Puts and/or Deletes atomically on a single row. Current Batch call does not support RowMutations as part of the batch.We should add this missing part. We should be able to batch RowMutations.</description>
      <version>1.2.6</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiParallel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Table.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MultiServerCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="18641" opendate="2017-8-21 00:00:00" fixdate="2017-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include block content verification logic used in lruCache in bucketCache</summary>
      <description>With off-heap/bucketCache being used to cache data blocks without going through on-heap cache, the logic used in lruCache to check the content of already cached block need to be included in bucketCache. Please see this discussion for details.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18718" opendate="2017-8-30 00:00:00" fixdate="2017-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the coprocessor.Export</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18749" opendate="2017-9-3 00:00:00" fixdate="2017-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply the CF specific TimeRange from Scan to filter the segment scanner</summary>
      <description>We can evict the unused segment scanner early.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SegmentScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Segment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MutableSegment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ImmutableSegment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompositeImmutableSegment.java</file>
    </fixedFiles>
  </bug>
  <bug id="18928" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backup delete command shows wrong number of deletes requested</summary>
      <description>./hbase backup delete backup_1506934800496 backup_15069347491632017-10-02 14:55:39,788 WARN &amp;#91;main&amp;#93; util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable2017-10-02 14:55:40,151 WARN &amp;#91;main&amp;#93; shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.Please make sure that backup is enabled on the cluster. To enable backup, in hbase-site.xml, set: hbase.backup.enable=truehbase.master.logcleaner.plugins=YOUR_PLUGINS,org.apache.hadoop.hbase.backup.master.BackupLogCleanerhbase.procedure.master.classes=YOUR_CLASSES,org.apache.hadoop.hbase.backup.master.LogRollMasterProcedureManagerhbase.procedure.regionserver.classes=YOUR_CLASSES,org.apache.hadoop.hbase.backup.regionserver.LogRollRegionServerProcedureManagerand restart the cluster2017-10-02 14:55:40,328 INFO &amp;#91;main&amp;#93; metrics.MetricRegistries: Loaded MetricRegistries class org.apache.hadoop.hbase.metrics.impl.MetricRegistriesImpl2017-10-02 14:55:41,876 INFO &amp;#91;main&amp;#93; impl.BackupAdminImpl: Deleting backup backup_1506934749163 ...2017-10-02 14:55:41,970 INFO &amp;#91;main&amp;#93; util.BackupUtils: No data has been found in hdfs://localhost:8020/test/backup_1506934749163/default/raj22.2017-10-02 14:55:41,998 INFO &amp;#91;main&amp;#93; impl.BackupAdminImpl: Delete backup backup_1506934749163 completed.Deleted 1 backups. Total requested: 3</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupCommands.java</file>
    </fixedFiles>
  </bug>
  <bug id="18929" opendate="2017-10-3 00:00:00" fixdate="2017-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hbase backup command doesn’t show debug option to enable backup in debug mode</summary>
      <description>./hbase backup create ...Usage: hbase backup create &lt;type&gt; &lt;backup_path&gt; &amp;#91;options&amp;#93; type "full" to create a full backup image "incremental" to create an incremental backup image backup_path Full path to store the backup imageOptions: -b &lt;arg&gt; Bandwidth per task (MapReduce task) in MB/s -q &lt;arg&gt; Yarn queue name to run backup create command on -s &lt;arg&gt; Backup set to backup, mutually exclusive with -t (table list) -t &lt;arg&gt; Table name list, comma-separated. -w &lt;arg&gt; Number of parallel MapReduce tasks to executeThe above help doesn't show that we can actually run backups in debug mode easily by just providing -d option which is very helpful in debugging.The following code works well./hbase backup create full hdfs://localhost:8020/test/ -t test1 -dand shows all debug logs.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupCommands.java</file>
    </fixedFiles>
  </bug>
  <bug id="18932" opendate="2017-10-4 00:00:00" fixdate="2017-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backups masking exception in a scenario and though it fails , it shows success message.</summary>
      <description>Scenario to reproduce : Take 1 full and incremental backup Delete the table Try to rerun the backups on it.It fails with following error given failure message in between but finally it shows backup successful.============================================================================2017-10-02 15:29:25,339 DEBUG &amp;#91;main&amp;#93; impl.BackupSystemTable: Restoring backup:system from snapshot2017-10-02 15:29:28,148 DEBUG &amp;#91;main&amp;#93; impl.BackupSystemTable: Done restoring backup system table2017-10-02 15:29:28,148 DEBUG &amp;#91;main&amp;#93; impl.BackupSystemTable: Deleting snapshot_backup_system from the system2017-10-02 15:29:28,166 DEBUG &amp;#91;main&amp;#93; impl.BackupSystemTable: Done deleting backup system table snapshot2017-10-02 15:29:28,166 DEBUG &amp;#91;main&amp;#93; impl.TableBackupClient: Trying to cleanup up target dir. Current backup phase: INCREMENTAL_COPY2017-10-02 15:29:28,169 DEBUG &amp;#91;main&amp;#93; impl.BackupSystemTable: Finish backup exclusive operation2017-10-02 15:29:28,172 ERROR &amp;#91;main&amp;#93; impl.TableBackupClient: Backup backup_1506938363641 failed.Backup session backup_1506938363641 finished. Status: SUCCESS.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="19027" opendate="2017-10-17 00:00:00" fixdate="2017-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Honor the CellComparator of ScanInfo in scanning over a store</summary>
      <description>All components in the scan path should honor the cell comparator from scaninfo rather than arbitrarily choose the comparator from store, scaninfo, or static COMPARATOR.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.querymatcher.TestScanWildcardColumnTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.querymatcher.ScanWildcardColumnTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="19038" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>precommit mvn install should run from root on patch</summary>
      <description>If the root pom changes we don't properly order the mvn install on the patch and this can lead to problems.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,1.1.13,2.0.0-alpha-4,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19039" opendate="2017-10-18 00:00:00" fixdate="2017-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor shadedjars test to only run on java changes.</summary>
      <description>per HBASE-18760 ; the shadejars clean check is currently broken due to a typo, we should excise it.needs to be done in all the branches.Also limit runs of the test to changes that alter poms or java files. probably that's all that can break this. Any edge cases will come up in nightly.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,1.1.13,2.0.0-alpha-4,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19118" opendate="2017-10-30 00:00:00" fixdate="2017-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use SaslUtil to set Sasl.QOP in &amp;#39;Thrift&amp;#39;</summary>
      <description>In Configure the Thrift Gateway, it says "set the property hbase.thrift.security.qop to one of the following three values: privacy, integrity, authentication", which would lead to failure of starting up a thrift server.In fact, the value of hbase.thrift.security.qop should be auth, auth-int, auth-conf, according to the documentation of Sasl.QOP</description>
      <version>1.2.6</version>
      <fixedVersion>1.3.2,2.0.0-beta-1,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="19141" opendate="2017-10-31 00:00:00" fixdate="2017-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[compat 1-2] getClusterStatus always return empty ClusterStatus</summary>
      <description>We are able to limit the scope to get part of ClusterStatus in 2.0. However the request sent by 1.x client has no specific scope info to retrieve any information from ClusterStatus.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientClusterStatus.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="19215" opendate="2017-11-8 00:00:00" fixdate="2017-11-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect exception handling on the client causes incorrect call timeouts and byte buffer allocations on the server</summary>
      <description>Ran into the situation of oome on the client : java.lang.OutOfMemoryError: Direct buffer memory.When we encounter an unhandled exception during channel write at RpcClientImplcheckIsOpen(); // Now we're checking that it didn't became idle in between. try { call.callStats.setRequestSizeBytes(IPCUtil.write(this.out, header, call.param, cellBlock)); } catch (IOException e) {we end up leaving the connection open. This becomes especially problematic when we get an unhandled exception between writing the length of our request on the channel and subsequently writing the params and cellblocks *dos.write(Bytes.toBytes(totalSize));* // This allocates a buffer that is the size of the message internally. header.writeDelimitedTo(dos); if (param != null) param.writeDelimitedTo(dos); if (cellBlock != null) dos.write(cellBlock.array(), 0, cellBlock.remaining()); dos.flush(); return totalSize;After reading the length rs allocates a bb and expects data to be filled. However when we encounter an exception during param write we release the writelock in rpcclientimpl and do not close the connection, the exception is handled at AbstractRpcClient.callBlockingMethod and retried. Now the next client request to the same rs writes to the channel however the server interprets this as part of the previous request and errors out during proto conversion when processing the request since its considered malformed(in the worst case this might be misinterpreted as wrong data?). Now the remaining data of the current request is read(the current request's size &gt; prev request's allocated partially filled bytebuffer) and is misinterpreted as the size of new request, in my case this was in gbs. All the client requests time out since this bytebuffer is never completely filled. We should close the connection for any Throwable and not just ioexception.</description>
      <version>1.3.1,1.2.6</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.BlockingRpcConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="19216" opendate="2017-11-8 00:00:00" fixdate="2017-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a general framework to execute remote procedure on RS</summary>
      <description>When building the basic framework for HBASE-19064, I found that the enable/disable peer is built upon the watcher of zk.The problem of using watcher is that, you do not know the exact time when all RSes in the cluster have done the change, it is a 'eventually done'. And for synchronous replication, when changing the state of a replication peer, we need to know the exact time as we can only enable read/write after that time. So I think we'd better use procedure to do this. Change the flag on zk, and then execute a procedure on all RSes to reload the flag from zk.Another benefit is that, after the change, zk will be mainly used as a storage, so it will be easy to implement another replication peer storage to replace zk so that we can reduce the dependency on zk.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionTransitionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.ExecutorType.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.EventType.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionServerStatus.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.RemoteProcedureDispatcher.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.LockedResourceType.java</file>
    </fixedFiles>
  </bug>
  <bug id="19260" opendate="2017-11-15 00:00:00" fixdate="2017-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add lock back to avoid parallel accessing meta to locate region</summary>
      <description>In branch-0.98 we have below codes to avoid accessing meta in parallel in HConnectionManager: Result regionInfoRow; // This block guards against two threads trying to load the meta // region at the same time. The first will load the meta region and // the second will use the value that the first one found. if (useCache) { if (TableName.META_TABLE_NAME.equals(parentTable) &amp;&amp; usePrefetch &amp;&amp; getRegionCachePrefetch(tableName)) { synchronized (regionLockObject) { // Check the cache again for a hit in case some other thread made the // same query while we were waiting on the lock. ... } } ...while in HBASE-10018 we removed such logic along with region-location-prefetching. We regard this as an unexpected behavior change and observed below phenomenon in our product env:1. Unnecessary connection setup to meta when multiple threads locating region in a client process2. Priority handler of the RS holding meta region exhausted, application keep retrying and cause a vicious circleTo resolve this problem, we propose to add the userRegionLock back and keep the behavior in accordance with 0.98</description>
      <version>1.3.1,1.2.6,2.0.0-alpha-3,1.1.12</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="19267" opendate="2017-11-15 00:00:00" fixdate="2017-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eclipse project import issues on 2.0</summary>
      <description>Trying to do a fresh import of branch-2 nets some errors..It seems like a previous change I made to clean up errors (HBASE-13236), specifically adding the maven-compiler-plugin lifecycle mapping for m2eclipse, is now causing Eclipse to not compile HBase as Java8. Removing the lifecycle mapping fixes this.I assume this only needs to happen for 2.0.I keep having issues with the JavaNature being ignored. Not yet sure if this is a result of something we're doing wrong (or just Eclipse being Eclipse).</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19344" opendate="2017-11-25 00:00:00" fixdate="2017-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve asyncWAL by using Independent thread for netty #IO in FanOutOneBlockAsyncDFSOutput</summary>
      <description>The logic now is that the netty #IO thread and asyncWal's thread are the same one.Improvement proposal:1, Split into two.2, All multiWal share the netty #IO thread pool.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestAsyncWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestAsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AsyncFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SecureAsyncProtobufLogWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.RingBufferTruck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSWALEntry.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
    </fixedFiles>
  </bug>
  <bug id="1956" opendate="2009-11-5 00:00:00" fixdate="2009-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export HDFS read and write latency as a metric</summary>
      <description>HDFS write latency spikes especially are an indicator of general cluster overloading. We see this where the WAL writer complains about writes taking &gt; 1 second, sometimes &gt; 4, etc. If for example the average write latency over the monitoring period is exported as a metric, then this can feed into alerting for or automatic provisioning of additional cluster hardware. While we're at it, export read side metrics as well.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="19929" opendate="2018-2-4 00:00:00" fixdate="2018-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call RS.stop on a session expired RS may hang</summary>
      <description>See the discussion in HBASE-19927. The problem is that, for a normal stop we will try to close all the regions and wait until they are all closed. But if the RS has already session expired, master will start the failover work which will move the WAL directory, and then we will be stuck in writing flush marker.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWALLockup.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestDrainBarrier.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.DrainBarrier.java</file>
    </fixedFiles>
  </bug>
  <bug id="20058" opendate="2018-2-23 00:00:00" fixdate="2018-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>improper quoting in presplitting command docs</summary>
      <description>http://hbase.apache.org/book.html#tricks.pre-splithbase&gt;create 't1','f',SPLITS =&gt; ['10','20',30']Missing a quote before the 30./</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.shell.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20059" opendate="2018-2-23 00:00:00" fixdate="2018-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure documentation is updated for the offheap Bucket cache usage</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">conf.hbase-env.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20196" opendate="2018-3-14 00:00:00" fixdate="2018-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain all regions with same size in memstore flusher</summary>
      <description>Here is the javadoc for getCopyOfOnlineRegionsSortedByOffHeapSize() : * the biggest. If two regions are the same size, then the last one found wins; i.e. this * method may NOT return all regions.Currently value type is HRegion - we only store one region per size.I think we should change value type to Collection&lt;HRegion&gt; so that we don't miss any region (potentially with big size).e.g. Suppose there are there regions (R1, R2 and R3) with sizes 100, 100 and 1, respectively.Using the current data structure, R2 would be stored in the Map, evicting R1 from the Map.This means that the current code would choose to flush regions R2 and R3, releasing 101 from memory.If value type is changed to Collection&lt;HRegion&gt;, we would flush both R1 and R2. This achieves faster memory reclamation.Confirmed with eshcar over in HBASE-20090</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20291" opendate="2018-3-27 00:00:00" fixdate="2018-4-27 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>Fix for The POM for net.minidev:json-smart:jar:2.3-SNAPSHOT is missing, no dependency information available with hadoop.profile=3.0</summary>
      <description>receiving messageThe POM for net.minidev:json-smart:jar:2.3-SNAPSHOT is missing, no dependency information availablewhen running withmvn clean install -DHBasePatchProcess -Dhadoop-three.version=3.0.0 -Dhadoop.profile=3.0 -DskipTests</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20434" opendate="2018-4-17 00:00:00" fixdate="2018-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Also remove remote wals when peer is in DA state</summary>
      <description>Consider we have two clusters in A and S state, and then we transit A to DA. And later we want to transit DA to A, since the remote cluster is in S, we should be able to do it. But there are some remote wals on the HDFS for the cluster in S state, so we need to remove them first before transiting the cluster in DA state to A.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.SyncReplicationWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AbstractFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20452" opendate="2018-4-18 00:00:00" fixdate="2018-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master UI: Table merge button should validate required fields before submit</summary>
      <description>In HBase master UI, whether the required fields are provided should be validated before the button is clicked. Also, it should avoid giving a false message that merge request has been submitted even if the validation fails later.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
    </fixedFiles>
  </bug>
  <bug id="20454" opendate="2018-4-18 00:00:00" fixdate="2018-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] Add note on perf to upgrade section</summary>
      <description>Add short note on YMMV regards perf and 2.0.0 but we working on it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20457" opendate="2018-4-19 00:00:00" fixdate="2018-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return immediately for a scan rpc call when we want to switch from pread to stream</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSwitchToStreamRead.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScannerContext.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.java</file>
    </fixedFiles>
  </bug>
  <bug id="23829" opendate="2020-2-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get `-PrunSmallTests` passing on JDK11</summary>
      <description>Start with the small tests, shaking out issues identified by the harness. So far it seems like -Dhadoop.profile=3.0 and -Dhadoop-three.version=3.3.0-SNAPSHOT maybe be required.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.log.TestLogLevel.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFutureUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
