<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="2700" opendate="2010-6-8 00:00:00" fixdate="2010-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle master failover for regions in transition</summary>
      <description>To this point in HBASE-2692 tasks we have moved everything for regions in transition into ZK, but we have not fully handled the master failover case. This is to deal with that and to write tests for it.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKAssign.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2703" opendate="2010-6-8 00:00:00" fixdate="2010-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ui not working in distributed context</summary>
      <description>UI is not showing when you put hbase on a cluster; this is since we renamed webapps dir as hbase-webapps.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="281" opendate="2008-1-30 00:00:00" fixdate="2008-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should allow deletions in .META. and -ROOT- tables</summary>
      <description>For administrative and debugging purposes, it would be nice to be able to delete rows from .META. via the shell. The alternative is writing custom java code to do such operations, which is just ridiculous. The reality of HBase's maturity is that from time to time we're going to have to reach into the .META. and ROOT tables to fix things, so I think the shell should be where that happens.Currently, attempting to delete from either table gives a "non-existant table" error.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3900" opendate="2011-5-19 00:00:00" fixdate="2011-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose progress of a major compaction in UI and/or in shell</summary>
      <description>A general recommendation is to turn off major compactions and run them externally only currently the only way to follow progress of a major compaction is by study of regionserver logs. Add a feature that gives a percentage complete of major compaction up in the UI or in shell.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="415" opendate="2008-2-6 00:00:00" fixdate="2008-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite leases to use DelayedBlockingQueue instead of polling</summary>
      <description>The current lease implementation is pretty broken. This change was previously included in HBASE-69, but should really be a separate patch.</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.Leases.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="426" opendate="2008-2-7 00:00:00" fixdate="2008-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase can&amp;#39;t find remote filesystem</summary>
      <description>If filesystem is remote, e.g. its an Hadoop DFS running "over there", there is no means of pointing hbase at it currently (unless you count copying hadoop-site.xml into hbase/conf). Should be possible to just set a fully qualified hbase.rootdir and that should be sufficient for hbase figuring the fs (needs to be backported to 0.1 too).</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMigrate.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestTimestamp.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestSplit.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestMergeMeta.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestLogRolling.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHStoreFile.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestGet2.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestGet.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestDeleteFamily.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestDeleteAll.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestCompaction.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableMapReduce.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableIndex.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConnectionManager.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="433" opendate="2008-2-9 00:00:00" fixdate="2008-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>region server should deleted restore log after successfull restore</summary>
      <description>Currently we do not remove the restore log "oldlogfile.log" after we reopen a region after a crashed region server.Suggestion would be to remove after we successfully flush of all the edits to a mapfileso something like:replay log memcache flushdeleted log</description>
      <version>0.1.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="436" opendate="2008-2-10 00:00:00" fixdate="2008-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>website</summary>
      <description>Make the hbase website. Base it on hadoop forrest templates.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docs.src.documentation.content.xdocs.tabs.xml</file>
      <file type="M">build.xml</file>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">docs.linkmap.html</file>
      <file type="M">docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="437" opendate="2008-2-11 00:00:00" fixdate="2008-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear Command should use system.out</summary>
      <description>Current clear command doesn't work, It should use system.out</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.ClearCommand.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4374" opendate="2011-9-12 00:00:00" fixdate="2011-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up default regions size from 256M to 1G</summary>
      <description>HBASE-4365 has some discussion of why we default for a table should tend to fewer bigger regions. It doesn't look like this issue will be done for 0.92. For 0.92, lets up default region size from 256M to 1G and talk up pre-split on table creation in manual.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="438" opendate="2008-2-11 00:00:00" fixdate="2008-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>XMLOutputter state should be initialized.</summary>
      <description>bash-3.00# bin/hbase shell --htmlHQL, 0.2.0-dev version.Copyright (c) 2008 by udanax, licensed to Apache Software Foundation.Type 'help;' for usage.hql &gt; show tables;&lt;table&gt; &lt;tr&gt; &lt;th&gt;Name &lt;/th&gt; &lt;th&gt;Descriptor &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;a &lt;/td&gt; &lt;td&gt;name: a, families: {b:={name: b, max versions: 3, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, bloom filter: none}} &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;1 table(s) in set. (0.21 sec)hql &gt; show tables;Exception in thread "main" java.lang.IllegalStateException: getState() == DOCUMENT_ENDED at org.znerd.xmlenc.XMLOutputter.startTag(Unknown Source) at org.apache.hadoop.hbase.hql.formatter.HtmlTableFormatter.header(HtmlTableFormatter.java:125) at org.apache.hadoop.hbase.hql.ShowCommand.execute(ShowCommand.java:66) at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:68) at org.apache.hadoop.hbase.Shell.main(Shell.java:114)bash-3.00#</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="446" opendate="2008-2-14 00:00:00" fixdate="2008-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fully qualified hbase.rootdir doesn&amp;#39;t work</summary>
      <description>Jim was setting up cluster w/ new hbase. Setting fully qualified hbase.rootdir was failing. Complaint was that the filesystems didn't match &amp;#8211; i.e. the hdfs of the fully qualified hbase.rootdir didn't jibe w/ the default hadoop file:///.Fix needs to be backported.The problem was that because the hadoop config files were not found (because they are in a different directory and not on the classpath) then fs.get(conf) returns file:///</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4461" opendate="2011-9-22 00:00:00" fixdate="2011-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose getRowOrBefore via Thrift</summary>
      <description>In order for fat Thrift-based clients to locate region locations they need to utilize the getRowOrBefore method.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4463" opendate="2011-9-22 00:00:00" fixdate="2011-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run more aggressive compactions during off peak hours</summary>
      <description>The number of iops on the disk and the top of the rack bandwidth utilization at off peak hours is much lower than at peak hours depending on the application usage pattern. We can utilize this knowledge to improve the performance of the HBase cluster by increasing the compact selection ratio to a much larger value during off-peak hours than otherwise - increasing hbase.hstore.compaction.ratio (1.2 default) to hbase.hstore.compaction.ratio.offpeak (5 default). This will help reduce the average number of files per store.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSelection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="452" opendate="2008-2-15 00:00:00" fixdate="2008-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"region offline" should throw IOException, not IllegalStateException</summary>
      <description>It would be nice if I could wrap my HTable.get calls in try {} catch (IOException e). But that doesn't work, since I also need to catch IllegalStateException. I think that any time there is something wrong with hbase, hbase calls should throw an IOException (or subclass thereof). Things like IllegalStateException should be reserved for programmer error.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4561" opendate="2011-10-9 00:00:00" fixdate="2011-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Maven documentation in book</summary>
      <description>The maven documentation is a little out of date and has recently led to some confusion about tests. This would cleanup the maven documents in the book to be more explicit about how maven should be used.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4720" opendate="2011-11-1 00:00:00" fixdate="2011-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement atomic update operations (checkAndPut, checkAndDelete) for REST client/server</summary>
      <description>I have several large application/HBase clusters where an application node will occasionally need to talk to HBase from a different cluster. In order to help ensure some of my consistency guarantees I have a sentinel table that is updated atomically as users interact with the system. This works quite well for the "regular" hbase client but the REST client does not implement the checkAndPut and checkAndDelete operations. This exposes the application to some race conditions that have to be worked around. It would be ideal if the same checkAndPut/checkAndDelete operations could be supported by the REST client.</description>
      <version>None</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RootResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndPutTableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndPutRowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndDeleteTableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndDeleteRowResource.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="4721" opendate="2011-11-1 00:00:00" fixdate="2011-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain Delete Markers after Major Compaction</summary>
      <description>There is a need to provide long TTLs for delete markers. This is useful when replicating hbase logs from one cluster to another. The receiving cluster shouldn't compact away the delete markers because the affected key-values might still be on the way.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="477" opendate="2008-2-29 00:00:00" fixdate="2008-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for an HBASE_CLASSPATH</summary>
      <description>We have mention of HBASE_CLASSPATH in hbase-env.sh but its not actually read anywhere. Make it work like HADOOP_CLASSPATH. See classpath discussion on this page, http://wiki.apache.org/hadoop/Hbase/Jython, for a use case.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="480" opendate="2008-2-29 00:00:00" fixdate="2008-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to manually merge two regions</summary>
      <description>hbase-471 needs a tool to merge two regions that have same start key. This tool may be of use elsewhere making repairs.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4830" opendate="2011-11-19 00:00:00" fixdate="2011-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver BLOCKED on WAITING DFSClient$DFSOutputStream.waitForAckedSeqno running 0.20.205.0+</summary>
      <description>Running 0.20.205.1 (I was not at tip of the branch) I ran into the following hung regionserver:"regionserver7003.logRoller" daemon prio=10 tid=0x00007fd98028f800 nid=0x61af in Object.wait() [0x00007fd987bfa000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:485) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3606) - locked &lt;0x00000000f8656788&gt; (a java.util.LinkedList) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3595) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3687) - locked &lt;0x00000000f8656458&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3626) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86) at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966) - locked &lt;0x00000000f8655998&gt; (a org.apache.hadoop.io.SequenceFile$Writer) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:214) at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:791) at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:578) - locked &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94) at java.lang.Thread.run(Thread.java:662)Other threads are like this (here's a sample):"regionserver7003.logSyncer" daemon prio=10 tid=0x00007fd98025e000 nid=0x61ae waiting for monitor entry [0x00007fd987cfb000] java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1074) - waiting to lock &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1195) at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1057) at java.lang.Thread.run(Thread.java:662)...."IPC Server handler 0 on 7003" daemon prio=10 tid=0x00007fd98049b800 nid=0x61b8 waiting for monitor entry [0x00007fd9872f1000] java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:1007) - waiting to lock &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1798) at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1668) at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2980) at sun.reflect.GeneratedMethodAccessor636.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1325)Looks like HDFS-1529? (Todd?)</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="490" opendate="2008-3-4 00:00:00" fixdate="2008-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doubly-assigned .META.; master uses one and clients another</summary>
      <description>Internal cluster has two .META.,,1 regions up (Its possible for a region to be added twice to the unassigned map if meta scans run close together). Worse is that the master is working with one .META. but when clients come in, they're being give the other. Makes for odd results.Made it a blocker. Still trying to track down how master doesn't see subsequent update of .META. info in ROOT.....</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RootScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.MetaScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5000" opendate="2011-12-9 00:00:00" fixdate="2011-12-9 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Speed up simultaneous reads of a block when block caching is turned off</summary>
      <description>With block caching, when one client starts reading a block and another one comes around asking for the same block, the second client waits for the first one to finish reading and returns the block from cache. This is achieved by locking on the block offset using IdLock, a "sparse lock" primitive allowing to lock on arbitrary long numbers. However, in case there is no block caching, there is no reason to wait for other clients that are reading the same block. One challenge optimizing this that we don't necessary have accurate information about whether other HFile API clients interested in the block would cache it.Setting priority as minor, as it is very unusual to turn off block caching.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="505" opendate="2008-3-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region assignments should never time out so long as the region server reports that it is processing the open request</summary>
      <description>Currently, when the master assigns a region to a region server, it extends the reassignment timeout when the region server reports that it is processing the open. This only happens once, and so if the region takes a long time to come on line due to a large set of transactions in the redo log or because the initial compaction takes a long time, the master will assign the region to another server when the reassignment timeout occurs.Assigning a region to multiple region servers can easily corrupt the region. For example:region server 1 is processing the redo log creating a new mapfile. It takes more than one interval to do so so the master assigns the region to region server 2. region server 2 starts processing the redo log creating essentially the same mapFile as region server 1, but with a different name. region server 2 can fail to open the region if region server 1 deletes the old log file or if it tries to open the new mapFile that region server 1 is creating.region server 1 can fail to open the region if it tries to open the mapFile that region server 2 is creating.Often region server 1 eventually succeeds and reports to the master that it has finished opening the region, but the master tells it to close that region because it has assigned it to another server. Region server 2 often fails to open the region, because the old log file has been deleted, or it fails to process the new map file created by region server 1.Proposed solution:During the open process the region server should send a MSG_PROCESS_OPEN with each heartbeat until the region is opened (when it sends MSG_REGION_OPEN). The master will extend the reassignment timeout with each MSG_PROCESS_OPEN it receives and will not assign the region to another server so long as it continues to receive heart beat messages from the region server processing the open.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5050" opendate="2011-12-16 00:00:00" fixdate="2011-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[rest] SPNEGO-based authentication</summary>
      <description>Currently the REST gateway can authenticate to a HBase cluster using a preconfigured principal. This provides a limited form of secure operation where one or more gateways can be deployed with distinct principals granting appropriate levels of privilege, but the service ports must be protected through network ACLs. This is at best a stopgap.SPNEGO is the standard mechanism for Kerberos authentication over HTTP. Enhance the REST gateway such that it provides this option, and issues requests to the HBase cluster with the established context.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="510" opendate="2008-3-13 00:00:00" fixdate="2008-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HConnectionManger.listTables returns empty list if exception (though there may be many tables present)</summary>
      <description>Its a problem because commonly a check for existence will get list of current tables.Yesterday saw problem when .META. went off line. A piece of client code was asking for list of tables when .META. was offline, it was getting back an empty list because listTables do while was seeing 'org.apache.hadoop.hbase.NotServingRegionException: .META.,,1'Problem is the do while in HCM.listTables goes as long as startRow does not equal LAST_ROW but startRow is initialized with EMPTY_START_ROW which is equal to LAST_ROW.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="515" opendate="2008-3-14 00:00:00" fixdate="2008-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>At least double default timeouts between regionserver and master</summary>
      <description>501 added logging of how long we sleep at the end of the HRegionServer main run method before we send heartback back to the master. Last night during upload I saw this:2008-03-13 00:03:50,884 WARN org.apache.hadoop.hbase.util.Sleeper: We slept ten times longer than scheduled: 3000Above log has since been improved but its saying that we slept &gt; 30 seconds, the default timeout on master/regionserver communications (When the lease expires, master starts giving regions to someone else and when this regionserver reports in, its told to close all its regions).Server was under load from other processes but still...upload rate was not that rabid.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5264" opendate="2012-1-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 0.92.0 upgrade guide</summary>
      <description>Add an upgrade guide for going from 0.90 to 0.92.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.images.favicon.ico</file>
      <file type="M">src.main.resources.hbase-webapps.static.favicon.ico</file>
      <file type="M">src.docbkx.upgrading.xml</file>
      <file type="M">src.docbkx.performance.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5300" opendate="2012-1-30 00:00:00" fixdate="2012-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Research website symolic links for renaming of "HBase book" to "HBase Reference Guide"</summary>
      <description>Creating this task per a conversation I had with stack last week.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.metrics.xml</file>
      <file type="M">src.site.xdoc.acid-semantics.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5304" opendate="2012-1-31 00:00:00" fixdate="2012-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pluggable split key policy</summary>
      <description>We need a way to specify custom policies to determine split keys.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5430" opendate="2012-2-18 00:00:00" fixdate="2012-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix licenses in 0.92.1 -- RAT plugin won&amp;#39;t pass</summary>
      <description>Use the -Drelease profile to see we are missing 30 or so license. Fix.</description>
      <version>None</version>
      <fixedVersion>0.92.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="548" opendate="2008-3-27 00:00:00" fixdate="2008-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to online single region</summary>
      <description>A sequence of events put a region offline in the middle of an online table. I was unable to get the region backon by running 'enable table'. Here is a little bit of code that I ran to bring the region back online and get the table running again. This issue is about adding it to hbase tools (A new package named 'tools'?). public static void main(String[] args) throws IOException { HBaseConfiguration c = new HBaseConfiguration(); c.set("hbase.master", args[0]); HTable t = new HTable(c, new Text(".META.")); Text row = new Text(args[1]); byte [] cell = t.get(row, new Text("info:regioninfo")); HRegionInfo info = Writables.getHRegionInfo(cell); LOG.info(info); long id = t.startUpdate(row); info.setOffline(false); t.put(id, COL_REGIONINFO, Writables.getBytes(info)); t.delete(id, COL_SERVER); t.delete(id, COL_STARTCODE); t.commit(id); }</description>
      <version>0.1.0,0.1.1,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="552" opendate="2008-3-29 00:00:00" fixdate="2008-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bloom filter bugs</summary>
      <description>There are some bugs in Bloom filters in the code that deals with initialization and (de)serialization.</description>
      <version>0.1.0</version>
      <fixedVersion>0.1.1,0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.onelab.filter.Key.java</file>
      <file type="M">src.java.org.onelab.filter.DynamicBloomFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="554" opendate="2008-3-30 00:00:00" fixdate="2008-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>filters generate StackOverflowException</summary>
      <description>Below is from list.You're doing nothing wrong.The filters as written recurse until they find a match. If long stretches between matching rows, then you will get a StackOverflowError. Filters need to be changed. Thanks for pointing this out. Can you do without them for the moment until we get a chance to fix it?St.AckDavid Alves wrote:&gt; Hi St.Ack and all&gt; &gt; The error always occurs when trying to see if there are more rows to&gt; process.&gt; Yes I'm using a filter(RegExpRowFilter) to select only the rows (any&gt; row key) that match a specific value in one of the columns.&gt; Then I obtain the scanner just test the hasNext method, close the&gt; scanner and return.&gt; Am I doing something wrong?&gt; Still StackOverflowError is not supposed to happen right?&gt;&gt; Regards&gt; David Alves&gt; On Thu, 2008-03-27 at 12:36 -0700, stack wrote:&gt;&gt; You are using a filter? If so, tell us more about it.&gt;&gt; St.Ack&gt;&gt;&gt;&gt; David Alves wrote:&gt;&gt;&gt; Hi guys &gt;&gt;&gt;&gt;&gt;&gt; I 'm using HBase to keep data that is later indexed.&gt;&gt;&gt; The data is indexed in chunks so the cycle is get XXXX records index&gt;&gt;&gt; them check for more records etc...&gt;&gt;&gt; When I tryed the candidate-2 instead of the old 0.16.0 (which I&gt;&gt;&gt; switched to do to the regionservers becoming unresponsive) I got the&gt;&gt;&gt; error in the end of this email well into an indexing job.&gt;&gt;&gt; So you have any idea why? Am I doing something wrong?&gt;&gt;&gt;&gt;&gt;&gt; David Alves&gt;&gt;&gt;&gt;&gt;&gt; java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException:&gt;&gt;&gt; java.io.IOException: java.lang.StackOverflowError&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at java.io.DataInputStream.readLong(DataInputStream.java:399)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.readChunk(DFSClient.java:735)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.read(DFSClient.java:658)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.readBuffer(DFSClient.java:1130)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.read(DFSClient.java:1166)&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at org.apache.hadoop.io.DataOutputBuffer&gt;&gt;&gt; $Buffer.write(DataOutputBuffer.java:56)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1829)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1729)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1775)&gt;&gt;&gt; at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:461)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $StoreFileScanner.getNext(HStore.java:2350)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.hbase.HAbstractScanner.next(HAbstractScanner.java:256)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $HStoreScanner.next(HStore.java:2561)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1807)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; ...</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5560" opendate="2012-3-10 00:00:00" fixdate="2012-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid RegionServer GC caused by timed-out calls</summary>
      <description>The HBaseRpcServer queues up rpc responses if the socket connection to the client is not yet ready to receive data. Calls are queued here until a 15 minute timeout occurs. I am able to generate a full GC when I artificially make a client read rpc-responses very slowly. This jira is to make this 15 minute time configurable.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5610" opendate="2012-3-21 00:00:00" fixdate="2012-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GA to hbase.apache.org</summary>
      <description>Lets add the bit of script necessary tracking hbase.apache.org in google analytics. I was going to get it going first then open it to the PMC for viewing.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.vm</file>
    </fixedFiles>
  </bug>
  <bug id="562" opendate="2008-4-4 00:00:00" fixdate="2008-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Exceptions to subpackages</summary>
      <description>Some exceptions are only master-side or regionserver-side or client-side. Those that are, move to subpackages.</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.WrongRegionException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.RegionServerRunningException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.NoServerForRegionException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RetryableMetaOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ModifyColumn.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.InvalidColumnNameException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.FileSystemVersionException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="569" opendate="2008-4-8 00:00:00" fixdate="2008-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DemoClient.php</summary>
      <description>Adding DemoClient.php implementation</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.README.txt</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="573" opendate="2008-4-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not read hadoop-*.xml for dfs configuration after moving out hadoop/contrib</summary>
      <description>When HBase was in hadoop/contrib, the hbase script set both HADOOP_CONF_DIRand HBASE_CONF_DIR to CLASSPATH, so that dfs's configuration can be loadedcorrectly. However, when moved out hadoop/contrib, it only sets HBASE_CONF_DIR.I can think of several possible solutions:1) set HADOOP_CONF_DIR in hbase-env.sh, then add HADOOP_CONF_DIR to CLASSPATH as before2) Instruct user to create links for hadoop-*.xml if they want to customize some dfs settings.3) If only a small set of dfs confs are related to dfs's client, maybe they can be set via hbase-site.xml, then hbase sets these for us when create a FileSystem obj.Please see the thread "# of dfs replications when using hbase" on hbase-user@.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="574" opendate="2008-4-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not load hadoop native libs</summary>
      <description>After moving out from hadoop/contrib, the standalone release does not include hadoop native libs in hbase/lib/native while it still includes hadoop-core.jar. I think they should be included as well to improve speed for compression and decompression.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="613" opendate="2008-5-2 00:00:00" fixdate="2008-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Timestamp-anchored scanning fails to find all records</summary>
      <description>If I add 3 versions of a cell and then scan across the first set of added cells using a timestamp that should only get values from the first upload, a bunch are missing (I added 100k on each of the three uploads). I thought it the fact that we set the number of cells found back to 1 in HStore when we move off current row/column but that doesn't seem to be it. I also tried upping the MAX_VERSIONs on my table and that seemed to have no effect. Need to look closer.Build a unit test because replicating on cluster takes too much time.</description>
      <version>0.1.0,0.1.1,0.1.2</version>
      <fixedVersion>0.1.3,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MultiRegionTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HAbstractScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="79" opendate="2008-1-30 00:00:00" fixdate="2008-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] When HBase needs to be migrated, it should display a message on stdout, not just in the logs</summary>
      <description>When you upgrade your HBase code version, there is occasionally the need to migrate the underlying data store to a new version. However, if you are unaware of this need, then you'll be very confused by what happens when you restart HBase. Using start-hbase.sh, you get messages indicating that the master and regionservers started as expected. However, in reality, it will have tried to start and failed due to a version mismatch. This information is displayed in the logs, but you won't know that until you go log diving.Instead, let's have the start-hbase.sh script do a check to see if the version number is wrong itself. That way, if it fails, it can write messages about startup failure to the console instead of to the logs. This will make new admins much happier.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
