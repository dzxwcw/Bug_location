<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="14086" opendate="2015-7-15 00:00:00" fixdate="2015-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14087" opendate="2015-7-15 00:00:00" fixdate="2015-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14249" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14250" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14251" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14338" opendate="2015-8-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14340" opendate="2015-8-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14348" opendate="2015-8-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14349" opendate="2015-9-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1437" opendate="2009-5-19 00:00:00" fixdate="2009-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>broken links in hbase.org</summary>
      <description>See http://hadoop.apache.org/hbase/docs/r0.19.2/. See along the LHS. E.g. metrics is at top level but here its relative and not found.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14544" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow HConnectionImpl to not refresh the dns on errors</summary>
      <description>Some clusters will have static ip addresses and forced dns lookup can cause extra instability. Allow users to tun that feature off, if wanted.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14594" opendate="2015-10-13 00:00:00" fixdate="2015-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use new DNS API introduced in HADOOP-12437</summary>
      <description>HADOOP-12437 introduced a new API to org.apache.hadoop.net.DNS: getDefaultHost(String, String, boolean).The purpose of this method (the boolean argument really) is to change the functionality so that when rDNS fails, InetAddress#getCanonicalHostName() is consulted which includes resolution via the hosts file.The direct application of this new method is relevant on hosts with multiples NICs and Kerberos enabled.Sadly, this method only exists in 2.8.0-SNAPSHOT, so to benefit from the fix without great pain, some reflection is required.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.16,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepJob.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.filter.AuthFilter.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14683" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batching in buffered mutator is awful when adding lists of mutations.</summary>
      <description>When adding a list of mutations to the buffer the limit is checked after every mutation is added.This leads to lots of tries to flush.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14684" opendate="2015-10-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try to remove all MiniMapReduceCluster in unit tests</summary>
      <description>As discussion in dev list, we will try to do MR job without MiniMapReduceCluster.Testcases will run faster and more reliable.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobSecureExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableMapReduceUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduceBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScanBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestSyncTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestRowCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultithreadedTableMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithVisibilityLabels.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithTTLs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithOperationAttributes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHRegionPartitioner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHashTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCopyTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCellCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14708" opendate="2015-10-27 00:00:00" fixdate="2015-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use copy on write Map for region location cache</summary>
      <description>Internally a co-worker profiled their application that was talking to HBase. &gt; 60% of the time was spent in locating a region. This was while the cluster was stable and no regions were moving.To figure out if there was a faster way to cache region location I wrote up a benchmark here: https://github.com/elliottneilclark/benchmark-hbase-cacheThis tries to simulate a heavy load on the location cache. 24 different threads. 2 Deleting location data 2 Adding location data Using floor to get the result.To repeat my work just run ./run.sh and it should produce a result.csvResults:ConcurrentSkiplistMap is a good middle ground. It's got equal speed for reading and writing.However most operations will not need to remove or add a region location. There will be potentially several orders of magnitude more reads for cached locations than there will be on clearing the cache.So I propose a copy on write tree map.</description>
      <version>1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MetaTableAccessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MetaCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="14758" opendate="2015-11-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UT case for unchecked error/exception thrown in AsyncProcess#sendMultiAction</summary>
      <description>This is an addendum for HBASE-14359, open a new JIRA to trigger HadoopQA run</description>
      <version>1.1.2,0.98.15</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="1476" opendate="2009-6-2 00:00:00" fixdate="2009-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scaling compaction with multiple threads</summary>
      <description>Was thinking we should build in support to be able to handle more then one thread for compactions this will allow us to keep up with compactions when we get to the point where we store Tb's of data per node and may regionsMaybe a configurable setting to set how many threads a region server can use for compactions.With compression turned on my compactions are limited by cpu speed with multi cores then it would be nice to be able to scale compactions to 2 or more cores.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14761" opendate="2015-11-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deletes with and without visibility expression do not delete the matching mutation</summary>
      <description>This is from the user list as reported by Anoop Sharma running into an issue related to visibility expressions and delete.Example run from hbase shell is listed below.Will appreciate any help on this issue.thanks.In the example below, user running queries has ‘MANAGER’ authorization.*First example:* add a column with visib expr ‘MANAGER’ delete it by passing in visibility of ‘MANAGER’ This works and scan doesn’t return anything.*Second example:* add a column with visib expr ‘MANAGER’ delete it by not passing in any visibility. This doesn’t delete the column. Scan doesn’t return the row but RAW scan shows the column marked as deleteColumn. Now if delete is done again with visibility of ‘MANAGER’, it still doesn’t delete it and scan returns the original column.*Example 1:*hbase(main):096:0&gt; create 'HBT1', 'cf'hbase(main):098:0* *put 'HBT1', 'John', 'cf:a', 'CA',{VISIBILITY=&gt;'MANAGER'}*hbase(main):099:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446154722055,value=CA1 row(s) in 0.0030 secondshbase(main):100:0&gt; *delete 'HBT1', 'John', 'cf:a', {VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0030 secondshbase(main):101:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL0 row(s) in 0.0030 seconds*Example 2:*hbase(main):010:0* *put 'HBT1', 'John', 'cf:a', 'CA',{VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0040 secondshbase(main):011:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0060 secondshbase(main):012:0&gt; *delete 'HBT1', 'John', 'cf:a'*0 row(s) in 0.0090 secondshbase(main):013:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0050 secondshbase(main):014:0&gt; *scan 'HBT1', {RAW =&gt; true}*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346519,type=DeleteColumn1 row(s) in 0.0060 secondshbase(main):015:0&gt; *delete 'HBT1', 'John', 'cf:a', {VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0030 secondshbase(main):016:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0040 secondshbase(main):017:0&gt; *scan 'HBT1', {RAW =&gt; true}*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346601,type=DeleteColumn1 row(s) in 0.0060 seconds</description>
      <version>1.0.0,1.0.1,1.1.0,1.0.2,1.1.2,0.98.15</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDeletes.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="14778" opendate="2015-11-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make block cache hit percentages not integer in the metrics system</summary>
      <description>Once you're close to the 90%+ it's hard to see a difference because getting a full percent change is rare.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14793" opendate="2015-11-10 00:00:00" fixdate="2015-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow limiting size of block into L1 block cache.</summary>
      <description>G1GC does really badly with long lived large objects. Lets allow limiting the size of a block that can be kept in the block cache.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14795" opendate="2015-11-10 00:00:00" fixdate="2015-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the spark-hbase scan operations</summary>
      <description>This is a sub-jira of HBASE-14789. This jira is to focus on the replacement of TableInputFormat for a more custom scan implementation that will make the following use case more effective.Use case:In the case you have multiple scan ranges on a single table with in a single query. TableInputFormat will scan the the outer range of the scan start and end range where this implementation can be more pointed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14796" opendate="2015-11-11 00:00:00" fixdate="2015-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the Gets in the connector</summary>
      <description>Current the Spark-Module Spark SQL implementation gets records from HBase from the driver if there is something like the following found in the SQL.rowkey = 123The reason for this original was normal sql will not have many equal operations in a single where clause.Zhan, had brought up too points that have value.1. The SQL may be generated and may have many many equal statements in it so moving the work to an executor protects the driver from load2. In the correct implementation the drive is connecting to HBase and exceptions may cause trouble with the Spark application and not just with the a single task execution</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseResources.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14804" opendate="2015-11-13 00:00:00" fixdate="2015-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell&amp;#39;s create table command ignores &amp;#39;NORMALIZATION_ENABLED&amp;#39; attribute</summary>
      <description>I am trying to create a new table and set the NORMALIZATION_ENABLED as true, but seems like the argument NORMALIZATION_ENABLED is being ignored. And the attribute NORMALIZATION_ENABLED is not displayed on doing a desc command on that tablehbase(main):020:0&gt; create 'test-table-4', 'cf', {NORMALIZATION_ENABLED =&gt; 'true'}An argument ignored (unknown or overridden): NORMALIZATION_ENABLED0 row(s) in 4.2670 seconds=&gt; Hbase::Table - test-table-4hbase(main):021:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4 COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0430 secondsHowever, on doing an alter command on that table we can set the NORMALIZATION_ENABLED attribute for that tablehbase(main):022:0&gt; alter 'test-table-4', {NORMALIZATION_ENABLED =&gt; 'true'}Unknown argument ignored: NORMALIZATION_ENABLEDUpdating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.3640 secondshbase(main):023:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4, {TABLE_ATTRIBUTES =&gt; {NORMALIZATION_ENABLED =&gt; 'true'} COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0190 secondsI think it would be better to have a single step process to enable normalization while creating the table itself, rather than a two step process to alter the table later on to enable normalization</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="14805" opendate="2015-11-13 00:00:00" fixdate="2015-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>status should show the master in shell</summary>
      <description>status 'simple' or 'detailed' only shows the regionservers and regions, but not the active master. Actually, there is no way to know about the active masters from the shell it seems.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="14837" opendate="2015-11-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Procedure Queue Improvement</summary>
      <description>Add event support to the procedure queue to avoid spinning and remove all the immutable object creations from the java classes Map, Tree, ... the queues that can't be executed because are waiting for an event (e.g. master initialized) or someone else have an exclusive lock are pulled out the run queuehttps://reviews.apache.org/r/40460/</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureFairRunQueues.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureSimpleRunQueue.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureRunnableSet.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureFairRunQueues.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ProcedureInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="14840" opendate="2015-11-19 00:00:00" fixdate="2015-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sink cluster reports data replication request as success though the data is not replicated</summary>
      <description>Scenario:Sink cluster is downCreate a table and enable table replicationPut some dataNow restart the sink clusterObservance:Data is not replicated in sink cluster but still source cluster updates the WAL log position in ZK, resulting in data loss in sink cluster.</description>
      <version>1.1.2,1.0.3,0.98.16</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="14849" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14922" opendate="2015-12-3 00:00:00" fixdate="2015-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delayed flush doesn&amp;#39;t work causing flush storms.</summary>
      <description>Starting all regionservers at the same time will mean that most PeriodicMemstoreFlusher's will be running at the same time. So all of these threads will queue flushes at about the same time.This was supposed to be mitigated by Delayed. However that isn't nearly enough. This results in the immediate filling up and then draining of the flush queues every hour.</description>
      <version>1.2.0,1.1.2,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestChoreService.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ChoreService.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14928" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14939" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document bulk loaded hfile replication</summary>
      <description>After HBASE-13153 is committed we need to add that information under the Cluster Replication section in HBase book.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14951" opendate="2015-12-8 00:00:00" fixdate="2015-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make hbase.regionserver.maxlogs obsolete</summary>
      <description>There was a discussion in HBASE-14388 related to maximum number of log files. It was an agreement that we should calculate this number in a code but still need to honor user's setting. Maximum number of log files now is calculated as following: maxLogs = HEAP_SIZE * memstoreRatio * 2/ LogRollSize</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="15021" opendate="2015-12-21 00:00:00" fixdate="2015-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105 � NullPointer Run 2: TestRSStatusServlet.testBasic:105 � NullPointer Run 3: TestRSStatusServlet.testBasic:105 � NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 2: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 3: TestRSStatusServlet.testWithRegions:119 � NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15027" opendate="2015-12-22 00:00:00" fixdate="2015-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor the way the CompactedHFileDischarger threads are created</summary>
      <description>As per suggestion given over in HBASE-14970, if we need to create a single thread pool service for the CompactionHFileDischarger we need to create an exectuor service in the RegionServer level and create discharger handler threads (Event handlers) and pass the Event to the new Exectuor service that we create for the compaction hfiles discharger. What should be the default number of threads here? If a HRS holds 100 of regions - will 10 threads be enough? This issue will try to resolve this with tests and discussions and suitable patch will be updated in HBASE-14970 for branch-1 once this is committed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactedHFilesDischarger.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionReplayEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactedHFilesDischarger.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MockRegionServerServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.OnlineRegions.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactedHFilesDischarger.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.ExecutorType.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.EventType.java</file>
    </fixedFiles>
  </bug>
  <bug id="15087" opendate="2016-1-9 00:00:00" fixdate="2016-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-common findbugs complaints</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestByteBufferUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.UnsafeAccess.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.DynamicClassLoader.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.DNS.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.types.CopyOnWriteArrayMap.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ProcedureInfo.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.OffheapKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="15115" opendate="2016-1-15 00:00:00" fixdate="2016-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs complaints in hbase-client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.MultiRowRangeFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.LongComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FuzzyRowFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetryingCallerInterceptorFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MetricsConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HConnectable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15118" opendate="2016-1-15 00:00:00" fixdate="2016-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs complaint in hbase-server</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.SortedList.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.jmx.JMXJsonServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HFileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.JMXListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HashTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.HFileLinkCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ExpiredMobFileCleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.ExpiredMobFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaState.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.UserQuotaState.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreChunkPool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.TimeRangeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.SizeCachedKeyValue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.ExportSnapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.IdReadWriteLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionMover.java</file>
    </fixedFiles>
  </bug>
  <bug id="1512" opendate="2009-6-12 00:00:00" fixdate="2009-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Coprocessors: Support aggregate functions</summary>
      <description>Chatting with jgray and holstad at the kitchen table about counts, sums, and other aggregating facility, facility generally where you want to calculate some meta info on your table, it seems like it wouldn't be too hard making a filter type that could run a function server-side and return the result ONLY of the aggregation or whatever.For example, say you just want to count rows, currently you scan, server returns all data to client and count is done by client counting up row keys. A bunch of time and resources have been wasted returning data that we're not interested in. With this new filter type, the counting would be done server-side and then it would make up a new result that was the count only (kinda like mysql when you ask it to count, it returns a 'table' with a count column whose value is count of rows). We could have it so the count was just done per region and return that. Or we could maybe make a small change in scanner too so that it aggregated the per-region counts.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.ExecResult.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.AggregationClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15120" opendate="2016-1-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport HBASE-14883 to branch-1.1</summary>
      <description>When checking branch-1.1 UT in HBASE-13590, found TestSplitTransactionOnCluster#testFailedSplit will fail at a 12/50 chance. The issue is fixed by HBASE-14883 but the change didn't go into branch-1.1</description>
      <version>1.1.2</version>
      <fixedVersion>1.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.jamon</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15145" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK and Replication should authenticate to zookepeer using server principal</summary>
      <description>In secure clusters, we protect znodes with the server principal in zk. However, if a user wants to add a replication peer or run HBCK, then she will get Auth exception. This was not a problem due to an earlier bug. For replication, the long term fix is HBASE-11392. However, we should still have a way to launch zkcli with the server principals for manual inspection / manipulation. HBCK should always assume the server principals. Thanks Koelli for reporting this.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="15163" opendate="2016-1-25 00:00:00" fixdate="2016-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sampling code and metrics for get/scan/multi/mutate count separately</summary>
      <description>This way we could see QPS of different kinds of requests, to better analyze what's causing hot spot in system</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15172" opendate="2016-1-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting storage policy in bulkload</summary>
      <description>When using tiered HFile storage, we should be able to generating hfile with correct storage type during bulkload. This JIRA is targeting at making it possible.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  <bug id="15173" opendate="2016-1-26 00:00:00" fixdate="2016-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute mergeRegions RPC call as the request user</summary>
      <description>This is follow up to HBASE-15132Master currently sends mergeRegions RPC to region server under user 'hbase'.This issue is to execute mergeRegions RPC call as the request userSee tail of HBASE-15132 for related discussion.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DispatchMergingRegionHandler.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15174" opendate="2016-1-26 00:00:00" fixdate="2016-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client Public API should not have PB objects in 2.0</summary>
      <description>Some more cleanup for the parent jira. We have leaked some PB structs in Admin (and possible other places). We should clean up these API before 2.0.Examples include: AdminProtos.GetRegionInfoResponse.CompactionState getCompactionState(final TableName tableName) throws IOException; .... void snapshot(final String snapshotName, final TableName tableName, HBaseProtos.SnapshotDescription.Type type) throws IOException, SnapshotCreationException, IllegalArgumentException; .... MasterProtos.SnapshotResponse takeSnapshotAsync(HBaseProtos.SnapshotDescription snapshot) throws IOException, SnapshotCreationException;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Triple.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestInterfaceAudienceAnnotations.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionLoadStats.java</file>
    </fixedFiles>
  </bug>
  <bug id="15871" opendate="2016-5-20 00:00:00" fixdate="2016-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memstore flush doesn&amp;#39;t finish because of backwardseek() in memstore scanner.</summary>
      <description>Sometimes in our production hbase cluster, it takes a long time to finish memstore flush.( for about more than 30 minutes)the reason is that a memstore flusher thread calls StoreScanner.updateReaders(), waits for acquiring a lock that store scanner holds in StoreScanner.next() and backwardseek() in memstore scanner runs for a long time.I think that this condition could occur in reverse scan by the following process.1) create a reversed store scanner by requesting a reverse scan.2) flush a memstore in the same HStore.3) puts a lot of cells in memstore and memstore is almost full.4) call the reverse scanner.next() and re-create all scanners in this store because all scanners was already closed by 2)'s flush() and backwardseek() with store's lastTop for all new scanners.5) in this status, memstore is almost full by 2) and all cells in memstore have sequenceID greater than this scanner's readPoint because of 2)'s flush(). this condition causes searching all cells in memstore, and seekToPreviousRow() repeatly seach cells that are already searched if a row has one column. (described this in more detail in a attached file.)6) flush a memstore again in the same HStore, and wait until 4-5) process finished, to update store files in the same HStore after flusing.I searched HBase jira. and found a similar issue. (HBASE-14497) but, HBASE-14497's fix can't solve this issue because that fix just changed recursive call to loop.(and already applied to our HBase version)</description>
      <version>1.1.2</version>
      <fixedVersion>2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SegmentScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15958" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ClaimQueues on top of HBase</summary>
      <description>Building on HBase-15883. Now implementing the claim queues procedure within an HBase table. Peer tracking will still be performed by ZooKeeper though. Revision at https://reviews.apache.org/r/48232/</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateHBaseImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesHBaseImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesArguments.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
    </fixedFiles>
  </bug>
  <bug id="16213" opendate="2016-7-12 00:00:00" fixdate="2016-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A new HFileBlock structure for fast random get</summary>
      <description>HFileBlock store cells sequential, current when to get a row from the block, it scan from the first cell until the row's cell.The new structure store every row's start offset with data, so it can find the exact row with binarySearch.I use EncodedSeekPerformanceTest test the performance.First use ycsb write 100w data, every row have only one qualifier, and valueLength=16B/64/256B/1k.Then use EncodedSeekPerformanceTest to test random read 1w or 100w row, and also record HFileBlock's dataSize/dataWithMetaSize in the encoding.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestSeekToBlockWithEncoders.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.SizeCachedNoTagsKeyValue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.SizeCachedKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.java</file>
    </fixedFiles>
  </bug>
  <bug id="16214" opendate="2016-7-12 00:00:00" fixdate="2016-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add in UI description of Replication Table</summary>
      <description>Add a description for the Replication Table in the HBase UI</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="16215" opendate="2016-7-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up references for EOM release lines</summary>
      <description>I've seen us state a few places that 1.0.z is EOM. We should clean up remaining references remove 1.0.z artifact from dist.apache remove it from the ref guide (java prereqs, hadoop prereqs, RM list) remove unreleased 1.0.z versions from JIRA archive released 1.0.z versions in JIRAedit: expanded to include 0.94.y first EOM DISCUSS, second EOM DISCUSS 0.98.y EOM DISCUSS</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16270" opendate="2016-7-21 00:00:00" fixdate="2016-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle duplicate clearing of snapshot in region replicas</summary>
      <description>We have an HBase (1.1.2) production cluster with 58 region servers and a staging cluster with 6 region servers.For both clusters, we enabled region replicas with the following settings:hbase.regionserver.storefile.refresh.period = 0hbase.region.replica.replication.enabled = truehbase.region.replica.replication.memstore.enabled = truehbase.master.hfilecleaner.ttl = 3600000hbase.master.loadbalancer.class = org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancerhbase.meta.replica.count = 3hbase.regionserver.meta.storefile.refresh.period = 30000hbase.region.replica.wait.for.primary.flush = truehbase.region.replica.storefile.refresh.memstore.multiplier = 4We then altered our HBase tables to have REGION_REPLICATION =&gt; 2Both clusters got into a state where a few region servers were spewing the following error below in the HBase logs. In one instance the error occurred over 70K times. At this time, these region servers would see 10x write traffic (the hadoop.HBase.RegionServer.Server.writeRequestCount metric) and in some instances would crash.At the same time, we would see secondary regions move and then go into the "reads are disabled" state for extended periods. It appears there is a race condition where the DefaultMemStore::clearSnapshot method might be called more than once in succession. The first call would set snapshotId to -1, but the second call would throw an exception. It seems the second call should just return if the snapshotId is already -1, rather than throwing an exception.Thu Jul 21 08:38:50 UTC 2016, RpcRetryingCaller{globalStartTime=1469090201543, pause=100, retries=35}, org.apache.hadoop.hbase.regionserver.UnexpectedStateException: org.apache.hadoop.hbase.regionserver.UnexpectedStateException: Current snapshot id is -1,passed 1469085004304 at org.apache.hadoop.hbase.regionserver.DefaultMemStore.clearSnapshot(DefaultMemStore.java:187) at org.apache.hadoop.hbase.regionserver.HStore.updateStorefiles(HStore.java:1054) at org.apache.hadoop.hbase.regionserver.HStore.access$500(HStore.java:128) at org.apache.hadoop.hbase.regionserver.HStore$StoreFlusherImpl.replayFlush(HStore.java:2270) at org.apache.hadoop.hbase.regionserver.HRegion.replayFlushInStores(HRegion.java:4487) at org.apache.hadoop.hbase.regionserver.HRegion.replayWALFlushCommitMarker(HRegion.java:4388) at org.apache.hadoop.hbase.regionserver.HRegion.replayWALFlushMarker(HRegion.java:4191) at org.apache.hadoop.hbase.regionserver.RSRpcServices.doReplayBatchOp(RSRpcServices.java:776) at org.apache.hadoop.hbase.regionserver.RSRpcServices.replay(RSRpcServices.java:1655) at org.apache.hadoop.hbase.protobuf.generated.AdminProtos$AdminService$2.callBlockingMethod(AdminProtos.java:22255) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2114) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101) at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130) at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.1.2</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AbstractMemStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="1664" opendate="2009-7-16 00:00:00" fixdate="2009-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable 1058 on catalog tables</summary>
      <description>See tail of hbase-1058 for discussion.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16643" opendate="2016-9-16 00:00:00" fixdate="2016-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reverse scanner heap creation may not allow MSLAB closure due to improper ref counting of segments</summary>
      <description>In the reverse scanner case,While doing 'initBackwardHeapIfNeeded' in MemstoreScanner for setting the backward heap, we do a if ((backwardHeap == null) &amp;&amp; (forwardHeap != null)) { forwardHeap.close(); forwardHeap = null; // before building the heap seek for the relevant key on the scanners, // for the heap to be built from the scanners correctly for (KeyValueScanner scan : scanners) { if (toLast) { res |= scan.seekToLastRow(); } else { res |= scan.backwardSeek(cell); } }forwardHeap.close(). This would internally decrement the MSLAB ref counter for the current active segment and snapshot segment.When the scan is actually closed again we do close() and that will again decrement the count. Here chances are there that the count would go negative and hence the actual MSLAB closure that checks for refCount==0 will fail. Apart from this, when the refCount becomes 0 after the firstClose if any other thread requests to close the segment, then we will end up in corrupted segment because the segment could be put back to the MSLAB pool.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingToCellArrayMapMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SegmentScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreCompactorIterator.java</file>
    </fixedFiles>
  </bug>
  <bug id="16750" opendate="2016-10-3 00:00:00" fixdate="2016-5-3 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>hbase compilation failed on power system</summary>
      <description>Hi,hbase compilation failed on IBM power system ppc64le architecture with below error:Hbase Failure:[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 04:33 min[INFO] Finished at: 2016-09-30T08:58:47-04:00[INFO] Final Memory: 215M/843M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc (output-pdf) on project hbase: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc (output-pdf) on project hbase: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288) at org.apache.maven.cli.MavenCli.main(MavenCli.java:199) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.PluginExecutionException: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:145) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207) ... 20 moreCaused by: org.jruby.exceptions.RaiseException: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.jruby.RubyFile.size(org/jruby/RubyFile.java:1108) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/document_state.rb:69) at RUBY.each(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/object_store.rb:70) at org.jruby.RubyArray.each(org/jruby/RubyArray.java:1613) at RUBY.each(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/object_store.rb:69) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/document_state.rb:68) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document/internals.rb:141) at RUBY.render(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:359) at RUBY.render_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:376) at org.jruby.RubyIO.open(org/jruby/RubyIO.java:1181) at RUBY.render_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:376) at RUBY.write(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/asciidoctor-pdf-1.5.0.alpha.6/lib/asciidoctor-pdf/converter.rb:1235) at RUBY.write(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor/document.rb:1051) at RUBY.convert(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor.rb:1504) at RUBY.convert_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor.rb:1562) at RUBY.convertFile(&lt;script&gt;:68) at org.jruby.gen.InterfaceImpl537412764.convertFile(org/jruby/gen/InterfaceImpl537412764.gen:13)[ERROR][ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionExceptionAs per this recommendation if we update asciidoctor version it fixes the problemhttps://github.com/asciidoctor/asciidoctorj/issues/402Please find the patch attached.</description>
      <version>1.1.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16751" opendate="2016-10-3 00:00:00" fixdate="2016-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tuning information to HBase Book</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16818" opendate="2016-10-12 00:00:00" fixdate="2016-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid multiple copies of binary data during the conversion from Result to Row</summary>
      <description>In the buildRow() of HBaseRelation, CellUtil.cloneValue will already create a copy of the data. If the data type is BinaryType, another copy is being made within Utils.hbaseFieldToScalaType in Utils.scala. Generally, binary data can be fairly large, so copying may be an expensive operation.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug id="17802" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note that minor versions can add methods to Interfaces</summary>
      <description>Clarify that adding methods to Interfaces in minor releases is allowed and that we'll always try to do it in a backward compatible way.Here is discussion from the list:http://search-hadoop.com/m/HBase/YGbbQfpjp1kozD7?subj=About+adding+methods+to+an+interface+which+is+part+of+our+public+API+in+minor+release</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17817" opendate="2017-3-21 00:00:00" fixdate="2017-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Regionservers log which tables it removed coprocessors from when aborting</summary>
      <description>When a coprocessor throws a runtime exception (e.g. NPE), the regionserver handles this according to hbase.coprocessor.abortonerror.If the coprocessor was loaded on a specific table, the output in the logs give no indication as to which table the coprocessor was removed from (or which version, or jarfile is the culprit). This causes longer debugging and recovery times.</description>
      <version>1.1.2</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
    </fixedFiles>
  </bug>
  <bug id="1854" opendate="2009-9-18 00:00:00" fixdate="2009-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the Region Historian</summary>
      <description>After much discussion on the mailing list and on IRC, it was decided that we remove the Region Historian until we come up with a better solution.</description>
      <version>None</version>
      <fixedVersion>0.20.1,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.table.jsp</file>
      <file type="M">src.webapps.master.regionhistorian.jsp</file>
      <file type="M">src.test.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.RegionHistorian.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18577" opendate="2017-8-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded client includes several non-relocated third party dependencies</summary>
      <description>we have some unexpected unrelocated third party dependencies in our shaded artifacts.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18614" opendate="2017-8-16 00:00:00" fixdate="2017-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting BUCKET_CACHE_COMBINED_KEY to false disables stats on RS UI</summary>
      <description>Enabling offheap cache and setting BUCKET_CACHE_COMBINED_KEY to false in site xml to make offheap cache a strict L2 cache to LRU cache, disables the L2 stats normally rendered on region server UI.</description>
      <version>1.1.2</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestBlockCacheReporting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="18765" opendate="2017-9-6 00:00:00" fixdate="2017-9-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The value of balancerRan is true even though no plans are executed</summary>
      <description>//We balance per group instead of per table List&lt;RegionPlan&gt; plans = new ArrayList&lt;&gt;(); for(Map.Entry&lt;TableName, Map&lt;ServerName, List&lt;HRegionInfo&gt;&gt;&gt; tableMap: getRSGroupAssignmentsByTable(groupName).entrySet()) { LOG.info("Creating partial plan for table " + tableMap.getKey() + ": " + tableMap.getValue()); List&lt;RegionPlan&gt; partialPlans = balancer.balanceCluster(tableMap.getValue()); LOG.info("Partial plan for table " + tableMap.getKey() + ": " + partialPlans); if (partialPlans != null) { plans.addAll(partialPlans); } } long startTime = System.currentTimeMillis(); balancerRan = plans != null;The plans is never null.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="18970" opendate="2017-10-9 00:00:00" fixdate="2017-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The version of jruby we use now can&amp;#39;t get interactive input from prompt</summary>
      <description>case 1: press enterhbase(main):002:0&gt; disable_all 'chia(.*)'chia_1 Disable the above 1 tables (y/n)?y^Mcase 2: press ctrl-jhbase(main):001:0&gt; disable_all 'chia(.*)'chia_1 Disable the above 1 tables (y/n)?y^J1 tables successfully disabledTook 5.0059 seconds</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.irb.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="20885" opendate="2018-7-13 00:00:00" fixdate="2018-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove entry for RPC quota from hbase:quota when RPC quota is removed.</summary>
      <description>When a RPC quota is removed (using LIMIT =&gt; 'NONE'), the entry from hbase:quota table is not completely removed. For e.g. see below:hbase(main):005:0&gt; create 't2','cf1'Created table t2Took 0.8000 seconds=&gt; Hbase::Table - t2hbase(main):006:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; '10M/sec'Took 0.1024 secondshbase(main):007:0&gt; list_quotasOWNER QUOTAS TABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINE1 row(s)Took 0.0622 secondshbase(main):008:0&gt; scan 'hbase:quota'ROW COLUMN+CELL t.t2 column=q:s, timestamp=1531513014463, value=PBUF\x12\x0B\x12\x09\x08\x04\x10\x80\x80\x80 \x05 \x021 row(s)Took 0.0453 secondshbase(main):009:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; 'NONE'Took 0.0097 secondshbase(main):010:0&gt; list_quotasOWNER QUOTAS0 row(s)Took 0.0338 secondshbase(main):011:0&gt; scan 'hbase:quota'ROW COLUMN+CELL t.t2 column=q:s, timestamp=1531513039505, value=PBUF\x12\x001 row(s)Took 0.0066 seconds</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.GlobalQuotaSettingsImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.ThrottleSettings.java</file>
    </fixedFiles>
  </bug>
  <bug id="20886" opendate="2018-7-14 00:00:00" fixdate="2018-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Auth] Support keytab login in hbase client</summary>
      <description>There're lots of questions about how to connect to kerberized hbase cluster through hbase-client api from user-mail and slack channel.hbase.client.keytab.file and hbase.client.keytab.principal are already existed in code base, but they are only used in Canary.This issue is to make use of two configs to support client-side keytab based login, after this issue resolved, hbase-client should directly connect to kerberized cluster without changing any code as long as hbase.client.keytab.file and hbase.client.keytab.principal are specified.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestUsersOperationsWithSecureHadoop.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.HBaseKerberosUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.UserProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="20949" opendate="2018-7-26 00:00:00" fixdate="2018-7-26 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Split/Merge table can be executed concurrently with DisableTableProcedure</summary>
      <description>The top flaky tests on the dashboard are all because of this.TestRestoreSnapshotFromClientTestSimpleRegionNormalizerOnClusterTheoretically this should not happen, need to dig more.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.LockAndQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="21017" opendate="2018-8-6 00:00:00" fixdate="2018-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit the expected states for open/close</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestEnableTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21707" opendate="2019-1-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix warnings in hbase-rsgroup module and also make the UTs more stable</summary>
      <description>By adding a sleep after the stopServer call, the test will hang there forever, need to dig more, maybe something wrong with the TRSP related stuffs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsKillRS.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBase.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBalance.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin2.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.master.balancer.TestRSGroupBasedLoadBalancer.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.master.balancer.RSGroupableBalancerTestBase.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="9393" opendate="2013-8-30 00:00:00" fixdate="2013-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region Server fails to properly close socket resulting in many CLOSE_WAIT to Data Nodes</summary>
      <description>HBase dose not close a dead connection with the datanode.This resulting in over 60K CLOSE_WAIT and at some point HBase can not connect to the datanode because too many mapped sockets from one host to another on the same port.The example below is with low CLOSE_WAIT count because we had to restart hbase to solve the porblem, later in time it will incease to 60-100K sockets on CLOSE_WAIT&amp;#91;root@hd2-region3 ~&amp;#93;# netstat -nap |grep CLOSE_WAIT |grep 21592 |wc -l13156&amp;#91;root@hd2-region3 ~&amp;#93;# ps -ef |grep 21592root 17255 17219 0 12:26 pts/0 00:00:00 grep 21592hbase 21592 1 17 Aug29 ? 03:29:06 /usr/java/jdk1.6.0_26/bin/java -XX:OnOutOfMemoryError=kill -9 %p -Xmx8000m -ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -Dhbase.log.dir=/var/log/hbase -Dhbase.log.file=hbase-hbase-regionserver-hd2-region3.swnet.corp.log ...</description>
      <version>0.94.2,0.98.0,1.0.1.1,1.1.2</version>
      <fixedVersion>1.4.0,1.3.2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
