<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="23774" opendate="2020-1-30 00:00:00" fixdate="2020-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Announce user-zh list</summary>
      <description>Let folks know about the new user-zh list that is dedicated for user questions in chinese (as opposed to the norm of english on user)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.9,2.2.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23957" opendate="2020-3-10 00:00:00" fixdate="2020-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[flakey test] client.TestMultiParallel fails to read hbase-site.xml</summary>
      <description>Saw this on a PreCommit run.Log file says2020-03-09 22:48:26,295 FATAL [Time-limited test] conf.Configuration(2853): error parsing conf hbase-site.xmljava.io.FileNotFoundException: /home/jenkins/jenkins-slave/workspace/Base-PreCommit-GitHub-PR_PR-1258@2/yetus-jdk8-hadoop2-check/src/hbase-server/target/test-classes/hbase-site.xml (No such file or directory) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:93) at sun.net.www.protocol.file.FileURLConnection.connect(FileURLConnection.java:90) at sun.net.www.protocol.file.FileURLConnection.getInputStream(FileURLConnection.java:188) at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2672) at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2746) at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2706) at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2579) at org.apache.hadoop.conf.Configuration.get(Configuration.java:1091) at org.apache.hadoop.conf.Configuration.getTrimmed(Configuration.java:1145) at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2363) at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2793) at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2810) at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100) at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849) at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389) at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:181) at org.apache.hadoop.hbase.fs.HFileSystem.&lt;init&gt;(HFileSystem.java:85) at org.apache.hadoop.hbase.fs.HFileSystem.get(HFileSystem.java:465) at org.apache.hadoop.hbase.HBaseTestingUtility.getTestFileSystem(HBaseTestingUtility.java:3180) at org.apache.hadoop.hbase.HBaseTestingUtility.getNewDataTestDirOnTestFS(HBaseTestingUtility.java:507) at org.apache.hadoop.hbase.HBaseTestingUtility.setupDataTestDirOnTestFS(HBaseTestingUtility.java:496) at org.apache.hadoop.hbase.HBaseTestingUtility.getDataTestDirOnTestFS(HBaseTestingUtility.java:469) at org.apache.hadoop.hbase.HBaseTestingUtility.getDataTestDirOnTestFS(HBaseTestingUtility.java:483) at org.apache.hadoop.hbase.HBaseTestingUtility.createDirsAndSetProperties(HBaseTestingUtility.java:651) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:603) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:586) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1039) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:1004) at org.apache.hadoop.hbase.client.TestMultiParallel.beforeClass(TestMultiParallel.java:96)This stage ran on H9.</description>
      <version>2.3.0,1.4.13,2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.5</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.resources.hbase-site2.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestUpdateConfiguration.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncClusterAdminApi.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdminBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="24077" opendate="2020-3-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When encounter RowTooBigException, log the row info.</summary>
      <description>Current when we encounter a big row and throw RowTooBigException, there is no information about the row, it hard to debug.</description>
      <version>2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="24078" opendate="2020-3-30 00:00:00" fixdate="2020-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpotBugs check automatically skip inapplicable modules</summary>
      <description>Our personality runs spotbugs on a per-module basis, rather than from the root module. It has a hard-coded list of modules to skip over. The comment says it only runs on modules that have content under src/main/java. Replace the fixed list with a find expression.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24189" opendate="2020-4-14 00:00:00" fixdate="2020-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALSplit recreates region dirs for deleted table with recovered edits data</summary>
      <description>Under the following scenario region directories in HDFS can be recreated with only recovered.edits in them: Create table "test" Put into "test" Delete table "test" Create table "test" again Crash the regionserver to which the put has went to force the WAL replay Region directory in old table is recreated in new table hbase hbck returns inconsistencyThis appears to happen due to the fact that WALs are not cleaned up once a table is deleted and they still contain the edits from old table. I've tried wal_roll command on the regionserver before crashing it, but it doesn't seem to help as under some circumstances there are still WAL files around. The only solution that works consistently is to restart regionserver before creating the table at step 4 because that triggers log cleanup on startup: https://github.com/apache/hbase/blob/f3ee9b8aa37dd30d34ff54cd39fb9b4b6d22e683/hbase-procedure/src/main/java/org/apache/hadoop/hbase/procedure2/store/wal/WALProcedureStore.java#L508Â Truncating a table also would be a workaround by in our case it's a no-go as we create and delete tables in our tests which run back to back (create table in the beginning of the test and delete in the end of the test).A nice option in our case would be to provide hbase shell utility to force clean up of log files manually as I realize that it's not really viable to clean all of those up every time some table is removed.</description>
      <version>2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CommonFSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="24208" opendate="2020-4-17 00:00:00" fixdate="2020-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove RS entry from zk draining servers node after RS been stopped</summary>
      <description>When a RS is been decommissioned, we will add an entry into the zk node. This will be there unless the same RS instance is recommissioned. But if we want to scale down a cluster, the best path would be to decommission the RSs in the scaling down nodes. The regions in these RSs will get moved to live RSs. In this case these decommissioned RSs will get stopped later. These will never get recommissioned. The zk nodes will still be there under draining servers path.We can remove this zk node when the RS is getting stopped.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2421" opendate="2010-4-7 00:00:00" fixdate="2010-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Put hangs for 10 retries on failed region servers</summary>
      <description>Since MultiPut got in, instead of calling getRegionLocationForRowWithRetries we now call getRegionServerWithRetries to send an array list of Puts. The problem is that if the region server failed, we'll still retry the 10 times in a backoff fashion even tho we get connections refused. This is also true for a single put since it's the same code path.Marking as critical since it almost disables our responsiveness to machine failures in certain cases where we are already sending a batch of edits when the server fails. Assigning to Ryan since he's been there recently.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestMultiParallelPut.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24212" opendate="2020-4-17 00:00:00" fixdate="2020-1-17 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>HMaster UI hangs when rsgorup is enabled and meta-region is not available</summary>
      <description>HMaster UI hangs when rsgroup is enabled and meta-region is not available.Steps to reproduce: Cluster: 1 Master, 3 RS Create rsgroup r1 and r2 Move rs1 to r1 and rs2 to r2 then all the regions are online on rs3 Stop rs3 Now accessÂ URL hmaster:Host:infoPort/master-status The page will not open.</description>
      <version>2.2.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="24218" opendate="2020-4-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hadoop 3.2.x in hadoop check</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.3.7,1.7.0,1.4.14,2.2.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24220" opendate="2020-4-20 00:00:00" fixdate="2020-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow that zk NOTEMPTY multi exception is retryable by running in-series</summary>
      <description>It looks like our zk behavior changed slightly on upgrade. In tests on occasion, I see a fail because the zk delete of dir fails because dir is not empty. Or, this flakey type we just ignored in the past (its rare enough &amp;#8211; I'm just seeing it of late because I'm running tests back-to-back on 4 different hardwares). Let me try allowing NOTEMPTY as a retryable. Lets see how it does.Threw in some debug while at it for other issues I've seen in test.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftHttpServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.AbstractTestDLS.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="24221" opendate="2020-4-21 00:00:00" fixdate="2020-6-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bulkLoadHFile by family</summary>
      <description>Support bulkLoadHFile by family to avoid long time waiting of bulkLoadHFile because of compacting at server side</description>
      <version>3.0.0-alpha-1,2.3.0,2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestBulkLoadHFilesByFamily.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestBulkLoadHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.BulkLoadHFilesTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="24252" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement proxyuser/doAs mechanism for hbase-http</summary>
      <description>The REST and Thrift interfaces for HBase already implement the standard hadoop ProxyUser mechanism for SPNEGO, but it is not implemented in hbase-httpserver.Implement it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.HttpServerFunctionalTest.java</file>
      <file type="M">hbase-http.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="24253" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-Find-Flaky-Tests job is failing because of can not build docker image</summary>
      <description>06:30:14 Step 3/4 : RUN DEBIAN_FRONTEND=noninteractive apt-get -qq -y update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get -qq -y install --no-install-recommends curl=7.58.0-2ubuntu3.8 python2.7=2.7.17-1~18.04 python-pip=9.0.1-2.3~ubuntu1.18.04.1 python-setuptools=39.0.1-2 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*06:30:14 ---&gt; Running in 484963d4232006:30:23 [91mE: Version '2.7.17-1~18.04' for 'python2.7' was not found06:30:23 [0mThe command '/bin/sh -c DEBIAN_FRONTEND=noninteractive apt-get -qq -y update &amp;&amp; DEBIAN_FRONTEND=noninteractive apt-get -qq -y install --no-install-recommends curl=7.58.0-2ubuntu3.8 python2.7=2.7.17-1~18.04 python-pip=9.0.1-2.3~ubuntu1.18.04.1 python-setuptools=39.0.1-2 &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/lib/apt/lists/*' returned a non-zero code: 100</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="24273" opendate="2020-4-27 00:00:00" fixdate="2020-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK&amp;#39;s "Orphan Regions on FileSystem" reports regions with referenced HFiles</summary>
      <description>This issue came up after merging regions. MergeTableRegionsProcedure removes the parent regions from hbase:meta and creates HFile references in child region to the old parent regions. Running `hbck_chore_run` right after the `merge_region` will show the parent regions in "Orphan Regions on FileSystem" until major compaction is run on child region which will remove HFile references and cause Catalog Janitor to clean up the parent regions.There are probably other situations which can cause the same issue (maybe region split?)Having "Orphan Regions on FileSystem" list parent regions and suggest to "hbase completebulkload" is dangerous in this case as completing bulk load will lead to stale HFile references in child region which will cause its OPEN to fail because referenced HFile doesn't exist.Figuring out these things for database administrators is tedious, so I think it would be reasonable to not consider regions with referenced HFiles to be orphans (or maybe could give an extra hint saying that it has referenced HFiles).</description>
      <version>2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMetaFixer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.HConnectionTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2430" opendate="2010-4-8 00:00:00" fixdate="2010-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable frag display in trunk, let HBASE-2165 replace it</summary>
      <description>HBASE-2165 is about working on fragmentation indicator to make it less intrusive. Currently it can get in way of displaying UI on big cluster.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.resources.webapps.master.table.jsp</file>
      <file type="M">core.src.main.resources.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24301" opendate="2020-5-1 00:00:00" fixdate="2020-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Apache POM to version 23</summary>
      <description>The most recent version of the Apache parent POM is v23. We should update to this one. There should not be big changes, except that it updates the rat-plugin to the version we already have.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24359" opendate="2020-5-12 00:00:00" fixdate="2020-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optionally ignore edits for deleted CFs for replication.</summary>
      <description>Replication will be stuck after we delete CFs from both the source and the sink, if the source still has outstanding edits that now it could not get rid of. Now all replication is backed up behind these unreplicatable edits.We should have an option to ignore edits for deleted CFs at the source.This issue is similar to HBASE-12091</description>
      <version>2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationDroppedTables.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="2436" opendate="2010-4-12 00:00:00" fixdate="2010-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] review Jersey and JSON dependencies</summary>
      <description>From Thomas Koch over on HBASE-2383:The stargate lib folder contains:asm-3.1.jar, jackson-asl-0.9.4.jar, jersey-core-1.1.0-ea.jar, jersey-server-1.1.0-ea.jar, persistence-api-1.0.jar, commons-codec-1.3.jar, jaxb-impl-2.1.10.jar, jersey-json-1.1.0-ea.jar, jsr311-api-1.1.jar, protobuf-java-2.1.0.jarIt seems, that the following jars are either not used or only used for the tests:asm-3.1.jar, jackson-asl-0.9.4.jar, jaxb-impl-2.1.10.jarThe following are already in Debian:commons-codec-1.3.jarpersistence-api-1.0.jar (libgeronimo-jpa-3.0-spec-java 1.1.1-1)protobuf-java-2.1.0.jar (libprotobuf-java 2.3.0-1)Which leaves the following to be packaged:jersey-core-1.1.0-ea.jar, jersey-server-1.1.0-ea.jar, jersey-json-1.1.0-ea.jar ( https://jersey.dev.java.net )jsr311-api-1.1.jar ( https://jsr311.dev.java.net )Upstream version of jersey is 1.1.5.1. Would stargate work with this version?java.net doesn't seem to release tarballs, so I could obtain the sources only from these jars:http://download.java.net/maven/2/com/sun/jersey/jersey-bundle/1.1.5.1/jersey-bundle-1.1.5.1-sources.jarhttp://download.java.net/maven/2/javax/ws/rs/jsr311-api/1.1.1/jsr311-api-1.1.1-sources.jarThe jars in the Stargate lib directory were added per the getting started recipe up on the Jersey wiki.Tasks: Update Jersey to 1.1.5.1. Remove json.org JSON dependencies and substitute as required. Prune unnecessary jars.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.core.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.model.ScannerModel.java</file>
      <file type="M">contrib.stargate.core.src.main.java.org.apache.hadoop.hbase.stargate.auth.ZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24381" opendate="2020-5-16 00:00:00" fixdate="2020-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The Size metrics in Master Webui is wrong if the size is 0</summary>
      <description>As shown in attachment, there is no storefiles on the last RS, but the StoreFile Size is as large as the previous RS.</description>
      <version>2.2.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.rsgroup.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="24428" opendate="2020-5-25 00:00:00" fixdate="2020-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Priority compaction for recently split daughter regions</summary>
      <description>We observe that under hotspotting conditions that splitting will proceed very slowly and the "Cannot split region due to reference files being there" log line will be logged excessively. (branch-1 based production.) This is because after a region is split it must be compacted before it can be split again. Reference files must be replaced by real HFiles, normal housekeeping performed during compaction. However if the regionserver is under excessive load, its compaction queues may become deep. The daughters of a recently split hotspotting region may themselves continue to hotspot and will rapidly need to split again. If the scheduled compaction work to remove/replace reference files is queued hundreds or thousands of compaction queue elements behind current, the recently split daughter regions will not be able to split again for a long time and may grow very large, producing additional complications (very large regions, very deep replication queues).To help avoid this condition we should prioritize the compaction of recently split daughter regions. Compaction requests include a priority field and CompactionRequest implements a comparator that sorts by this field. We already detect when a compaction request involves a region that has reference files, to ensure that it gets selected to be eligible for compaction, but we do not seem to prioritize the requests for post-split housekeeping. Split work should be placed at the top of the queue. Ensure that this is happening.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.SortedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequestImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="24506" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>async client deadlock</summary>
      <description>I ran into an issue where one instance in a cluster of application servers backed by HBase stopped processing requests. Looking at a thread dump, it seems HBase client threads are deadlocked:https://pastebin.com/raw/B3FJL1AgThe deadlock seemed to happen at the same time that a region server was abruptly stopped (the physical server was restarted unexpectedly).I'm using the hbase async API. The hbase client version is 2.2.4. The server is running 2.2.4 as well.</description>
      <version>3.0.0-alpha-1,2.3.0,2.2.4,2.4.0,2.2.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.ipc.TestNettyRpcConnection.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableGetMultiThreaded.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.ipc.TestIPCUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.NettyRpcConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.IPCUtil.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
