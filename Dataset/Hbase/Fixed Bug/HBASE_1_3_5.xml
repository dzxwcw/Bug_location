<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="22280" opendate="2019-4-21 00:00:00" fixdate="2019-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate read/write handler for priority request(especially for meta).</summary>
      <description>Client may give too many read pressure on meta, so blocking master write meta for region open.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestReportOnlineRegionsRace.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22281" opendate="2019-4-21 00:00:00" fixdate="2019-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failed shell UTs</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.table.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.hbase.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="22380" opendate="2019-5-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>break circle replication when doing bulkload</summary>
      <description>when enabled master-master bulkload replication, HFiles will be replicated circularly between two clusters</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,1.4.10,2.0.5,2.3.0,2.1.5,1.3.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.BulkLoadHFilesTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.AsyncClusterConnection.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestBulkLoadHFilesSplitRecovery.java</file>
    </fixedFiles>
  </bug>
  <bug id="22384" opendate="2019-5-8 00:00:00" fixdate="2019-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formatting issues in administration section of book</summary>
      <description>The administration section in the book (64.3.2. Administration) has some formatting issues. Due to that issues the list count is not accurate, as well as the indentation of some code snippets.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22585" opendate="2019-6-14 00:00:00" fixdate="2019-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure javax.annotation doesn&amp;#39;t get include in shaded artifacts when built with Java 11</summary>
      <description>Master &amp; branch-2 build fails on Java 11. Complaints about the hbase-shaded-check-invariants. Will paste the stacktrace if needed in the comments. </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22594" opendate="2019-6-16 00:00:00" fixdate="2019-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up for backup examples</summary>
      <description>The PR for HBASE-7003 included some clean ups. They should be brought in without moving the files.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.ZKTableArchiveClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.LongTermArchivingHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.HFileArchiveManager.java</file>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22595" opendate="2019-6-16 00:00:00" fixdate="2019-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use full qualified name in Checkstyle suppressions</summary>
      <description>Currently the Checkstyle suppressions file uses only the simple file name to suppress issues. Better would be to use the full qualified name of the class to ensure that not another class with the same name in a different package gets suppressed.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22596" opendate="2019-6-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Chore] Separate the execution period between CompactionChecker and PeriodicMemStoreFlusher</summary>
      <description>The story started at tuning RS performance where I found:public static final String THREAD_WAKE_FREQUENCY = "hbase.server.thread.wakefrequency";...this.threadWakeFrequency = conf.getInt(HConstants.THREAD_WAKE_FREQUENCY, 10 * 1000);...this.compactionChecker = new CompactionChecker(this, this.threadWakeFrequency, this);this.periodicFlusher = new PeriodicMemStoreFlusher(this.threadWakeFrequency, this);CompactionChecker and PeriodicMemStoreFlusher execution period are bound together. (as well as LeaseChecker)This issue is going to introduce two new parameters such that user/admin can tune them according to business workload.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22617" opendate="2019-6-22 00:00:00" fixdate="2019-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recovered WAL directories not getting cleaned up</summary>
      <description>While colocating the recovered edits directory with hbase.wal.dir, BASE_NAMESPACE_DIR got missed. This results in recovered edits being put in a separate directory rather than the default region directory even if the hbase.wal.dir is not overridden. Eg. if data is stored in /hbase/data/namespace/table1, recovered edits are put in  /hbase/namespace/table1. This also messes up the regular cleaner chores which never operate on this new directory and these directories will never be deleted, even for split parents or dropped tables. We should change the default back to have the base namespace directory in path.</description>
      <version>1.3.3,2.2.0,1.4.8,2.1.1,1.4.9,2.1.2,1.4.10,2.1.3,1.3.4,2.1.4,2.1.5,1.3.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRecoveredEdits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.GCRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CommonFSUtils.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.util.BackupUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22628" opendate="2019-6-25 00:00:00" fixdate="2019-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the custom WAL directory (hbase.wal.dir) usage</summary>
      <description>Custom WAL directory usage must be documented, otherwise it may lead to inconsistent data during migrating to new WAL dir path. You can consider below scenario while migrating to custom WAL directory. Setup HBase cluster with the default setting (all WAL files are under the root directory ie. /hbase/WALs). Create table 't1' and insert few records Flush meta table (so that table region entries persist in FS) Forcibly kill HBase processes (HM &amp; RS). Configure the hbase.wal.dir to outside the root dir (say /hbaseWAL) Start the HBase servers Scan 't1'Ideally HMaster should submit split task of old RS(s) WAL files (created under /hbase/WALs) and old data should be replayed. But currently, during HM startup we populate the previous dead servers from the current WAL dir ( hbase.wal.dir -&gt; /hbaseWAL). Since WAL dir path is new, so you need to copy RegionServer WAL directories manualy from old WAL dir to new path. </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="2265" opendate="2010-2-25 00:00:00" fixdate="2010-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HFile and Memstore should maintain minimum and maximum timestamps</summary>
      <description>In order to fix HBASE-1485 and HBASE-29, it would be very helpful to have HFile and Memstore track their maximum and minimum timestamps. This has the following nice properties: for a straight Get, if an entry has been already been found with timestamp X, and X &gt;= HFile.maxTimestamp, the HFile doesn't need to be checked. Thus, the current fast behavior of get can be maintained for those who use strictly increasing timestamps, but "correct" behavior for those who sometimes write out-of-order. for a scan, the "latest timestamp" of the storage can be used to decide which cell wins, even if the timestamp of the cells is equal. In essence, rather than comparing timestamps, instead you are able to compare tuples of (row timestamp, storage.max_timestamp) in general, min_timestamp(storage A) &gt;= max_timestamp(storage B) if storage A was flushed after storage B.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22656" opendate="2019-7-4 00:00:00" fixdate="2019-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Metrics] Tabe metrics &amp;#39;BatchPut&amp;#39; and &amp;#39;BatchDelete&amp;#39; are never updated</summary>
      <description>public void updatePutBatch(TableName tn, long t) { if (tableMetrics != null &amp;&amp; tn != null) { tableMetrics.updatePut(tn, t); // Here should use updatePutBatch } ... } public void updateDeleteBatch(TableName tn, long t) { if (tableMetrics != null &amp;&amp; tn != null) { tableMetrics.updateDelete(tn, t); // Here should use updateDeleteBatch } ... }</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2273" opendate="2010-2-27 00:00:00" fixdate="2010-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] export metrics via Hadoop metrics, JMX, and ZooKeeper</summary>
      <description>Export metrics via Hadoop metrics, JMX, and ZooKeeper.At the moment, just "requests": requests per second. Put up ephemeral znodes in Zookeeper which include requests metric to facilitate monitoring and load balancing.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.VersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.TableResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterVersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterStatusResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.SchemaResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerInstanceResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RootResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RegionsResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.Main.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.Constants.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.ZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.JDBCAuthenticator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22891" opendate="2019-8-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HBaseQA in HBase-PreCommit-GitHub-PR job</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11,2.0.7</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  <bug id="22911" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fewer concurrent github PR builds</summary>
      <description>we've been regularly getting 4-5 concurrent builds of PRs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11,2.0.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  <bug id="22913" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hadoop label for nightly builds</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23829" opendate="2020-2-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get `-PrunSmallTests` passing on JDK11</summary>
      <description>Start with the small tests, shaking out issues identified by the harness. So far it seems like -Dhadoop.profile=3.0 and -Dhadoop-three.version=3.3.0-SNAPSHOT maybe be required.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.log.TestLogLevel.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFutureUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
