<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="11195" opendate="2014-5-16 00:00:00" fixdate="2014-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potentially improve block locality during major compaction for old regions</summary>
      <description>This might be a specific use case. But we have some regions which are no longer written to (due to the key). Those regions have 1 store file and they are very old, they haven't been written to in a while. We still use these regions to read from so locality would be nice. I propose putting a configuration option: something likehbase.hstore.min.locality.to.skip.major.compact &amp;#91;between 0 and 1&amp;#93;such that you can decide whether or not to skip major compaction for an old region with a single store file.I'll attach a patch, let me know what you guys think.</description>
      <version>1.0.0,0.94.26,0.98.10,2.0.0</version>
      <fixedVersion>1.0.0,0.98.10,0.94.27</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="11196" opendate="2014-5-16 00:00:00" fixdate="2014-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update description of -ROOT- in ref guide</summary>
      <description>Since the resolution of HBASE-3171, &amp;#45;ROOT- is no longer used to store the location(s) of .META. . Unfortunately, not all of our documentation has been updated to reflect this change in architecture.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12102" opendate="2014-9-26 00:00:00" fixdate="2014-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate keys in HBase.RegionServer metrics JSON</summary>
      <description>The JSON returned by /jmx on the RegionServer contains duplicate 'tag.Context' keys for various HBase.RegionServer metrics. Regions:{ "name" : "Hadoop:service=HBase,name=RegionServer,sub=Regions", "modelerType" : "RegionServer,sub=Regions", "tag.Context" : "regionserver", "tag.Context" : "regionserver", "tag.Hostname" : "some.host.name", ...}Server:"name" : "Hadoop:service=HBase,name=RegionServer,sub=Server", "modelerType" : "RegionServer,sub=Server", "tag.Context" : "regionserver", "tag.zookeeperQuorum" : "some.zookeeper.quorum.peers", "tag.serverName" : "some.server.name", "tag.clusterId" : "88c186ea-2308-4713-8b5f-5a3e829cbb10", "tag.Context" : "regionserver", ...}IPC:{ "name" : "Hadoop:service=HBase,name=IPC,sub=IPC", "modelerType" : "IPC,sub=IPC", "tag.Context" : "ipc", "tag.Context" : "ipc", "tag.Hostname" : "some.host.name", ...}This can cause issues with some JSON parsers. We should avoid emitting duplicate keys if it is under our control.</description>
      <version>1.0.0,0.98.10,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionAggregateSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="12709" opendate="2014-12-18 00:00:00" fixdate="2014-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[mvn] Add unit test excludes command line flag to the build.</summary>
      <description>I've added a simple way to specify unit test classes to skip when executing unit test runs. I've added a -D variable called test.exclude.pattern that you can using like this:mvn test -Dtest.exclude.pattern=**/TestFoo.java,**/TestBar.javato exclude the unit tests form TestFoo and TestBar in this run. By default there is nothing excluded.</description>
      <version>1.0.0,0.98.10,2.0.0</version>
      <fixedVersion>1.0.0,hbase-11339,0.98.10,1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1271" opendate="2009-3-20 00:00:00" fixdate="2009-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow multiple tests to run on one machine</summary>
      <description>Currently, if we try to run two tests on one machine (e.g. in two checkouts) the second one will fail because its servers won't be able to bind to ports. We should use random ports in our servers in the tests to fix this.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniZooKeeperCluster.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="12711" opendate="2014-12-18 00:00:00" fixdate="2014-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix new findbugs warnings in hbase-thrift module</summary>
      <description>From https://builds.apache.org/job/PreCommit-HBASE-Build/12121/artifact/patchprocess/newPatchFindbugsWarningshbase-thrift.html , there were 5 findbugs warnings introduced.This issue fixes the new warnings.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftHttpServlet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="12862" opendate="2015-1-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uppercase "wals" in RegionServer webUI</summary>
      <description>This cauught my eye in RS UI. We should make wals uppercase in line with the other tabs.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="12863" opendate="2015-1-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master info port on RS UI is always 0</summary>
      <description>Not sure the reason. The link at the bottom of the RS UI back to master always lists 0 as the info port, thus fails when clicked. Any takers ?</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestActiveMasterManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="12864" opendate="2015-1-15 00:00:00" fixdate="2015-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IntegrationTestTableSnapshotInputFormat fails</summary>
      <description>IntegrationTestTableSnapshotInputFormat fails with first: 2015-01-15 03:56:36,175 INFO [main] mapreduce.Job: Task Id : attempt_1420685782128_0080_m_000014_2, Status : FAILEDError: java.io.IOException: java.lang.NoClassDefFoundError: com/yammer/metrics/core/MetricsRegistry at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionStores(HRegion.java:858) at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:756) at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:729) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4885) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4851) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4822) at org.apache.hadoop.hbase.client.ClientSideRegionScanner.&lt;init&gt;(ClientSideRegionScanner.java:60) at org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl$RecordReader.initialize(TableSnapshotInputFormatImpl.java:190) at org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat$TableSnapshotRegionRecordReader.initialize(TableSnapshotInputFormat.java:139) at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:545) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:783) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)and then when that is fixed with: 2015-01-15 04:15:58,576|beaver.machine|INFO|28451|139674165233408|MainThread|Error: java.io.IOException: java.lang.IllegalStateException: bucketCacheSize &lt;= 0; Check hbase.bucketcache.size setting and/or server java heap size2015-01-15 04:15:58,576|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionStores(HRegion.java:858)2015-01-15 04:15:58,576|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:756)2015-01-15 04:15:58,577|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:729)2015-01-15 04:15:58,577|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4885)2015-01-15 04:15:58,577|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4851)2015-01-15 04:15:58,577|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:4822)2015-01-15 04:15:58,577|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.client.ClientSideRegionScanner.&lt;init&gt;(ClientSideRegionScanner.java:60)2015-01-15 04:16:22,764|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.io.hfile.CacheConfig.getL2(CacheConfig.java:491)2015-01-15 04:16:22,764|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(CacheConfig.java:536)2015-01-15 04:16:22,764|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.io.hfile.CacheConfig.&lt;init&gt;(CacheConfig.java:186)2015-01-15 04:16:22,764|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HStore.&lt;init&gt;(HStore.java:250)2015-01-15 04:16:22,764|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:3762)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:832)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:829)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at java.util.concurrent.FutureTask.run(FutureTask.java:262)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at java.util.concurrent.FutureTask.run(FutureTask.java:262)2015-01-15 04:16:22,765|beaver.machine|INFO|28451|139674165233408|MainThread|at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)2015-01-15 04:16:22,766|beaver.machine|INFO|28451|139674165233408|MainThread|at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)2015-01-15 04:16:22,766|beaver.machine|INFO|28451|139674165233408|MainThread|at java.lang.Thread.run(Thread.java:745)ndimiduk do you know about the second failure? We can setting block cache size to 0.</description>
      <version>None</version>
      <fixedVersion>1.0.0,0.98.10,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12891" opendate="2015-1-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel execution for Hbck checkRegionConsistency</summary>
      <description>We have a lot of regions on our cluster ~500k and noticed that hbck took quite some time in checkAndFixConsistency(). davelatham patched our cluster to do this check in parallel to speed things up. I'll attach the patch.</description>
      <version>0.98.10,1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2015-1-21 00:00:00" fixdate="2015-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimum memstore size is a percentage</summary>
      <description>We have a cluster which is optimized for random reads. Thus we have a large block cache and a small memstore. Currently our heap is 20GB and we wanted to configure the memstore to take 4% or 800MB. Right now the minimum memstore size is 5%. What do you guys think about reducing the minimum size to 1%? Suppose we log a warning if the memstore is below 5% but allow it?What do you folks think?</description>
      <version>0.98.10,1.1.0,2.0.0</version>
      <fixedVersion>1.0.0,1.1.0,0.98.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.util.HeapMemorySizeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12949" opendate="2015-1-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanner can be stuck in infinite loop if the HFile is corrupted</summary>
      <description>We've encountered problem where compaction hangs and never completes.After looking into it further, we found that the compaction scanner was stuck in a infinite loop. See stack below.org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:296)org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:257)org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:697)org.apache.hadoop.hbase.regionserver.StoreScanner.seekToNextRow(StoreScanner.java:672)org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)org.apache.hadoop.hbase.regionserver.compactions.Compactor.performCompaction(Compactor.java:223)We identified the hfile that seems to be corrupted. Using HFile tool shows the following:[biadmin@hdtest009 bin]$ hbase org.apache.hadoop.hbase.io.hfile.HFile -v -k -m -f /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b715/01/23 11:53:17 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available15/01/23 11:53:18 INFO util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc3215/01/23 11:53:18 INFO util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C15/01/23 11:53:18 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFSScanning -&gt; /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b7WARNING, previous row is greater then current row filename -&gt; /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b7 previous -&gt; \x00/20110203-094231205-79442793-1410161293068203000\x0Aattributes16794406\x00\x00\x01\x00\x00\x00\x00\x00\x00 current -&gt;Exception in thread "main" java.nio.BufferUnderflowException at java.nio.Buffer.nextGetIndex(Buffer.java:489) at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:347) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.readKeyValueLen(HFileReaderV2.java:856) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next(HFileReaderV2.java:768) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.scanKeysValues(HFilePrettyPrinter.java:362) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.processFile(HFilePrettyPrinter.java:262) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.run(HFilePrettyPrinter.java:220) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.main(HFilePrettyPrinter.java:539) at org.apache.hadoop.hbase.io.hfile.HFile.main(HFile.java:802)Turning on Java Assert shows the following:Exception in thread "main" java.lang.AssertionError: Key 20110203-094231205-79442793-1410161293068203000/attributes:16794406/1099511627776/Minimum/vlen=15/mvcc=0 followed by a smaller key //0/Minimum/vlen=0/mvcc=0 in cf attributes at org.apache.hadoop.hbase.regionserver.StoreScanner.checkScanOrder(StoreScanner.java:672)It shows that the hfile seems to be corrupted &amp;#8211; the keys don't seem to be right.But Scanner is not able to give a meaningful error, but stuck in an infinite loop in here:KeyValueHeap.generalizedSeek()while ((scanner = heap.poll()) != null) {}</description>
      <version>0.94.3,0.98.10</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13002" opendate="2015-2-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make encryption cipher configurable</summary>
      <description>Make encryption cipher configurable currently it is hard coded to AES, so that user can configure his/her own algorithm.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckEncryption.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestEncryptionTest.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionRandomKeying.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileEncryption.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.io.crypto.TestEncryption.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.io.crypto.TestCipherProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.crypto.Encryption.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.security.TestEncryptionUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.EncryptionUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13009" opendate="2015-2-11 00:00:00" fixdate="2015-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase REST UI inaccessible</summary>
      <description>Since the movement of HBase REST code into its own module, the UI is no longer accessible because the web.xml does not get bundled.</description>
      <version>0.98.10</version>
      <fixedVersion>1.0.0,1.1.0,0.98.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.components.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13038" opendate="2015-2-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the java doc warning continuously reported by Hadoop QA</summary>
      <description>Fix the java doc warning introduced after HBASE-12978 commit.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="13057" opendate="2015-2-17 00:00:00" fixdate="2015-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide client utility to easily enable and disable table replication</summary>
      <description>Currently to enable replication for a table user has to 1) Disable the table2) Set the replication scope3) enable the tableand also create a table with same name and CFs in peer cluster(s).To help user we can provide client API's to enable and disable table replication.So that user has to call only one API i.e., enable table replication which will,1. Create a table with same name and CFs in the peer cluster(s)2. Disable the table3. Set the replication scope4. Enable the tableIf user wants to disable table replication then he/she only has to call disable table replication API which will,1. Disable the table2. Set the replication scope3. Enable the table.</description>
      <version>0.98.10</version>
      <fixedVersion>1.1.0,1.0.2,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.shell.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.replication.admin.rb</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="13058" opendate="2015-2-17 00:00:00" fixdate="2015-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hbase shell command &amp;#39;scan&amp;#39; for non existent table shows unnecessary info for one unrelated existent table.</summary>
      <description>When scanning for a non existent table in hbase shell, error message in shell sometimes(based on META table content) displays completely unrelated table info , which seems to be unnecessary and inconsistent with other error messages:hbase(main):016:0&gt; scan 'noTable'ROW COLUMN+CELLERROR: Unknown table Table 'noTable' was not found, got: hbase:namespace.!---------------------------------------------------------------------------------------------hbase(main):017:0&gt; scan '01_noTable'ROW COLUMN+CELLERROR: Unknown table 01_noTable!--------------------------------------------------Its happening when doing a META table scan (to locate input table ) and scanner stops at row of another table (beyond which table can not exist) in ConnectionManager.locateRegionInMeta:private RegionLocations locateRegionInMeta(TableName tableName, byte[] row, boolean useCache, boolean retry, int replicaId) throws IOException {.................................................................// possible we got a region of a different table... if (!regionInfo.getTable().equals(tableName)) { throw new TableNotFoundException( "Table '" + tableName + "' was not found, got: " + regionInfo.getTable() + "."); }..............................................................Here, we can simply put a debug message(if required) and just throw the TableNotFoundException(tableName) with only tableName instead of with scanner positioned row.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.clone.snapshot.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.rb</file>
    </fixedFiles>
  </bug>
  <bug id="13074" opendate="2015-2-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleaned up usage of hbase.master.lease.thread.wakefrequency</summary>
      <description>While checking for configs to tweak, I ran into hbase.master.lease.thread.wakefrequency, but it has been deprecated. There are however still references of it in a few tests classes so just cleaning it up..</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-spark.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-server.src.test.resources.hbase-site2.xml</file>
      <file type="M">hbase-server.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface.java</file>
      <file type="M">hbase-rest.src.test.resources.hbase-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13085" opendate="2015-2-23 00:00:00" fixdate="2015-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Security issue in the implementation of Rest gataway &amp;#39;doAs&amp;#39; proxy user support</summary>
      <description>When 'hbase.rest.support.proxyuser' is turned on, HBase Rest gateway support 'doAs' proxy user from the Rest client.The current implementation checks to see if the 'rest server user' is authorized to impersonate the 'doAs' user (the user in the 'doAs' Rest query string).if (doAsUserFromQuery != null) { Configuration conf = servlet.getConfiguration(); if (!servlet.supportsProxyuser()) { throw new ServletException("Support for proxyuser is not configured"); } UserGroupInformation ugi = servlet.getRealUser(); // create and attempt to authorize a proxy user (the client is attempting // to do proxy user) ugi = UserGroupInformation.createProxyUser(doAsUserFromQuery, ugi); // validate the proxy user authorization try { ProxyUsers.authorize(ugi, request.getRemoteAddr(), conf); } catch(AuthorizationException e) { throw new ServletException(e.getMessage()); } servlet.setEffectiveUser(doAsUserFromQuery); } The current implementation allows anyone from the rest client side to impersonate another user by 'doAs'. For example, potentially, 'user1' can 'doAs=admin'The correct implementation should check to see if the rest client user is authorized to do impersonation.</description>
      <version>1.0.0,0.98.10,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServletContainer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13086" opendate="2015-2-23 00:00:00" fixdate="2015-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show ZK root node on Master WebUI</summary>
      <description>Currently we show a well-formed ZK quorum on the master webUI but not the root node. Root node can be changed based on deployment, so we should list it here explicitly. This information is helpful for folks playing around with phoenix.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="13289" opendate="2015-3-19 00:00:00" fixdate="2015-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>typo in splitSuccessCount metric</summary>
      <description>Our split metrics have a misspelled Count and it shows up in our jmx metrics "splitSuccessCounnt" : 0,</description>
      <version>1.0.0,0.98.10,1.1.0,0.98.11,0.98.12,0.98.10.1,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="13419" opendate="2015-4-7 00:00:00" fixdate="2015-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift gateway should propagate text from exception causes.</summary>
      <description>Exceptions passed back from the thrift gateway only include the message text of the toplevel exception. Information from the cause chain is lost.In some cases, the top-level exception text is useless but there is some very specific and useful information provided in some of the cause exceptions, so it would be very helpful to flatten all of the messages into the exception text returned to the user.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13601" opendate="2015-4-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connection leak during log splitting</summary>
      <description>Ran into an issue where Region server died with the following exception2015-04-29 17:10:11,856 WARN [nector@0.0.0.0:60030] mortbay.log - EXCEPTIONjava.io.IOException: Too many open files at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method) at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241) at org.mortbay.jetty.nio.SelectChannelConnector$1.acceptChannel(SelectChannelConnector.java:75) at org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:686) at org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192) at org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124) at org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708) at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)Realized that all the tcp sockets on the system were used out due to the regionserver trying to split the log and failing multiple times and leaving a connection open -java.io.IOException: Got error for OP_READ_BLOCK, self=/10..99.3:50695, remote=/10.232.99.36:50010, for file /hbase/WALs/host1,60020,1425930917890-splitting/host1%2C60020%2C1425930917890.1429358890944, for pool BP-181199659-10.232.99.2-1411124363096 block 1074497051_756497 at org.apache.hadoop.hdfs.RemoteBlockReader2.checkSuccess(RemoteBlockReader2.java:432) at org.apache.hadoop.hdfs.RemoteBlockReader2.newBlockReader(RemoteBlockReader2.java:397) at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReader(BlockReaderFactory.java:786) at org.apache.hadoop.hdfs.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:665) at org.apache.hadoop.hdfs.BlockReaderFactory.build(BlockReaderFactory.java:325) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:567) at org.apache.hadoop.hdfs.DFSInputStream.seekToNewSource(DFSInputStream.java:1446) at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:769) at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:799) at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:840) at java.io.DataInputStream.read(DataInputStream.java:100) at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:124) at org.apache.hadoop.hbase.regionserver.wal.HLogFactory.createReader(HLogFactory.java:91) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:660) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.getReader(HLogSplitter.java:569) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:282) at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFile(HLogSplitter.java:225) at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:143) at org.apache.hadoop.hbase.regionserver.handler.HLogSplitterHandler.process(HLogSplitterHandler.java:82) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.0.0,0.98.10</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="13604" opendate="2015-4-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bin/hbase mapredcp does not include yammer-metrics jar</summary>
      <description>Noticed this while testing hive integration with HBase snapshots. The yammer metrics jar is needed for working with snapshots, but we don't include it in the bin/hbase mapredcp output. That means the necessary classes are not on Hive's classpath by default.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13871" opendate="2015-6-9 00:00:00" fixdate="2015-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change RegionScannerImpl to deal with Cell instead of byte[], int, int</summary>
      <description>This is also a sub item for splitting HBASE-13387 into smaller chunks.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ReversedRegionScannerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
