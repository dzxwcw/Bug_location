<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="12735" opendate="2014-12-20 00:00:00" fixdate="2014-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor TAG so it can live as unit test and as an integration test</summary>
      <description>Parent task moved TAG to IT wholesale. This is about keeping a bit of ACID coverage going in UT. Jon offered to refactor TAG so can live in IT and UT.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestAcidGuarantees.java</file>
    </fixedFiles>
  </bug>
  <bug id="12738" opendate="2014-12-22 00:00:00" fixdate="2014-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chunk Ref Guide into file-per-chapter</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.upgrading.xml</file>
      <file type="M">src.main.docbkx.performance.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">src.main.docbkx.hbase-default.xml</file>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12891" opendate="2015-1-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel execution for Hbck checkRegionConsistency</summary>
      <description>We have a lot of regions on our cluster ~500k and noticed that hbck took quite some time in checkAndFixConsistency(). davelatham patched our cluster to do this check in parallel to speed things up. I'll attach the patch.</description>
      <version>0.98.10,1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="12897" opendate="2015-1-21 00:00:00" fixdate="2015-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimum memstore size is a percentage</summary>
      <description>We have a cluster which is optimized for random reads. Thus we have a large block cache and a small memstore. Currently our heap is 20GB and we wanted to configure the memstore to take 4% or 800MB. Right now the minimum memstore size is 5%. What do you guys think about reducing the minimum size to 1%? Suppose we log a warning if the memstore is below 5% but allow it?What do you folks think?</description>
      <version>0.98.10,1.1.0,2.0.0</version>
      <fixedVersion>1.0.0,1.1.0,0.98.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.util.HeapMemorySizeUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="12991" opendate="2015-2-9 00:00:00" fixdate="2015-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HBase 1.0 interfaces in hbase-rest</summary>
      <description>hbase-rest uses HTable and HBaseAdmin under the covers. They should use the new hbase 1.0 interfaces instead.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestTableResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestStatusResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestScannersWithFilters.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestGzipFilter.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.RowResourceBase.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.SchemaResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="12999" opendate="2015-2-10 00:00:00" fixdate="2015-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make foreground_start return the correct exit code</summary>
      <description></description>
      <version>1.1.0,0.98.11,2.0.0</version>
      <fixedVersion>1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13014" opendate="2015-2-11 00:00:00" fixdate="2015-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java Tool For Region Moving</summary>
      <description>As per discussion on HBASE-12989 we should move the functionality of region_mover.rb into a Java tool and use region_mover.rb only only as a wrapper around it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestRegionMover.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionMover.java</file>
    </fixedFiles>
  </bug>
  <bug id="13044" opendate="2015-2-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configuration option for disabling coprocessor loading</summary>
      <description>Some users would like complete assurance coprocessors cannot be loaded. Add a configuration option that prevents coprocessors from ever being loaded by ignoring any load directives found in the site file or table metadata.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13067" opendate="2015-2-18 00:00:00" fixdate="2015-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix caching of stubs to allow IP address changes of restarted remote servers</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13074" opendate="2015-2-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleaned up usage of hbase.master.lease.thread.wakefrequency</summary>
      <description>While checking for configs to tweak, I ran into hbase.master.lease.thread.wakefrequency, but it has been deprecated. There are however still references of it in a few tests classes so just cleaning it up..</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-spark.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-server.src.test.resources.hbase-site2.xml</file>
      <file type="M">hbase-server.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface.java</file>
      <file type="M">hbase-rest.src.test.resources.hbase-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13081" opendate="2015-2-20 00:00:00" fixdate="2015-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Branch precommit builds are not updating to branch head before patch application</summary>
      <description>See for example https://builds.apache.org/job/PreCommit-HBASE-Build/12922//consolegit checkout 0.98Previous HEAD position was 03d8918... HBASE-13069 Thrift Http Server returns an error code of 500 instead of 401 when authentication fails (Srikanth Srungarapu)Switched to branch '0.98'Your branch is behind 'origin/0.98' by 48 commits, and can be fast-forwarded. (use "git pull" to update your local branch)git statusOn branch 0.98Your branch is behind 'origin/0.98' by 48 commits, and can be fast-forwarded. (use "git pull" to update your local branch)Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) patchprocess/nothing added to commit but untracked files present (use "git add" to track)Because the local tree is 48 commits behind the head of the 0.98 branch, the contributor's patch based on the head of 0.98 branch cannot cleanly apply.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13103" opendate="2015-2-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ergonomics] add region size balancing as a feature of master</summary>
      <description>Often enough, folks miss-judge split points or otherwise end up with a suboptimal number of regions. We should have an automated, reliable way to "reshape" or "balance" a table's region boundaries. This would be for tables that contain existing data. This might look like:Admin#reshapeTable(TableName, int numSplits);or from the shell:&gt; reshape TABLE, numSplitsBetter still would be to have a maintenance process, similar to the existing Balancer that runs AssignmentManager on an interval, to run the above "reshape" operation on an interval. That way, the cluster will automatically self-correct toward a desirable state.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13109" opendate="2015-2-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make better SEEK vs SKIP decisions during scanning</summary>
      <description>I'm re-purposing this issue to add a heuristic as to when to SEEK and when to SKIP Cells. This has come up in various issues, and I think I have a way to finally fix this now. HBASE-9778, HBASE-12311, and friends are related.&amp;#8212; Old description &amp;#8212;This is a continuation of HBASE-9778.We've seen a scenario of a very slow scan over a region using a timerange that happens to fall after the ts of any Cell in the region.Turns out we spend a lot of time seeking.Tested with a 5 column table, and the scan is 5x faster when the timerange falls before all Cells' ts.We can use the lookahead hint introduced in HBASE-9778 to do opportunistic SKIPing before we actually seek.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestExplicitColumnTracker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBlocksRead.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockWithScanInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="13111" opendate="2015-2-26 00:00:00" fixdate="2015-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>truncate_preserve command is failing with undefined method error</summary>
      <description>hbase(main):001:0&gt; truncate_preserve 't1'Truncating 't1' table (it may take a while):ERROR: undefined method `getTable' for nil:NilClassHere is some help for this command: Disables, drops and recreates the specified table while still maintaing the previous region boundaries.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="13123" opendate="2015-2-27 00:00:00" fixdate="2015-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor bug in ROW bloom filter</summary>
      <description>While checking the code for Bloom filter found that while checking if a key passes the ROW bloom check we try to create a bloom key. The bloom key should be constructed only with the row part of the key. But try to form the bloom key including the meta data part of the key.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="13156" opendate="2015-3-5 00:00:00" fixdate="2015-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix minor rat violation recently introduced (asciidoctor.css).</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13202" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - core framework</summary>
      <description>core package, part of HBASE-12439this is just the proc-v2 submodule. it depends only on hbase-common.https://reviews.apache.org/r/27703/</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureFairRunQueues.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.util.StreamUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13203" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - master create/delete table</summary>
      <description>master side, part of HBASE-12439starts up the procedure executor on the masterand replaces the create/delete table handlers with the procedure version.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ModifyRegionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13204" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - client create/delete table sync</summary>
      <description>client side, part of HBASE-12439/HBASE-13203it uses the new procedure code to be know when the procedure is completed, and have a proper sync behavior on create/delete table.Review: https://reviews.apache.org/r/32391/</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13206" opendate="2015-3-11 00:00:00" fixdate="2015-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TableLock tableName log format</summary>
      <description>The TableName log from Table Lock result in something like:[tableName=^Gdefault^R^Cfoo, lockOwner=localhost,60000because it uses tableNameProto.toByteArray()the fix will result in the proper view[tableName=default:testMissingLastRegion, lockOwner=localh</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.TableLockChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13208" opendate="2015-3-11 00:00:00" fixdate="2015-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Patch build should match the patch filename and not the whole relative URL in findBranchNameFromPatchName</summary>
      <description>In HBASE-1319 we saw that the patch got applied to the wrong branch, the problem is findBranchNameFromPatchName matching a regex that contains wildcard symbols against the whole URL, in this case the regex is 0.94 and the relativePatchURL is /jira/secure/attachment/12703942/HBASE-13193-v4.patch where 0394 is a match.Thanks to jonathan.lawlor for reporting this.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13209" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - master Add/Modify/Delete Column Family</summary>
      <description>master side, part of HBASE-12439starts up the procedure executor on the masterand replaces the add/modify/delete handlers with the procedure version.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestTableLockManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.handler.TestTableDescriptorModification.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.handler.TestTableDeleteFamilyHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="1321" opendate="2009-4-12 00:00:00" fixdate="2009-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-1234 broke TestCompaction; fix and reenable</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.DisableTestCompaction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13210" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - master Modify table</summary>
      <description>master side, part of HBASE-12439starts up the procedure executor on the masterand replaces the modify table handlers with the procedure version.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13211" opendate="2015-3-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - master Enable/Disable table</summary>
      <description>master side, part of HBASE-12439starts up the procedure executor on the masterand replaces the enable/disable table handlers with the procedure version.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TableProcedureInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13218" opendate="2015-3-12 00:00:00" fixdate="2015-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the syntax shown for using ExportSnapshot tool in the book</summary>
      <description>It is $ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16It should be$ bin/hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13227" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadIncrementalHFile should skip non-files inside a possible family-dir</summary>
      <description>if we have random files/dirs inside the bulkload family dir, we should try to skip them.</description>
      <version>1.0.0,1.1.0,0.98.11,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug id="13228" opendate="2015-3-13 00:00:00" fixdate="2015-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create procedure v2 branch and add it to QA branch list</summary>
      <description>to develop Procedure V2 quickly, we are going to commit stuff to an hbase-12439 branch.In theory we can have QA running if the patch name is HBASE-xyz-hbase-12439.patch</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13244" opendate="2015-3-15 00:00:00" fixdate="2015-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test delegation token generation with kerberos enabled</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.HBaseKerberosUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="13262" opendate="2015-3-17 00:00:00" fixdate="2015-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResultScanner doesn&amp;#39;t return all rows in Scan</summary>
      <description>Tried to write a simple Java client again 1.1.0-SNAPSHOT. Write 1M rows, each row with 1 family, and 10 qualifiers (values &amp;#91;0-9&amp;#93;), for a total of 10M cells written Read back the data from the table, ensure I saw 10M cellsRunning it against 04ac1891 (and earlier) yesterday, I would get ~20% of the actual rows. Running against 1.0.0, returns all 10M records as expected.Code I was running for the curious.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallReversedScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13265" opendate="2015-3-17 00:00:00" fixdate="2015-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make thrift2 usable from c++</summary>
      <description>Currently the c++ code generated from our thrift2 idl doesn't compile. Mostly this is a naming issue for parameters.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="13281" opendate="2015-3-19 00:00:00" fixdate="2015-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;hbase.bucketcache.size&amp;#39; description in hbase book is not correct</summary>
      <description>The description about 'hbase.bucketcache.size' is not correct. We either specify it as a float or in MB's and the default value that is mentioned is never used.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13289" opendate="2015-3-19 00:00:00" fixdate="2015-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>typo in splitSuccessCount metric</summary>
      <description>Our split metrics have a misspelled Count and it shows up in our jmx metrics "splitSuccessCounnt" : 0,</description>
      <version>1.0.0,0.98.10,1.1.0,0.98.11,0.98.12,0.98.10.1,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="13290" opendate="2015-3-19 00:00:00" fixdate="2015-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - client enable/disable table sync</summary>
      <description>client side, part of HBASE-12439/HBASE-13211it uses the new procedure code to be know when the procedure is completed, and have a proper sync/async behavior on enable/disable table. For 1.1, It has to be binary compatible (1.0 client can talk to 1.1 server &amp;&amp; 1.1 client can talk to 1.0 server). Binary compatible is TBD for 2.0.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="13307" opendate="2015-3-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Making methods under ScannerV2#next inlineable, faster</summary>
      <description>See parent issue for patch and evidence.I was looking at graphs of our scan and found that methods were 'too big' to be inlined (looking at jvm compilation and inlining output flags &amp;#8211; see parent for list). Changing method size helped some. Let me commit the resultant patch.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestTags.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
    </fixedFiles>
  </bug>
  <bug id="13309" opendate="2015-3-21 00:00:00" fixdate="2015-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some tests do not reset EnvironmentEdgeManager</summary>
      <description>while playing with client timeouts, I hit lots of flakys in HBaseFsck. the cause was just an EnvironmentEdge.inject() not followed by a reset().looks like TestFsUtils and TestHRegion are the only other two that have a missing reset(). all the other tests using inject() seems to be written properly.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="1331" opendate="2009-4-20 00:00:00" fixdate="2009-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower the default scanner caching value</summary>
      <description>The current default value for the scanner caching is 30; this causing headaches to many new users who may use a row from a scanner for more than 60 seconds. Let's set it at 1, then folks will be able to optimize.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13316" opendate="2015-3-23 00:00:00" fixdate="2015-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the downtime on planned moves of regions</summary>
      <description>The current behavior of a region move shuts down a region and then starts is up in another regionserver. This causes increased latency and possibly timeouts till the new region's cache is fully warmed up. We can make a region move less disruptive by warming the cache in the destination region server before shutting dow the old region.</description>
      <version>1.0.1,1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13326" opendate="2015-3-24 00:00:00" fixdate="2015-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disabled table can&amp;#39;t be enabled after HBase is restarted</summary>
      <description>The folks at Intel discovered a pretty nasty bug in 1.0 and 1.1 (but not master). Steps to reproduce:1. Create a table, any table.2. Disable the table.3. Restart HBase.4. Try enabling the table.The table won't become enabled and the master web UI will indicate a never-ending region in transition. Also worth noting is that mbertozzi dug in and noted that this isn't happening in the master branch.</description>
      <version>1.0.0,1.1.0,0.98.12</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13328" opendate="2015-3-24 00:00:00" fixdate="2015-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadIncrementalHFile.doBulkLoad(Path,HTable) should handle managed connections</summary>
      <description>This seems to be a regression from HBASE-12783 discovered in testing Phoenix with 1.1.0-SNAPSHOT. Phoenix uses an HTable (with managed connection) to pass to doBulkLoad() which throws NeedUnmanagedConnectionException (see IndexToolIT.java and IndexTool.java in Phoenix).</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug id="13339" opendate="2015-3-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default Hadoop version to latest for master</summary>
      <description>Current default Hadoop version is getting a little long in the tooth. We should update to the latest version. The latest version is backwards compatible with 2.5.1's dfs and mr so this should be painless.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">dev-support.test-patch.properties</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13342" opendate="2015-3-26 00:00:00" fixdate="2015-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect interface annotations</summary>
      <description>Still some old annotations. Have slipped in. Lets remove them and add in a patch check for them.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RegionStateListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceStateManager.java</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13355" opendate="2015-3-28 00:00:00" fixdate="2015-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QA bot reports checking javac twice</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13369" opendate="2015-3-31 00:00:00" fixdate="2015-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose scanNext stats to region server level</summary>
      <description>Regions have a scanNext_num_ops stat but there is no such stat at the regionserver level. The num_ops metrics at the RS level (exposed through JMX) are as follows: "SyncTime_num_ops" : 54, "AppendSize_num_ops" : 56, "AppendTime_num_ops" : 56, "Append_num_ops" : 0, "Delete_num_ops" : 0, "Mutate_num_ops" : 0, "Get_num_ops" : 0, "Replay_num_ops" : 0, "Increment_num_ops" : 0, "QueueCallTime_num_ops" : 53, "ProcessCallTime_num_ops" : 53,We need a scanNext_num_ops, along with 95th percentile stat, at the RS level.</description>
      <version>None</version>
      <fixedVersion>1.1.0,0.98.13,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="13372" opendate="2015-4-1 00:00:00" fixdate="2015-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit tests for SplitTransaction and RegionMergeTransaction listeners</summary>
      <description>We have new Listener interfaces in SplitTransaction and RegionMergeTransaction. There are no use cases for these yet, nor unit tests. We should have unit tests for these that do something just a bit nontrivial so as to provide a useful example.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMergeTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="13374" opendate="2015-4-1 00:00:00" fixdate="2015-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small scanners (with particular configurations) do not return all rows</summary>
      <description>I recently ran into a couple data loss issues with small scans. Similar to HBASE-13262, these issues only appear when scans are configured in such a way that the max result size limit is reached before the caching limit is reached. As far as I can tell, this issue affects branches 0.98+I should note that after investigation it looks like the root cause of these issues is not the same as HBASE-13262. Rather, these issue are caused by errors in the small scanner logic (I will explain in more depth below). Furthermore, I do know that the solution from HBASE-13262 has not made its way into small scanners (it is being addressed in HBASE-13335). As a result I made sure to test these issues with the patch from HBASE-13335 applied and I saw that they were still present.The following two issues have been observed (both lead to data loss):1. When a small scan is configured with a caching value of Integer.MAX_VALUE, and a maxResultSize limit that is reached before the region is exhausted, integer overflow will occur. This eventually leads to a preemptive skip of the regions.2. When a small scan is configured with a maxResultSize that is smaller than the size of a single row, the small scanner will jump between regions preemptively. This issue seems to be because small scanners assume that, unless a region is exhausted, at least 2 rows will be returned from the server. This assumption isn't clearly state in the small scanners but is implied through the use of skipRowOfFirstResult.Again, I would like to stress that the root cause of these issues is NOT related to the cause of HBASE-13262. These issues occur because of inappropriate assumption made in the small scanner logic. The inappropriate assumptions are:1. Integer overflow will not occur when incrementing caching2. At least 2 rows will be returned from the server unless the region has been exhaustedI am attaching a patch that contains tests to display these issues. If these issues should be split into separate JIRAs please let me know.</description>
      <version>1.0.0,1.1.0,0.98.13,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallReversedScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13375" opendate="2015-4-1 00:00:00" fixdate="2015-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide HBase superuser higher priority over other users in the RPC handling</summary>
      <description>HBASE-13351 annotates Master RPCs so that RegionServer RPCs are treated with a higher priority compared to user RPCs (and they are handled by a separate set of handlers, etc.). It may be good to stretch this to users too - hbase superuser (configured via hbase.superuser) gets higher priority over other users in the RPC handling. That way the superuser can always perform administrative operations on the cluster even if all the normal priority handlers are occupied (for example, we had a situation where all the master's handlers were tied up with many simultaneous createTable RPC calls from multiple users and the master wasn't able to perform any operations initiated by the admin). (Discussed this some with enis and elserj).Does this make sense to others?</description>
      <version>None</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQosFunction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterPriorityRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.PriorityFunction.java</file>
    </fixedFiles>
  </bug>
  <bug id="13409" opendate="2015-4-6 00:00:00" fixdate="2015-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add categories to uncategorized tests</summary>
      <description>A couple tests without categories were flagged recently by TestCheckTestClasses in a precommit build.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.filter.TestLongComparator.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientExponentialBackoff.java</file>
    </fixedFiles>
  </bug>
  <bug id="1341" opendate="2009-4-23 00:00:00" fixdate="2009-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HTable Pooler</summary>
      <description>A client class that takes care of properly pooling HTable references for use in multi-threaded, low-latency Java clients.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13455" opendate="2015-4-11 00:00:00" fixdate="2015-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - master truncate table</summary>
      <description>master side, part of HBASE-12439and replaces the truncate table handlers with the procedure version.https://reviews.apache.org/r/33102</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="13473" opendate="2015-4-15 00:00:00" fixdate="2015-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleted cells come back alive after the stripe compaction</summary>
      <description>during the STRIPE compaction,deletes(tombstones) are being dropped in 2 cases.1. Compaction including L0 (includeL0 == true)2. L0 has no files (canDropDeletesWithoutL0 == true)To drop delete marker and keep the consistency during compaction, All of HFiles in the stripe has to be selected, just like major compaction.otherwise, after the compaction only delete markers would be gone, and deleted cells (which is in the not-selected HFiles) are going to be alive again.In my cluster, there was no file on L0(canDropDeletesWithoutL0==true) and not all files are selected for compaction, so some of deleted rows have come back alive and appears when i get or scan after compactions.I made a patch about it.it checks if all files are selected before we set the majorRange of compaction request .</description>
      <version>1.0.1,1.1.0,0.98.12,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.13,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="13481" opendate="2015-4-15 00:00:00" fixdate="2015-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master should respect master (old) DNS/bind related configurations</summary>
      <description>This is a continuation of parent HBASE-13453. We should continue respecting the following parameters that 1.0.0 does not: hbase.master.dns.interfacehbase.master.dns.nameserverhbase.master.ipc.addressCredit goes to jerryhe for pointing that out.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerHostname.java</file>
    </fixedFiles>
  </bug>
  <bug id="13515" opendate="2015-4-20 00:00:00" fixdate="2015-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle FileNotFoundException in region replica replay for flush/compaction events</summary>
      <description>I had this patch laying around that somehow dropped from my plate. We should skip replaying compaction / flush and region open event markers if the files (from flush or compaction) can no longer be found from the secondary. If we do not skip, the replay will be retried forever, effectively blocking the replication further. Bulk load already does this, we just need to do it for flush / compaction and region open events as well.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionReplayEvents.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="13516" opendate="2015-4-20 00:00:00" fixdate="2015-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase PermSize to 128MB</summary>
      <description>HBase uses ~40MB, and with Phoenix we use ~56MB of Perm space out of 64MB by default. Every Filter and Coprocessor increases that.Running out of perm space triggers a stop the world full GC of the entire heap. We have seen this in misconfigured cluster. Should we default to -XX:PermSize=128m -XX:MaxPermSize=128m out of the box as a convenience for users?</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.hbase-env.cmd</file>
    </fixedFiles>
  </bug>
  <bug id="13518" opendate="2015-4-21 00:00:00" fixdate="2015-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in hbase.hconnection.meta.lookup.threads.core parameter</summary>
      <description>A possible typo coming from patch in HBASE-13036. I think we want hbase.hconnection.meta.lookup.threads.core, not hbase.hconnection.meta.lookup.threads.max.core to be in line with the regular thread pool configuration. //To start with, threads.max.core threads can hit the meta (including replicas). //After that, requests will get queued up in the passed queue, and only after //the queue is full, a new thread will be started this.metaLookupPool = getThreadPool( conf.getInt("hbase.hconnection.meta.lookup.threads.max", 128), conf.getInt("hbase.hconnection.meta.lookup.threads.max.core", 10),</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="13527" opendate="2015-4-22 00:00:00" fixdate="2015-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The default value for hbase.client.scanner.max.result.size is never actually set on Scans</summary>
      <description>Now that max result size is driven from the client side like caching (HBASE-13362), we also need to set Scan.maxResultSize to the default value of hbase.client.scanner.max.result.size which is never performed. I think this has gone unnoticed because the server used to read the configuration hbase.client.scanner.max.result.size for itself, but now we expect the serialized Scan sent from the client side to contain this information. Realistically this should have been set on the Scans even before HBASE-13362, it's surprising that it's not as the scanner code seems to indicate otherwise.Ultimately, the end result is that, by default, scan RPC's are limited by hbase.server.scanner.max.result.size (note this is the new server side config not the client side config) which has a default value of 100 MB. The scan RPC's should instead be limited by hbase.client.scanner.max.result.size which has a default value of 2 MB.The reason why this issue occurs is because, by default, a new Scan() initializes Scan.maxResultSize to -1. This initial value of -1 will never be changed unless Scan#setMaxResultSize() is called. In the event that this value is not changed, the Scan that is serialized and sent to the server will also have Scan.maxResultSize = -1. Then, when the server is deciding what size limit should be enforced, it sees that Scan.maxResultSize = -1 so it uses the most relaxed size restriction possible, which is hbase.server.scanner.max.result.size (default value 100 MB).</description>
      <version>1.0.0,1.1.0,0.98.12,1.2.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.12,1.0.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.TableConfiguration.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13529" opendate="2015-4-22 00:00:00" fixdate="2015-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - WAL Improvements</summary>
      <description>from the discussion in HBASE-12439 the wal was resulting slow. there is an error around the awake of the slotCond.await(), causing more wait then necessary ArrayBlockingQueue is dog slow, replace it with ConcurrentLinkedQueue roll the wal only if reaches a threshold (conf ops) to amortize the cost hsync() is used by default, when the normal wal is using just hflush() make it tunable via conf</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.util.StringUtils.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="13554" opendate="2015-4-24 00:00:00" fixdate="2015-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book clarifying API stability guarantees</summary>
      <description>From the "Clarifying interface evolution freedom in patch releases" thread on dev@h.a.oSeems we have consensus that "HBase uses Semantic Versioning" isn't quite correct (or desired) at the moment. Update the documentation to make sure we're not misrepresenting any guarantees to users.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13561" opendate="2015-4-25 00:00:00" fixdate="2015-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ITBLL.Verify doesn&amp;#39;t actually evaluate counters after job completes</summary>
      <description>Was digging through ITBLL and noticed this oddity:The Verify Tool doesn't actually call Verify#verify(long) like the Loop Tool does. Granted, it doesn't know the total number of KVs that were written given the current arguments, it's not even checking to see if there things like UNDEFINED records found.It seems to me that Verify should really be doing some checking on the counters like Loop does and not just leaving it up to the visual inspection of whomever launched the task.Am I missing something?</description>
      <version>1.0.0,1.1.0,0.98.12,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  <bug id="13563" opendate="2015-4-25 00:00:00" fixdate="2015-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing table owner to AC tests.</summary>
      <description>As per the description.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
    </fixedFiles>
  </bug>
  <bug id="13569" opendate="2015-4-26 00:00:00" fixdate="2015-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct errors reported with mvn site</summary>
      <description>There are a number of Javadoc errors reported when running mvn site.</description>
      <version>None</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKSplitLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ServerRegionReplicaUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.MultiHConnection.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HFileV1Detector.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationThrottler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.HBaseReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Region.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.NonReversedNonLazyKeyValueScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreChunkPool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.IncreasingToUpperBoundRegionSplitPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HeapMemoryManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DeleteTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactionRequestor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ColumnTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.WALLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruCachedBlockQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HFileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.InterProcessLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.TimeoutException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateImplementation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.SplitLogManagerCoordination.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.constraint.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.constraint.ConstraintException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.HFileArchiveTableMonitor.java</file>
      <file type="M">hbase-protocol.src.main.java.com.google.protobuf.HBaseZeroCopyByteString.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.SequentialProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.Procedure.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.util.vint.UVLongTool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.util.vint.UVIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.util.vint.UFIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.scanner.ReversibleCellScanner.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.scanner.CellSearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeCodec.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.tokenize.TokenizerNode.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.tokenize.Tokenizer.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.row.RowNodeWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.PrefixTreeEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.column.ColumnSectionWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.column.ColumnNodeWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.decode.ArraySearcherPool.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseSource.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.OrderedBytes.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.KeyLocker.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.DefaultEnvironmentEdge.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Base64.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Addressing.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.types.Struct.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.TableName.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AbstractClientScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallReversedScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.coprocessor.AggregationClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Durability.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTableMultiplexer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.package-info.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ResultScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetryingCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.coprocessor.ColumnInterpreter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FamilyFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.MultiRowRangeFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AsyncRpcClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.ConnectionId.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.ServerRpcController.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MetaTableAccessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.QuotaTableUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.regionserver.BloomType.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeers.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.visibility.CellVisibility.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.Cell.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellComparator.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellScannable.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellScanner.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.CellOutputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.hadoopbackport.ThrottledInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.TimeRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.util.Dictionary.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValueUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
    </fixedFiles>
  </bug>
  <bug id="13586" opendate="2015-4-28 00:00:00" fixdate="2015-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book on Hadoop and Java supported versions for 1.1.x</summary>
      <description>Should update http://hbase.apache.org/book.html#basic.prerequisites with the latest info.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13593" opendate="2015-4-29 00:00:00" fixdate="2015-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quota support for namespace should take snapshot restore and clone into account</summary>
      <description>Quota support for namespace should take restore and clone snapshot into account.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceStateManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceAuditor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1361" opendate="2009-4-30 00:00:00" fixdate="2009-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable bloom filters</summary>
      <description>Bloomfilters currently have problems and nobody uses them. Disable this feature for now. Bring it back as part of continuing/ongoing work on the read and write paths, e.g. 1249 and HFile related open issues.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13611" opendate="2015-5-2 00:00:00" fixdate="2015-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update clover to work for current versions</summary>
      <description>our clover profile uses a super old version of the clover plugin, sufficiently old that it requires maven 2.update to use ~4.x clover version and make sure things work.</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.SplitLogCounters.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestInterfaceAudienceAnnotations.java</file>
    </fixedFiles>
  </bug>
  <bug id="13614" opendate="2015-5-4 00:00:00" fixdate="2015-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid temp KeyOnlyKeyValue temp objects creations in read hot path</summary>
      <description>As part of HBASE-10800, move to new CellComparator, we are temp creating a Cell out of byte[]s so that the Comparator can compare. In read hot path, we can try minimize the object creations. The parent Jira added some such cases, which we can solve. This Jira will solve all such cases.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="13618" opendate="2015-5-5 00:00:00" fixdate="2015-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReplicationSource is too eager to remove sinks</summary>
      <description>Looking at the replication for some other reason I noticed that the replication source might be a bit too eager to remove sinks from the list of valid sinks.The current logic allows a sink to fail N times (default 3) and then it will be remove from the sinks. But note that this failure count is never reduced, so given enough runtime and some network glitches every sink will eventually be removed. When all sink are removed the source pick new sinks and the counter is set to 0 for all of them.I think we should change to reset the counter each time we successfully replicate something to the sink (which proves the sink isn't dead). Or we could decrease the counter each time we successfully replication, that might be better - if we consistently fail more attempts than we succeed the sink should be removed.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSinkManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="13622" opendate="2015-5-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>document upgrade rollback</summary>
      <description>I have some docs on doing a rollback of an hbase upgrade that are currently vendor specific. polish them up, make them suitable for HBase generally, and add them to the ref guide.</description>
      <version>0.98.0,1.0.0,1.1.0</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="1366" opendate="2009-5-3 00:00:00" fixdate="2009-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-1342 broke splitting (broke build)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HAbstractScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13661" opendate="2015-5-11 00:00:00" fixdate="2015-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct binary compatibility issues discovered in 1.1.0RC0</summary>
      <description>Over on the 1.1.0RC0 VOTE thread, Enis discovered some errors in InterfaceAudience annotations. Let's fix them.Filed by ndimiduk on enis's behalf.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceAuditor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMaster.java</file>
      <file type="M">hbase-rest.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.StorageClusterStatusMessage.proto</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.CorruptedWALProcedureStoreException.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSource.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
    </fixedFiles>
  </bug>
  <bug id="13662" opendate="2015-5-11 00:00:00" fixdate="2015-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RSRpcService.scan() throws an OutOfOrderScannerNext if the scan has a retriable failure</summary>
      <description>while fixing HBASE-13651 I noticed that if we have a failure inside the RSRpcService.scan(), when the request has a hasNextCallSeq() the nextCallSeq is incremented and not rolledback, which means that the client retry will send a request with a nextCallSeq not up to date, which result in an OutOfOrderScannerNextException.if (rows &gt; 0) { if (request.hasNextCallSeq()) { if (request.getNextCallSeq() != rsh.nextCallSeq) { throw new OutOfOrderScannerNextException(...) } // Increment the nextCallSeq value which is the next expected from client. rsh.nextCallSeq++; }}try { ...scan code...}after the scanner heartbeat patches HBASE-13090, we seems to be able to recover from that OutOfOrder exception, but the error show up anyway.After a discussion with saint.ack@gmail.com we ended up saying that decrementing the callSeq on exception seems to be fine. but we had the open question about having that nextCallSeq to be atomic, if that was supposed to prevent concurrent requests with the same id. any thoughts?</description>
      <version>1.0.1,1.1.0,0.98.10.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="13666" opendate="2015-5-11 00:00:00" fixdate="2015-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>book.pdf is not renamed during site build</summary>
      <description>Noticed this while testing HBASE-13665. Looks like the post-site hook is broken or not executed, so the file book.pdf is not copied over to the expected name apache_hbase_reference_guide.pdf.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.1,1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.make.rc.sh</file>
    </fixedFiles>
  </bug>
  <bug id="13694" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallQueueSize is incorrectly decremented until the response is sent</summary>
      <description>We should decrement the CallQueueSize as soon as we no longer need the call around, e.g. after RpcServer.CurCall.set(null) otherwise we will be only pushing back other client requests while we send the response back to the client that originated the call.</description>
      <version>1.1.0,0.98.12,1.0.2,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13698" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RegionLocator methods to Thrift2 proxy.</summary>
      <description>Thrift2 doesn't provide the same functionality as the java client for getting region locations. We should change that.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13699" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand information about HBase quotas</summary>
      <description>See HBASE-13398 and http://blog.cloudera.com/blog/2014/12/new-in-cdh-5-2-improvements-for-running-multiple-workloads-on-a-single-hbase-cluster/.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13716" opendate="2015-5-19 00:00:00" fixdate="2015-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop using Hadoop&amp;#39;s FSConstants</summary>
      <description>the FSConstants class was removed in HDFS-8135 (currently slated for Hadoop 2.8.0). I'm trying to have it reverted in branch-2, but we should migrate off of it sooner rather htan later.</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="13725" opendate="2015-5-20 00:00:00" fixdate="2015-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[documentation] Pseudo-Distributed Local Install can link to hadoop instructions</summary>
      <description>The below is no longer true, we can link to http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.htmlHadoop ConfigurationThis procedure assumes that you have configured Hadoop and HDFS on your local system and or a remote system, and that they are running and available. It also assumes you are using Hadoop 2. Currently, the documentation on the Hadoop website does not include a quick start for Hadoop 2, but the guide at link:http://www.alexjf.net/blog/distributed-systems/hadoop-yarn-installation-definitive-guide is a good starting point.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13728" opendate="2015-5-20 00:00:00" fixdate="2015-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove use of Hadoop&amp;#39;s GenericOptionsParser</summary>
      <description>GenericOptionsParser has been IA.Private for all of Hadoop 2 (handled in HADOOP-6668) we shouldn't be using it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestRowCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCopyTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCellCounter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.WALPlayer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Export.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CellCounter.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestImportTsv.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.mapreduce.SampleUploader.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.mapreduce.IndexBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="1373" opendate="2009-5-5 00:00:00" fixdate="2009-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to use compact/framed protocol</summary>
      <description>TCompactProtocol/TFramedTransport and nonblocking server option promises better efficiency and performance improvements. Consider moving HBase Thrift bits to this when full platform support is ready for TCompactProtocol.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.package.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13746" opendate="2015-5-22 00:00:00" fixdate="2015-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>list_replicated_tables command is not listing table in hbase shell.</summary>
      <description>IN HBase shell prompt execute the following commandlist_replicated_tableshbase(main):014:0&gt; list_replicated_tablesTABLE:COLUMNFAMILY ReplicationTypeERROR: undefined method `TNAME' for Java::OrgApacheHadoopHbaseClientReplication::ReplicationAdmin:ClassHere is some help for this command:List all the tables and column families replicated from this cluster hbase&gt; list_replicated_tables hbase&gt; list_replicated_tables 'abc.*' list.select {|s| pattern.match(s.get(ReplicationAdmin.TNAME))}</description>
      <version>1.1.0,0.98.13,1.0.2,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.replication.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="13770" opendate="2015-5-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Programmatic JAAS configuration option for secure zookeeper may be broken</summary>
      <description>While verifying the patch fix for HBASE-13768 we were unable to successfully test the programmatic JAAS configuration option for secure ZooKeeper integration. Unclear if that was due to a bug or incorrect test configuration.Update the security section of the online book with clear instructions for setting up the programmatic JAAS configuration option for secure ZooKeeper integration.Verify it works.Fix as necessary.</description>
      <version>1.0.1,1.1.0,0.98.13,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13780" opendate="2015-5-26 00:00:00" fixdate="2015-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default to 700 for HDFS root dir permissions for secure deployments</summary>
      <description>Secure mode deployments should protect the files under HDFS root dir. We should check and set the root dirs permissions on a kerberos setup so that users does not have to. We have hbase.data.umask.enable and hbase.data.umask for data files, but those are not that useful since we should protect dir listing, and access to WAL files, snapshot files, etc. See HBASE-13768 which has an integration test for this.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13801" opendate="2015-5-28 00:00:00" fixdate="2015-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop src checksum is shown instead of HBase src checksum in master / RS UI</summary>
      <description>Simple bug. We are showing the Hadoop's source MD5 checksum in the master UI instead of the HBase's one.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="1381" opendate="2009-5-6 00:00:00" fixdate="2009-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove onelab and bloom filters files from hbase</summary>
      <description>Bloom filters were broken in 0.19 hbase. They probably won't be in 0.20.0 because discussion and experimentation has them using lots of RAM for questionable benefit. We'll reexamine in 0.21. Meantime, purge the onelab stuff and the bloom filter map files from hbase. They haven't been working and when we go back to bloomfilters, we'll pick up the onelab from the hadoop core project (or use jgrays' bloomfilter implementation because it might be more efficient).</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.onelab.test.TestFilter.java</file>
      <file type="M">src.test.org.onelab.test.StringKey.java</file>
      <file type="M">src.java.org.onelab.filter.RetouchedBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.RemoveScheme.java</file>
      <file type="M">src.java.org.onelab.filter.Key.java</file>
      <file type="M">src.java.org.onelab.filter.HashFunction.java</file>
      <file type="M">src.java.org.onelab.filter.Filter.java</file>
      <file type="M">src.java.org.onelab.filter.DynamicBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.CountingBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.BloomFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HalfMapFileReader.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.BloomFilterMapFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13812" opendate="2015-5-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deleting of last Column Family of a table should not be allowed</summary>
      <description>In this test run (without master killing) of IntegrationTestDDLMasterFailover of HBASE-13470, the DeleteColumnFamilyAction tries to delete one of the column families of a selected table. When there is only 1 column family left, the delete didn't fail.console_tue_nokill_r3.log2015-05-20 02:34:30,934 INFO [Thread-5] hbase.IntegrationTestMasterFailover: Deleting column family: {NAME =&gt; 'cf-882743293', DATA_BLOCK_ENCODING =&gt; 'PREFIX', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', COMPRESSION =&gt; 'NONE', VERSIONS =&gt; '1', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'} from table: ittable-0296278350...2015-05-20 02:34:35,269 INFO [Thread-5] hbase.IntegrationTestMasterFailover: Deleted column family: {NAME =&gt; 'cf-882743293', DATA_BLOCK_ENCODING =&gt; 'PREFIX', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', COMPRESSION =&gt; 'NONE', VERSIONS =&gt; '1', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'} from table: ittable-0296278350...2015-05-20 02:34:35,383 INFO [Thread-14] hbase.IntegrationTestMasterFailover: No column families in table: 'ittable-0296278350'...Number of Tables: 49 Table: ittable-0372763896 rw families: 1 Table: ittable-0824340809 rw families: 1 Table: ittable-1400154361 rw families: 1 Table: ittable-1625415605 rw families: 0 Table: ittable-1501441540 rw families: 1 Table: ittable-0296278350 rw families: 0This should not be allowed.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
    </fixedFiles>
  </bug>
  <bug id="13816" opendate="2015-5-30 00:00:00" fixdate="2015-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build shaded modules only in release profile</summary>
      <description>Shaded modules are great, but not needed in the regular build + test cycle. I noticed that around 30-40% of the build time goes in the actual shading. I think we can just build the shaded jars in the release profile. hadoopqe and mvn publishing should not be affected.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13817" opendate="2015-5-31 00:00:00" fixdate="2015-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ByteBufferOuputStream - add writeInt support</summary>
      <description>While writing Cells to this stream, to make the CellBlock ByteBuffer, we do write length of the cell as int. We use StreamUtils to do this which will write each byte one after the other. So 4 write calls on Stream.(OutputSteam has only this support) With ByteBufferOuputStream we have the overhead of checking for size limit and possible grow with every write call. Internally this stream writes to a ByteBuffer. Again inside the ByteBuffer implementations there is position limit checks. If we do write these length as int in one go we can reduce this overhead.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.NoTagsKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValueUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ByteBufferOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="13821" opendate="2015-6-1 00:00:00" fixdate="2015-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WARN if hbase.bucketcache.percentage.in.combinedcache is set</summary>
      <description>HBASE-11520 improved configuration of bucket cache to no longer require hbase.bucketcache.percentage.in.combinedcache. This was done rather aggressively, with this previously mandatory configuration being ignored. This can result in RS crashes for unsuspecting users. We should add a WARN when hbase.bucketcache.percentage.in.combinedcache is set to make debugging the crash more straight forward.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="13839" opendate="2015-6-4 00:00:00" fixdate="2015-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix AssgnmentManagerTmpl.jamon issues (coloring, content etc.)</summary>
      <description>The template for the RIT in the Master status page, AssignmentManagerTmpl.jamon) has a few issues: The oldest RIT should not be red, looks like a failed entryThe RIT entries should be for example yellow/amber when over the threshold time, and red if 2x the threshold - or red for the oldest once over the threshold. Region count over RIT threshold should only be colored if &gt; 0The summary line (first of two) should not be colored unless there is a value &gt; 0 in it. Color is overriden by table-stripped CSS style!The Bootstrap stylesheet cancels out the hardcoded coloring! The table-stripped resets the conditional coloring and should be fixed. Best is to use "alert-warning" etc. that come from the Bootstrap theme stylesheet. That should maybe already work in combination with the "table-stripped" from the same. Should sort descending by timeCurrently the list of regions is sorted by encoded region name. Better is to have the table sorted by RIT time descending.We should also think about a pagination option for the currently hardcoded 100 entries max. Maybe a separate issue?</description>
      <version>1.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="13861" opendate="2015-6-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BucketCacheTmpl.jamon has wrong bucket free and used labels</summary>
      <description>See this from the template, and note the label and actual values for the last two columns.&lt;table class="table table-striped"&gt; &lt;tr&gt; &lt;th&gt;Bucket Offset&lt;/th&gt; &lt;th&gt;Allocation Size&lt;/th&gt; &lt;th&gt;Free Count&lt;/th&gt; &lt;th&gt;Used Count&lt;/th&gt; &lt;/tr&gt;&lt;%for Bucket bucket: buckets %&gt; &lt;tr&gt; &lt;td&gt;&lt;% bucket.getBaseOffset() %&gt;&lt;/td&gt; &lt;td&gt;&lt;% bucket.getItemAllocationSize() %&gt;&lt;/td&gt; &lt;td&gt;&lt;% bucket.getFreeBytes() %&gt;&lt;/td&gt; &lt;td&gt;&lt;% bucket.getUsedBytes() %&gt;&lt;/td&gt; &lt;/tr&gt;They are labeled "counts" but are "bytes", duh.</description>
      <version>1.1.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="13863" opendate="2015-6-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-wal feature breaks reported number and size of HLogs</summary>
      <description>When multi-wal is enabled the number and size of retained HLogs is always reported as zero.We should fix this so that the numbers are the sum of all retained logs.</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DefaultWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.BoundedRegionGroupingProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13877" opendate="2015-6-9 00:00:00" fixdate="2015-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interrupt to flush from TableFlushProcedure causes dataloss in ITBLL</summary>
      <description>ITBLL with 1.25B rows failed for me (and Stack as reported in https://issues.apache.org/jira/browse/HBASE-13811?focusedCommentId=14577834&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14577834) HBASE-13811 and HBASE-13853 fixed an issue with WAL edit filtering. The root cause this time seems to be different. It is due to procedure based flush interrupting the flush request in case the procedure is cancelled from an exception elsewhere. This leaves the memstore snapshot intact without aborting the server. The next flush, then flushes the previous memstore with the current seqId (as opposed to seqId from the memstore snapshot). This creates an hfile with larger seqId than what its contents are. Previous behavior in 0.98 and 1.0 (I believe) is that after flush prepare and interruption / exception will cause RS abort.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionMergeRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Region.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.flush.RegionServerFlushTableProcedureManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  <bug id="13889" opendate="2015-6-11 00:00:00" fixdate="2015-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-shaded-client artifact so it works on hbase-downstreamer</summary>
      <description>The hbase-shaded-client artifact was introduced in HBASE-13517. Thank you very much for this, as I am new to Java building and was having a very slow-moving time resolving conflicts. However, the shaded client artifact seems to be missing javax.xml.transform.TransformerException. I examined the JAR, which does not have this package/class.Steps to reproduce:Java: package com.mycompany.app; import org.apache.hadoop.conf.Configuration; import org.apache.hadoop.hbase.HBaseConfiguration; import org.apache.hadoop.hbase.client.Connection; import org.apache.hadoop.hbase.client.ConnectionFactory; public class App { public static void main( String[] args ) throws java.io.IOException { Configuration config = HBaseConfiguration.create(); Connection connection = ConnectionFactory.createConnection(config); } }POM:&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.mycompany.app&lt;/groupId&gt; &lt;artifactId&gt;my-app&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;my-app&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-shaded-client&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; Run:$ mvn exec:java -Dexec.mainClass="com.mycompany.app.App"[INFO] Scanning for projects...[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building my-app 1.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- exec-maven-plugin:1.4.0:java (default-cli) @ my-app ---[WARNING] java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:293) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/shaded/javax/xml/transform/TransformerException at com.mycompany.app.App.main(App.java:17) ... 6 moreCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.shaded.javax.xml.transform.TransformerException at java.net.URLClassLoader$1.run(URLClassLoader.java:366) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 7 more[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 2.388 s[INFO] Finished at: 2015-06-11T13:23:21-04:00[INFO] Final Memory: 10M/111M[INFO] ------------------------------------------------------------------------</description>
      <version>1.1.0,1.1.0.1</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13893" opendate="2015-6-12 00:00:00" fixdate="2015-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace HTable with Table in client tests</summary>
      <description>Many client tests reference the HTable implementation instead of the generic Table implementation. It is now not possible to reuse the tests for another Table implementation. This issue focusses on all HTable instances in relevant client tests and thus not all HTable instances.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestRegionSplitter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestProcessBasedCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestMetaTableAccessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestHBaseOnOtherDfsCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestGlobalMemStoreSize.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationKillRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestRegionReplicaReplicationEndpointNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerWithBulkload.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerOnlineConfigChange.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPerColumnFamilyFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactionWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaThrottle.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRollingRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterTransitions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterRestartAfterDisablingTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestGetLastFlushedSequenceId.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduceBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScanBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestSyncTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestRowCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultithreadedTableMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultiTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithVisibilityLabels.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithTTLs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithOperationAttributes.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Action.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.coprocessor.Batch.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestOperation.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestWithCellVisibilityLoadAndVerify.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.RowResourceBase.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestStatusResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSizeCalculator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestCheckAndMutate.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientPushback.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSideNoCodec.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTableMultiplexer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTableMultiplexerFlushCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMetaWithReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiParallel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicaWithCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestResultSizeEstimation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotCloneIndependence.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestAggregateProtocol.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestBigDecimalColumnInterpreter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestDoubleColumnInterpreter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithRemove.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestMultiRowRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestLoadAndSwitchEncodeOnDisk.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCellCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCopyTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHashTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  <bug id="13907" opendate="2015-6-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to deploy a coprocessor</summary>
      <description>Capture this information:&gt; Where are the dependencies located for these classes? Is there a path on HDFS or local disk that dependencies need to be placed so that each RegionServer has access to them?It is suggested to bundle them as a single jar so that RS can load the whole jar and resolve dependencies. If you are not able to do that, you need place the dependencies in regionservers class path so that they are loaded during RS startup. Do either of these options work for you? Btw, you can load the coprocessors/filters into path specified by hbase.dynamic.jars.dir &amp;#91;1&amp;#93;, so that they are loaded dynamically by regionservers when the class is accessed (or you can place them in the RS class path too, so that they are loaded during RS JVM startup).&gt; How would one deploy these using an automated system? (puppet/chef/ansible/etc)You can probably use these tools to automate shipping the jars to above locations?&gt; Tests our developers have done suggest that simply disabling a coprocessor, replacing the jar with a different version, and enabling the coprocessor again does not load the newest version. With that in mind how does one know which version is currently deployed and enabled without resorting to parsing `hbase shell` output or restarting hbase?Actually this is a design issue with current classloader. You can't reload a class in a JVM unless you delete all the current references to it. Since the current JVM (classloader) has reference to it, you can't overwrite it unless you kill the JVM, which is equivalent to restarting it. So you still have the older class loaded in place. For this to work, classloader design should be changed. If it works for you, you can rename the coprocessor class name and the new version of jar and RS loads it properly.&gt; Where does logging go, and how does one access it? Does logging need to be configured in a certain way?Can you please specify which logging you are referring to?&gt; Where is a good location to place configuration files?Same as above, are these hbase configs or something else? If hbase configs, are these gateway configs/server side?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13930" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Findbugs packages from shaded jars</summary>
      <description>Looking at 1.1.1RC0 shaded artifacts, looks like classes from find bugs are under the edu prefix and are not shaded. We should exclude find bugs from the shaded builds, and/or shade shade the edu prefix as well.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13933" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBE&amp;#39;s seekBefore with tags corrupts the tag&amp;#39;s offset information thus leading to incorrect results</summary>
      <description>The problem occurs with moveToPrevious() case and incase of tags we copy the previous pointer's tag info to the current because already decoded the tags.Will check once again before I post other details. I have a test case to reproduce the problem. Found this while working with MultibyteBuffers and verified if this is present in trunk - it is in all branches where we have tags compression (I suppose) will verify</description>
      <version>1.0.0,1.0.1,1.1.0,0.98.13,1.0.1.1,1.1.0.1,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.1,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekTo.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="14021" opendate="2015-7-3 00:00:00" fixdate="2015-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quota table has a wrong description on the UI</summary>
      <description></description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="14041" opendate="2015-7-8 00:00:00" fixdate="2015-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client MetaCache is cleared if a ThrottlingException is thrown</summary>
      <description>During performance test with the request throttling, I saw that hbase:meta table had been read a lot. Currently the MetaCache of the client is cleared, if a ThrottlingException is thrown. It seems to be not needed.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14155" opendate="2015-7-25 00:00:00" fixdate="2015-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError in reverse scan</summary>
      <description>A stack overflow may occur when a reverse scan is done. To reproduce (on a Mac), use the following steps: Download the Phoenix 4.5.0 RC here: https://dist.apache.org/repos/dist/dev/phoenix/phoenix-4.5.0-HBase-1.1-rc0/bin/ Copy the phoenix-4.5.0-HBase-1.1-server.jar into the HBase lib directory (removing any earlier Phoenix version if there was one installed) Stop and restart HBase From the bin directory of the Phoenix binary distribution, start sqlline like this: ./sqlline.py localhost Create a new table and populate it like this:create table desctest (k varchar primary key desc);upsert into desctest values ('a');upsert into desctest values ('ab');upsert into desctest values ('b'); Note that the following query works fine at this point:select * from desctest order by k;+------------------------------------------+| K |+------------------------------------------+| a || ab || b |+------------------------------------------+ Stop and start HBase Rerun the above query again and you'll get a StackOverflowError at StoreFileScanner.seekToPreviousRow()select * from desctest order by k;java.lang.RuntimeException: org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: org.apache.hadoop.hbase.DoNotRetryIOException: DESCTEST,,1437847235264.a74d70e6a8b36e24d1ea1a70edb0cdf7.: null at org.apache.phoenix.util.ServerUtil.createIOException(ServerUtil.java:84) at org.apache.phoenix.util.ServerUtil.throwIOException(ServerUtil.java:52) at org.apache.phoenix.coprocessor.BaseScannerRegionObserver$2.nextRaw(BaseScannerRegionObserver.java:352) at org.apache.phoenix.coprocessor.DelegateRegionScanner.nextRaw(DelegateRegionScanner.java:77) at org.apache.hadoop.hbase.regionserver.RSRpcServices.scan(RSRpcServices.java:2393) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32205) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2112) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:101) at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130) at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.StackOverflowError at org.apache.hadoop.hbase.io.hfile.ChecksumUtil.numChunks(ChecksumUtil.java:201) at org.apache.hadoop.hbase.io.hfile.ChecksumUtil.numBytes(ChecksumUtil.java:189) at org.apache.hadoop.hbase.io.hfile.HFileBlock.totalChecksumBytes(HFileBlock.java:1826) at org.apache.hadoop.hbase.io.hfile.HFileBlock.getBufferReadOnly(HFileBlock.java:356) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getEncodedBuffer(HFileReaderV2.java:1211) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$EncodedScannerV2.getFirstKeyInBlock(HFileReaderV2.java:1307) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekBefore(HFileReaderV2.java:657) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$AbstractScannerV2.seekBefore(HFileReaderV2.java:646) at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToPreviousRow(StoreFileScanner.java:425) at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToPreviousRow(StoreFileScanner.java:449) at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToPreviousRow(StoreFileScanner.java:449) at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekToPreviousRow(StoreFileScanner.java:449)I've attempted to reproduce this in a standalone HBase unit test, but have not been able to (but I'll attach my attempt which mimics what Phoenix is doing).</description>
      <version>1.1.0</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="14193" opendate="2015-8-7 00:00:00" fixdate="2015-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove support for direct upgrade from pre-0.96 versions</summary>
      <description>As discussed on the mailing list this will remove all support for upgrades from pre-0.96 versions.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSTableDescriptorMigrationToSubdir.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">bin.hbase.cmd</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="14260" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t build javadocs for hbase-protocol module</summary>
      <description>I'm not sure I have all the affected versions, but it seems that something is amiss in making our javadocs: mvn -Papache-release -Prelease -DskipTests clean package... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache HBase ....................................... SUCCESS [ 11.149 s][INFO] Apache HBase - Checkstyle .......................... SUCCESS [ 1.249 s][INFO] Apache HBase - Resource Bundle ..................... SUCCESS [ 0.539 s][INFO] Apache HBase - Annotations ......................... SUCCESS [ 4.438 s][INFO] Apache HBase - Protocol ............................ SUCCESS [10:15 min][INFO] Apache HBase - Common .............................. SUCCESS [ 48.465 s][INFO] Apache HBase - Procedure ........................... SUCCESS [ 14.375 s][INFO] Apache HBase - Client .............................. SUCCESS [ 45.187 s][INFO] Apache HBase - Hadoop Compatibility ................ SUCCESS [ 6.998 s][INFO] Apache HBase - Hadoop Two Compatibility ............ SUCCESS [ 14.891 s][INFO] Apache HBase - Prefix Tree ......................... SUCCESS [ 14.214 s][INFO] Apache HBase - Server .............................. SUCCESS [02:01 min][INFO] Apache HBase - Testing Util ........................ SUCCESS [ 12.779 s][INFO] Apache HBase - Thrift .............................. SUCCESS [01:15 min][INFO] Apache HBase - Shell ............................... SUCCESS [ 6.649 s][INFO] Apache HBase - Integration Tests ................... SUCCESS [ 6.429 s][INFO] Apache HBase - Examples ............................ SUCCESS [ 13.200 s][INFO] Apache HBase - Rest ................................ SUCCESS [ 27.831 s][INFO] Apache HBase - Assembly ............................ SUCCESS [ 19.400 s][INFO] Apache HBase - Shaded .............................. SUCCESS [ 0.419 s][INFO] Apache HBase - Shaded - Client ..................... SUCCESS [ 23.707 s][INFO] Apache HBase - Shaded - Server ..................... SUCCESS [ 43.654 s][INFO] Apache HBase - Spark ............................... SUCCESS [02:22 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 21:13 min[INFO] Finished at: 2015-08-19T15:48:00-05:00[INFO] Final Memory: 181M/1513M[INFO] ------------------------------------------------------------------------</description>
      <version>0.98.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14261" opendate="2015-8-19 00:00:00" fixdate="2015-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance Chaos Monkey framework by adding zookeeper and datanode fault injections.</summary>
      <description>One of the shortcomings of existing ChaosMonkey framework is lack of fault injections for hbase dependencies like zookeeper, hdfs etc. This patch attempts to solve this problem partially by adding datanode and zk node fault injections.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseCluster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.RESTApiClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.DistributedHBaseCluster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.ClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.MonkeyFactory.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.RestartActionBaseAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.Action.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKServerTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="14271" opendate="2015-8-20 00:00:00" fixdate="2015-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Nexus staging instructions</summary>
      <description>Refine the Nexus staging instructions a bit. (A promise I made a long time ago.)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14587" opendate="2015-10-9 00:00:00" fixdate="2015-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Attach a test-sources.jar for hbase-server</summary>
      <description>It'd be nice to attach a test-sources jar alongside the others as part of the build, to provide test resources.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14696" opendate="2015-10-26 00:00:00" fixdate="2015-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting allowPartialResults in mapreduce Mappers</summary>
      <description>It is currently impossible to get partial results in mapreduce mapper jobs.When setting setAllowPartialResults(true) for scan jobs, they still fail with OOME on large rows.The reason is that Scan field allowPartialResults is lost during job creation: 1. User creates a Job and sets a scan object via TableMapReduceUtil.initTableMapperJob(table_name, scanObj,...) -&gt; which puts a result of TableMapReduceUtil.convertScanToString(scanObj) to the job config. 2. When the job starts - method TableInputFormat.setConfig retrieves a scan string from config and converts it to Scan object by calling TableMapReduceUtil.convertStringToScan - which results in a Scan object with a field allowPartialResults always set to false.I have tried to experiment and modify a TableInputFormat method setConfig() by forcing all scans to allow partial results and after this all jobs succeeded with no more OOME and I also noticed that mappers began to get partial results (Result.isPartial()).My use case is very simple - I just have large rows and expect a mapper to get them partially - to get same rowid several times with different key/value records.This would allow me not to worry about implementing my own result partitioning solution, which i would encounter in case the big amount of result key values could be transparently returned for a single large row.And from the other side - if a Scan object can return several records for the same rowid (partial results), perhaps the mapper should do the same.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="14745" opendate="2015-11-2 00:00:00" fixdate="2015-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade the last few dependencies in hbase-shaded-client</summary>
      <description>junit hadoop common</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.13,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14761" opendate="2015-11-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deletes with and without visibility expression do not delete the matching mutation</summary>
      <description>This is from the user list as reported by Anoop Sharma running into an issue related to visibility expressions and delete.Example run from hbase shell is listed below.Will appreciate any help on this issue.thanks.In the example below, user running queries has MANAGER authorization.*First example:* add a column with visib expr MANAGER delete it by passing in visibility of MANAGER This works and scan doesnt return anything.*Second example:* add a column with visib expr MANAGER delete it by not passing in any visibility. This doesnt delete the column. Scan doesnt return the row but RAW scan shows the column marked as deleteColumn. Now if delete is done again with visibility of MANAGER, it still doesnt delete it and scan returns the original column.*Example 1:*hbase(main):096:0&gt; create 'HBT1', 'cf'hbase(main):098:0* *put 'HBT1', 'John', 'cf:a', 'CA',{VISIBILITY=&gt;'MANAGER'}*hbase(main):099:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446154722055,value=CA1 row(s) in 0.0030 secondshbase(main):100:0&gt; *delete 'HBT1', 'John', 'cf:a', {VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0030 secondshbase(main):101:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL0 row(s) in 0.0030 seconds*Example 2:*hbase(main):010:0* *put 'HBT1', 'John', 'cf:a', 'CA',{VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0040 secondshbase(main):011:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0060 secondshbase(main):012:0&gt; *delete 'HBT1', 'John', 'cf:a'*0 row(s) in 0.0090 secondshbase(main):013:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0050 secondshbase(main):014:0&gt; *scan 'HBT1', {RAW =&gt; true}*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346519,type=DeleteColumn1 row(s) in 0.0060 secondshbase(main):015:0&gt; *delete 'HBT1', 'John', 'cf:a', {VISIBILITY=&gt;'MANAGER'}*0 row(s) in 0.0030 secondshbase(main):016:0&gt; *scan 'HBT1'*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346473,value=CA1 row(s) in 0.0040 secondshbase(main):017:0&gt; *scan 'HBT1', {RAW =&gt; true}*ROWCOLUMN+CELL John column=cf:a, timestamp=1446155346601,type=DeleteColumn1 row(s) in 0.0060 seconds</description>
      <version>1.0.0,1.0.1,1.1.0,1.0.2,1.1.2,0.98.15</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDeletes.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityScanDeleteTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="15036" opendate="2015-12-23 00:00:00" fixdate="2015-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase Spark documentation to include bulk load with thin records</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15441" opendate="2016-3-10 00:00:00" fixdate="2016-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix WAL splitting when region has moved multiple times</summary>
      <description>Currently WAL splitting is broken when a region has been opened multiple times in recent minutes.Region open and region close write event markers to the wal. These markers should have the sequence id in them. However it is currently getting 1. That means that if a region has moved multiple times in the last few mins then multiple split log workers will try and create the recovered edits file for sequence id 1. One of the workers will fail and on failing they will delete the recovered edits. Causing all split wal attempts to fail.We need to: make sure that close get the correct sequence id for open. Filter all region events from recovered editsIt appears that the close event with a sequence id of one is coming from region warm up.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15479" opendate="2016-3-17 00:00:00" fixdate="2016-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No more garbage or beware of autoboxing</summary>
      <description>Quick journey with JMC in a profile mode revealed very interesting and unexpected heap polluter on a client side. Patch will shortly follow.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,0.98.19,1.1.5,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="15666" opendate="2016-4-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded dependencies for hbase-testing-util</summary>
      <description>Folks that make use of our shaded client but then want to test things using the hbase-testing-util end up getting all of our dependencies again in the test scope.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE.txt</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-it.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15667" opendate="2016-4-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more clarity to Reference Guide related to importing Eclipse Formatter</summary>
      <description>In Hbase Reference Guide: 141.1.1. Code Formattingin procedure bullet point 2: It is not clear what the menu item is. It should be changed to the following:"In Preferences, click Java-&gt;Code Style-&gt;Formatter"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15925" opendate="2016-5-31 00:00:00" fixdate="2016-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compat-module maven variable not evaluated</summary>
      <description>Looks like we've regressed on HBASE-8488. Have a look at the dependency artifacts list on http://mvnrepository.com/artifact/org.apache.hbase/hbase-testing-util/1.2.1. Notice the direct dependency's artifactId is ${compat.module}.</description>
      <version>1.0.0,1.1.0,1.2.0,1.2.1,1.0.3,1.1.5</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15984" opendate="2016-6-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Given failure to parse a given WAL that was closed cleanly, replay the WAL.</summary>
      <description>subtask for a general work around for "underlying reader failed / is in a bad state" just for the case where a WAL 1) was closed cleanly and 2) we can tell that our current offset ought not be the end of parseable entries.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,0.98.23,1.2.4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15985" opendate="2016-6-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clarify promises about edits from replication in ref guide</summary>
      <description>we should make clear in a call out that replication only provides at-least-once delivery and doesn't guarantee ordering so that e.g. folks using increments aren't surprised.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15989" opendate="2016-6-8 00:00:00" fixdate="2016-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hbase.online.schema.update.enable</summary>
      <description>Pre discussion on HBASE-15981, the configuration hbase.online.schema.update.enable is an artifact of a bygone era. Lets rip it out.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">hbase-shell.src.test.rsgroup.org.apache.hadoop.hbase.client.rsgroup.TestShellRSGroups.java</file>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.AbstractTestShell.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.alter.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDefaultVisLabelService.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelReplicationWithExpAsString.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestTableLockManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestChangingEncoding.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="16495" opendate="2016-8-24 00:00:00" fixdate="2016-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When accessed via Thrift, all column families have timeToLive equal to -1</summary>
      <description>When accessed via Thrift, all column families have timeToLive equal to -1. This happens because converter function colDescFromHbase forgets to copy timeToLive from HColumnDescriptor. One line change fixes that.</description>
      <version>None</version>
      <fixedVersion>1.4.0,0.98.22,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="18049" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It is not necessary to re-open the region when MOB files cannot be found</summary>
      <description>In HBASE-17712, we try to re-open the region when store files cannot be found. This is useful for store files in a region, but is not necessary when the MOB files cannot be found, because the store files in a region only contain the references to the MOB files and a re-open of a region doesn't help the lost MOB files.In this JIRA, we will directly throw DNRIOE only when the MOB files are not found in MobStoreScanner and ReversedMobStoreScanner. Other logics keep the same.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="7332" opendate="2012-12-11 00:00:00" fixdate="2012-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[webui] HMaster webui should display the number of regions a table has.</summary>
      <description>Pre-0.96/trunk hbase displayed the number of regions per table in the table listing. Would be good to have this back.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
</bugrepository>
