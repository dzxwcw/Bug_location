<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="2195" opendate="2010-2-9 00:00:00" fixdate="2010-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support cyclic replication</summary>
      <description>We need to support cyclic replication by using the cluster id of each HlogKey and stop replicating when it goes back to the original cluster.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSink.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogMethods.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogKey.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3085" opendate="2010-10-6 00:00:00" fixdate="2010-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaResource broken on TRUNK up on HUDSON</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3133" opendate="2010-10-19 00:00:00" fixdate="2010-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only log compaction requests when a request is actually added to the queue</summary>
      <description>We always log compaction requests, even if a compaction has already been requested for the specified region.Propose only logging the big compaction request log line if a compaction request is actually added to the queue.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3134" opendate="2010-10-20 00:00:00" fixdate="2010-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[replication] Add the ability to enable/disable streams</summary>
      <description>This jira was initially in the scope of HBASE-2201, but was pushed out since it has low value compared to the required effort (and when want to ship 0.90.0 rather soonish).We need to design a way to enable/disable replication streams in a determinate fashion.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.TestReplication.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.ReplicationSourceDummy.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">src.main.ruby.shell.commands.list.peers.rb</file>
      <file type="M">src.main.ruby.shell.commands.enable.peer.rb</file>
      <file type="M">src.main.ruby.shell.commands.disable.peer.rb</file>
      <file type="M">src.main.ruby.hbase.replication.admin.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="3135" opendate="2010-10-20 00:00:00" fixdate="2010-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make our MR jobs implement Tool and use ToolRunner so can do -D trickery, etc.</summary>
      <description>Our TIF can take a bunch of config. If our MR jobs &amp;#8211; rowcounter, export, import, etc. &amp;#8211; all implemented Tool/ToolRunner, then we'd pick up the Tool cmdline parse of -D that sets config. Small change. Lots of utility.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.WALPlayer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Export.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CellCounter.java</file>
    </fixedFiles>
  </bug>
  <bug id="3170" opendate="2010-10-29 00:00:00" fixdate="2010-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionServer confused about empty row keys</summary>
      <description>I'm no longer sure about the expected behavior when using an empty row key (e.g. a 0-byte long byte array). I assumed that this was a legitimate row key, just like having an empty column qualifier is allowed. But it seems that the RegionServer considers the empty row key to be whatever the first row key is.Version: 0.89.20100830, r0da2890b242584a8a5648d83532742ca7243346b, Sat Sep 18 15:30:09 PDT 2010hbase(main):001:0&gt; scan 'tsdb-uid', {LIMIT =&gt; 1}ROW COLUMN+CELL \x00 column=id:metrics, timestamp=1288375187699, value=foo \x00 column=id:tagk, timestamp=1287522021046, value=bar \x00 column=id:tagv, timestamp=1288111387685, value=qux 1 row(s) in 0.4610 secondshbase(main):002:0&gt; get 'tsdb-uid', ''COLUMN CELL id:metrics timestamp=1288375187699, value=foo id:tagk timestamp=1287522021046, value=bar id:tagv timestamp=1288111387685, value=qux 3 row(s) in 0.0910 secondshbase(main):003:0&gt; get 'tsdb-uid', "\000"COLUMN CELL id:metrics timestamp=1288375187699, value=foo id:tagk timestamp=1287522021046, value=bar id:tagv timestamp=1288111387685, value=qux 3 row(s) in 0.0550 secondsThis isn't a parsing problem with the command-line of the shell. I can reproduce this behavior both with plain Java code and with my asynchbase client.Since I don't actually have a row with an empty row key, I expected that the first get would return nothing.</description>
      <version>0.89.20100621,0.89.20100924,0.90.0,0.90.1,0.90.2,0.90.3,0.90.4,0.90.5,0.90.6,0.92.0,0.92.1</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
    </fixedFiles>
  </bug>
  <bug id="3240" opendate="2010-11-16 00:00:00" fixdate="2010-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation of importtsv and bulk loads</summary>
      <description>Right now our bulk load features are a little confusing. We have loadtable.rb for new tables and completebulkload for existing tables. The docs only talk about the incremental case, and there are basically no docs for the ruby script. We should conslidate these things and make the documentation a little more clear on the full story.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3328" opendate="2010-12-9 00:00:00" fixdate="2010-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Admin API: Explicit Split Points</summary>
      <description>Add the ability to explicitly split an existing region at a user-specified point. Currently, you can disable automated splitting and can presplit a newly-created table at explicit boundaries, but cannot explicitly bound a split of an existing region.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3337" opendate="2010-12-13 00:00:00" fixdate="2010-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore HBCK fix of unassignment and dupe assignment for new master</summary>
      <description>HBCK fixing of region unassignment and duplicate assignment was broken with the move to the new master.We've seen it happen doing testing of 0.90 RCs like that over in HBASE-3332.Rather than the old "clear everything out approach" which relied on the BaseScanner, in the new master we should just manipulate unassigned ZK nodes and let the master handle the transition.</description>
      <version>None</version>
      <fixedVersion>0.90.0,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3417" opendate="2011-1-5 00:00:00" fixdate="2011-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CacheOnWrite is using the temporary output path for block names, need to use a more consistent block naming scheme</summary>
      <description>Currently the block names used in the block cache are built using the filesystem path. However, for cache on write, the path is a temporary output file.The original COW patch actually made some modifications to block naming stuff to make it more consistent but did not do enough. Should add a separate method somewhere for generating block names using some more easily mocked scheme (rather than just raw path as we generate a random unique file name twice, once for tmp and then again when moved into place).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3419" opendate="2011-1-5 00:00:00" fixdate="2011-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If re-transition to OPENING during log replay fails, server aborts. Instead, should just cancel region open.</summary>
      <description>The Progressable used on region open to tickle the ZK OPENING node to prevent the master from timing out a region open operation will currently abort the RegionServer if this fails for some reason. However it could be "normal" for an RS to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open.We had a cluster trip over some other issue (for some reason, the tickle was not happening in &lt; 30 seconds, so master was timing out every time). Because of the abort on BadVersion, this eventually led to every single RS aborting itself eventually taking down the cluster.</description>
      <version>0.90.0,0.92.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3429" opendate="2011-1-7 00:00:00" fixdate="2011-1-7 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>HBaseObjectWritable should support arrays of any Writable or Serializable, not just Writable[].</summary>
      <description>Presently, an exception is actually thrown when deserializing the output of HBaseObjectWritable.writeObject(SomeSubclassOfWritable[]).The issue is pretty difficult to debug.This patch adds support for arrays whose contents are subtypes of both Serializable and Writable.</description>
      <version>0.92.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
    </fixedFiles>
  </bug>
  <bug id="3443" opendate="2011-1-13 00:00:00" fixdate="2011-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ICV optimization to look in memstore first and then store files (HBASE-3082) does not work when deletes are in the mix</summary>
      <description>For incrementColumnValue() HBASE-3082 adds an optimization to check memstores first, and only if not present in the memstore then check the store files. In the presence of deletes, the above optimization is not reliable.If the column is marked as deleted in the memstore, one should not look further into the store files. But currently, the code does so.Sample test code outline:admin.createTable(desc)table = HTable.new(conf, tableName)table.incrementColumnValue(Bytes.toBytes("row"), cf1name, Bytes.toBytes("column"), 5);admin.flush(tableName)sleep(2)del = Delete.new(Bytes.toBytes("row"))table.delete(del)table.incrementColumnValue(Bytes.toBytes("row"), cf1name, Bytes.toBytes("column"), 5);get = Get.new(Bytes.toBytes("row"))keyValues = table.get(get).raw()keyValues.each do |keyValue| puts "Expect 5; Got Value=#{Bytes.toLong(keyValue.getValue())}";endThe above prints:Expect 5; Got Value=10</description>
      <version>0.90.0,0.90.1,0.90.2,0.90.3,0.90.4,0.90.5,0.90.6,0.92.0,0.92.1</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="3444" opendate="2011-1-14 00:00:00" fixdate="2011-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to prove Bytes.toBytesBinary and Bytes.toStringBinary() is reversible</summary>
      <description>Bytes.toStringBinary() doesn't escape \.Otherwise the transformation isn't reversiblebyte[] a = {'\', 'x' , '0', '0'}Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
    </fixedFiles>
  </bug>
  <bug id="3572" opendate="2011-2-26 00:00:00" fixdate="2011-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>memstore lab can leave half inited data structs (bad!)</summary>
      <description>in Chunk.init() if new byte[] fails it leaves the Chunk in its uninitialized state, other threads will assume someone else will init it and get stuck in an infinite loop.</description>
      <version>0.90.1,0.92.0</version>
      <fixedVersion>0.90.2,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreLAB.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3573" opendate="2011-2-26 00:00:00" fixdate="2011-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move shutdown messaging OFF hearbeat; prereq for fix of hbase-1502</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestSerialization.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestHMsg.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3584" opendate="2011-3-1 00:00:00" fixdate="2011-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow atomic put/delete in one call</summary>
      <description>Right now we have the following calls:put(Put)delete(Delete)increment(Increments)But we cannot combine all of the above in a single call, complete with a single row lock. It would be nice to do that.It would also allow us to do a CAS where we could do a put/increment if the check succeeded.Amendment:Since Increment does not currently support MVCC it cannot be included in an atomic operation.So this for Put and Delete only.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RowMutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="3612" opendate="2011-3-8 00:00:00" fixdate="2011-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseAdmin::isTableAvailable( name ) returns true when the table does not exist</summary>
      <description>HBaseAdmin::isTableAvailable( name ) returns true for a table in which HBaseAdmin::tableExists( name ) returns false.It appears from the code that the default return value from isTableAvailable() is true and false is only returned in the case where the table is found and not all the region servers are online.</description>
      <version>0.90.1,0.92.0</version>
      <fixedVersion>0.90.2,0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3624" opendate="2011-3-11 00:00:00" fixdate="2011-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only one coprocessor of each priority type can be loaded for a table</summary>
      <description>Coprocessors are added to HBase using a TreeSet that is initialized with an EnvironmentPriorityComparator. The net effect is that only one coprocessor of a given priority can be loaded at a time for a given table. This appears to be due to how the TreeSet uses the EnvironmentPriorityComparator to determine whether there are duplicate entries - if the coprocessors have the same priority (e.g., User), they are considered the same and won't be added to the Set.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorEnvironment.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3631" opendate="2011-3-14 00:00:00" fixdate="2011-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLONE - HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell</summary>
      <description>HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell0.90 was fixed but in trunk there is still bug</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3653" opendate="2011-3-16 00:00:00" fixdate="2011-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallelize Server Requests on HBase Client</summary>
      <description>Vastly improves HBCK performance. Although we are parallelizing getRegionAssignment() calls, getHRegionConnection()gets a big lock. Cache misses can be expensive on heavily-loaded servers because they need to setup a proxy connection. This hurts cache hits on a cache miss &amp; serializes all cache misses. We should parallelize both situations.</description>
      <version>0.90.2,0.92.0</version>
      <fixedVersion>0.90.2,0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3827" opendate="2011-4-28 00:00:00" fixdate="2011-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-1502, removing heartbeats, broke master joining a running cluster and was returning master hostname for rs to use</summary>
      <description>A couple of silly issues in hbase-1502 turned up by cluster testing TRUNK.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RootRegionTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3835" opendate="2011-4-30 00:00:00" fixdate="2011-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch web pages to Jamon template engine instead of JSP</summary>
      <description>Jamon (http://jamon.org) is a template engine that I think is preferable to JSP. You can read an interview with some comparisons vs JSP here: http://www.artima.com/lejava/articles/jamon.htmlIn particular, I think it will give us the following advantages: Since we'll have a servlet in front of each template, it will encourage us to write less inline Java code and do more code in the servlets. Makes proper unit testing easier since you can trivially render a template and pass in mock arguments without having to start a whole HTTP stack Static typing of template arguments makes it easier to know at compile-time if you've made a mistake.Thoughts? I converted the Master UI yesterday and only took a couple hours.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.regionserver.regionserver.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.regionserver.index.html</file>
      <file type="M">src.main.resources.hbase-webapps.master.master.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.master.index.html</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="3836" opendate="2011-4-30 00:00:00" fixdate="2011-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add facility to track currently progressing actions/workflows</summary>
      <description>A lot of troubleshooting involves answering the question "well, what is your server doing right now?" Today, that involves some combination of interpreting jstack output and/or trudging through logs. Problems with these methods are: (a) users may not have direct ssh access to regionserver machines in production environments, (b) logs are very verbose, so hard to separate what's still going on vs stuff that might have completed, and (c) interpreting jstack requires a pretty good knowledge of the codebase plus diving into source code.I'd like to add a singleton (for now) which takes care of tracking any major actions going on in the region server and master.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestActiveMasterManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3837" opendate="2011-4-30 00:00:00" fixdate="2011-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose regionsInTransition on master UI</summary>
      <description>There have been some bugs in the past that can cause a region to get "stuck" in transition. It's currently hard to see this without tailing the logs and noticing periodic timeout messages, etc.I'd like to expose the regionsInTransition map on the master UI, so ops can quickly identify what might be causing a region to get "stuck".</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3838" opendate="2011-5-1 00:00:00" fixdate="2011-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionCoprocesorHost.preWALRestore throws npe in case there is no RegionObserver registered.</summary>
      <description>It seems the check to bypass the Observers chain is at wrong place in case of pre/post WALRestore. It should be inside the "if statement" that checks whether the CP is instance of RegionObserver or not.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3839" opendate="2011-5-1 00:00:00" fixdate="2011-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose in-progress tasks on web UIs</summary>
      <description>HBASE-3836 adds a TaskMonitor class which collects info about what's going on inside processes. This ticket is to expose the task monitor info on the web UIs.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.static.hbase.css</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3842" opendate="2011-5-1 00:00:00" fixdate="2011-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor Coprocessor Compaction API</summary>
      <description>After HBASE-3797, the compaction logic flow has been significantly altered. Because of this, the current compaction coprocessor API is insufficient for gaining full insight into compaction requests/results. Refactor coprocessor API after HBASE-3797 is committed to be more extensible and increase visibility.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestClassLoading.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3843" opendate="2011-5-2 00:00:00" fixdate="2011-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>splitLogWorker starts too early</summary>
      <description>splitlogworker should be started in startServiceThreads() instead of in initializeZookeeper(). This will ensure that the region server accepts a split-logging tasks only after it has successfully done reportForDuty() to the master.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3844" opendate="2011-5-2 00:00:00" fixdate="2011-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Book.xml (removing link to defunct wiki) and Performance.xml (adding client tip)</summary>
      <description>Book.xml - in the FAQ it had a link to a "Frequently Seen Errors" wiki page. This page is labeled as defunct and doesn't even have anything useful on it anyway. Removed the link to that page.Performance.xml - added tip in Performance under client about attribute selection. This is one of those "obvious but not so obvious" topics, if you only need 3 attributes don't select the entire column family.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.performance.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3864" opendate="2011-5-6 00:00:00" fixdate="2011-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename of hfile.min.blocksize.size in HBASE-2899 reverted in HBASE-1861</summary>
      <description>HBASE-2899 renamed hfile.min.blocksize.size to hbase.mapreduce.hfileoutputformat.blocksize. However, HBASE-1861 (committed after HBASE-2899) reverted this change.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3877" opendate="2011-5-11 00:00:00" fixdate="2011-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Determine Proper Defaults for Compaction ThreadPools</summary>
      <description>With the introduction of HBASE-1476, we now have multithreaded compactions + 2 different ThreadPools for large and small compactions. However, this is disabled by default until we can determine a proper default throttle point. Opening this JIRA to log all discussion on how to select a good default for this case.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3890" opendate="2011-5-16 00:00:00" fixdate="2011-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled tasks in distributed log splitting not in sync with ZK</summary>
      <description>This is in continuation to HBASE-3889:Note that there must be more slightly off here. Although the splitlogs znode is now empty the master is still stuck here:Doing distributed log split in hdfs://localhost:8020/hbase/.logs/10.0.0.65,60020,1305406356765 - Waiting for distributed tasks to finish. scheduled=2 done=1 error=0 4380sMaster startup - Splitting logs after master startup 4388sThere seems to be an issue with what is in ZK and what the TaskBatch holds. In my case it could be related to the fact that the task was already in ZK after many faulty restarts because of the NPE. Maybe it was added once (since that is keyed by path, and that is unique on my machine), but the reference count upped twice? Now that the real one is done, the done counter has been increased, but will never match the scheduled.The code could also check if ZK is actually depleted, and therefore treat the scheduled task as bogus? This of course only treats the symptom, not the root cause of this condition.</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="3923" opendate="2011-5-26 00:00:00" fixdate="2011-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-1502 Broke Shell&amp;#39;s status &amp;#39;simple&amp;#39; and &amp;#39;detailed&amp;#39;</summary>
      <description>This is due to the JRuby code using the now removed HServerInfo. Also getServers() is now getServersSize() etc.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3929" opendate="2011-5-27 00:00:00" fixdate="2011-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to HFile tool to produce basic stats</summary>
      <description>In looking at HBASE-3421 I wrote a small tool to scan an HFile and produce some basic statistics about it: min/mean/max key size, value size (uncompressed) min/mean/max number of columns per row (uncompressed) min/mean/max number of bytes per row (uncompressed) the key of the largest row</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3939" opendate="2011-6-1 00:00:00" fixdate="2011-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some crossports of Hadoop IPC fixes</summary>
      <description>A few fixes from Hadoop IPC that we should probably cross-port into our copy: HADOOP-7227: remove the protocol version check at call time HADOOP-7146: fix a socket leak in server HADOOP-7121: fix behavior when response serialization throws an exception HADOOP-7346: send back nicer error response when client is using an out of date IPC version</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.ipc.TestDelayedRpc.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.WritableRpcEngine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.VersionedProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Invocation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.CoprocessorProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateImplementation.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3940" opendate="2011-6-1 00:00:00" fixdate="2011-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase daemons should log version info at startup and possibly periodically</summary>
      <description>As someone who deals with different clusters running different versions (sometimes pre-releases), I often end up with an error log but not great info as the exact revision the log came from. This is even more tricky with things like rolling update.To help with this, the RS and Master should log their complete version string at startup. Potentially, they could also log it once every 12 hours as well, so that even with log rotation we can tell the current version.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.VersionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3949" opendate="2011-6-2 00:00:00" fixdate="2011-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "Master" link to RegionServer pages</summary>
      <description>Use the ZK info where the master is to add a UI link on the top of each RegionServer page. Currently you cannot navigate directly to the Master UI once you are on a RS page.Not sure if the info port is exposed OTTOMH, but we could either use the RS local config setting for that or add it to ZK to enable lookup.</description>
      <version>0.90.3,0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="3970" opendate="2011-6-9 00:00:00" fixdate="2011-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address HMaster crash/failure half way through meta migration</summary>
      <description>When HMaster tries to migrate (after HBASE-451 goes live) the old HRI (with HTD) to new HRI (with out HTD) and if the Master or the migration process crashes/fails midway, it will leave the .META. in a corrupt state and may not allow successful cluster startup.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestMetaMigration.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3973" opendate="2011-6-9 00:00:00" fixdate="2011-6-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase IRB shell: Don&amp;#39;t pretty-print the output when stdout isn&amp;#39;t a TTY</summary>
      <description>In the HBase shell, when the output isn't a TTY, the shell assumes the "terminal" to be 100 characters wide. The way the shell wraps things around makes it very hard to script the output of the shell (e.g. redirect the output to a file and then work on that file, or pipe the output to another command).When stdout isn't a TTY, the shell shouldn't try to wrap things around.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3983" opendate="2011-6-13 00:00:00" fixdate="2011-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>list command in shell seems broken</summary>
      <description>hbase(main):007:0&gt; listERROR: wrong number of arguments (1 for 2)</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4014" opendate="2011-6-21 00:00:00" fixdate="2011-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Coprocessors: Flag the presence of coprocessors in logged exceptions</summary>
      <description>For some initial triage of bug reports for core versus for deployments with loaded coprocessors, we need something like the Linux kernel's taint flag, and list of linked in modules that show up in the output of every OOPS, to appear above or below exceptions that appear in the logs.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithRemove.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithRemove.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithAbort.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4036" opendate="2011-6-27 00:00:00" fixdate="2011-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implementing a MultipleColumnPrefixFilter</summary>
      <description>Implementing a MultipleColumnPrefixFilter so that a user can now specify multiple column prefixes. If the qualifier matches any of the prefixes - it will be accepted</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4037" opendate="2011-6-27 00:00:00" fixdate="2011-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeout annotations to preempt surefire killing all tests</summary>
      <description>Let me add the timeout annotation to a few of the tests that are currently without them and that go on so long, the maven surefire plugin steps in and kills the whole test run (Comes of discussion up on mailing list: http://search-hadoop.com/m/ALF0L1yBgA5/Build+failed+in+Jenkins%253A+HBase-TRUNK+%25231989&amp;subj=Re+Build+failed+in+Jenkins+HBase+TRUNK+1892)</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">pom.xml</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="405" opendate="2008-2-4 00:00:00" fixdate="2008-5-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIF and TOF use log4j directly rather than apache commons-logging</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4078" opendate="2011-7-7 00:00:00" fixdate="2011-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Silent Data Offlining During HDFS Flakiness</summary>
      <description>See HBASE-1436 . The bug fix for this JIRA is a temporary workaround for improperly moving partially-written files from TMP into the region directory when a FS error occurs. Unfortunately, the fix is to ignore all IO exceptions, which masks off-lining due to FS flakiness. We need to permanently fix the problem that created HBASE-1436 &amp; then at least have the option to not open a region during times of flakey FS.</description>
      <version>0.89.20100924,0.90.3,0.92.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4079" opendate="2011-7-7 00:00:00" fixdate="2011-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTableUtil - helper class for loading data</summary>
      <description>A pattern that we use at Explorys is to chunk up Puts, and then bucket Puts by RegionServer. This reduces the number of RPC calls per writeBuffer flush, because the flushes will typically be going to one region with this approach.I didn't think adding such utility methods to HTable was the right approach, so I created an HTableUtil (in the .client package) that contained such functionality.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4124" opendate="2011-7-22 00:00:00" fixdate="2011-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZK restarted while a region is being assigned, new active HM re-assigns it but the RS warns &amp;#39;already online on this server&amp;#39;.</summary>
      <description>ZK restarted while assigning a region, new active HM re-assign it but the RS warned 'already online on this server'.Issue:The RS failed besause of 'already online on this server' and return; The HM can not receive the message and report 'Regions in transition timed out'.</description>
      <version>None</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4126" opendate="2011-7-22 00:00:00" fixdate="2011-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make timeoutmonitor timeout after 30 minutes instead of 3</summary>
      <description>See J-D comment here https://issues.apache.org/jira/browse/HBASE-4064?focusedCommentId=13069098&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13069098 where he thinks we should just turn off timeout monitor because it only ever wrecks havoc. Lets make it 30 minutes for 0.90.4.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4127" opendate="2011-7-22 00:00:00" fixdate="2011-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseAdmin : Don&amp;#39;t modify table&amp;#39;s name away</summary>
      <description>One of the developers was using the default constructor for HTableDescriptor, which is sadly a bad constructor that should never be used. It made the tablename empty in META &amp; caused an ERROR cycle as region onlining kept failing. We should have never let this happen. Don't do table modifications if the HTableDescriptor name doesn't match the table name passed in.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ModifyTableHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4163" opendate="2011-8-4 00:00:00" fixdate="2011-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Split Strategy for YCSB Benchmark</summary>
      <description>Talked with Lars about how we can make it easier for users to run the YCSB benchmarks against HBase &amp; get realistic results. Currently, HBase is optimized for the random/uniform read/write case, which is the YCSB load. The initial reason why we perform bad when users test against us is because they do not presplit regions &amp; have the split ratio really low. We need a one-line way for a user to create a table that is pre-split to 200 regions (or some decent number) by default &amp; disable splitting. Realistically, this is how a uniform load cluster should scale, so it's not a hack. This will also give us a good use case to point to for how users should pre-split regions.</description>
      <version>0.90.3,0.92.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4168" opendate="2011-8-5 00:00:00" fixdate="2011-8-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A client continues to try and connect to a powered down regionserver</summary>
      <description>Experiment-1Started a dev cluster - META is on the same regionserver as my key-value. I kill the regionserver process but donot power down the machine.The META is able to migrate to a new regionserver and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-2Started a dev cluster - META is on a different regionserver as my key-value. I kill the regionserver process but donot power down the machine.The META remains where it is and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-3Started a dev cluster - META is on a different regionserver as my key-value. I power down the machine hosting this regionserver.The META remains where it is and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-4 (This is the problematic one)Started a dev cluster - META is on the same regionserver as my key-value. I power down the machine hosting this regionserver.The META is able to migrate to a new regionserver - however - it takes a really long time (~30 minutes)The regions on that regionserver DONOT reopen (I waited for 1 hour)The client is able to find the new location of the META, however, the META keeps redirecting the client to powered downregionserver as the location of the key-value it is trying to get. Thus the client's get is unsuccessful.</description>
      <version>None</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4171" opendate="2011-8-6 00:00:00" fixdate="2011-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell broken in trunk</summary>
      <description>The shell displays for any command entered:ERROR: undefined method `getZooKeeper' for #&lt;Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0x1c904f75&gt;</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4174" opendate="2011-8-6 00:00:00" fixdate="2011-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Developer.xml - adding a sub-section for setting jira priorities</summary>
      <description>Porting a wiki page to HBase book on this subject.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4215" opendate="2011-8-17 00:00:00" fixdate="2011-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS requestsPerSecond counter seems to be off</summary>
      <description>In testing trunk, I had YCSB reporting some 40,000 requests/second, but the summary info on the master webpage was consistently indicating somewhere around 3x that. I'm guessing that we may have a bug where we forgot to divide by time.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4219" opendate="2011-8-17 00:00:00" fixdate="2011-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Per-Column Family Metrics</summary>
      <description>Right now, we have region server level statistics. However, the read/write flow varies a lot based on the column family involved. We should add dynamic, per column family metrics to JMX so we can track each column family individually.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV1.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV1.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CachedBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockType.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4224" opendate="2011-8-18 00:00:00" fixdate="2011-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a flush by regionserver rather than by table option</summary>
      <description>This evening needed to clean out logs on the cluster. logs are by regionserver. to let go of logs, we need to have all edits emptied from memory. only flush is by table or region. We need to be able to flush the regionserver. Need to add this.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="424" opendate="2008-2-7 00:00:00" fixdate="2008-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should be able to enable/disable .META. table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4243" opendate="2011-8-23 00:00:00" fixdate="2011-4-23 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>HADOOP_HOME should be auto-detected</summary>
      <description>Now that HBASE-3465 has been integrated, perhaps we should try to auto-detect the HADOOP_HOME setting if it is not given explicitly. Something along the lines of:# check for hadoop in the path141 HADOOP_IN_PATH=`which hadoop 2&gt;/dev/null`142 if [ -f ${HADOOP_IN_PATH} ]; then143 HADOOP_DIR=`dirname "$HADOOP_IN_PATH"`/..144 fi145 # HADOOP_HOME env variable overrides hadoop in the path146 HADOOP_HOME=${HADOOP_HOME:-$HADOOP_DIR}147 if [ "$HADOOP_HOME" == "" ]; then148 echo "Cannot find hadoop installation: \$HADOOP_HOME must be set or hadoop must be in the path";149 exit 4;150 fiThoughts?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="4263" opendate="2011-8-27 00:00:00" fixdate="2011-9-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New config property for user-table only RegionObservers.</summary>
      <description>It turns out that a RegionObserver can interfere with - ROOT - and .META.That seems weird and should be prevented.(The one use case for this that I could come up with is access control by Region by intercepting actions on .META., and I don't think that's a particularly strong use case).I'll attach a patch as soon as I get to it.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4269" opendate="2011-8-29 00:00:00" fixdate="2011-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests and restore semantics to TableInputFormat/TableRecordReader</summary>
      <description>HBASE-4196 Modified the semantics of failures in TableImportFormat/TableRecordReader, and had no tests cases. This patch restores semantics to rethrow when a DoNotRetryIOException is triggered and adds test cases.</description>
      <version>0.90.5,0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4270" opendate="2011-8-29 00:00:00" fixdate="2011-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IOE ignored during flush-on-close causes dataloss</summary>
      <description>If the RS experiences an exception during the flush of a region while closing it, it currently catches the exception, logs a warning, and keeps going. If the exception was a DroppedSnapshotException, this means that it will silently drop any data that was in memstore when the region was closed.Instead, the RS should do a hard abort so that its logs will be replayed.</description>
      <version>0.90.4,0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.handler.TestOpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4272" opendate="2011-8-29 00:00:00" fixdate="2011-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbck feature to only inspect and try to repair META and ROOT</summary>
      <description>In cases that META is stuck unassigned, hbck currently crashes. It should be able to handle mis-deployed META and ROOT.</description>
      <version>0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4275" opendate="2011-8-29 00:00:00" fixdate="2011-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS should communicate fatal "aborts" back to the master</summary>
      <description>When a region server aborts, it should attempt to send an RPC to the master that contains (a) the reason for aborting, and (b) the last several KB of log messages, if available. This should help a lot in debugging.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterRegionInterface.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4281" opendate="2011-8-29 00:00:00" fixdate="2011-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add facility to dump current state of all executors</summary>
      <description>For debugging, it's useful to be able to understand the current state of all of the executors. This is like the /stacks servlet, but with executor-specific information, including what's queued and not yet running.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.executor.TestExecutorService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.EventHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4291" opendate="2011-8-30 00:00:00" fixdate="2011-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve display of regions in transition in UI to be more readable</summary>
      <description>Currently the web UI shows which regions are in transition, including the timestamp of when they entered that state. It would be better to show this timestamp as a human-readable date and relative time ("15 seconds ago") in this context.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4292" opendate="2011-8-30 00:00:00" fixdate="2011-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a debugging dump servlet to the master and regionserver</summary>
      <description>For debugging clusters, it would be nice to have a single text-only page that can be "curled" and used for debugging. This servlet will include the following info: build version monitored task status server list recently aborted region servers (see HBASE-4275) regions in transition executor pool status (see HBASE-4281) stack trace of all threads configuration last N KB of the server's logs</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.VersionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4296" opendate="2011-8-30 00:00:00" fixdate="2011-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecate HTable[Interface].getRowOrBefore(...)</summary>
      <description>HTable's getRowOrBefore(...) internally calls into Store.getRowKeyAtOrBefore. That method was created to allow our scanning of .META. (see HBASE-2600).Store.getRowKeyAtOrBefore(...) lists a bunch of requirements for this to be performant that a user of HTable will not be aware of.I propose deprecating this in the public interface in 0.92 and removing it from the public interface in 0.94. If we don't get to HBASE-2600 in 0.94 it will still remain as internal interface for scanning meta.Comments?</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4301" opendate="2011-8-30 00:00:00" fixdate="2011-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>META migration from 0.90 to trunk fails</summary>
      <description>I started a trunk cluster as an upgrade from 0.90.4ish, and now I can't scan my .META. table, etc, and other operations fail.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSTableDescriptors.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4302" opendate="2011-8-30 00:00:00" fixdate="2011-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only run Snappy compression tests if Snappy is available</summary>
      <description>The presence of the Snappy CODEC it does not mean that the Snappy JNI-bindings and native library are available.Because of this it is not possible to assert SNAPPY compression</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestCompressionTest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4304" opendate="2011-8-30 00:00:00" fixdate="2011-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>requestsPerSecond counter stuck at 0</summary>
      <description>Running trunk @ r1163343, all of the requestsPerSecond counters are showing 0 both in the master UI and in the RS UI. The writeRequestsCount metric is properly updating in the RS UI.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.metrics.TestMetricsMBeanBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.MetricsRate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4327" opendate="2011-9-2 00:00:00" fixdate="2011-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile HBase against hadoop 0.22</summary>
      <description>Pom contains a profile for hadoop-0.20 and one for hadoop-0.23, but not one for hadoop-0.22.When overriding hadoop.version to 0.22, then the (compile-time) dependency on hadoop-annotations cannot be met.That exists on 0.23 and 0.24/trunk, but not on 0.22.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4330" opendate="2011-9-3 00:00:00" fixdate="2011-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix races in slab cache</summary>
      <description>A few races are still lingering in the slab cache. Here are some tests and proposed fixes.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.slab.TestSlabCache.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.slab.TestSlab.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.slab.TestSingleSizeCache.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.CacheTestUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SlabCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.Slab.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4331" opendate="2011-9-4 00:00:00" fixdate="2011-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bypassing default actions in prePut fails sometimes with HTable client</summary>
      <description>While testing some other scenario I found calling CoprocessorEnvironment.bypass() fails if all trailing puts in a batch are bypassed that way. By extension a single bypassed put will also fail.The problem is that the puts are removed from the batch in a way that does not align them with the result-status, and in addition the result is never marked as success.A possible fix is to just mark bypassed puts as SUCCESS and filter them in the following logic.(I also contemplated a new BYPASSED OperationStatusCode, but that turned out to be not necessary).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4339" opendate="2011-9-7 00:00:00" fixdate="2011-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve eclipse documentation and project file generation</summary>
      <description>Import via m2eclipse ask a few build path fixes. This should be documented in the hbase book.mvn eclipse:eclipse is helped with the build-helper-maven-plugin plugin where additional folder (target/...) are listed. The listed jamon folder is wrong.(Putting these 2 concerns on same jira as they are more or less related, avoiding jira proliferation).</description>
      <version>0.92.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4348" opendate="2011-9-7 00:00:00" fixdate="2011-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics for regions in transition</summary>
      <description>The following metrics would be useful for monitoring the master: the number of regions in transition the number of regions in transition that have been in transition for more than a minute how many seconds has the oldest region-in-transition been in transition</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.metrics.MasterMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="4357" opendate="2011-9-8 00:00:00" fixdate="2011-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region stayed in transition - in closing state</summary>
      <description>Got the following during testing, 1. On a given machine, kill "RS process id". Then kill "HMaster process id".2. Start RS first via "bin/hbase-daemon.sh --config ./conf start regionserver.". Start HMaster via "bin/hbase-daemon.sh --config ./conf start master".One region of a table stayed in closing state.According to zookeeper,794a6ff17a4de0dd0a19b984ba18eea9 miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. state=CLOSING, ts=Wed Sep 07 17:21:44 PDT 2011 (75701s ago), server=sea-esxi-0,60000,1315428682281 According to .META. table, the region has been assigned to from sea-esxi-0 to sea-esxi-4.miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. sea-esxi-4:60030 H\xB49X\x10bM\xB1 I7K\xC6\xA7\xEF\x9D\x90 0</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.MockRegionServerServices.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.handler.TestCloseRegionHandler.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRootHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4379" opendate="2011-9-13 00:00:00" fixdate="2011-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck] Does not complain about tables with no end region [Z,]</summary>
      <description>hbck does not detect or have an error condition when the last region of a table is missing (end key != '').</description>
      <version>0.90.5,0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.92.2,0.94.1,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandlerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.TableIntegrityErrorHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="4381" opendate="2011-9-13 00:00:00" fixdate="2011-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor split decisions into a split policy class</summary>
      <description>This is a semantics-preserving refactor that moves the code that decides when and where to split into a new split policy class.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="4387" opendate="2011-9-13 00:00:00" fixdate="2011-9-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error while syncing: DFSOutputStream is closed</summary>
      <description>In a billion-row load on ~25 servers, I see "error while syncing" reasonable often with the error "DFSOutputStream is closed" around a roll. We have some race where a roll at the same time as heavy inserts causes a problem.</description>
      <version>0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4403" opendate="2011-9-14 00:00:00" fixdate="2011-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adopt interface stability/audience classifications from Hadoop</summary>
      <description>As HBase gets more widely used, we need to be more explicit about which APIs are stable and not expected to break between versions, which APIs are still evolving, etc. We also have many public classes that are really internal to the RS or Master and not meant to be used by users. Hadoop has adopted a classification scheme for audience (public, private, or limited-private) as well as stability (stable, evolving, unstable). I think we should copy these annotations to HBase and start to classify our public classes.</description>
      <version>0.90.5,0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Row.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperListener.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKSplitLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKServerTool.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKConfig.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKAssign.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.SchemaChangeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RootRegionTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RegionServerTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MetaNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MasterSchemaChangeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.DrainingServerTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ClusterStatusTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ClusterId.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ZooKeeperConnectionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.YouAreDeadException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.VersionAnnotation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Writables.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.VersionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Threads.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Strings.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.SoftValueSortedMap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ServerCommandLine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.RetryCounterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.RetryCounter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.RegionSplitCalculator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.PoolMap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.PairOfSameType.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Pair.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Objects.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.MurmurHash.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Methods.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.MD5Hash.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ManualEnvironmentEdge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.KeyRange.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Keying.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.JvmVersion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.IncrementingEnvironmentEdge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.IdLock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HMerge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseConfTool.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HasThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HashedBytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSTableDescriptors.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSMapRUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSHDFSUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FileSystemVersionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.EnvironmentEdgeManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.EnvironmentEdge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.DirectMemoryUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.DefaultEnvironmentEdge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompoundBloomFilterWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompoundBloomFilterBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompoundBloomFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CollectionBackedScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Classes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CancelableProgressable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ByteBufferOutputStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ByteBloomFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Base64.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Addressing.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.AbstractHBaseTool.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.UnknownScannerException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.UnknownRowLockException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.UnknownRegionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.HThreadedSelectorServerArgs.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.CallQueue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableNotFoundException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableNotEnabledException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableNotDisabledException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableDescriptors.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.Stoppable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.Server.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.security.User.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.security.TokenInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.security.KerberosInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.VersionResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.StorageClusterVersionResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.StorageClusterStatusResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.SchemaResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ScannerResultGenerator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ScannerInstanceResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RowSpec.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RowResultGenerator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RootResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ResultGenerator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ResourceConfig.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ResourceBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RegionsResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.provider.producer.ProtobufMessageBodyProducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.provider.producer.PlainTextMessageBodyProducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.provider.JAXBContextResolver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ProtobufMessageHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.MultiRowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.VersionModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.TableSchemaModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.TableRegionModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.TableModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.TableListModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.TableInfoModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.RowModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.ColumnSchemaModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.CellSetModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.model.CellModel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.metrics.RESTStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.metrics.RESTMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.filter.GZIPResponseWrapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.filter.GZIPResponseStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.filter.GZIPRequestWrapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.filter.GZIPRequestStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.filter.GzipFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ExistsResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Constants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.Response.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.RemoteAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.Cluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.Client.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.RemoteExceptionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.WrongRegionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALActionsListener.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.OrphanHLogAfterSplitException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogPrettyPrinter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogKey.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.TimeRangeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ShutdownHook.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RSStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RSDumpServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ReplicationSourceService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ReplicationSinkService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ReplicationService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerStoppedException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerServices.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerRunningException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerAccounting.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionOpeningState.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionAlreadyInTransitionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.OperationStatus.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.OnlineRegions.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MultiVersionConsistencyControl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaConfigured.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreLAB.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Leases.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LeaseListener.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LeaseException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueSkipListSet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyPrefixRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.InternalScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.InternalScan.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServerCommandLine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRootHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenMetaHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRootHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseMetaHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.FlushRequester.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.DeleteTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.DebugPrint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactSelection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactionRequestor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ColumnTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ColumnCount.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ChangedReadersObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.RegionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.PleaseHoldException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.NotServingRegionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.ThreadMonitoring.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.StateDumpServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTask.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.LogMonitoring.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.migration.HRegionInfo090x.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.PersistentMetricsTimeVaryingRate.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.MetricsString.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.MetricsRate.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.MetricsMBeanBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.HBaseInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.UnAssignCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.TimeToLiveLogCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerAndLoad.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.RegionPlan.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.metrics.MasterStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.metrics.MasterMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterDumpServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LogCleanerDelegate.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LogCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancerFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TotesHRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableModifyFamilyHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableEventHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableAddFamilyHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.SplitRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ModifyTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.DeleteTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.DefaultLoadBalancer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.DeadServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.BulkReOpen.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.BulkAssigner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.MasterNotRunningException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.MasterAddressTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TsvImporterMapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableSplit.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableReducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.PutSortReducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.IdentityTableMapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.Export.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.Driver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.CellCounter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.WritableRpcEngine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.VersionedProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Status.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ServerNotRunningYetException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.RpcEngine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ResponseFlag.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.RequestContext.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ProtocolSignature.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Invocation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCStatistics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ExecRPCInvoker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Delayable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.CoprocessorProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ConnectionHeader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.WritableWithSize.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.TimeRange.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.Reference.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.ImmutableBytesWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SlabItemActionWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SlabCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.Slab.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.SimpleBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.ReusableStreamGzipCodec.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.InvalidHFileException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.InlineBlockWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV1.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV1.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.DoubleBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.Compression.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CachedBlockQueue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CachedBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CacheableDeserializer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.Cacheable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockType.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheKey.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheColumnFamilySummary.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HeapSize.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseMapWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.PrefixKeyDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.EncoderBufferTooSmallException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DiffKeyDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.CopyKeyDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.CompressionState.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.DoubleOutputStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.DataOutputOutputStream.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.CodeToClassAndBack.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.InvalidFamilyOperationException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionLocation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HDFSBlocksDistribution.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.WritableByteArrayComparable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.WhileMatchFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ValueFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.TimestampsFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueExcludeFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.RowFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.RandomRowFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.QualifierFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ParseFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ParseConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.NullComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.MultipleColumnPrefixFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.KeyOnlyFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.InvalidRowFilterException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.IncompatibleFilterException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.FilterBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.FamilyFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.CompareFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ColumnRangeFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ColumnPrefixFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ColumnPaginationFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ColumnCountGetFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BitComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.RegionTransitionData.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.EventHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.EmptyWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.DoNotRetryIOException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.WALObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.WALCoprocessorEnvironment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.RegionCoprocessorEnvironment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.ObserverContext.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.MultiRowMutationProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.MasterCoprocessorEnvironment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.ColumnInterpreter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.BaseEndpointCoprocessor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateImplementation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.constraint.Constraints.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.constraint.ConstraintProcessor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.constraint.ConstraintException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.constraint.Constraint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.constraint.BaseConstraint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ClusterStatus.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ClockOutOfSyncException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHColumnDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ScannerTimeoutException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RowMutations.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RowLock.java</file>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">bin.region.mover.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.Abortable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.AvroServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.avro.AvroUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaMigrationRemovingHTD.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.RootLocationEditor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.AbstractClientScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Action.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Append.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Attributes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.AggregationClient.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.Batch.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.Exec.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.ExecResult.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Get.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableInterfaceFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Increment.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.IsolationLevel.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.metrics.ScanMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MultiAction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MultiPut.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MultiResponse.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Mutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.NoServerForRegionException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Operation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.OperationWithAttributes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RegionOfflineException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ResultScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException.java</file>
    </fixedFiles>
  </bug>
  <bug id="4414" opendate="2011-9-15 00:00:00" fixdate="2011-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region splits by size not being triggered in at least some cases</summary>
      <description>We seem to have lost the triggering of region splits by size somewhere in trunk.Running a simple test to load data only:1. create 'usertable', 'f1' in hbase shell2. run a YCSB load of 10M recordsI wind up with a single region containing all records, around 13GB, despite max region size being configured to 640MB. ip-10-160-217-155.us-west-1.compute.internal:8120 1316045713501 requestsPerSecond=0, numberOfOnlineRegions=1, usedHeapMB=1544, maxHeapMB=2962 usertable,,1316045755455.1e11a9f71072113258942e03dabaa468. numberOfStores=1, numberOfStorefiles=16, storefileUncompressedSizeMB=13611, storefileSizeMB=13621, compressionRatio=1.0007, memstoreSizeMB=50, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=1930, rootIndexSizeKB=108, totalStaticIndexSizeKB=10511, totalStaticBloomSizeKB=0, totalCompactingKVs=3356000, currentCompactedKVs=3356000, compactionProgressPct=1.0As best I can tell, the changes introduced in HBASE-3797 and HBASE-1476 dropped some cases where we were triggering region splits when we didn't compact.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4418" opendate="2011-9-15 00:00:00" fixdate="2011-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show all the hbase configuration in the web ui</summary>
      <description>The motivation is to show ALL the HBase configuration, which takes effect in the run time, in a global place.So we can easily know which configuration takes effect and what the value is.The configuration shows all the HBase and DFS configuration entry in the configuration file and also includes all the HBase default setting in the code, which is not the config file.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4419" opendate="2011-9-16 00:00:00" fixdate="2011-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve build warning messages</summary>
      <description>This item is created to clean up the build log.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4445" opendate="2011-9-19 00:00:00" fixdate="2011-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not passing --config when checking if distributed mode or not</summary>
      <description>If config other than under hbase and we set distributed mode, we were not passing the config to our little property value setter</description>
      <version>0.90.4,0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.start-hbase.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4447" opendate="2011-9-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow hbase.version to be passed in as command-line argument</summary>
      <description>Currently the build always produces the jars and tarball according to the version baked into the POM.When we modify this to allow the version to be passed in as a command-line argument, it can still default to the same behavior, yet give the flexibility for an internal build to tag on own version.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4487" opendate="2011-9-26 00:00:00" fixdate="2011-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The increment operation can release the rowlock before sync-ing the Hlog</summary>
      <description>This allows for better throughput when there are hot rows.I have seen this change make a single row update improve from 400 increments/sec/server to 4000 increments/sec/server.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4509" opendate="2011-9-29 00:00:00" fixdate="2011-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck] Improve region map output</summary>
      <description>HBASE-4375 added a region coverage visualization to hbck in details mode. When users have binary row keys the output is difficult to parse (awk/sed) or pull into programs (numeric, excel) capable of handling tsv formatted data.This patch improves output by using Bytes.toStringBinary (which escapes binary) instead of Bytes.toString when printing keys, suggests some repair actions, and collects "problem group" that groups regions that are overlapping.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4552" opendate="2011-10-7 00:00:00" fixdate="2011-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>multi-CF bulk load is not atomic across column families</summary>
      <description>Currently the bulk load API simply imports one HFile at a time. With multi-column-family support, this is inappropriate, since different CFs show up separately. Instead, the IPC endpoint should take a of CF -&gt; HFiles, so we can online them all under a single region-wide lock.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4553" opendate="2011-10-7 00:00:00" fixdate="2011-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The update of .tableinfo is not atomic; we remove then rename</summary>
      <description>This comes of HBASE-4547. The rename in 0.20 hdfs fails if file exists already. In 0.20+ its better but still 'some' issues if existing reader when file is renamed. This issue is about fixing this (though we depend on fix first being in hdfs).</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestMergeTable.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestFSTableDescriptors.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestFSTableDescriptorForceCreation.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestDrainingServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionInfo.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HMerge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSTableDescriptors.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4556" opendate="2011-10-8 00:00:00" fixdate="2011-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix all incorrect uses of InternalScanner.next(...)</summary>
      <description>There are cases all over the code where InternalScanner.next(...) is not used correctly.I see this a lot:while(scanner.next(...)) {}The correct pattern is:boolean more = false;do { more = scanner.next(...);} while (more);</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HMerge.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4577" opendate="2011-10-11 00:00:00" fixdate="2011-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region server reports storefileSizeMB bigger than storefileUncompressedSizeMB</summary>
      <description>Minor issue while looking at the RS metrics:numberOfStorefiles=8, storefileUncompressedSizeMB=2418, storefileSizeMB=2420, compressionRatio=1.0008I guess there's a truncation somewhere when it's adding the numbers up.FWIW there's no compression on that table.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileWriter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4583" opendate="2011-10-12 00:00:00" fixdate="2011-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate RWCC with Append and Increment operations</summary>
      <description>Currently Increment and Append operations do not work with RWCC and hence a client could see the results of multiple such operation mixed in the same Get/Scan.The semantics might be a bit more interesting here as upsert adds and removes to and from the memstore.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="4588" opendate="2011-10-13 00:00:00" fixdate="2011-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The floating point arithmetic to validate memory allocation configurations need to be done as integers</summary>
      <description>The floating point arithmetic to validate memory allocation configurations need to be done as integers.On our cluster, we had block cache = 0.6 and memstore = 0.2. It was saying this was &gt; 0.8 when it is actually equal.Minor bug but annoying nonetheless.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4603" opendate="2011-10-17 00:00:00" fixdate="2011-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uneeded sleep time for tests in hbase.master.ServerManager#waitForRegionServers</summary>
      <description>This functions waits for at least 2 times hbase.master.wait.on.regionservers.interval, defaulted at 3 seconds, i.e. 6 seconds for every mini hbase cluster starts.In the context of a mini cluster, it's not useful, as the regions servers are created locally.Changing this to a lower value such as 100ms gives 5.8 second per HBase cluser start. It should lower the build time on the apache server by more than 8%.Beeing more aggressive (removing all the wait time) could be possible as well. To be studied later.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.resources.hbase-site.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4642" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Apache License Header</summary>
      <description>executing mvn apache-rat:check fails with &amp;#91;ERROR&amp;#93; Failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: Too many unapproved licenses: 84 -&gt; &amp;#91;Help 1&amp;#93;org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: Too many unapproved licenses: 84there are about 70 + files which are missing the Apache License Headers and rest of them should be added to the exclude list.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.set.meta.memstore.size.rb</file>
      <file type="M">bin.set.meta.block.caching.rb</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
    </fixedFiles>
  </bug>
  <bug id="4645" opendate="2011-10-21 00:00:00" fixdate="2011-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Edits Log recovery losing data across column families</summary>
      <description>There is a data loss happening (for some of the column families) when we do the replay logs.The bug seems to be from the fact that during replay-logs we only choose to replaythe logs from the maximumSequenceID across ALL the stores. This is wrong. If acolumn family is ahead of others (because the crash happened before all the columnfamilies were flushed), then we lose data for the column families that have not yetcaught up.The correct logic for replay should begin the replay from the minimum across themaximum in each store.</description>
      <version>0.89.20100924,0.92.0</version>
      <fixedVersion>0.89-fb,0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4679" opendate="2011-10-26 00:00:00" fixdate="2011-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift null mutation error</summary>
      <description>When using null as a value for a mutation, HBasse thrift client failed and threw an error. We should instad check for a null byte buffer.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4690" opendate="2011-10-27 00:00:00" fixdate="2011-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intermittent TestRegionServerCoprocessorExceptionWithAbort#testExceptionFromCoprocessorDuringPut failure</summary>
      <description>See https://builds.apache.org/view/G-L/view/HBase/job/HBase-0.92/83/testReport/junit/org.apache.hadoop.hbase.coprocessor/TestRegionServerCoprocessorExceptionWithAbort/testExceptionFromCoprocessorDuringPut/Somehow getRSForFirstRegionInTable() wasn't able to retrieve the region server.One fix for this issue is to spin up MiniCluster with 1 region server so that we don't need to search for the region server where first region is hosted.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4691" opendate="2011-10-28 00:00:00" fixdate="2011-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove more unnecessary byte[] copies from KeyValues</summary>
      <description>Just looking through the code I found some more spots where we unnecessarily copy byte[] rather than just passing offset and length around.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4692" opendate="2011-10-28 00:00:00" fixdate="2011-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-4300 broke the build</summary>
      <description>Rebasing my patch I missed encoding of master name up in zk. Creating a separate issue so can try patch build with my fix rather than run all tests locally.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestServerName.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestActiveMasterManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Addressing.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4694" opendate="2011-10-28 00:00:00" fixdate="2011-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some cleanup of log messages in RS and M</summary>
      <description>I did a little edit of logging. We do way too much but am not going to do a big overhaul just yet. Here's a few small changes saving a few lines, some redundancy, and making others look like surrounding log lines.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.DefaultLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="4708" opendate="2011-10-31 00:00:00" fixdate="2011-11-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert safemode related pieces of hbase-4510</summary>
      <description>This thread in dev has us backing out the safemode related portions of hbase-4510 commit: http://search-hadoop.com/m/7WOjpVyG5F/Hmaster+can%2527t+start+for+the+latest+trunk+version&amp;subj=Hmaster+can+t+start+for+the+latest+trunk+version</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4716" opendate="2011-11-1 00:00:00" fixdate="2011-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve locking for single column family bulk load</summary>
      <description>HBASE-4552 changed the locking behavior for single column family bulk load, namely we don't need to take write lock.A read lock would suffice in this scenario.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4734" opendate="2011-11-2 00:00:00" fixdate="2011-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[bulk load] Warn if bulk load directory contained no files</summary>
      <description>Bulk load exits if no files are found in the specified directory. This can happen if a directory has been bulk loaded already (bulk load renames/moves files). It would be good to provide some sort of warning when this happens.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4785" opendate="2011-11-15 00:00:00" fixdate="2011-11-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve recovery time of HBase client when a region server dies.</summary>
      <description>When a region server dies, the HBase client waits until the RPC timesout before learning that it needs to check META to find the new location of the region. And it incurs this timeout cost for every region being served by the dead region server. Remove this overhead by clearing the entries in cache that have the dead region server as their values.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.SoftValueSortedMap.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4791" opendate="2011-11-15 00:00:00" fixdate="2011-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Secure Zookeeper JAAS configuration to be programmatically set (rather than only by reading JAAS configuration file)</summary>
      <description>In the currently proposed fix for HBASE-2418, there must be a JAAS file specified in System.setProperty("java.security.auth.login.config"). However, it might be preferable to construct a JAAS configuration programmatically, as is done with secure Hadoop (see https://github.com/apache/hadoop-common/blob/a48eceb62c9b5c1a5d71ee2945d9eea2ed62527b/src/java/org/apache/hadoop/security/UserGroupInformation.java#L175).This would have the benefit of avoiding a usage of a system property setting, and allow instead an HBase-local configuration setting.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="4797" opendate="2011-11-16 00:00:00" fixdate="2011-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[availability] Skip recovered.edits files with edits we know older than what region currently has</summary>
      <description>Testing 0.92, I crashed all servers out. Another bug makes it so WALs are not getting cleaned so I had 7000 regions to replay. The distributed split code did a nice job and cluster came back but interesting is that some hot regions ended up having loads of recovered.edits files &amp;#8211; tens if not hundreds &amp;#8211; to replay against the region (can we bulk load recovered.edits instead of replaying them?). Each recovered.edits file is taking about a second to process (though only about 30 odd edits per file it seems). The region is unavailable during this time.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="4808" opendate="2011-11-17 00:00:00" fixdate="2011-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to Ensure Expired Deletes Don&amp;#39;t Override Puts</summary>
      <description>We originally thought we had a bug where expired delete markers would early-out valid puts. It ended up being a false alarm, but we added a unit test to ensure that this behavior is correctly maintained.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="4809" opendate="2011-11-17 00:00:00" fixdate="2011-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Per-CF set RPC metrics</summary>
      <description>Porting per-CF set metrics for RPC times and response sizes from 0.89-fb to trunk. For each "mutation signature" (a set of column families involved in an RPC request) we increment several metrics, allowing to monitor access patterns. We deal with guarding against an explosion of the number of metrics in HBASE-4638 (which might even be implemented as part of this JIRA).</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.metrics.TestSchemaMetrics.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.metrics.TestSchemaConfigured.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="481" opendate="2008-2-29 00:00:00" fixdate="2008-5-29 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Use smaller dataset in TestTableIndex and TestTableMapReduce</summary>
      <description>TestTableIndex and TestTableMapReduce rely on MakeMultiRegionTable, which in turn calls addContent in HBaseTestCase. This method produces 17k rows in the table, and something like 10 regions. That's a lot more than we actually need to prove the functionality of TTI and TTMR. Can we reduce the number of rows that we use by shortening the run of addContent? Is there any risk in changing addContent globally to produce less data?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">conf.hbase-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4815" opendate="2011-11-17 00:00:00" fixdate="2011-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable online altering by default, create a config for it</summary>
      <description>There's a whole class of bugs that we've been revealing from trying out online altering in conjunction with other operations like splitting. HBASE-4729, HBASE-4794, and HBASE-4814 are examples.It's not so much that the online altering code is buggy, but that it wasn't tested in an environment that permits splitting.I think we should mark online altering as experimental in 0.92 and add a config to enable it (so it would be disabled by default, requiring people to enable for altering table schema).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableEventHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4816" opendate="2011-11-18 00:00:00" fixdate="2011-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver wouldn&amp;#39;t go down because split happened exactly at same time we issued bulk user region close call on our way out</summary>
      <description>A regionserver wouldn't go down because it was waiting on a user region to close only the user-space region had just been opened as part of a split transaction &amp;#8211; it was a new daughter &amp;#8211; just as we'd issued the bulk close to all user regions on receipt of a cluster shutdown call.We need to add a check for this condition &amp;#8211; user tables that did not get the close.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4819" opendate="2011-11-18 00:00:00" fixdate="2011-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestShell broke in trunk; typo</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="4820" opendate="2011-11-18 00:00:00" fixdate="2011-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed log splitting coding enhancement to make it easier to understand, no semantics change</summary>
      <description>In reviewing distributed log splitting feature, we found some cosmetic issues. They make the code hard to understand.It will be great to fix them. For this issue, there should be no semantic change.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="4851" opendate="2011-11-22 00:00:00" fixdate="2011-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop maven dependency needs to be an optional one</summary>
      <description>Given that HBase 0.92/0.94 is likely to be used with at least 3 different versions of Hadoop (0.20, 0.22 and 0.23) it seems appropriate to make hadoop maven dependencies into optional ones (IOW, the build of HBase will see NO changes in behavior, but any component that has HBase as a dependency will be in control of what version of Hadoop gets used).</description>
      <version>0.92.0,0.92.1,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4854" opendate="2011-11-23 00:00:00" fixdate="2011-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>it seems that CLASSPATH elements coming from Hadoop change HBase behaviour</summary>
      <description>It looks like HBASE-3465 introduced a slight change in behavior. The ordering of classpath elements makes Hadoop ones go before the HBase ones, which leads to log4j properties picked up from the wrong place, etc. It seems that the easies way to fix that would be to revert the ordering of classpath.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="4856" opendate="2011-11-23 00:00:00" fixdate="2011-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper to 3.4.0 release</summary>
      <description>Zookeeper 3.4.0 has been released.We should upgade.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4859" opendate="2011-11-23 00:00:00" fixdate="2011-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correctly PreWarm HBCK ThreadPool</summary>
      <description>See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="4861" opendate="2011-11-24 00:00:00" fixdate="2011-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some misspells and extraneous characters in logs; set some to TRACE</summary>
      <description>Some small clean up in logs.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.SplitRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
    </fixedFiles>
  </bug>
  <bug id="4878" opendate="2011-11-27 00:00:00" fixdate="2011-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master crash when splitting hlog may cause data loss</summary>
      <description>Let's see the code of HlogSplitter#splitLog(final FileStatus[] logfiles)private List&lt;Path&gt; splitLog(final FileStatus[] logfiles) throws IOException { try { for (FileStatus log : logfiles) { parseHLog(in, logPath, entryBuffers, fs, conf, skipErrors); } archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf); } finally { status.setStatus("Finishing writing output logs and closing down."); splits = outputSink.finishWritingAndClose(); }}If master is killed, after finishing archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf), but before finishing splits = outputSink.finishWritingAndClose();Log date would loss!</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4881" opendate="2011-11-28 00:00:00" fixdate="2011-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unhealthy region is on service caused by rollback of region splitting</summary>
      <description>If region splitting is failed in the state of JournalEntry.CLOSED_PARENT_REGIONIt will be rollback as the following steps:1.case CLOSED_PARENT_REGION: this.parent.initialize(); break;2.case CREATE_SPLIT_DIR: this.parent.writestate.writesEnabled = true; cleanupSplitDir(fs, this.splitdir); break;3.case SET_SPLITTING_IN_ZK: if (server != null &amp;&amp; server.getZooKeeper() != null) { cleanZK(server, this.parent.getRegionInfo()); } break;If this.parent.initialize() throws IOException in step 1,If check filesystem is ok. it will do nothing.However, the parent region is on service now.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="4927" opendate="2011-12-1 00:00:00" fixdate="2011-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CatalogJanior:SplitParentFirstComparator doesn&amp;#39;t sort as expected, for the last region when the endkey is empty</summary>
      <description>When reviewing HBASE-4238 backporting, Jon found this issue.What happens if the split points are (empty end key is the last key, empty start key is the first key)Parent [A,)L daughter [A,B), R daughter [B,)When sorted, we gets to end key comparision which results in this incorrector order:[A,B), [A,), [B,) we wanted:[A,), [A,B), [B,)</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionInfo.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="4934" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display Master server and Regionserver start time on respective info servers.</summary>
      <description>With operations like rolling restart or master failovers, it is difficult to tell if a server is the "old" instance or the "new" restarted instance. Adding a start date stamp on the info web pages would be helpful for determining this.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="4937" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error in Quick Start Shell Exercises</summary>
      <description>The shell exercises in the Quick Start (http://hbase.apache.org/book/quickstart.html) startshbase(main):003:0&gt; create 'test', 'cf'0 row(s) in 1.2200 secondshbase(main):003:0&gt; list 'table'test1 row(s) in 0.0550 secondsIt looks like the second command is wrong. Running it, the actual output ishbase(main):001:0&gt; create 'test', 'cf'0 row(s) in 0.3630 secondshbase(main):002:0&gt; list 'table'TABLE 0 row(s) in 0.0100 secondsThe argument to list should be 'test', not 'table', and the output in the example is missing the TABLE line.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.getting.started.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4944" opendate="2011-12-4 00:00:00" fixdate="2011-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optionally verify bulk loaded HFiles</summary>
      <description>We rely on users to produce properly formatted HFiles for bulk import. Attached patch adds an optional code path, toggled by a configuration property, that verifies the HFile under consideration for import is properly sorted. The default maintains the current behavior, which does not scan the file for correctness.Patch is against trunk but can apply against all active branches.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
    </fixedFiles>
  </bug>
  <bug id="4990" opendate="2011-12-9 00:00:00" fixdate="2011-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document secure HBase setup</summary>
      <description></description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5009" opendate="2011-12-12 00:00:00" fixdate="2011-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure of creating split dir if it already exists prevents splits from happening further</summary>
      <description>The scenario is-&gt; The split of a region takes a long time-&gt; The deletion of the splitDir fails due to HDFS problems.-&gt; Subsequent splits also fail after that.private static void createSplitDir(final FileSystem fs, final Path splitdir) throws IOException { if (fs.exists(splitdir)) throw new IOException("Splitdir already exits? " + splitdir); if (!fs.mkdirs(splitdir)) throw new IOException("Failed create of " + splitdir); }Correct me if am wrong? If it is an issue can we change the behaviour of throwing exception?Pls suggest.</description>
      <version>None</version>
      <fixedVersion>0.90.6,0.92.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.Reference.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5021" opendate="2011-12-14 00:00:00" fixdate="2011-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce upper bound on timestamp</summary>
      <description>We have been getting hit with performance problems on our time-series database due to invalid timestamps being inserted by the timestamp. We are working on adding proper checks to app server, but production performance could be severely impacted with significant recovery time if something slips past. Since timestamps are considered a fundamental part of the HBase schema &amp; multiple optimizations use timestamp information, we should allow the option to sanity check the upper bound on the server-side in HBase.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RowResultGenerator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="5040" opendate="2011-12-15 00:00:00" fixdate="2011-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HBase builds fail</summary>
      <description>I saw the following in HBase-0.92-security build #39:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure[ERROR] &lt;https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&gt;:[590,4] method does not override or implement a method from a supertype[ERROR] -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure&lt;https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&gt;:[590,4] method does not override or implement a method from a supertypeThe above was probably introduced by HBASE-5006</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5052" opendate="2011-12-16 00:00:00" fixdate="2011-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path where a dynamically loaded coprocessor jar is copied on the local file system depends on the region name (and implicitly, the start key)</summary>
      <description>When loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. The name is "cleaned", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
    </fixedFiles>
  </bug>
  <bug id="5055" opendate="2011-12-16 00:00:00" fixdate="2011-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build against hadoop 0.22 broken</summary>
      <description>I got the following when compiling TRUNK against hadoop 0.22:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:compile (default-compile) on project hbase: Compilation failure: Compilation failure:[ERROR] /Users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java:[37,39] cannot find symbol[ERROR] symbol : class DFSInputStream[ERROR] location: class org.apache.hadoop.hdfs.DFSClient[ERROR] [ERROR] /Users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java:[109,37] cannot find symbol[ERROR] symbol : class DFSInputStream[ERROR] location: class org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.WALReader.WALReaderFSDataInputStream</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5062" opendate="2011-12-18 00:00:00" fixdate="2011-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing logons if security is enabled</summary>
      <description>Somehow the attached changes are missing from the security integration.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Strings.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
    </fixedFiles>
  </bug>
  <bug id="5063" opendate="2011-12-18 00:00:00" fixdate="2011-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionServers fail to report to backup HMaster after primary goes down.</summary>
      <description>Setup cluster with two HMasters Observe that HM1 is up and that all RS's are in the RegionServer list on web page. Kill (not even -9) the active HMaster Wait for ZK to time out (default 3 minutes). Observe that HM2 is now active. Tables may show up but RegionServers never report on web page. Existing connections are fine. New connections cannot find regionservers.Note: If we replace a new HM1 in the same place and kill HM2, the cluster functions normally again after recovery. This sees to indicate that regionservers are stuck trying to talk to the old HM1.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5068" opendate="2011-12-19 00:00:00" fixdate="2011-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RC1 can not build its hadoop-0.23 profile</summary>
      <description>The hadoop .23 version needs to be bumped to 0.23.1-SNAPSHOT</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5076" opendate="2011-12-20 00:00:00" fixdate="2011-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell hangs when creating some &amp;#39;illegal&amp;#39; tables.</summary>
      <description>In hbase shell. These commands hang:create 'hbase.version','foo'create 'splitlog','foo'Interestinglycreate 'hbase.id','foo'create existingtablename, 'foo'create '.META.','foo'create '-ROOT-','foo'are properly rejected.We should probably either rename to make the files illegal table names (hbase.version to .hbase.version and splitlog to .splitlog) or we could add more special cases.</description>
      <version>0.92.0,0.94.1,0.94.2,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="5077" opendate="2011-12-20 00:00:00" fixdate="2011-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SplitLogWorker fails to let go of a task, kills the RS</summary>
      <description>I hope I didn't break spacetime continuum, I got this while testing 0.92.0:2011-12-20 03:06:19,838 FATAL org.apache.hadoop.hbase.regionserver.SplitLogWorker: logic error - end task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A9100%2Fhbase%2F.logs%2Fsv4r14s38%2C62023%2C1324345935047-splitting%2Fsv4r14s38%252C62023%252C1324345935047.1324349363814 done failed because task doesn't existorg.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A9100%2Fhbase%2F.logs%2Fsv4r14s38%2C62023%2C1324345935047-splitting%2Fsv4r14s38%252C62023%252C1324345935047.1324349363814 at org.apache.zookeeper.KeeperException.create(KeeperException.java:111) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1228) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:372) at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:654) at org.apache.hadoop.hbase.regionserver.SplitLogWorker.endTask(SplitLogWorker.java:372) at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:280) at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197) at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:165) at java.lang.Thread.run(Thread.java:662)I'll post more logs in a moment. What I can see is that the master shuffled that task around a bit and one of the region servers died on this stack trace while the others were able to interrupt themselves.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5078" opendate="2011-12-20 00:00:00" fixdate="2011-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DistributedLogSplitter failing to split file because it has edits for lots of regions</summary>
      <description>Testing 0.92.0RC, ran into interesting issue where a log file had edits for many regions and just opening the file per region was taking so long, we were never updating our progress and so the split of the log just kept failing; in this case, the first 40 edits in a file required our opening 35 files &amp;#8211; opening 35 files took longer than the hard-coded 25 seconds its supposed to take "acquiring" the task.First, here is master's view:2011-12-20 17:54:09,184 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: task not yet acquired /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 ver = 0...2011-12-20 17:54:09,233 INFO org.apache.hadoop.hbase.master.SplitLogManager: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 acquired by sv4r27s44,7003,1324365396664...2011-12-20 17:54:35,475 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: task not yet acquired /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403573033 ver = 3Master then gives it elsewhere.Over on the regionserver we see:2011-12-20 17:54:09,233 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: worker sv4r27s44,7003,1324365396664 acquired task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679....2011-12-20 17:54:10,714 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Path=hdfs://sv4r11s38:7000/hbase/splitlog/sv4r27s44,7003,1324365396664_hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679/TestTable/6b6bfc2716dff952435ab26f018648b2/recovered.edits/0000000000000278862.temp, syncFs=true, hflush=false........ and so on till:2011-12-20 17:54:36,876 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 preempted from sv4r27s44,7003,1324365396664, current task state and owner=owned sv4r28s44,7003,1324365396678....2011-12-20 17:54:37,112 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: Failed to heartbeat the task/hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679....When above happened, we'd only processed 40 edits. As written, we only heatbeat every 1024 edits.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5081" opendate="2011-12-21 00:00:00" fixdate="2011-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed log splitting deleteNode races against splitLog retry</summary>
      <description>Recently, during 0.92 rc testing, we found distributed log splitting hangs there forever. Please see attached screen shot.I looked into it and here is what happened I think:1. One rs died, the servershutdownhandler found it out and started the distributed log splitting;2. All three tasks failed, so the three tasks were deleted, asynchronously;3. Servershutdownhandler retried the log splitting;4. During the retrial, it created these three tasks again, and put them in a hashmap (tasks);5. The asynchronously deletion in step 2 finally happened for one task, in the callback, it removed onetask in the hashmap;6. One of the newly submitted tasks' zookeeper watcher found out that task is unassigned, and it is notin the hashmap, so it created a new orphan task.7. All three tasks failed, but that task created in step 6 is an orphan so the batch.err counter was one short,so the log splitting hangs there and keeps waiting for the last task to finish which is never going to happen.So I think the problem is step 2. The fix is to make deletion sync, instead of async, so that the retry will havea clean start.Async deleteNode will mess up with split log retrial. In extreme situation, if async deleteNode doesn't happensoon enough, some node created during the retrial could be deleted.deleteNode should be sync.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKSplitLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5087" opendate="2011-12-22 00:00:00" fixdate="2011-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up the 0.92RC zk to 3.4.1RC0</summary>
      <description>ZK just found bad bug in 3.4.1 zookeeper-1333. They put up a fix and new rc, 3.4.1(Andrew, you saw Todds query asking if it'd possible to hold to zk 3.3.4 and just have 3.4.1 for secure installs?)</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5094" opendate="2011-12-26 00:00:00" fixdate="2011-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The META can hold an entry for a region with a different server name from the one actually in the AssignmentManager thus making the region inaccessible.</summary>
      <description>RegionState rit = this.services.getAssignmentManager().isRegionInTransition(e.getKey()); ServerName addressFromAM = this.services.getAssignmentManager() .getRegionServerOfRegion(e.getKey()); if (rit != null &amp;&amp; !rit.isClosing() &amp;&amp; !rit.isPendingClose()) { // Skip regions that were in transition unless CLOSING or // PENDING_CLOSE LOG.info("Skip assigning region " + rit.toString()); } else if (addressFromAM != null &amp;&amp; !addressFromAM.equals(this.serverName)) { LOG.debug("Skip assigning region " + e.getKey().getRegionNameAsString() + " because it has been opened in " + addressFromAM.getServerName()); }In ServerShutDownHandler we try to get the address in the AM. This address is initially null because it is not yet updated after the region was opened .i.e. the CAll back after node deletion is not yet done in the master side.But removal from RIT is completed on the master side. So this will trigger a new assignment.So there is a small window between the online region is actually added in to the online list and the ServerShutdownHandler where we check the existing address in AM.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5111" opendate="2011-12-31 00:00:00" fixdate="2011-1-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper to 3.4.2 release</summary>
      <description>Zookeeper 3.4.2 has just been released.We should upgrade to this release.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5119" opendate="2012-1-3 00:00:00" fixdate="2012-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the TimeoutMonitor&amp;#39;s timeout back down</summary>
      <description>The TimeoutMonitor used to be extremely racy and caused more troubles than it fixed, but most of this has been fixed I believe in the context of 0.92 so I think we should set it down back to a useful level. Currently it's 30 minutes, what should the new value be?I think 5 minutes should be good, will do some testing.</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="512" opendate="2008-3-13 00:00:00" fixdate="2008-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add configuration for global aggregate memcache size</summary>
      <description>Currently, we have a configuration parameter for the size a Memcache must reach before it is flushed. This leads to pretty even sized mapfiles when flushes run, which is nice. However, as noted in the parent issue, we can often get to a point where we run out of memory because too much data is hanging around in Memcaches.I think that we should add a new configuration parameter that governs the total amount of memory that the region server should spend on Memcaches. This would have to be some number less than the heap size - we'll have to discover the proper values through experimentation. Then, when a put comes in, if the global aggregate size of all the Memcaches for all the stores is at the threshold, then we should block the current and any subsequent put operations from completing until forced flushes cause the memory usage to go back down to a safe level. The existing strategy for triggering flushes will still be in play, just augmented with this blocking behavior.This approach has the advantage of helping us avoid OOME situations by warning us well in advance of overflow. Additionally, it becomes something of a performance tuning knob, allowing you to allocate more memory to improve write performance. This is superior to the previously suggested PhantomReference approach because that would possibly causes us to bump into further OOMEs while we're trying to flush to avoid them.</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Flusher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5143" opendate="2012-1-8 00:00:00" fixdate="2012-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix config typo in pluggable load balancer factory</summary>
      <description>HBASE-4240 made LoadBalancer pluggable.Configuration it loads seems to be wrongly named and carries a typo: "hbase.maser.loadBalancer.class"Could rather be "hbase.master.loadbalancer.class"Luckily 0.92 is not out yet and we should fix it asap, before folks start using it. Attaching patch.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancerFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5162" opendate="2012-1-10 00:00:00" fixdate="2012-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Basic client pushback mechanism</summary>
      <description>The current blocking we do when we are close to some limits (memstores over the multiplier factor, too many store files, global memstore memory) is bad, too coarse and confusing. After hitting HBASE-5161, it really becomes obvious that we need something better.I did a little brainstorm with Stack, we came up quickly with two solutions: Send some exception to the client, like OverloadedException, that's thrown when some situation happens like getting past the low memory barrier. It would be thrown when the client gets a handler and does some check while putting or deleting. The client would treat this a retryable exception but ideally wouldn't check .META. for a new location. It could be fancy and have multiple levels of pushback, like send the exception to 25% of the clients, and then go up if the situation persists. Should be "easy" to implement but we'll be using a lot more IO to send the payload over and over again (but at least it wouldn't sit in the RS's memory). Send a message alongside a successful put or delete to tell the client to slow down a little, this way we don't have to do back and forth with the payload between the client and the server. It's a cleaner (I think) but more involved solution.In every case the RS should do very obvious things to notify the operators of this situation, through logs, web UI, metrics, etc.Other ideas?</description>
      <version>0.92.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestRegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.HConnectionTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestFastFailWithoutTestUtil.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MultiAction.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionAdapter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClusterConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientPushback.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.StatsTrackingRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5167" opendate="2012-1-10 00:00:00" fixdate="2012-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We shouldn&amp;#39;t be injecting &amp;#39;Killing [daemon]&amp;#39; into logs, when we aren&amp;#39;t doing that.</summary>
      <description>HBASE-4209 changed the behavior of the scripts such that we do not kill the daemons away anymore. We should have also changed the message shown in the logs.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5175" opendate="2012-1-10 00:00:00" fixdate="2012-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DoubleColumnInterpreter</summary>
      <description>DoubleColumnInterpreter was requested by Royston Sellman.</description>
      <version>None</version>
      <fixedVersion>0.98.1,0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="5186" opendate="2012-1-12 00:00:00" fixdate="2012-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics to ThriftServer</summary>
      <description>It will be useful to have some metrics (queue length, waiting time, processing time ...) similar to Hadoop RPC server. This allows us to monitor system health also provide a tool to diagnose the problem where thrift calls are slow.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5189" opendate="2012-1-13 00:00:00" fixdate="2012-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics to keep track of region-splits in RS</summary>
      <description>For write-heavy workload with region-size 1 GB, region-split is considerably high. We do normally grep the NN log (grep "mkdir*.split" NN.log | sort | uniq -c) to get the count.I would like to have a counter incremented each time region-split execution succeeds and this counter exposed via the metrics stuff in HBase. regionSplitSuccessCount regionSplitFailureCount (will help us to correlate the timestamp range in RS logs across all RS)</description>
      <version>0.90.5,0.92.0</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="5195" opendate="2012-1-13 00:00:00" fixdate="2012-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Coprocessors] preGet hook does not allow overriding or wrapping filter on incoming Get</summary>
      <description>Without the ability to wrap the internal Scan on the Get, we can't override (or protect, in the case of access control) Gets as we can Scans. The result is inconsistent behavior.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="5196" opendate="2012-1-13 00:00:00" fixdate="2012-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failure in region split after PONR could cause region hole</summary>
      <description>If region split fails after PONR, it relies on the master ServerShutdown handler to fix it. However, if the master doesn't get a chance to fix it. There will be a hole in the region chain.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.90.6,0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5213" opendate="2012-1-17 00:00:00" fixdate="2012-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"hbase master stop" does not bring down backup masters</summary>
      <description>Typing "hbase master stop" produces the following message:"stop Start cluster shutdown; Master signals RegionServer shutdown"It seems like backup masters should be considered part of the cluster, but they are not brought down by "hbase master stop"."stop-hbase.sh" does correctly bring down the backup masters.The same behavior is observed when a client app makes use of the client API HBaseAdmin.shutdown() http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#shutdown() &amp;#8211; this isn't too surprising since I think "hbase master stop" just calls this API.It seems like HBASE-1448 address this; perhaps there was a regression?</description>
      <version>0.90.5,0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestActiveMasterManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ActiveMasterManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="5228" opendate="2012-1-18 00:00:00" fixdate="2012-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[REST] Rip out "transform" feature</summary>
      <description>The 'transform' feature, where REST can be instructed, via a table attribute, to apply a transformation (e.g. base64 encoding or decoding) to a (sub)set of column values before serving them up to a client or storing them into HBase, was added some time ago at the request of Jack Levin. I have since come to regret it, it was not a well thought out feature: This is really an application concern. It adds significant overhead to request processing: Periodically a HBaseAdmin is used to retrieve the table descriptor, in order to scan through table attributes for transformation directives.I think it is best to rip it out, its a real problem area, and REST should be no more concerned about data formats than the Java API. I doubt anyone uses this, not even Jack. Will need to follow up with him to confirm.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.90.6,0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestTransform.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.transform.Transform.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.transform.NullTransform.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.transform.Base64.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.MultiRowResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="5256" opendate="2012-1-23 00:00:00" fixdate="2012-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use WritableUtils.readVInt() in RegionLoad.readFields()</summary>
      <description>Currently in.readInt() is used in RegionLoad.readFields()More metrics would be added to RegionLoad in the future, we should utilize WritableUtils.readVInt() to reduce the amount of data exchanged between Master and region servers.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
    </fixedFiles>
  </bug>
  <bug id="5259" opendate="2012-1-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize the RegionLocation in TableInputFormat by the reverse DNS lookup.</summary>
      <description>Assuming the HBase and MapReduce running in the same cluster, the TableInputFormat is to override the split function which divides all the regions from one particular table into a series of mapper tasks. So each mapper task can process a region or one part of a region. Ideally, the mapper task should run on the same machine on which the region server hosts the corresponding region. That's the motivation that the TableInputFormat sets the RegionLocation so that the MapReduce framework can respect the node locality. The code simply set the host name of the region server as the HRegionLocation. However, the host name of the region server may have different format with the host name of the task tracker (Mapper task). The task tracker always gets its hostname by the reverse DNS lookup. And the DNS service may return different host name format. For example, the host name of the region server is correctly set as a.b.c.d while the reverse DNS lookup may return a.b.c.d. (With an additional doc in the end).So the solution is to set the RegionLocation by the reverse DNS lookup as well. No matter what host name format the DNS system is using, the TableInputFormat has the responsibility to keep the consistent host name format with the MapReduce framework.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="5266" opendate="2012-1-24 00:00:00" fixdate="2012-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ColumnRangeFilter</summary>
      <description>There are only a few lines of documentation for ColumnRangeFilter.Given the usefulness of this filter for efficient intra-row scanning (see HBASE-5229 and HBASE-4256), we should make this filter more prominent in the documentation.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5267" opendate="2012-1-24 00:00:00" fixdate="2012-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a configuration to disable the slab cache by default</summary>
      <description>From what I commented at the tail of HBASE-4027:I changed the release note, the patch doesn't have a "hbase.offheapcachesize" configuration and it's enabled as soon as you set -XX:MaxDirectMemorySize (which is actually a big problem when you consider this: http://hbase.apache.org/book.html#trouble.client.oome.directmemory.leak). We need to add hbase.offheapcachesize and set it to false by default.Marking as a blocker for 0.92.1 and assigning to Li Pi at Todd's request.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">src.docbkx.upgrading.xml</file>
      <file type="M">conf.hbase-env.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5274" opendate="2012-1-24 00:00:00" fixdate="2012-1-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter out the expired store file scanner during the compaction</summary>
      <description>During the compaction time, HBase will generate a store scanner which will scan a list of store files. And it would be more efficient to filer out the expired store file since there is no need to read any key values from these store files.This optimization has been already implemented on 89-fb and this is the building block for HBASE-5199 as well. It is supposed to be no-ops to compact the expired store files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingTTL.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="5278" opendate="2012-1-25 00:00:00" fixdate="2012-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell script refers to removed "migrate" functionality</summary>
      <description>$ hbase migrateException in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/util/MigrateCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.util.Migrateat java.net.URLClassLoader$1.run(URLClassLoader.java:202)at java.security.AccessController.doPrivileged(Native Method)at java.net.URLClassLoader.findClass(URLClassLoader.java:190)at java.lang.ClassLoader.loadClass(ClassLoader.java:306)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)at java.lang.ClassLoader.loadClass(ClassLoader.java:247)Could not find the main class: org.apache.hadoop.hbase.util.Migrate. Program will exit.The 'hbase' shell script has docs referring to a 'migrate' command which no longer exists.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="5283" opendate="2012-1-26 00:00:00" fixdate="2012-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Request counters may become negative for heavily loaded regions</summary>
      <description>Requests counter showing negative count, example under 'Requests' column: -645470239Name Region Server Start Key End Key Requestsusertable,user2037516127892189021,1326756873774.16833e4566d1daef109b8fdcd1f4b5a6. xxx.com:60030 user2037516127892189021 user2296868939942738705 -645470239RegionLoad.readRequestsCount and RegionLoad.writeRequestsCount are of int type. Our Ops has been running lots of heavy load operation. RegionLoad.getRequestsCount() overflows int.MAX_VALUE. It is set to D986E7E1. In table.jsp, RegionLoad.getRequestsCount() is assigned to long type. D986E7E1 is converted to long FFFFFFFFD986E7E1 which is -645470239 in decimal.Suggested fix is to make readRequestsCount and writeRequestsCount long type.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
    </fixedFiles>
  </bug>
  <bug id="5294" opendate="2012-1-27 00:00:00" fixdate="2012-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure javadoc is included in tarball bundle when we release</summary>
      <description>0.92.0 doesn't have javadoc in the tarball. Fix.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5298" opendate="2012-1-30 00:00:00" fixdate="2012-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thrift metrics to thrift2</summary>
      <description>We have added thrift metrics collection in HBASE-5186.It will be good to have them in thrift2 as well.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestCallQueue.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="5345" opendate="2012-2-7 00:00:00" fixdate="2012-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CheckAndPut doesn&amp;#39;t work when value is empty byte[]</summary>
      <description>When a value contains an empty byte[] and then a checkAndPut is performed with an empty byte[] , the operation will fail.For example:Put put = new Put(row1);put.add(fam1, qf1, new byte&amp;#91;0&amp;#93;);table.put(put);put = new Put(row1);put.add(fam1, qf1, val1);table.checkAndPut(row1, fam1, qf1, new byte&amp;#91;0&amp;#93;, put); ---&gt; falseI think this is related to HBASE-3793 and HBASE-3468.Note that you will also get into this situation when first putting a null value ( put.add(fam1,qf1,null) ), as this value will then be regarded and returned as an empty byte[] upon a get.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5349" opendate="2012-2-7 00:00:00" fixdate="2012-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automagically tweak global memstore and block cache sizes based on workload</summary>
      <description>Hypertable does a neat thing where it changes the size given to the CellCache (our MemStores) and Block Cache based on the workload. If you need an image, scroll down at the bottom of this link: http://www.hypertable.com/documentation/architecture/That'd be one less thing to configure.</description>
      <version>0.92.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreChunkPool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.FlushRequester.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.DoubleBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="5350" opendate="2012-2-7 00:00:00" fixdate="2012-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix jamon generated package names</summary>
      <description>Previously, jamon was creating the template files in "org.apache.hbase", but it should be "org.apache.hadoop.hbase", so it's in line with rest of source files.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RSStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterStatusServlet.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.common.TaskMonitorTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="5364" opendate="2012-2-9 00:00:00" fixdate="2012-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix source files missing licenses in 0.92 and trunk</summary>
      <description>running 'mvn rat:check' shows that a few files have snuck in that do not have proper apache licenses. Ideally we should fix these before we cut another release/release candidate.This is a blocker for 0.94, and probably should be for the other branches as well.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.90.6,0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.constraint.RuntimeFailConstraint.java</file>
      <file type="M">src.packages.deb.conf-pseudo.control.control</file>
      <file type="M">src.main.python.hbase.merge.conf.py</file>
      <file type="M">dev-support.findHangingTest.sh</file>
      <file type="M">bin.hbase-jruby</file>
    </fixedFiles>
  </bug>
  <bug id="5421" opendate="2012-2-17 00:00:00" fixdate="2012-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>use hadoop-client/hadoop-minicluster artifacts for Hadoop 0.23 build</summary>
      <description>Hadoop recently added hadoop-client and hadoop-minicluster artifacts for Hadoop 0.23+ that don't export all the internal dependencies (HADOOP-8009).Let's use them instead of manually specifying transitive dependency exclusion lists (which is error prone and annoying).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5423" opendate="2012-2-17 00:00:00" fixdate="2012-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver may block forever on waitOnAllRegionsToClose when aborting</summary>
      <description>If closeRegion throws any exception (It would be caused by FS ) when RS is aborting, RS will block forever on waitOnAllRegionsToClose().</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5427" opendate="2012-2-17 00:00:00" fixdate="2012-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade our zk to 3.4.3</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5428" opendate="2012-2-17 00:00:00" fixdate="2012-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for custom filters to be registered within the Thrift interface</summary>
      <description>Custom filters work within the Java client API, but are not accessible within the Thrift API. Attempting to use one will generate a "Filter Name x not supported"Attached patch allows a user to specify a list of custom filters that are registered at Thrift server startup time within the HBase configuration files:&lt;property&gt; &lt;name&gt;hbase.thrift.filters&lt;/name&gt; &lt;value&gt;MyFilter:com.foo.Filter,OtherFilter:com.foo.OtherFilter&lt;/value&gt;&lt;/property&gt;Patch created off SVN r1245727</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.filter.TestParseFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ParseFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="5436" opendate="2012-2-20 00:00:00" fixdate="2012-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Right-size the map when reading attributes.</summary>
      <description></description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.OperationWithAttributes.java</file>
    </fixedFiles>
  </bug>
  <bug id="5464" opendate="2012-2-23 00:00:00" fixdate="2012-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log warning message when thrift calls throw exceptions</summary>
      <description>Currently there is no logging message when client calls throw exceptions. It will be easier to debug if we have them.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5466" opendate="2012-2-23 00:00:00" fixdate="2012-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Opening a table also opens the metatable and never closes it.</summary>
      <description>Having upgraded to CDH3U3 version of hbase we found we had a zookeeper connection leak, tracking it down we found that closing the connection will only close the zookeeper connection if all calls to get the connection have been closed, there is incCount and decCount in the HConnection class,When a table is opened it makes a call to the metascanner class which opens a connection to the meta table, this table never gets closed.This caused the count in the HConnection class to never return to zero meaning that the zookeeper connection will not close when we close all the tables or callingHConnectionManager.deleteConnection(config, true);</description>
      <version>0.90.5,0.92.0</version>
      <fixedVersion>0.90.7,0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5483" opendate="2012-2-27 00:00:00" fixdate="2012-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow configurable host to bind to for starting REST server from commandline</summary>
      <description>Current implementation binds to all interfaces, use config to store hbase.rest.host and hbase.rest.port</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
    </fixedFiles>
  </bug>
  <bug id="5497" opendate="2012-2-29 00:00:00" fixdate="2012-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add protobuf as M/R dependency jar (mapred)</summary>
      <description>Getting this from M/R jobs (Export for example):Error: java.lang.ClassNotFoundException: com.google.protobuf.Message at java.net.URLClassLoader$1.run(URLClassLoader.java:217) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:205) at java.lang.ClassLoader.loadClass(ClassLoader.java:321) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294) at java.lang.ClassLoader.loadClass(ClassLoader.java:266) at org.apache.hadoop.hbase.io.HbaseObjectWritable.&lt;clinit&gt;(HbaseObjectWritable.java:262)</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="5498" opendate="2012-3-1 00:00:00" fixdate="2012-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure Bulk Load</summary>
      <description>Design doc: https://cwiki.apache.org/confluence/display/HCATALOG/HBase+Secure+Bulk+LoadShort summary:Security as it stands does not cover the bulkLoadHFiles() feature. Users calling this method will bypass ACLs. Also loading is made more cumbersome in a secure setting because of hdfs privileges. bulkLoadHFiles() moves the data from user's directory to the hbase directory, which would require certain write access privileges set.Our solution is to create a coprocessor which makes use of AuthManager to verify if a user has write access to the table. If so, launches a MR job as the hbase user to do the importing (ie rewrite from text to hfiles). One tricky part this job will have to do is impersonate the calling user when reading the input files. We can do this by expecting the user to pass an hdfs delegation token as part of the secureBulkLoad() coprocessor call and extend an inputformat to make use of that token. The output is written to a temporary directory accessible only by hbase and then bulkloadHFiles() is called.</description>
      <version>None</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug id="5508" opendate="2012-3-2 00:00:00" fixdate="2012-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to allow test output to show on the terminal</summary>
      <description>Sometimes it is useful to directly see the test results on the terminal.We can add a property to achieve that.mvn test -Dtest.output.tofile=false</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5511" opendate="2012-3-2 00:00:00" fixdate="2012-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More doc on maven release process</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5519" opendate="2012-3-5 00:00:00" fixdate="2012-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect warning in splitlogmanager</summary>
      <description>because of recently added behavior - where the splitlogmanager timeout thread get's data from zk node just to check that the zk node is there ... we might have multiple watches firing without the task znode expiring.remove the poor warning message. (internally, there was an assert that failed in Mikhail's tests)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="5521" opendate="2012-3-5 00:00:00" fixdate="2012-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move compression/decompression to an encoder specific encoding context</summary>
      <description>As part of working on HBASE-5313, we want to add a new columnar encoder/decoder. It makes sense to move compression to be part of encoder/decoder:1) a scanner for a columnar encoded block can do lazy decompression to a specific part of a key value object2) avoid an extra bytes copy from encoder to hblock-writer. If there is no encoder specified for a writer, the HBlock.Writer will use a default compression-context to do something very similar to today's code.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileDataBlockEncoder.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockCompatibility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.encoding.TestDataBlockEncoders.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoderImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.Compression.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.PrefixKeyDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DiffKeyDeltaEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.CopyKeyDataBlockEncoder.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="5522" opendate="2012-3-5 00:00:00" fixdate="2012-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase 0.92 test artifacts are missing from Maven central</summary>
      <description>Could someone with enough karma, please, publish the test artifacts for 0.92.0?</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5523" opendate="2012-3-5 00:00:00" fixdate="2012-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Delete Timerange logic for KEEP_DELETED_CELLS</summary>
      <description>A Delete at time T marks a Put at time T as deleted.In parent I invented special logic that insert a virtual millisecond into the tr if the encountered KV is a delete marker.This was so that there is a way to specify a timerange that would allow to see the put but not the delete:if (kv.isDelete()) { if (!keepDeletedCells) { // first ignore delete markers if the scanner can do so, and the // range does not include the marker boolean includeDeleteMarker = seePastDeleteMarkers ? // +1, to allow a range between a delete and put of same TS tr.withinTimeRange(timestamp+1) : tr.withinOrAfterTimeRange(timestamp);Discussed this today with a coworker and he convinced me that this is very confusing and also not needed.When we have a Delete and Put at the same time T, there is not timerange that can include the Put but not the Delete.So I will change the code to this (and fix the tests):if (kv.isDelete()) { if (!keepDeletedCells) { // first ignore delete markers if the scanner can do so, and the // range does not include the marker boolean includeDeleteMarker = seePastDeleteMarkers ? tr.withinTimeRange(timestamp) : tr.withinOrAfterTimeRange(timestamp);It's easier to understand, and does not lead to strange scenarios when the TS is used as a controlled counter.Needs to be done before 0.94 goes out.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestKeepDeletes.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="5524" opendate="2012-3-5 00:00:00" fixdate="2012-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a couple of more filters to our rat exclusion set</summary>
      <description>Build up on jenkins is failing because I just enabled the rat/license check as part of our build. We're failing because CP is writing test data into top-level at ./test.</description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5529" opendate="2012-3-6 00:00:00" fixdate="2012-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR test failures becuase MALLOC_ARENA_MAX is not set</summary>
      <description>When running unit tests on CentOS 6 I get a bunch of unit test failures in mapreduce-related tests due to:2012-03-03 00:14:18,776 WARN &amp;#91;Container Monitor&amp;#93;monitor.ContainersMonitorImpl$MonitoringThread(436): Container&amp;#91;pid=21446,containerID=container_1330762435849_0002_01_000001&amp;#93; isrunning beyond virtual memory limits. Current usage: 223.1mb of 2.0gbphysical memory used; 6.9gb of 4.2gb virtual memory used. Killingcontainer.Note: this also came up in the mapreduce project. See: https://issues.apache.org/jira/browse/MAPREDUCE-3933Patch coming shortly</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5547" opendate="2012-3-9 00:00:00" fixdate="2012-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t delete HFiles when in "backup mode"</summary>
      <description>This came up in a discussion I had with Stack.It would be nice if HBase could be notified that a backup is in progress (via a znode for example) and in that case either:1. rename HFiles to be delete to &lt;file&gt;.bck2. rename the HFiles into a special directory3. rename them to a general trash directory (which would not need to be tied to backup mode).That way it should be able to get a consistent backup based on HFiles (HDFS snapshots or hard links would be better options here, but we do not have those).#1 makes cleanup a bit harder.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestLogsCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TimeToLiveLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.LogCleanerDelegate.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.LogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.example.TestZooKeeperTableArchiveClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.example.TableHFileArchiveTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="5552" opendate="2012-3-9 00:00:00" fixdate="2012-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up our jmx view; its a bit of a mess</summary>
      <description>Fix before we release 0.92.1</description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.HBaseInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCStatistics.java</file>
    </fixedFiles>
  </bug>
  <bug id="5555" opendate="2012-3-9 00:00:00" fixdate="2012-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a pointer to a dns verification utility in hbase book/dns</summary>
      <description>DNS should work correctly in a Hbase cluster. I have a simple DNS checker utility, that verifies DNS on all machines of the cluster. https://github.com/sujee/hadoop-dns-checkeradd a pointer to the tool in hbase book : http://hbase.apache.org/book.html#dns</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5589" opendate="2012-3-15 00:00:00" fixdate="2012-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add of the offline call to the Master Interface</summary>
      <description>Hbck from HBASE-5128 requires an offline method on the master to properly cleanup state during certain assignment repair operations. This will this method will be added to recent and older versions of HBase.</description>
      <version>0.90.6,0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
    </fixedFiles>
  </bug>
  <bug id="559" opendate="2008-4-3 00:00:00" fixdate="2008-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR example job to count table rows</summary>
      <description>The Lars' import is a little messy; he's not sure how many records were imported. Running a select takes a couple of hours. He happens to have an idle MR cluster standing by. An example MR job that just did a count of records would be generally useful. Could even output row keys so you'd have a list of what made it in. Later, if this tool becomes popular with derivatives and similiars, we can bundle a jar of MR jobs to run against your tables that can answer common queries and that are amenable to subclassing/modification.</description>
      <version>None</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5591" opendate="2012-3-16 00:00:00" fixdate="2012-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="5604" opendate="2012-3-20 00:00:00" fixdate="2012-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>M/R tool to replay WAL files</summary>
      <description>Just an idea I had. Might be useful for restore of a backup using the HLogs.This could an M/R (with a mapper per HLog file).The tool would get a timerange and a (set of) table(s). We'd pick the right HLogs based on time before the M/R job is started and then have a mapper per HLog file.The mapper would then go through the HLog, filter all WALEdits that didn't fit into the time range or are not any of the tables and then uses HFileOutputFormat to generate HFiles.Would need to indicate the splits we want, probably from a live table.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="5897" opendate="2012-4-29 00:00:00" fixdate="2012-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>prePut coprocessor hook causing substantial CPU usage</summary>
      <description>I was running an insert workload against trunk under oprofile and saw that a significant portion of CPU usage was going to calling the "prePut" coprocessor hook inside doMiniBatchPut, even though I don't have any coprocessors installed. I ran a million-row insert and collected CPU time spent in the RS after commenting out the preput hook, and found CPU usage reduced by 33%.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.2,0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="600" opendate="2008-4-23 00:00:00" fixdate="2008-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filters have excessive DEBUG logging</summary>
      <description>Downgrade most to trace</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.WhileMatchRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.StopRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RowFilterSet.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.PageRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.InclusiveStopRowFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6265" opendate="2012-6-25 00:00:00" fixdate="2012-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling getTimestamp() on a KV in cp.prePut() causes KV not to be flushed</summary>
      <description>There is an issue when you call getTimestamp() on any KV handed into a Coprocessor's prePut(). It initializes the internal "timestampCache" variable. When you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). The TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call.</description>
      <version>0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="6314" opendate="2012-7-3 00:00:00" fixdate="2012-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fast fail behavior for unauthenticated user</summary>
      <description>In case of unauthenticated users in secure hbase, hbase shell does a connection retry at two levels:a) HConnection: It retries hbase.client.retries.number times in the getMaster()b) HBaseAdmin: it again retries hbase.client.retries.number times in its ctrSo, hbase shell retries square number of times of the configured setting. We can make it failfast (no retries) in case the user is not authenticated (no valid kerberos credentials).</description>
      <version>0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="6341" opendate="2012-7-6 00:00:00" fixdate="2012-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publicly expose HConnectionKey</summary>
      <description>HBASE-3777 introduced the concept of a HConnectionKey to quickly identify and compare Configuration instances which lack equals/hashCode values.We currently have use cases where being able to key Configuration objects in a similar way would be helpful. An example of this would be maintain a cache of HTablePool instances based on the HConnectionKey instead of the Configuration instance.I propose that HConnectionKey be made publicly available instead of its current package scope. Or another possibility would be to move it from being a static inner class to being part of the API.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.2,0.94.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="6471" opendate="2012-7-29 00:00:00" fixdate="2012-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression caused by HBASE-4054</summary>
      <description>The patch in HBASE-4054 switches the PooledHTable to extend HTable as opposed to implement HTableInterface.Since HTable does not have an empty constructor, the patch added a call to the super() constructor, which though does trigger the ZooKeeper and META scan, causing a considerable delay. With multiple threads using the pool in parallel, the first thread is holding up all the subsequent ones, in effect it negates the whole reason we have a HTable pool.We should complete HBASE-5728, or alternatively add a protected, empty constructor the HTable. I am +1 for the former.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTablePool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RegionsResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
    </fixedFiles>
  </bug>
  <bug id="670" opendate="2008-6-6 00:00:00" fixdate="2008-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Historian deadlocks if regionserver is at global memory boundary and is hosting .META.</summary>
      <description>The global memory unit test was deadlocking because historian was trying to update .META. with flush info &amp;#8211; only the single regionserver was the one hosting the .META. and the regionserver global lock was in place while memory is full</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Flusher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.RegionHistorian.java</file>
    </fixedFiles>
  </bug>
  <bug id="6716" opendate="2012-9-4 00:00:00" fixdate="2012-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopqa is hosed</summary>
      <description>See this thread on list: http://search-hadoop.com/m/PtDLC19vEd62/%2522Looks+like+HadoopQA+is+hosed%2522&amp;subj=Looks+like+HadoopQA+is+hosed+Lots of the hadoopqa builds are failing complaining about missing dir.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="6856" opendate="2012-9-21 00:00:00" fixdate="2012-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the LeaseException thrown in scanner next</summary>
      <description>In some situations clients that fetch data from a RS get a LeaseException instead of the usual ScannerTimeoutException/UnknownScannerException.This particular case should be documented in the HBase guide.Some key points the source of exception is: org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:230) it happens in the context of a slow/freezing RS#next it can be prevented by having hbase.rpc.timeout &gt; hbase.regionserver.lease.periodHarsh J investigated the issue and has some conclusions, seehttp://mail-archives.apache.org/mod_mbox/hbase-user/201209.mbox/%3CCAOcnVr3R-LqtKhFsk8Bhrm-YW2i9O6J6Fhjz2h7q6_sxvwd2yw%40mail.gmail.com%3E</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6858" opendate="2012-9-21 00:00:00" fixdate="2012-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the incorrect BADVERSION checking in the recoverable zookeeper</summary>
      <description>Thanks for Stack and Kaka's reporting that there is a bug in the recoverable zookeeper when handling BADVERSION exception for setData(). It shall compare the ID payload of the data in zk with its own identifier.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
    </fixedFiles>
  </bug>
  <bug id="7060" opendate="2012-10-29 00:00:00" fixdate="2012-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region load balancing by table does not handle the case where a table&amp;#39;s region count is lower than the number of the RS in the cluster</summary>
      <description>When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:avg-regions-per-RS=0.5min-RS-per-RS=0max-RS-per-RS=1when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always &gt;=min, so it will always skip, therefore, no underloaded RS are found.Map&lt;ServerName, Integer&gt; underloadedServers = new HashMap&lt;ServerName, Integer&gt;();for (Map.Entry&lt;ServerAndLoad, List&lt;HRegionInfo&gt;&gt; server:serversByLoad.entrySet()) {int regionCount = server.getKey().getLoad();if (regionCount &gt;= min) { break; }underloadedServers.put(server.getKey().getServerName(), min - regionCount);}Later the function returns since underloaded RS size is 0if (serverUnerloaded ==0) return regionsToReturn;</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.DefaultLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7129" opendate="2012-11-8 00:00:00" fixdate="2012-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need documentation for REST atomic operations (HBASE-4720)</summary>
      <description>HBASE-4720 added checkAndPut/checkAndDelete capability to the REST interface, but the REST documentation (in the package summary) needs to be updated so people know that this feature exists and how to use it.http://wiki.apache.org/hadoop/Hbase/Stargatehttp://hbase.apache.org/book/rest.html</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="7452" opendate="2012-12-28 00:00:00" fixdate="2012-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change ForeignException#receive(String, FE) to only be #receive(FE)</summary>
      <description>This was suggested but not completely finished before HBASE-7206 got committed. This finishes the job.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.errorhandling.TestTimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.errorhandling.TestForeignExceptionDispatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="7453" opendate="2012-12-28 00:00:00" fixdate="2012-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-7423 snapshot followup</summary>
      <description>HBASE-7423 change the arguments for one method used by restore code</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ModifyRegionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
