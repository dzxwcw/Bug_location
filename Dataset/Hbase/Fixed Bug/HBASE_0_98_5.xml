<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="10786" opendate="2014-3-18 00:00:00" fixdate="2014-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If snapshot verification fails with &amp;#39;Regions moved&amp;#39;, the message should contain the name of region causing the failure</summary>
      <description>I was trying to find cause for test failure in https://builds.apache.org/job/PreCommit-HBASE-Build/9036//testReport/org.apache.hadoop.hbase.snapshot/TestSecureExportSnapshot/testExportRetry/ :org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: org.apache.hadoop.hbase.snapshot.HBaseSnapshotException: Snapshot { ss=emptySnaptb0-1395177346656 table=testtb-1395177346656 type=FLUSH } had an error. Procedure emptySnaptb0-1395177346656 { waiting=[] done=[] } at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:342) at org.apache.hadoop.hbase.master.HMaster.isSnapshotDone(HMaster.java:3007) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:40494) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2020) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:98) at org.apache.hadoop.hbase.ipc.FifoRpcScheduler$1.run(FifoRpcScheduler.java:73) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)Caused by: org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException via Failed taking snapshot { ss=emptySnaptb0-1395177346656 table=testtb-1395177346656 type=FLUSH } due to exception:Regions moved during the snapshot '{ ss=emptySnaptb0-1395177346656 table=testtb-1395177346656 type=FLUSH }'. expected=9 snapshotted=8:org.apache.hadoop.hbase.snapshot.CorruptedSnapshotException: Regions moved during the snapshot '{ ss=emptySnaptb0-1395177346656 table=testtb-1395177346656 type=FLUSH }'. expected=9 snapshotted=8 at org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.rethrowException(ForeignExceptionDispatcher.java:83) at org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.rethrowExceptionIfFailed(TakeSnapshotHandler.java:320) at org.apache.hadoop.hbase.master.snapshot.SnapshotManager.isSnapshotDone(SnapshotManager.java:332) ... 11 moreHowever, it is not clear which region caused the verification to fail.I searched for log from balancer but found none.The exception message should include region name which caused the verification to fail.</description>
      <version>None</version>
      <fixedVersion>0.98.1,0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.java</file>
    </fixedFiles>
  </bug>
  <bug id="11726" opendate="2014-8-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master should fail-safe if starting with a pre 0.96 layout</summary>
      <description>We recently saw this: If user inadvertently starts the HBase Master after deploying new HBase binaries (any version that supports namespaces), the HMaster will start the migration to PBs the the hbase.version file per HBASE-5453 and that will write a new version file PB-serialized but with the old version number. Further restarts of the master will fail because the hbase version file has been migrated to PBs and there will be version mismatch. The right approach should be to fail safe the master if we find an old hbase.version file in order to force user to run upgrade tool.</description>
      <version>0.96.2,0.99.0,0.98.5,2.0.0</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="11727" opendate="2014-8-12 00:00:00" fixdate="2014-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assignment wait time error in case of ServerNotRunningYetException</summary>
      <description>maxWaitTime = this.server.getConfiguration(). getLong("hbase.regionserver.rpc.startup.waittime", 60000);It should add the current time.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="11735" opendate="2014-8-14 00:00:00" fixdate="2014-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Configurable Bucket Sizes in bucketCache</summary>
      <description></description>
      <version>0.99.0,0.98.4,0.98.5</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11736" opendate="2014-8-14 00:00:00" fixdate="2014-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document SKIP_FLUSH snapshot option</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11737" opendate="2014-8-14 00:00:00" fixdate="2014-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document callQueue improvements from HBASE-11355 and HBASE-11724</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.performance.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11738" opendate="2014-8-14 00:00:00" fixdate="2014-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document improvements to LoadTestTool and PerformanceEvaluation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11739" opendate="2014-8-14 00:00:00" fixdate="2014-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document blockCache contents report in the UI</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11757" opendate="2014-8-15 00:00:00" fixdate="2014-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a common base abstract class for both RegionObserver and MasterObserver</summary>
      <description>Some security coprocessors extend both RegionObserver and MasterObserver, unfortunately only one of the two can use the available base abstract class implementations. Provide a common base abstract class for both the RegionObserver and MasterObserver interfaces. Update current coprocessors that extend both interfaces to use the new common base abstract class.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug id="11774" opendate="2014-8-19 00:00:00" fixdate="2014-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid allocating unnecessary tag iterators</summary>
      <description>We can avoid an unnecessary object allocation, sometimes in hot code paths, by not creating a tag iterator if the cell's tag area is of length zero, signifying no tags present.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityLabelFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug id="11788" opendate="2014-8-20 00:00:00" fixdate="2014-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase is not deleting the cell when a Put with a KeyValue, KeyValue.Type.Delete is submitted</summary>
      <description>Code executed: @Test public void testHbasePutDeleteCell() throws Exception { TableName tableName = TableName.valueOf("my_test"); Configuration configuration = HBaseConfiguration.create(); HTableInterface table = new HTable(configuration, tableName); final String rowKey = "12345"; final byte[] familly = Bytes.toBytes("default"); // put one row Put put = new Put(Bytes.toBytes(rowKey)); put.add(familly, Bytes.toBytes("A"), Bytes.toBytes("a")); put.add(familly, Bytes.toBytes("B"), Bytes.toBytes("b")); put.add(familly, Bytes.toBytes("C"), Bytes.toBytes("c")); table.put(put); // get row back and assert the values Get get = new Get(Bytes.toBytes(rowKey)); Result result = table.get(get); Assert.isTrue(Bytes.toString(result.getValue(familly, Bytes.toBytes("A"))).equals("a"), "Column A value should be a"); Assert.isTrue(Bytes.toString(result.getValue(familly, Bytes.toBytes("B"))).equals("b"), "Column B value should be b"); Assert.isTrue(Bytes.toString(result.getValue(familly, Bytes.toBytes("C"))).equals("c"), "Column C value should be c"); // put the same row again with C column deleted put = new Put(Bytes.toBytes(rowKey)); put.add(familly, Bytes.toBytes("A"), Bytes.toBytes("a")); put.add(familly, Bytes.toBytes("B"), Bytes.toBytes("b")); put.add(new KeyValue(Bytes.toBytes(rowKey), familly, Bytes.toBytes("C"), HConstants.LATEST_TIMESTAMP, KeyValue.Type.DeleteColumn)); table.put(put); // get row back and assert the values get = new Get(Bytes.toBytes(rowKey)); result = table.get(get); Assert.isTrue(Bytes.toString(result.getValue(familly, Bytes.toBytes("A"))).equals("a"), "Column A value should be a"); Assert.isTrue(Bytes.toString(result.getValue(familly, Bytes.toBytes("B"))).equals("b"), "Column A value should be b"); Assert.isTrue(result.getValue(familly, Bytes.toBytes("C")) == null, "Column C should not exists"); }This assertion fails, the cell is not deleted but rather the value is empty:hbase(main):029:0&gt; scan 'my_test'ROW COLUMN+CELL 12345 column=default:A, timestamp=1408473082290, value=a 12345 column=default:B, timestamp=1408473082290, value=b 12345 column=default:C, timestamp=1408473082290, value= This behavior is different than previous 4.8.x Cloudera version and is currently corrupting all hive queries involving is null or is not null operators on the columns mapped to hbase</description>
      <version>0.99.0,0.96.1.1,0.98.5,2.0.0</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="11796" opendate="2014-8-21 00:00:00" fixdate="2014-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add client support for atomic checkAndMutate</summary>
      <description>Currently HBase has support for atomic checkAndPut as well as checkAndDelete operations on individual rows. HBase also supports the atomic submission of multiple operations through mutateRow. It would be nice to support atomic checkAndMutate(RowMutations) as well.</description>
      <version>None</version>
      <fixedVersion>0.98.7,0.99.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.HTablePool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTableWrapper.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Table.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="11814" opendate="2014-8-24 00:00:00" fixdate="2014-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestAssignmentManager.testCloseFailed() and testOpenCloseRacing() is flaky</summary>
      <description>Andrew found this at https://issues.apache.org/jira/browse/HBASE-11546?focusedCommentId=14108237&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14108237That was for 0.98 but trunk has to fix testCloseFailed(). Need to do the same as in HBASE-11615</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="11815" opendate="2014-8-25 00:00:00" fixdate="2014-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flush and compaction could just close the tmp writer if there is an exception</summary>
      <description>A minor change. try { flushed = performFlush(scanner, writer, smallestReadPoint); } finally { finalizeWriter(writer, cacheFlushId, status); }Whenever there is a failure during flush we should close the writer but adding the meta data and setting the status would not be needed. status.setStatus("Flushing " + store + ": appending metadata"); writer.appendMetadata(cacheFlushSeqNum, false); status.setStatus("Flushing " + store + ": closing flushed file"); writer.close();</description>
      <version>None</version>
      <fixedVersion>0.98.7,0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="11869" opendate="2014-9-1 00:00:00" fixdate="2014-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support snapshot owner</summary>
      <description>In current codebase, the table snapshot operations only can be done by the global admin , not by the table admin.There is a multi-tenant hbase cluster, each table has different snapshot policies, eg: do snapshot per week, or snapshot after the new data are imported. We want to release the snapshot permission to each table admin.According to mbertozzi's suggestion, we implement the snapshot owner feature. The user with table admin permission can create snapshot and the owner of this snapshot is this user. The owner of snapshot can delete and restore the snapshot. Only the user with global admin permission can clone a snapshot, for this operation creates a new table.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="12349" opendate="2014-10-27 00:00:00" fixdate="2014-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Maven build support module for a custom version of error-prone</summary>
      <description>Add a new Maven build support module that builds and publishes a custom error-prone artifact for use by the rest of the build.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-spark-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-resource-bundle.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-archetypes.pom.xml</file>
      <file type="M">hbase-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
