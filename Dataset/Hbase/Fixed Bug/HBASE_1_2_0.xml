<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="13515" opendate="2015-4-20 00:00:00" fixdate="2015-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle FileNotFoundException in region replica replay for flush/compaction events</summary>
      <description>I had this patch laying around that somehow dropped from my plate. We should skip replaying compaction / flush and region open event markers if the files (from flush or compaction) can no longer be found from the secondary. If we do not skip, the replay will be retried forever, effectively blocking the replication further. Bulk load already does this, we just need to do it for flush / compaction and region open events as well.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionReplayEvents.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="13516" opendate="2015-4-20 00:00:00" fixdate="2015-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase PermSize to 128MB</summary>
      <description>HBase uses ~40MB, and with Phoenix we use ~56MB of Perm space out of 64MB by default. Every Filter and Coprocessor increases that.Running out of perm space triggers a stop the world full GC of the entire heap. We have seen this in misconfigured cluster. Should we default to -XX:PermSize=128m -XX:MaxPermSize=128m out of the box as a convenience for users?</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.hbase-env.cmd</file>
    </fixedFiles>
  </bug>
  <bug id="13527" opendate="2015-4-22 00:00:00" fixdate="2015-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The default value for hbase.client.scanner.max.result.size is never actually set on Scans</summary>
      <description>Now that max result size is driven from the client side like caching (HBASE-13362), we also need to set Scan.maxResultSize to the default value of hbase.client.scanner.max.result.size which is never performed. I think this has gone unnoticed because the server used to read the configuration hbase.client.scanner.max.result.size for itself, but now we expect the serialized Scan sent from the client side to contain this information. Realistically this should have been set on the Scans even before HBASE-13362, it's surprising that it's not as the scanner code seems to indicate otherwise.Ultimately, the end result is that, by default, scan RPC's are limited by hbase.server.scanner.max.result.size (note this is the new server side config not the client side config) which has a default value of 100 MB. The scan RPC's should instead be limited by hbase.client.scanner.max.result.size which has a default value of 2 MB.The reason why this issue occurs is because, by default, a new Scan() initializes Scan.maxResultSize to -1. This initial value of -1 will never be changed unless Scan#setMaxResultSize() is called. In the event that this value is not changed, the Scan that is serialized and sent to the server will also have Scan.maxResultSize = -1. Then, when the server is deciding what size limit should be enforced, it sees that Scan.maxResultSize = -1 so it uses the most relaxed size restriction possible, which is hbase.server.scanner.max.result.size (default value 100 MB).</description>
      <version>1.0.0,1.1.0,0.98.12,1.2.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.12,1.0.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.TableConfiguration.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13554" opendate="2015-4-24 00:00:00" fixdate="2015-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book clarifying API stability guarantees</summary>
      <description>From the "Clarifying interface evolution freedom in patch releases" thread on dev@h.a.oSeems we have consensus that "HBase uses Semantic Versioning" isn't quite correct (or desired) at the moment. Update the documentation to make sure we're not misrepresenting any guarantees to users.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13560" opendate="2015-4-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Large compaction queue should steal from small compaction queue when idle</summary>
      <description>If you tune compaction threads so that a server is never over commited when large and small compaction threads are busy then it should be possible to have the large compactions steal work.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="13586" opendate="2015-4-28 00:00:00" fixdate="2015-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book on Hadoop and Java supported versions for 1.1.x</summary>
      <description>Should update http://hbase.apache.org/book.html#basic.prerequisites with the latest info.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13614" opendate="2015-5-4 00:00:00" fixdate="2015-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid temp KeyOnlyKeyValue temp objects creations in read hot path</summary>
      <description>As part of HBASE-10800, move to new CellComparator, we are temp creating a Cell out of byte[]s so that the Comparator can compare. In read hot path, we can try minimize the object creations. The parent Jira added some such cases, which we can solve. This Jira will solve all such cases.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="13618" opendate="2015-5-5 00:00:00" fixdate="2015-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ReplicationSource is too eager to remove sinks</summary>
      <description>Looking at the replication for some other reason I noticed that the replication source might be a bit too eager to remove sinks from the list of valid sinks.The current logic allows a sink to fail N times (default 3) and then it will be remove from the sinks. But note that this failure count is never reduced, so given enough runtime and some network glitches every sink will eventually be removed. When all sink are removed the source pick new sinks and the counter is set to 0 for all of them.I think we should change to reset the counter each time we successfully replicate something to the sink (which proves the sink isn't dead). Or we could decrease the counter each time we successfully replication, that might be better - if we consistently fail more attempts than we succeed the sink should be removed.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSinkManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HBaseInterClusterReplicationEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="13647" opendate="2015-5-8 00:00:00" fixdate="2015-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default value for hbase.client.operation.timeout is too high</summary>
      <description>Default value for hbase.client.operation.timeout is too high, it is LONG.Max.That value will block any service calls to coprocessor endpoints indefinitely.Should we introduce better default value for that?</description>
      <version>1.0.1,0.98.13,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="13694" opendate="2015-5-15 00:00:00" fixdate="2015-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallQueueSize is incorrectly decremented until the response is sent</summary>
      <description>We should decrement the CallQueueSize as soon as we no longer need the call around, e.g. after RpcServer.CurCall.set(null) otherwise we will be only pushing back other client requests while we send the response back to the client that originated the call.</description>
      <version>1.1.0,0.98.12,1.0.2,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="13755" opendate="2015-5-23 00:00:00" fixdate="2015-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide single super user check implementation</summary>
      <description>Followup for HBASE-13375.</description>
      <version>None</version>
      <fixedVersion>1.1.0,0.98.14,1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.ExpAsStringVisibilityLabelServiceImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQosFunction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13759" opendate="2015-5-24 00:00:00" fixdate="2015-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve procedure yielding</summary>
      <description>Adds the ability to yield the procedure every execution step.by default, a procedure will try to go begin to end without stopping.This allows procedures to be nice to other procedures.one usage example is ServerShutdownHandler where we want everyone to make some progress.Allows procedure to throw InterruptedException, the default handling will be:"ask the master if there is an abort of stop. If there is, stop executions and exit. Else, clear the IE and carryon executing. the interruted procedure will retry".If the procedure implementor wants a different behavior, the IE can be catched and custom handling can be performed.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.SequentialProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.Procedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="13760" opendate="2015-5-24 00:00:00" fixdate="2015-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup Findbugs keySet iterator warnings</summary>
      <description>Cleanup Findbugs keySet iterator warnings</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="13767" opendate="2015-5-25 00:00:00" fixdate="2015-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ZKAclReset to set and not just clear ZK ACLs</summary>
      <description>The ZKAclReset tool allows to clear ZK ACLs, which is useful if you are migrating from a secure to unsecure cluster setup.If you want to make sure that your znode ACLs are correct, a -set-acls option, which allows to enforce the proper ACLs on the znodes in a secure setup, can be useful too.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZkAclReset.java</file>
    </fixedFiles>
  </bug>
  <bug id="13768" opendate="2015-5-25 00:00:00" fixdate="2015-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znodes are bootstrapped with insecure ACLs in a secure configuration</summary>
      <description>A logic error causes HBase in most secure configuration deployments to handle its coordination state in ZooKeeper via insecure ACLs. Anyone with remote unauthenticated network access to the ZooKeeper quorum, which by definition includes all HBase clients, can make use of this opening to violate the operational integrity of the system. For example, critical znodes can be deleted, causing outages. It is possible to introduce rogue replication endpoints. It is possible to direct the distributed log splitting facility to split arbitrary files in HDFS.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,0.98.12.1,1.0.1.1,1.1.0.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
    </fixedFiles>
  </bug>
  <bug id="1377" opendate="2009-5-6 00:00:00" fixdate="2009-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS address is null in master web UI</summary>
      <description>My patch in HBASE-1279 was targeted for branch 0.19 and was missing a line to make it work for trunk in the copy constructor of HSI.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13770" opendate="2015-5-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Programmatic JAAS configuration option for secure zookeeper may be broken</summary>
      <description>While verifying the patch fix for HBASE-13768 we were unable to successfully test the programmatic JAAS configuration option for secure ZooKeeper integration. Unclear if that was due to a bug or incorrect test configuration.Update the security section of the online book with clear instructions for setting up the programmatic JAAS configuration option for secure ZooKeeper integration.Verify it works.Fix as necessary.</description>
      <version>1.0.1,1.1.0,0.98.13,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
    </fixedFiles>
  </bug>
  <bug id="13776" opendate="2015-5-26 00:00:00" fixdate="2015-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting illegal versions for HColumnDescriptor does not throw IllegalArgumentException</summary>
      <description>HColumnDescriptor hcd = new HColumnDescriptor( new HColumnDescriptor(HConstants.CATALOG_FAMILY) .setInMemory(true) .setScope(HConstants.REPLICATION_SCOPE_LOCAL) .setBloomFilterType(BloomType.NONE) .setCacheDataInL1(true)); final int minVersions = 123; final int maxVersions = 234; hcd.setMaxVersions(minVersions); hcd.setMinVersions(maxVersions);//no exception throw</description>
      <version>0.98.14,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="13780" opendate="2015-5-26 00:00:00" fixdate="2015-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default to 700 for HDFS root dir permissions for secure deployments</summary>
      <description>Secure mode deployments should protect the files under HDFS root dir. We should check and set the root dirs permissions on a kerberos setup so that users does not have to. We have hbase.data.umask.enable and hbase.data.umask for data files, but those are not that useful since we should protect dir listing, and access to WAL files, snapshot files, etc. See HBASE-13768 which has an integration test for this.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13801" opendate="2015-5-28 00:00:00" fixdate="2015-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop src checksum is shown instead of HBase src checksum in master / RS UI</summary>
      <description>Simple bug. We are showing the Hadoop's source MD5 checksum in the master UI instead of the HBase's one.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="1381" opendate="2009-5-6 00:00:00" fixdate="2009-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove onelab and bloom filters files from hbase</summary>
      <description>Bloom filters were broken in 0.19 hbase. They probably won't be in 0.20.0 because discussion and experimentation has them using lots of RAM for questionable benefit. We'll reexamine in 0.21. Meantime, purge the onelab stuff and the bloom filter map files from hbase. They haven't been working and when we go back to bloomfilters, we'll pick up the onelab from the hadoop core project (or use jgrays' bloomfilter implementation because it might be more efficient).</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.onelab.test.TestFilter.java</file>
      <file type="M">src.test.org.onelab.test.StringKey.java</file>
      <file type="M">src.java.org.onelab.filter.RetouchedBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.RemoveScheme.java</file>
      <file type="M">src.java.org.onelab.filter.Key.java</file>
      <file type="M">src.java.org.onelab.filter.HashFunction.java</file>
      <file type="M">src.java.org.onelab.filter.Filter.java</file>
      <file type="M">src.java.org.onelab.filter.DynamicBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.CountingBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.BloomFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HalfMapFileReader.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.BloomFilterMapFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="13812" opendate="2015-5-30 00:00:00" fixdate="2015-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deleting of last Column Family of a table should not be allowed</summary>
      <description>In this test run (without master killing) of IntegrationTestDDLMasterFailover of HBASE-13470, the DeleteColumnFamilyAction tries to delete one of the column families of a selected table. When there is only 1 column family left, the delete didn't fail.console_tue_nokill_r3.log2015-05-20 02:34:30,934 INFO [Thread-5] hbase.IntegrationTestMasterFailover: Deleting column family: {NAME =&gt; 'cf-882743293', DATA_BLOCK_ENCODING =&gt; 'PREFIX', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', COMPRESSION =&gt; 'NONE', VERSIONS =&gt; '1', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'} from table: ittable-0296278350...2015-05-20 02:34:35,269 INFO [Thread-5] hbase.IntegrationTestMasterFailover: Deleted column family: {NAME =&gt; 'cf-882743293', DATA_BLOCK_ENCODING =&gt; 'PREFIX', BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', COMPRESSION =&gt; 'NONE', VERSIONS =&gt; '1', TTL =&gt; 'FOREVER', MIN_VERSIONS =&gt; '0', KEEP_DELETED_CELLS =&gt; 'FALSE', BLOCKSIZE =&gt; '65536', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'} from table: ittable-0296278350...2015-05-20 02:34:35,383 INFO [Thread-14] hbase.IntegrationTestMasterFailover: No column families in table: 'ittable-0296278350'...Number of Tables: 49 Table: ittable-0372763896 rw families: 1 Table: ittable-0824340809 rw families: 1 Table: ittable-1400154361 rw families: 1 Table: ittable-1625415605 rw families: 0 Table: ittable-1501441540 rw families: 1 Table: ittable-0296278350 rw families: 0This should not be allowed.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
    </fixedFiles>
  </bug>
  <bug id="13816" opendate="2015-5-30 00:00:00" fixdate="2015-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build shaded modules only in release profile</summary>
      <description>Shaded modules are great, but not needed in the regular build + test cycle. I noticed that around 30-40% of the build time goes in the actual shading. I think we can just build the shaded jars in the release profile. hadoopqe and mvn publishing should not be affected.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13817" opendate="2015-5-31 00:00:00" fixdate="2015-6-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ByteBufferOuputStream - add writeInt support</summary>
      <description>While writing Cells to this stream, to make the CellBlock ByteBuffer, we do write length of the cell as int. We use StreamUtils to do this which will write each byte one after the other. So 4 write calls on Stream.(OutputSteam has only this support) With ByteBufferOuputStream we have the overhead of checking for size limit and possible grow with every write call. Internally this stream writes to a ByteBuffer. Again inside the ByteBuffer implementations there is position limit checks. If we do write these length as int in one go we can reduce this overhead.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.NoTagsKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValueUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ByteBufferOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="13821" opendate="2015-6-1 00:00:00" fixdate="2015-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WARN if hbase.bucketcache.percentage.in.combinedcache is set</summary>
      <description>HBASE-11520 improved configuration of bucket cache to no longer require hbase.bucketcache.percentage.in.combinedcache. This was done rather aggressively, with this previously mandatory configuration being ignored. This can result in RS crashes for unsuspecting users. We should add a WARN when hbase.bucketcache.percentage.in.combinedcache is set to make debugging the crash more straight forward.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="13826" opendate="2015-6-2 00:00:00" fixdate="2015-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to create table when group acls are appropriately set.</summary>
      <description>Steps for reproducing the issue. Create user 'test' and group 'hbase-admin'. Grant global create permissions to 'hbase-admin'. Add user 'test' to 'hbase-admin' group. Create table operation for 'test' user will throw ADE.</description>
      <version>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="13846" opendate="2015-6-4 00:00:00" fixdate="2015-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run MiniCluster on top of other MiniDfsCluster</summary>
      <description>Similar to how we don't start a mini-zk cluster when we already have one specified, this will skip starting a mini-dfs cluster if the user specifies a different one.</description>
      <version>0.98.14,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="13895" opendate="2015-6-12 00:00:00" fixdate="2015-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DATALOSS: Region assigned before WAL replay when abort</summary>
      <description>Opening a place holder till finish analysis.I have dataloss running ITBLL at 3B (testing HBASE-13877). Most obvious culprit is the double-assignment that I can see.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerAbortedException.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.WALPlayer.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerStoppedException.java</file>
    </fixedFiles>
  </bug>
  <bug id="13898" opendate="2015-6-14 00:00:00" fixdate="2015-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct additional javadoc failures under java 8</summary>
      <description>Java 8's javadoc gives an error where previous javadoc tools gave a more lenient WARN. HBASE-13569 took down the majority of the gaps, but while that issue was in review we gained some more.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Region.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.MultiByteBuffer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DefaultWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.HFileCorruptionChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterChunk.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.WriteSinkCoprocessor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.TableEventHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.BaseHFileCleanerDelegate.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MultiTableSnapshotInputFormatImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.lib.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.jmx.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.jmx.JMXJsonServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.InfoServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteAdmin.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufMagic.java</file>
      <file type="M">hbase-protocol.src.main.java.com.google.protobuf.HBaseZeroCopyByteString.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStore.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.scanner.CellSearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.tokenize.TokenizerNode.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.BulkDeleteEndpoint.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.metrics.ServerSideScanMetrics.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.coprocessor.package-info.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13899" opendate="2015-6-14 00:00:00" fixdate="2015-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jacoco instrumentation fails under jdk8</summary>
      <description>Moving the post-commit build for master to also cover jdk8 shows failures when attempting to instrument test for jacoco coverage.example: Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:386) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:401)Caused by: java.lang.RuntimeException: Class java/util/UUID could not be instrumented. at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:138) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:99) at org.jacoco.agent.rt.internal_5d10cad.PreMain.createRuntime(PreMain.java:51) at org.jacoco.agent.rt.internal_5d10cad.PreMain.premain(PreMain.java:43) ... 6 moreCaused by: java.lang.NoSuchFieldException: $jacocoAccess at java.lang.Class.getField(Class.java:1695) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:136) ... 9 moreFATAL ERROR in native method: processing of -javaagent failedAborted</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13910" opendate="2015-6-16 00:00:00" fixdate="2015-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add branch-1.2 to precommit branches</summary>
      <description>update the precommit test properties so that patches targeting branch-1.2 work</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="13911" opendate="2015-6-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.2 to prereq tables in ref guide</summary>
      <description>update the ref guide to have a column for 1.2.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13930" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Findbugs packages from shaded jars</summary>
      <description>Looking at 1.1.1RC0 shaded artifacts, looks like classes from find bugs are under the edu prefix and are not shaded. We should exclude find bugs from the shaded builds, and/or shade shade the edu prefix as well.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13948" opendate="2015-6-22 00:00:00" fixdate="2015-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand hadoop2 versions built on the pre-commit</summary>
      <description>For the HBase 1.1 line I've been validating builds against the following hadoop versions: 2.2.0 2.3.0 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0. Let's do the same in pre-commit.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="1395" opendate="2009-5-8 00:00:00" fixdate="2009-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>InfoServers no longer put up a UI</summary>
      <description>InfoServers do not work in an vanilla Hadoop 0.20 and HBase 0.20-dev installation. Directory listings instead of UI.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="13950" opendate="2015-6-23 00:00:00" fixdate="2015-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a NoopProcedureStore for testing</summary>
      <description>Add a NoopProcedureStore and an helper in ProcedureTestingUtil to submitAndWait() a procedure without having to do anything else.This is useful to avoid extra code like in case of TestAssignmentManager.processServerShutdownHandler()</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="13954" opendate="2015-6-23 00:00:00" fixdate="2015-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HTableInterface#getRowOrBefore related server side code</summary>
      <description>As part of HBASE-13214 review, anoop.hbase had a review comment on the review board to remove all the server side related code for getRowOrBefore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestWithDisabledAuthorization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSideNoCodec.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Region.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTableWrapper.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestGet.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientNoCluster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Get.java</file>
    </fixedFiles>
  </bug>
  <bug id="13956" opendate="2015-6-23 00:00:00" fixdate="2015-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add myself as 1.1 release manager</summary>
      <description>Just saw we have an RM section. Add myself.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13967" opendate="2015-6-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add jdk profiles for jdk.tools dependency</summary>
      <description>Right now hbase-annotations uses jdk7 jdk.tools and exposes that to downstream via hbase-client. We need it for building and using our custom doclet, but we should be using a jdk.tools version based on our java version (use jdk activated profiles to set it)</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13968" opendate="2015-6-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated methods from BufferedMutator class</summary>
      <description>As part of HBASE-13214 review, anoop.hbase had a review comment on the review board to remove BufferedMutator#getWriteBuffer method as it deprecated since 1.0.0So as part of this jira we can remove all the deprecated and unused methods from BufferedMutator class.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="13975" opendate="2015-6-26 00:00:00" fixdate="2015-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.2 RM to docs</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13976" opendate="2015-6-26 00:00:00" fixdate="2015-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>release manager list in ref guide is missing 0.94 line</summary>
      <description>0.94 has slowed substantially, but we haven't EOLed it yet so we should make sure folks looking to get a fix included know where to look.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="13977" opendate="2015-6-26 00:00:00" fixdate="2015-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert getKey and related APIs to Cell</summary>
      <description>During the course of changes for HBASE-11425 felt that more APIs can be converted to return Cell instead of BB like getKey, getLastKey. We can also rename the getKeyValue to getCell.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMajorCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHalfStoreFileReader.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekTo.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileSeek.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileEncryption.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockIndex.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestDataBlockEncoders.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HFilePerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeMultiFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="13990" opendate="2015-6-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up remaining errors for maven site goal</summary>
      <description>maven sit goal still fails with a problem about resolving mockito. work through any remaining issues to get a successful site execution.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="13992" opendate="2015-6-29 00:00:00" fixdate="2015-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate SparkOnHBase into HBase</summary>
      <description>This Jira is to ask if SparkOnHBase can find a home in side HBase core.Here is the github: https://github.com/cloudera-labs/SparkOnHBaseI am the core author of this project and the license is Apache 2.0A blog explaining this project is herehttp://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/A spark Streaming example is herehttp://blog.cloudera.com/blog/2014/11/how-to-do-near-real-time-sessionization-with-spark-streaming-and-apache-hadoop/A real customer using this in produce is blogged herehttp://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/Please debate and let me know what I can do to make this happen.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="14013" opendate="2015-7-2 00:00:00" fixdate="2015-7-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry when RegionServerNotYetRunningException rather than go ahead with assign so for sure we don&amp;#39;t skip WAL replay</summary>
      <description>Patches are copied from parent. They were done by enis +1 from. They continue the theme of the parent applying it to RegionServerNotYetRunningException as well as the new region aborting exception .. added in parent issue.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="14026" opendate="2015-7-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify "Web API" in version and compatibility docs</summary>
      <description>per discussion on HBASE-13861, update our version and compatibility section to clarify under operational compatibility that by "Web page API" we mean the /jmx endpoint.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14053" opendate="2015-7-9 00:00:00" fixdate="2015-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable DLR in branch-1+</summary>
      <description>Disable Distributed Log Replay in branch-1, branch-1.2, and master until it is known reliable (See HBASE-14028 for current list of issues and HBASE-7006 for overview on DLR). Disabling was discussed out on the dev list. See the thread here: http://apache-hbase.679495.n3.nabble.com/DISCUSS-Distributed-Log-Replay-in-branch-1-td4073003.html</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="14058" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stabilizing default heap memory tuner</summary>
      <description>The memory tuner works well in general cases but when we have a work load that is both read heavy as well as write heavy the tuner does too many tuning. We should try to control the number of tuner operation and stabilize it. The main problem was that the tuner thinks it is in steady state even if it sees just one neutral tuner period thus does too many tuning operations and too many reverts that too with large step sizes(step size was set to maximum even after one neutral period). So to stop this I have thought of these steps:1) The division created by  + /2 and  - /2 is too small. Statistically ~62% periods will lie outside this range, which means 62% of the data points are considered either high or low which is too much. Use  + *0.8 and  - *0.8 instead. On expectations it will decrease number of tuner operations per 100 periods from 19 to just 10. If we use /2 then 31% of data values will be considered to be high and 31% will be considered to be low (2*0.31 * 0.31 = 0.19), on the other hand if we use *0.8 then 22% will be low and 22% will be high(2*0.22*0.22 ~ 0.10).2) Defining proper steady state by looking at past few periods(it is equal to hbase.regionserver.heapmemory.autotuner.lookup.periods) rather than just last tuner operation. We say tuner is in steady state when last few tuner periods were NEUTRAL. We keep decreasing step size unless it is extremely low. Then leave system in that state for some time.3) Rather then decreasing step size only while reverting, decrease the magnitude of step size whenever we are trying to revert tuning done in last few periods(sum the changes of last few periods and compare to current step) rather than just looking at last period. When its magnitude gets too low then make tuner steps NEUTRAL(no operation). This will cause step size to continuously decrease unless we reach steady state. After that tuning process will restart (tuner step size rests again when we reach steady state).4) The tuning done in last few periods will be decaying sum of past tuner steps with sign. This parameter will be positive for increase in memstore and negative for increase in block cache. Rather than using arithmetic mean we use this to give more priority to recent tuner steps.Please see the attachments. One represents the size of memstore(green) and size of block cache(blue) adjusted by tuner without these modification and other with the above modifications. The x-axis is time axis and y-axis is the fraction of heap memory available to memstore and block cache at that time(it always sums up to 80%). I configured min/max ranges for both components to 0.1 and 0.7 respectively(so in the plots the y-axis min and max is 0.1 and 0.7). In both cases the tuner tries to distribute memory by giving ~15% to memstore and ~65% to block cache. But the modified one does it much more smoothly.I got these results from YCSB test. The test was doing approximately 5000 inserts and 500 reads per second (for one region server). The results can be further fine tuned and number of tuner operation can be reduced with these changes in configuration.For more fine tuning:a) lower max step size (suggested = 4%)b) lower min step size ( default if also fine )To further decrease frequency of tuning operations:c) increase the number of lookup periods ( in the tests it was just 10, default is 60 )d) increase tuner period ( in the tests it was just 20 secs, default is 60secs)I used smaller tuner period/ number of look up periods to get more data points.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.java</file>
    </fixedFiles>
  </bug>
  <bug id="14077" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add package to hbase-protocol protobuf files.</summary>
      <description>c++ generated code is currently in the default namespace. That's bad practice; so lets fix it</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.MapReduce.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.VisibilityLabels.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Tracing.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Snapshot.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.SecureBulkLoad.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RPC.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RowProcessor.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RegionServerStatus.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Quota.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Procedure.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.MultiRowMutation.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.generated.BulkDeleteProtos.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.generated.ExampleProtos.java</file>
      <file type="M">hbase-examples.src.main.protobuf.BulkDelete.proto</file>
      <file type="M">hbase-examples.src.main.protobuf.Examples.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AggregateProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.CellProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ComparatorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.EncryptionProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ErrorHandlingProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FilterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FSProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HFileProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.LoadBalancerProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ProcedureProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RPCProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.TracingProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.WALProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Aggregate.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Authentication.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Cell.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterId.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Comparator.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Encryption.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ErrorHandling.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Filter.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.FS.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.HFile.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.LoadBalancer.proto</file>
    </fixedFiles>
  </bug>
  <bug id="14086" opendate="2015-7-15 00:00:00" fixdate="2015-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14087" opendate="2015-7-15 00:00:00" fixdate="2015-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14097" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log link to client scan troubleshooting section when scanner exceptions happen.</summary>
      <description>As per description.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="14110" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CostFunction for balancing primary region replicas</summary>
      <description>Currently replicas for the same region are spread across region servers.However, one region server may serve disproportionately high number of primary region replicas. This has been observed in production cluster.This issue adds PrimaryRegionCountSkewCostFunction which facilitates balancing primary region replicas evenly across region servers.Thanks to Enis for fruitful discussion.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14211" opendate="2015-8-11 00:00:00" fixdate="2015-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more rigorous integration tests of splits</summary>
      <description>Add a chaos action that will turn down region size. Eventually this will cause regions to split a lot. It will need to have a min region size.Add a chaos monkey action that will change split policy Change between Uniform and SplittingUpTo and backAdd chaos monkey action that will request splits of every region. When regions all reach the size a the exact same time the compactions add a lot of work. This simulates a very well distributed write pattern reaching the region size.Add the ability to start with fewer regions than normal to ITBLL</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.StressAssignmentManagerMonkeyFactory.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.SlowDeterministicMonkeyFactory.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.MonkeyConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="14224" opendate="2015-8-14 00:00:00" fixdate="2015-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix coprocessor handling of duplicate classes</summary>
      <description>While discussing with misty over on HBASE-13907 we noticed some inconsistency when copros are loaded. Sometimes you can load them more than once, sometimes you can not. Need to consolidate.</description>
      <version>1.0.1,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>1.0.2,1.2.0,1.3.0,0.98.15,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14230" opendate="2015-8-17 00:00:00" fixdate="2015-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>replace reflection in FSHlog with HdfsDataOutputStream#getCurrentBlockReplication()</summary>
      <description>As comment TODO said, we use HdfsDataOutputStream#getCurrentBlockReplication and DFSOutputStream.getPipeLine to replace reflection in FSHlog</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14239" opendate="2015-8-18 00:00:00" fixdate="2015-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Branch-1.2 AM can get stuck when meta moves</summary>
      <description>When regions are moving master can get totally stuck trying to talk to meta.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterPriorityRpc.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14249" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14250" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14251" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14260" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t build javadocs for hbase-protocol module</summary>
      <description>I'm not sure I have all the affected versions, but it seems that something is amiss in making our javadocs: mvn -Papache-release -Prelease -DskipTests clean package... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache HBase ....................................... SUCCESS [ 11.149 s][INFO] Apache HBase - Checkstyle .......................... SUCCESS [ 1.249 s][INFO] Apache HBase - Resource Bundle ..................... SUCCESS [ 0.539 s][INFO] Apache HBase - Annotations ......................... SUCCESS [ 4.438 s][INFO] Apache HBase - Protocol ............................ SUCCESS [10:15 min][INFO] Apache HBase - Common .............................. SUCCESS [ 48.465 s][INFO] Apache HBase - Procedure ........................... SUCCESS [ 14.375 s][INFO] Apache HBase - Client .............................. SUCCESS [ 45.187 s][INFO] Apache HBase - Hadoop Compatibility ................ SUCCESS [ 6.998 s][INFO] Apache HBase - Hadoop Two Compatibility ............ SUCCESS [ 14.891 s][INFO] Apache HBase - Prefix Tree ......................... SUCCESS [ 14.214 s][INFO] Apache HBase - Server .............................. SUCCESS [02:01 min][INFO] Apache HBase - Testing Util ........................ SUCCESS [ 12.779 s][INFO] Apache HBase - Thrift .............................. SUCCESS [01:15 min][INFO] Apache HBase - Shell ............................... SUCCESS [ 6.649 s][INFO] Apache HBase - Integration Tests ................... SUCCESS [ 6.429 s][INFO] Apache HBase - Examples ............................ SUCCESS [ 13.200 s][INFO] Apache HBase - Rest ................................ SUCCESS [ 27.831 s][INFO] Apache HBase - Assembly ............................ SUCCESS [ 19.400 s][INFO] Apache HBase - Shaded .............................. SUCCESS [ 0.419 s][INFO] Apache HBase - Shaded - Client ..................... SUCCESS [ 23.707 s][INFO] Apache HBase - Shaded - Server ..................... SUCCESS [ 43.654 s][INFO] Apache HBase - Spark ............................... SUCCESS [02:22 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 21:13 min[INFO] Finished at: 2015-08-19T15:48:00-05:00[INFO] Final Memory: 181M/1513M[INFO] ------------------------------------------------------------------------</description>
      <version>0.98.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14261" opendate="2015-8-19 00:00:00" fixdate="2015-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance Chaos Monkey framework by adding zookeeper and datanode fault injections.</summary>
      <description>One of the shortcomings of existing ChaosMonkey framework is lack of fault injections for hbase dependencies like zookeeper, hdfs etc. This patch attempts to solve this problem partially by adding datanode and zk node fault injections.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseCluster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.RESTApiClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.DistributedHBaseCluster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.ClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.MonkeyFactory.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.RestartActionBaseAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.Action.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKServerTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="14271" opendate="2015-8-20 00:00:00" fixdate="2015-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Nexus staging instructions</summary>
      <description>Refine the Nexus staging instructions a bit. (A promise I made a long time ago.)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14313" opendate="2015-8-26 00:00:00" fixdate="2015-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>After a Connection sees ConnectionClosingException it never recovers</summary>
      <description></description>
      <version>1.2.0,1.1.0.1</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14314" opendate="2015-8-26 00:00:00" fixdate="2015-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metrics for block cache should take region replicas into account</summary>
      <description>Currently metrics for block cache are aggregates in the sense that they don't distinguish primary from secondary / tertiary replicas.This JIRA separates the block cache metrics for primary region replica from the aggregate.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHeapMemoryManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheKey.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14322" opendate="2015-8-27 00:00:00" fixdate="2015-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master still not using more than it&amp;#39;s priority threads</summary>
      <description>Master and regionserver will be running as the same user.Superusers by default adds the current user as a super user.Super users' requests always go to the priority threads.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQosFunction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterPriorityRpc.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="14325" opendate="2015-8-27 00:00:00" fixdate="2015-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add snapshotinfo command to hbase script</summary>
      <description>Since we already have commands like hbck, hfile, wal etc. that are used for getting various types of information about HBase components it make sense to me to add SnapshotInfo tool to collection. If nobody objects i would add patch for this.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="14326" opendate="2015-8-27 00:00:00" fixdate="2015-10-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book: fix definition of max min size to compact</summary>
      <description>I think we need to change wording/definition of these config parameters in HBase book, they are misleading:hbase.hstore.compaction.min.sizeDescriptionA StoreFile smaller than this size will always be eligible for minor compaction. HFiles this size or larger are evaluated by hbase.hstore.compaction.ratio to determine if they are eligible. Because this limit represents the "automatic include"limit for all StoreFiles smaller than this value, this value may need to be reduced in write-heavy environments where many StoreFiles in the 1-2 MB range are being flushed, because every StoreFile will be targeted for compaction and the resulting StoreFiles may still be under the minimum size and require further compaction. If this parameter is lowered, the ratio check is triggered more quickly. This addressed some issues seen in earlier versions of HBase but changing this parameter is no longer necessary in most situations. Default: 128 MB expressed in bytes.Default134217728hbase.hstore.compaction.max.sizeDescriptionA StoreFile larger than this size will be excluded from compaction. The effect of raising hbase.hstore.compaction.max.size is fewer, larger StoreFiles that do not get compacted often. If you feel that compaction is happening too often without much benefit, you can try raising this value. Default: the value of LONG.MAX_VALUE, expressed in bytes.hbase.hstore.compaction.ratioDescriptionFor minor compaction, this ratio is used to determine whether a given StoreFile which is larger than hbase.hstore.compaction.min.size is eligible for compaction. Its effect is to limit compaction of large StoreFiles. The value of hbase.hstore.compaction.ratio is expressed as a floating-point decimal. A large ratio, such as 10, will produce a single giant StoreFile. Conversely, a low value, such as .25, will produce behavior similar to the BigTable compaction algorithm, producing four StoreFiles. A moderate value of between 1.0 and 1.4 is recommended. When tuning this value, you are balancing write costs with read costs. Raising the value (to something like 1.4) will have more write costs, because you will compact larger StoreFiles. However, during reads, HBase will need to seek through fewer StoreFiles to accomplish the read. Consider this approach if you cannot take advantage of Bloom filters. Otherwise, you can lower this value to something like 1.0 to reduce the background cost of writes, and use Bloom filters to control the number of StoreFiles touched during reads. For most cases, the default value is appropriate.Default1.2FFor details, see HBASE-14263.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14334" opendate="2015-8-28 00:00:00" fixdate="2015-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Memcached block cache in to it&amp;#39;s own optional module.</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14338" opendate="2015-8-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14340" opendate="2015-8-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14346" opendate="2015-8-31 00:00:00" fixdate="2015-10-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in FamilyFilter</summary>
      <description>I think there's a typo. "qualifier name" should read "column family name"Family Filter This filter takes a compare operator and a comparator. It compares each qualifier name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that column.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.thrift.filter.language.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14348" opendate="2015-8-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14349" opendate="2015-9-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14385" opendate="2015-9-9 00:00:00" fixdate="2015-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close the sockets that is missing in connection closure.</summary>
      <description>As per heading. Due credit to one of our awesome customers for digging into this and helping me craft the unit test.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14387" opendate="2015-9-9 00:00:00" fixdate="2015-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction improvements: Maximum off-peak compaction size</summary>
      <description>Make max compaction size for peak and off peak separate config options.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerOnlineConfigChange.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="14459" opendate="2015-9-21 00:00:00" fixdate="2015-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add request and response sizes metrics</summary>
      <description>Adding metrics that should be useful:Request sizeResponse size</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestRpcMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14473" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute region locality in parallel</summary>
      <description>Right now on large clusters it's necessary to turn off the locality balance cost as it takes too long to compute the region locality. This is because it's computed when need in serial.We should compute this in parallel before it's needed.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java</file>
    </fixedFiles>
  </bug>
  <bug id="14515" opendate="2015-9-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow spark module unit tests to be skipped with a profile</summary>
      <description>we can skip the tests for individual modules with profile, e.g. skipServerTests, we should have the same for spark.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14516" opendate="2015-9-30 00:00:00" fixdate="2015-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>categorize hadoop-compat tests</summary>
      <description>the hadoop-compat and hadoop2-compat modules do not rely on the hbase annotations test-jar and their tests aren't categorized.this causes things to fail if you attempt to specify one of our test categories to run.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.thrift.TestMetricsThriftServerSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.rest.TestMetricsRESTSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestMetricsReplicationSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestMetricsReplicationSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestMetricsWALSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.metrics.TestBaseSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.master.TestMetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.thrift.TestMetricsThriftServerSourceFactory.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.TestCompatibilitySingletonFactory.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.rest.TestMetricsRESTSource.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestMetricsReplicationSourceFactory.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestMetricsWALSource.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServerSourceFactory.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.master.TestMetricsMasterSourceFactory.java</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14517" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show regionserver&amp;#39;s version in master status page</summary>
      <description>In production env, regionservers may be removed from the cluster for hardware problems and rejoined the cluster after the repair. There is a potential risk that the version of rejoined regionserver may diff from others because the cluster has been upgraded through many versions. To solve this, we can show the all regionservers' version in the server list of master's status page, and highlight the regionserver when its version is different from the master's version, similar to HDFS-3245Suggestions are welcome~</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
      <file type="M">hbase-protocol.src.main.protobuf.RPC.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RPCProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="14529" opendate="2015-9-30 00:00:00" fixdate="2015-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Respond to SIGHUP to reload config</summary>
      <description>SIGHUP is the way everyone since the dawn of unix has done config reload.Lets not be a special unique snowflake.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.conf.ConfigurationManager.java</file>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="1453" opendate="2009-5-27 00:00:00" fixdate="2009-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HADOOP-4681 to our bundled hadoop, add to &amp;#39;gettting started&amp;#39; recommendation that hbase users backport</summary>
      <description>HADOOP-4681, while not perfect, should help with our too frequent 'no such block' message incidence.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.hadoop-0.20.0-core.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14544" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow HConnectionImpl to not refresh the dns on errors</summary>
      <description>Some clusters will have static ip addresses and forced dns lookup can cause extra instability. Allow users to tun that feature off, if wanted.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14545" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMasterFailover often times out</summary>
      <description>Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 301.644 sec &lt;&lt;&lt; FAILURE! - in org.apache.hadoop.hbase.master.TestMasterFailovertestMasterFailoverWithMockedRIT(org.apache.hadoop.hbase.master.TestMasterFailover) Time elapsed: 240.112 sec &lt;&lt;&lt; ERROR!org.junit.runners.model.TestTimedOutException: test timed out after 240000 milliseconds at java.lang.Thread.sleep(Native Method) at org.apache.hadoop.hbase.util.Threads.sleep(Threads.java:146) at org.apache.hadoop.hbase.MiniHBaseCluster.waitForActiveAndReadyMaster(MiniHBaseCluster.java:535) at org.apache.hadoop.hbase.HBaseCluster.waitForActiveAndReadyMaster(HBaseCluster.java:280) at org.apache.hadoop.hbase.master.TestMasterFailover.testMasterFailoverWithMockedRIT(TestMasterFailover.java:400)Results :Tests in error: TestMasterFailover.testMasterFailoverWithMockedRIT:400  TestTimedOut test tim...Tests run: 7, Failures: 0, Errors: 1, Skipped: 0</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.16,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="14547" opendate="2015-10-2 00:00:00" fixdate="2015-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more debug/trace to zk-procedure</summary>
      <description>add more debug/trace logs to the zk-procedure/online-snapshot flow</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.snapshot.RegionServerSnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.ZKProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.ZKProcedureCoordinatorRpcs.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.ProcedureMember.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.ProcedureCoordinator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.Procedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="14548" opendate="2015-10-3 00:00:00" fixdate="2015-7-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand how table coprocessor jar and dependency path can be specified</summary>
      <description>Currently you can specify the location of the coprocessor jar in the table coprocessor attribute.The problem is that it only allows you to specify one jar that implements the coprocessor. You will need to either bundle all the dependencies into this jar, or you will need to copy the dependencies into HBase lib dir.The first option may not be ideal sometimes. The second choice can be troublesome too, particularly when the hbase region sever node and dirs are dynamically added/created.There are a couple things we can expand here. We can allow the coprocessor attribute to specify a directory location, probably on hdfs.We may even allow some wildcard in there.</description>
      <version>1.2.0</version>
      <fixedVersion>1.4.0,0.98.21,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorClassLoader.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CoprocessorClassLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="14557" opendate="2015-10-6 00:00:00" fixdate="2015-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce WALPlayer issue with NoTagsKeyValue</summary>
      <description>Running MapReduce WALPlayer to convert WAL into HFiles:15/10/05 20:28:08 INFO mapred.JobClient: Task Id : attempt_201508031611_0029_m_000000_0, Status : FAILEDjava.io.IOException: Type mismatch in value from map: expected org.apache.hadoop.hbase.KeyValue, recieved org.apache.hadoop.hbase.NoTagsKeyValue at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:997) at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:689) at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89) at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112) at org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper.map(WALPlayer.java:111) at org.apache.hadoop.hbase.mapreduce.WALPlayer$WALKeyValueMapper.map(WALPlayer.java:96) at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:140) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:751) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:368) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(AccessController.java:369) at javax.security.auth.Subject.doAs(Subject.java:572) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1502) at org.apache.hadoop.mapred.Child.main(Child.java:249)</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepReducer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.MemStoreWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValueUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="1458" opendate="2009-5-29 00:00:00" fixdate="2009-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>64 commit logs as upper bound is too many -- make it half</summary>
      <description>Running an upload, 64 commit logs as upper bound before we force flushes to clear the oldest edit is too much. I can see an upload running in my little cluster and even after running for an hour we still have not hit the 64 logs max. The more logs we have, the longer recovery on crash. We should halve the number I'd say.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14597" opendate="2015-10-13 00:00:00" fixdate="2015-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Groups cache in multi-threaded env</summary>
      <description>UGI doesn't hash based on the user as expected so since we have lots of ugi potentially created the cache doesn't do it's job.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestUser.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.UserProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
    </fixedFiles>
  </bug>
  <bug id="14633" opendate="2015-10-16 00:00:00" fixdate="2015-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try fluid width UI</summary>
      <description>Our UI is often too long. Lets give it more room if available.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-rest.src.main.resources.hbase-webapps.rest.rest.jsp</file>
    </fixedFiles>
  </bug>
  <bug id="14636" opendate="2015-10-17 00:00:00" fixdate="2015-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear HFileScannerImpl#prevBlocks in between Compaction flow</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="14683" opendate="2015-10-23 00:00:00" fixdate="2015-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batching in buffered mutator is awful when adding lists of mutations.</summary>
      <description>When adding a list of mutations to the buffer the limit is checked after every mutation is added.This leads to lots of tries to flush.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14684" opendate="2015-10-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try to remove all MiniMapReduceCluster in unit tests</summary>
      <description>As discussion in dev list, we will try to do MR job without MiniMapReduceCluster.Testcases will run faster and more reliable.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobSecureExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableMapReduceUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduceBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScanBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestSyncTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestRowCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultithreadedTableMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithVisibilityLabels.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithTTLs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithOperationAttributes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHRegionPartitioner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHashTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCopyTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCellCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="14690" opendate="2015-10-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix css so there&amp;#39;s no left/right scroll bar</summary>
      <description>2 em of extra padding needs to be reomved.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
    </fixedFiles>
  </bug>
  <bug id="14693" opendate="2015-10-24 00:00:00" fixdate="2015-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add client-side metrics for received pushback signals</summary>
      <description>HBASE-12911 added client side metrics. HBASE-5162 added a mechanism for sending advisory backpressure signals to clients when the server is heavily loaded, and HBASE-12702 and subtasks backported this to all active branches. Add client-side metrics for received pushback signal.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.16,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientPushback.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MetricsConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="14695" opendate="2015-10-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some easy HTML warnings</summary>
      <description>There are a few links to top-level pages but missing the trailing /, and a few please where we link to pages like replication.html even though we have a single-page book now. There are also a few broken links in the APIdocs, probably due to code we have specifically filtered out of the Javadoc:#------------------------------------------------------------# ERROR 4 files had broken links#------------------------------------------------------------/devapidocs/org/apache/hadoop/hbase/wal/NamespaceGroupingStrategy.htmlhad 1 broken link /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/org/apache/hadoop/hbase/wal/package-tree.htmlhad 1 broken link /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/overview-tree.htmlhad 2 broken links /devapidocs/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.SeekerState.html /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/serialized-form.htmlhad 26 broken links /devapidocs/src-html/com/google/protobuf/DescriptorProtos.DescriptorProto.ExtensionRange.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.DescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumValueDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumValueOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FieldDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FieldOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileDescriptorSet.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MessageOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MethodDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MethodOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.ServiceDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.ServiceOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.SourceCodeInfo.Location.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.SourceCodeInfo.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.UninterpretedOption.NamePart.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.UninterpretedOption.html /devapidocs/src-html/com/google/protobuf/GeneratedMessage.html /devapidocs/src-html/com/google/protobuf/GeneratedMessageLite.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/ColumnFilter$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/HBaseContext$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/RowKeyFilter$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/SchemaQualifierDefinition$.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSMapRUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.package-info.java</file>
    </fixedFiles>
  </bug>
  <bug id="1470" opendate="2009-6-1 00:00:00" fixdate="2009-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase and HADOOP-4379, dhruba&amp;#39;s flush/sync</summary>
      <description>This covers work with HADOOP-4379</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14700" opendate="2015-10-26 00:00:00" fixdate="2015-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support a "permissive" mode for secure clusters to allow "simple" auth clients</summary>
      <description>When implementing HBase security for an existing cluster, it can be useful to support mixed secure and insecure clients while all client configurations are migrated over to secure authentication. We currently have an option to allow secure clients to fallback to simple auth against insecure clusters. By providing an analogous setting for servers, we would allow a phased rollout of security: First, security can be enabled on the cluster servers, with the "permissive" mode enabled Clients can be converting to using secure authentication incrementally The server audit logs allow identification of clients still using simple auth to connect Finally, when sufficient clients have been converted to secure operation, the server-side "permissive" mode can be removed, allowing completely secure operation.Obviously with this enabled, there is no effective access control, but this would still be a useful tool to enable a smooth operational rollout of security. Permissive mode would of course be disabled by default. Enabling it should provide a big scary warning in the logs on startup, and possibly be flagged on relevant UIs.</description>
      <version>None</version>
      <fixedVersion>1.2.0,0.98.16,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestSecureRPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14723" opendate="2015-10-29 00:00:00" fixdate="2015-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix IT tests split too many times</summary>
      <description>Splitting the whole table is happening too often. Lets make this happen less frequently as there are more regions.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.SplitAllRegionOfTableAction.java</file>
    </fixedFiles>
  </bug>
  <bug id="14725" opendate="2015-10-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vet categorization of tests so they for sure go into the right small/medium/large buckets</summary>
      <description>I tried doing runSmallTests, runMediumTests, etc., and I noticed that some tests are larger than our categorization. At least for small tests it means they area all running in the one JVM. I also noticed that the categorization only takes effect in hbase-server.This patch makes it so runSmallTests runs all the small tests only in each category. Also moves tests that were larger than small out to medium so they don't run in the one JVM anymore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollingNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMultiVersionConcurrencyControl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestColumnSeeking.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestHFileLinkCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestRpcClientLeaks.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.util.TestTimeoutBlockingQueue.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.master.TestMetricsMasterSourceFactory.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="14745" opendate="2015-11-2 00:00:00" fixdate="2015-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade the last few dependencies in hbase-shaded-client</summary>
      <description>junit hadoop common</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.13,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1476" opendate="2009-6-2 00:00:00" fixdate="2009-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scaling compaction with multiple threads</summary>
      <description>Was thinking we should build in support to be able to handle more then one thread for compactions this will allow us to keep up with compactions when we get to the point where we store Tb's of data per node and may regionsMaybe a configurable setting to set how many threads a region server can use for compactions.With compression turned on my compactions are limited by cpu speed with multi cores then it would be nice to be able to scale compactions to 2 or more cores.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14772" opendate="2015-11-5 00:00:00" fixdate="2015-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve zombie detector; be more discerning</summary>
      <description>Currently, any surefire process with the hbase flag is a potential zombie. Our zombie check currently takes a reading and if it finds candidate zombies, it waits 30 seconds and then does another reading. If a concurrent build going on, in both cases the zombie detector will come up positive though the adjacent test run may be making progress; i.e. the cast of surefire processes may have changed between readings but our detector just sees presence of hbase surefire processes.Here is example:Suspicious java process found - waiting 30s to see if there are just slow to stopThere appear to be 5 zombie tests, they should have been killed by surefire but survived12823 surefirebooter852180186418035480.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true7653 surefirebooter8579074445899448699.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true12614 surefirebooter136529596936417090.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true7836 surefirebooter3217047564606450448.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true13566 surefirebooter2084039411151963494.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true************ BEGIN zombies jstack extract************ END zombies jstack extract5 is the number of forked processes we allow when doing medium and large tests.... so an adjacent build will always show as '5 zombies'.Need to add discerning if list of processes changes between readings.Can I also add a tag per build run that all forked processes pick up so I can look at the current builds progeny only?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.zombie-detector.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14773" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HBase shell tests are skipped when skipping server tests.</summary>
      <description>Looks like a copy pasta error.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14778" opendate="2015-11-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make block cache hit percentages not integer in the metrics system</summary>
      <description>Once you're close to the 90%+ it's hard to see a difference because getting a full percent change is rare.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14781" opendate="2015-11-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn per cf flushing on for ITBLL by default</summary>
      <description></description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  <bug id="14793" opendate="2015-11-10 00:00:00" fixdate="2015-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow limiting size of block into L1 block cache.</summary>
      <description>G1GC does really badly with long lived large objects. Lets allow limiting the size of a block that can be kept in the block cache.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14795" opendate="2015-11-10 00:00:00" fixdate="2015-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the spark-hbase scan operations</summary>
      <description>This is a sub-jira of HBASE-14789. This jira is to focus on the replacement of TableInputFormat for a more custom scan implementation that will make the following use case more effective.Use case:In the case you have multiple scan ranges on a single table with in a single query. TableInputFormat will scan the the outer range of the scan start and end range where this implementation can be more pointed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14796" opendate="2015-11-11 00:00:00" fixdate="2015-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the Gets in the connector</summary>
      <description>Current the Spark-Module Spark SQL implementation gets records from HBase from the driver if there is something like the following found in the SQL.rowkey = 123The reason for this original was normal sql will not have many equal operations in a single where clause.Zhan, had brought up too points that have value.1. The SQL may be generated and may have many many equal statements in it so moving the work to an executor protects the driver from load2. In the correct implementation the drive is connecting to HBase and exceptions may cause trouble with the Spark application and not just with the a single task execution</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseResources.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14804" opendate="2015-11-13 00:00:00" fixdate="2015-12-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell&amp;#39;s create table command ignores &amp;#39;NORMALIZATION_ENABLED&amp;#39; attribute</summary>
      <description>I am trying to create a new table and set the NORMALIZATION_ENABLED as true, but seems like the argument NORMALIZATION_ENABLED is being ignored. And the attribute NORMALIZATION_ENABLED is not displayed on doing a desc command on that tablehbase(main):020:0&gt; create 'test-table-4', 'cf', {NORMALIZATION_ENABLED =&gt; 'true'}An argument ignored (unknown or overridden): NORMALIZATION_ENABLED0 row(s) in 4.2670 seconds=&gt; Hbase::Table - test-table-4hbase(main):021:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4 COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0430 secondsHowever, on doing an alter command on that table we can set the NORMALIZATION_ENABLED attribute for that tablehbase(main):022:0&gt; alter 'test-table-4', {NORMALIZATION_ENABLED =&gt; 'true'}Unknown argument ignored: NORMALIZATION_ENABLEDUpdating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.3640 secondshbase(main):023:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4, {TABLE_ATTRIBUTES =&gt; {NORMALIZATION_ENABLED =&gt; 'true'} COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0190 secondsI think it would be better to have a single step process to enable normalization while creating the table itself, rather than a two step process to alter the table later on to enable normalization</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="14805" opendate="2015-11-13 00:00:00" fixdate="2015-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>status should show the master in shell</summary>
      <description>status 'simple' or 'detailed' only shows the regionservers and regions, but not the active master. Actually, there is no way to know about the active masters from the shell it seems.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="14819" opendate="2015-11-16 00:00:00" fixdate="2015-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-it tests failing with OOME; permgen</summary>
      <description>Let me up the heap used when failsafe forks.Here is example OOME doing ITBLL:2015-11-16 03:09:15,073 INFO [Thread-694] actions.BatchRestartRsAction(69): Starting region server:asf905.gq1.ygridcore.net2015-11-16 03:09:15,099 INFO [Thread-694] client.ConnectionUtils(104): regionserver/asf905.gq1.ygridcore.net/67.195.81.149:0 server-side HConnection retries=3502015-11-16 03:09:15,099 INFO [Thread-694] ipc.SimpleRpcScheduler(128): Using deadline as user call queue, count=12015-11-16 03:09:15,101 INFO [Thread-694] ipc.RpcServer$Listener(607): regionserver/asf905.gq1.ygridcore.net/67.195.81.149:0: started 3 reader(s) listening on port=361142015-11-16 03:09:15,103 INFO [Thread-694] fs.HFileSystem(252): Added intercepting call to namenode#getBlockLocations so can do block reordering using class class org.apache.hadoop.hbase.fs.HFileSystem$ReorderWALBlocks2015-11-16 03:09:15,104 INFO [Thread-694] zookeeper.RecoverableZooKeeper(120): Process identifier=regionserver:36114 connecting to ZooKeeper ensemble=localhost:501392015-11-16 03:09:15,117 DEBUG [Thread-694-EventThread] zookeeper.ZooKeeperWatcher(554): regionserver:361140x0, quorum=localhost:50139, baseZNode=/hbase Received ZooKeeper Event, type=None, state=SyncConnected, path=null2015-11-16 03:09:15,118 DEBUG [Thread-694] zookeeper.ZKUtil(492): regionserver:361140x0, quorum=localhost:50139, baseZNode=/hbase Set watcher on existing znode=/hbase/master2015-11-16 03:09:15,119 DEBUG [Thread-694] zookeeper.ZKUtil(492): regionserver:361140x0, quorum=localhost:50139, baseZNode=/hbase Set watcher on existing znode=/hbase/running2015-11-16 03:09:15,119 DEBUG [Thread-694-EventThread] zookeeper.ZooKeeperWatcher(638): regionserver:36114-0x1510e2c6f1d0029 connected2015-11-16 03:09:15,120 INFO [RpcServer.responder] ipc.RpcServer$Responder(926): RpcServer.responder: starting2015-11-16 03:09:15,121 INFO [RpcServer.listener,port=36114] ipc.RpcServer$Listener(738): RpcServer.listener,port=36114: starting2015-11-16 03:09:15,121 DEBUG [Thread-694] ipc.RpcExecutor(115): B.default Start Handler index=0 queue=02015-11-16 03:09:15,121 DEBUG [Thread-694] ipc.RpcExecutor(115): B.default Start Handler index=1 queue=02015-11-16 03:09:15,121 DEBUG [Thread-694] ipc.RpcExecutor(115): B.default Start Handler index=2 queue=02015-11-16 03:09:15,122 DEBUG [Thread-694] ipc.RpcExecutor(115): B.default Start Handler index=3 queue=02015-11-16 03:09:15,122 DEBUG [Thread-694] ipc.RpcExecutor(115): B.default Start Handler index=4 queue=02015-11-16 03:09:15,122 DEBUG [Thread-694] ipc.RpcExecutor(115): Priority Start Handler index=0 queue=02015-11-16 03:09:15,123 DEBUG [Thread-694] ipc.RpcExecutor(115): Priority Start Handler index=1 queue=12015-11-16 03:09:15,123 DEBUG [Thread-694] ipc.RpcExecutor(115): Priority Start Handler index=2 queue=02015-11-16 03:09:15,123 DEBUG [Thread-694] ipc.RpcExecutor(115): Priority Start Handler index=3 queue=12015-11-16 03:09:15,124 DEBUG [Thread-694] ipc.RpcExecutor(115): Priority Start Handler index=4 queue=02015-11-16 03:09:15,124 DEBUG [Thread-694] ipc.RpcExecutor(115): Replication Start Handler index=0 queue=02015-11-16 03:09:15,124 DEBUG [Thread-694] ipc.RpcExecutor(115): Replication Start Handler index=1 queue=02015-11-16 03:09:15,124 DEBUG [Thread-694] ipc.RpcExecutor(115): Replication Start Handler index=2 queue=02015-11-16 03:09:15,761 DEBUG [RS:0;asf905:36114] client.ConnectionManager$HConnectionImplementation(715): connection construction failedjava.io.IOException: java.lang.OutOfMemoryError: PermGen space at org.apache.hadoop.hbase.client.RegistryFactory.getRegistry(RegistryFactory.java:43) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.setupRegistry(ConnectionManager.java:886) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.&lt;init&gt;(ConnectionManager.java:692) at org.apache.hadoop.hbase.client.ConnectionUtils$2.&lt;init&gt;(ConnectionUtils.java:154) at org.apache.hadoop.hbase.client.ConnectionUtils.createShortCircuitConnection(ConnectionUtils.java:154) at org.apache.hadoop.hbase.regionserver.HRegionServer.createClusterConnection(HRegionServer.java:689) at org.apache.hadoop.hbase.regionserver.HRegionServer.setupClusterConnection(HRegionServer.java:720) at org.apache.hadoop.hbase.regionserver.HRegionServer.preRegistrationInitialization(HRegionServer.java:733) at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:889) at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.runRegionServer(MiniHBaseCluster.java:156) at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.access$000(MiniHBaseCluster.java:108) at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer$1.run(MiniHBaseCluster.java:140) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:356) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594) at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:334) at org.apache.hadoop.hbase.MiniHBaseCluster$MiniHBaseClusterRegionServer.run(MiniHBaseCluster.java:138) at java.lang.Thread.run(Thread.java:745)Caused by: java.lang.OutOfMemoryError: PermGen space at sun.misc.Unsafe.defineClass(Native Method) at sun.reflect.ClassDefiner.defineClass(ClassDefiner.java:63) at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:399) at sun.reflect.MethodAccessorGenerator$1.run(MethodAccessorGenerator.java:396) at java.security.AccessController.doPrivileged(Native Method) at sun.reflect.MethodAccessorGenerator.generate(MethodAccessorGenerator.java:395) at sun.reflect.MethodAccessorGenerator.generateConstructor(MethodAccessorGenerator.java:94) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:48) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:383) at org.apache.hadoop.hbase.client.RegistryFactory.getRegistry(RegistryFactory.java:41) ... 17 more</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestAcidGuarantees.java</file>
      <file type="M">hbase-it.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14837" opendate="2015-11-19 00:00:00" fixdate="2015-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Procedure Queue Improvement</summary>
      <description>Add event support to the procedure queue to avoid spinning and remove all the immutable object creations from the java classes Map, Tree, ... the queues that can't be executed because are waiting for an event (e.g. master initialized) or someone else have an exclusive lock are pulled out the run queuehttps://reviews.apache.org/r/40460/</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerProcedureInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureFairRunQueues.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureSimpleRunQueue.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureRunnableSet.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureFairRunQueues.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ProcedureInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="1484" opendate="2009-6-5 00:00:00" fixdate="2009-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>commit log split writes files with newest edits first (since hbase-1430); should be other way round</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHLog.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="14849" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14869" opendate="2015-11-21 00:00:00" fixdate="2015-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better request latency and size histograms</summary>
      <description>I just discussed this with a colleague.The get, put, etc, histograms that each region server keeps are somewhat useless (depending on what you want to achieve of course), as they are aggregated and calculated by each region server.It would be better to record the number of requests in certainly latency bands in addition to what we do now.For example the number of gets that took 0-5ms, 6-10ms, 10-20ms, 20-50ms, 50-100ms, 100-1000ms, &gt; 1000ms, etc. (just as an example, should be configurable).That way we can do further calculations after the fact, and answer questions like: How often did we miss our SLA? Percentage of requests that missed an SLA, etc.Comments?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Brainstorming</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.test.MetricsAssertHelperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.thrift.MetricsThriftServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.wal.MetricsWALSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.wal.MetricsEditsReplaySourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsSnapshotSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterFilesystemSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.balancer.MetricsBalancerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.test.MetricsAssertHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="1491" opendate="2009-6-6 00:00:00" fixdate="2009-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper errors: "Client has seen zxid 0xe our last zxid is 0xd"</summary>
      <description>We have been seeing a lot of these messages in tests:&amp;#91;junit&amp;#93; 2009-06-02 11:57:23,658 ERROR &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(514): Client has seen zxid 0xe our last zxid is 0xdThey usually repeat in a seemingly endless loop, such as: &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,083 INFO &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(737): Priming connection to java.nio.channels.SocketChannel&amp;#91;connected local=/0:0:0:0:0:0:0:1%0:56511 remote=localhost/0:0:0:0:0:0:0:1:21810&amp;#93; &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,084 INFO &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(889): Server connection successful &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,093 INFO &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(532): Connected to /0:0:0:0:0:0:0:1%0:56511 lastZxid 16 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 ERROR &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(543): Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 WARN &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(444): Exception causing close of session 0x0 due to java.io.IOException: Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 DEBUG &amp;#91;NIOServerCxn.Facto777ry:21810&amp;#93; server.NIOServerCnxn(447): IOException stack trace &amp;#91;junit&amp;#93; java.io.IOException: Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:544) &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:331) &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:176) &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 INFO &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(777): closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel&amp;#91;connected local=/0:0:0:0:0:0:0:1%0:21810 remote=/0:0:0:0:0:0:0:1%0:56511&amp;#93; &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,097 WARN &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(919): Exception closing session 0x121a2a7c43a0002 to sun.nio.ch.SelectionKeyImpl@2c662b4e &amp;#91;junit&amp;#93; java.io.IOException: Read error rc = -1 java.nio.DirectByteBuffer&amp;#91;pos=0 lim=4 cap=4&amp;#93; &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:653) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:897) &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,097 WARN &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(953): Ignoring exception during shutdown input &amp;#91;junit&amp;#93; java.net.SocketException: Socket is not connected &amp;#91;junit&amp;#93; at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) &amp;#91;junit&amp;#93; at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:640) &amp;#91;junit&amp;#93; at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:951) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:922)</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14922" opendate="2015-12-3 00:00:00" fixdate="2015-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delayed flush doesn&amp;#39;t work causing flush storms.</summary>
      <description>Starting all regionservers at the same time will mean that most PeriodicMemstoreFlusher's will be running at the same time. So all of these threads will queue flushes at about the same time.This was supposed to be mitigated by Delayed. However that isn't nearly enough. This results in the immediate filling up and then draining of the flush queues every hour.</description>
      <version>1.2.0,1.1.2,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestChoreService.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ChoreService.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.JitterScheduledThreadPoolExecutorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="14928" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14942" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow turning off BoundedByteBufferPool</summary>
      <description>The G1 does a great job of compacting, there's no reason to use the BoundedByteBufferPool when the JVM can it for us. So we should allow turning this off for people who are running new jvm's where the G1 is working well.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14946" opendate="2015-12-8 00:00:00" fixdate="2015-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow multi&amp;#39;s to over run the max result size.</summary>
      <description>If a user puts a list of tons of different gets into a table we will then send them along in a multi. The server un-wraps each get in the multi. While no single get may be over the size limit the total might be.We should protect the server from this. We should batch up on the server side so each RPC is smaller.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RetryImmediatelyException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MultiActionResultTooLarge.java</file>
    </fixedFiles>
  </bug>
  <bug id="14949" opendate="2015-12-8 00:00:00" fixdate="2015-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve name conflict when splitting if there are duplicated WAL entries</summary>
      <description>The AsyncFSHLog introduced in HBASE-14790 may write same WAL entries to different WAL files. WAL entry itself is idempotent so replay is not a problem but the intermediate file name and final name when splitting is constructed using the lowest or highest sequence id of the WAL entries written, so it is possible that different WAL files will have same intermediate or final file name when splitting. In the currentm implementation, this will cause split fail or data loss. We need to solve this.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14952" opendate="2015-12-8 00:00:00" fixdate="2015-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-assembly source artifact has some incorrect modules</summary>
      <description>After generating a tarball we noticed: that hbase-external-blockcache was missing. that hbase-spark is missing that there are duplicate hbase-shaded-{client,server} modules</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14974" opendate="2015-12-14 00:00:00" fixdate="2015-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Total number of Regions in Transition number on UI incorrect</summary>
      <description>Total number of Regions in Transition shows 100 when there are 100 or more regions in transition.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="14975" opendate="2015-12-14 00:00:00" fixdate="2015-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t color the total RIT line yellow if it&amp;#39;s zero</summary>
      <description>Right now if there are regions in transition, sometimes the RIT over 60 seconds line is colored yellow. It shouldn't be colored yellow if there are no regions that have been in transition too long.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="14976" opendate="2015-12-14 00:00:00" fixdate="2015-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add RPC call queues to the web ui</summary>
      <description>The size of the call queue for the regionserver is a critical metric to see if things are going too slowly. We should add the call queue size to the ui under the queues tab.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="14978" opendate="2015-12-15 00:00:00" fixdate="2015-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow Multi to retain too many blocks</summary>
      <description>Scans and Multi's have limits on the total size of cells that can be returned. However if those requests are not all pointing at the same blocks then the KeyValues can keep alive a lot more data than their size.Take the following example:A multi with a list of 10000 gets to a fat row. Each column being returned in in a different block. Each column is small 32 bytes or so.So the total cell size will be 32 * 10000 = ~320kb. However if each block is 128k then total retained heap size will be almost 2gigs.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="14979" opendate="2015-12-15 00:00:00" fixdate="2015-3-15 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Update to the newest Zookeeper release</summary>
      <description>ZOOKEEPER-706 is nice to have for anyone running replication that sometimes gets stalled. We should update to the latest patch version.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14984" opendate="2015-12-15 00:00:00" fixdate="2015-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow memcached block cache to set optimze to false</summary>
      <description>In order to keep latency consistent it might not be good to allow the spy memcached client to optimize.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="14999" opendate="2015-12-17 00:00:00" fixdate="2015-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove ref to org.mortbay.log.Log</summary>
      <description>I could see some 2 or 3 src files and many test files referring to org.mortbay.log.Log instead of commons Log. Patch correct all such places</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitWalDataLoss.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestDataBlockEncoders.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHBaseAdminNoCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ClientSideRegionScanner.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestClassFinder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.FlushRegionCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15005" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use value array in computing block length for 1.2 and 1.3</summary>
      <description>Follow on to HBASE-14978</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
    </fixedFiles>
  </bug>
  <bug id="15007" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update Hadoop support matrix to list 2.6.1+ as supported</summary>
      <description>The Hadoop community responded very well to our request for more maintenance releases and have now put out 2.6.1 - 2.6.3. The first of those included the fix for our catastrophic failure under HDFS encryption.We should update the book to point out those versions are fine.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15015" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checktyle plugin shouldn&amp;#39;t check Jamon-generated Java classes</summary>
      <description></description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15021" opendate="2015-12-21 00:00:00" fixdate="2015-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105  NullPointer Run 2: TestRSStatusServlet.testBasic:105  NullPointer Run 3: TestRSStatusServlet.testBasic:105  NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119  NullPointer Run 2: TestRSStatusServlet.testWithRegions:119  NullPointer Run 3: TestRSStatusServlet.testWithRegions:119  NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15036" opendate="2015-12-23 00:00:00" fixdate="2015-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase Spark documentation to include bulk load with thin records</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="1506" opendate="2009-6-9 00:00:00" fixdate="2009-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[performance] Make splits faster</summary>
      <description>Regionservers run splits. They close the region to split, divide it, and then tell master about the two new regions. Master then assigns new regions. New regions need to come up in new locations. Both regions are offline during this time.Instead, regionserver might run split as it does now but new, deploy the lower-half on the current regionserver immediately. It'd then inform master that it had split, and that it was serving the lower half. Master would then take care of assigning the upper half.Benefits would be that clients who were accessing the lower half of the split would not need to go through recalibration. They'd just keep working. There'd be disruption for those keys that landed in the top half of the split only.</description>
      <version>None</version>
      <fixedVersion>0.20.1,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15100" opendate="2016-1-13 00:00:00" fixdate="2016-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master WALProcs still never clean up</summary>
      <description>bin/hdfs dfs -ls /hbase/MasterProcWALs | wc -l218631</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormatReader.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormat.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.Procedure.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ProcedureInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="15104" opendate="2016-1-14 00:00:00" fixdate="2016-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Occasional failures due to NotServingRegionException in IT tests</summary>
      <description>IntegrationTestAcidGuarantees fails when trying to cleanup with NotServerRegionExceptions giving up (after 36 attempts) .5/11/09 09:19:24 INFO client.AsyncProcess: #33, waiting for some tasks to finish. Expected max=0, tasksInProgress=915/11/09 09:19:33 INFO client.AsyncProcess: #45, table=TestAcidGuarantees, attempt=10/35 failed=1ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region TestAcidGuarantees,test_row_1,1447089367019.032439ef4f3353cb894d20337ba043bc. is not online on node-4.internal,22101,1447089152259 at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2786) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:922) at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:1893) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32213) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2035) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107) at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130) at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107) at java.lang.Thread.run(Thread.java:745)...Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Mon Nov 09 09:19:53 PST 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68104: row 'test_row_1'Looked at the RS log, the following exception is found:2015-11-10 10:07:49,091 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=TestAcidGuarantees,,1447177733243.f1be6b850fe3958c5c9b5e330b5dfb00., starting to roll back the global memstore size.org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:102) at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:6011) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5995) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5967) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5938) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5894) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5845) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:356) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:126) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.2.0</version>
      <fixedVersion>0.94.28,1.2.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeCompressionAction.java</file>
    </fixedFiles>
  </bug>
  <bug id="15115" opendate="2016-1-15 00:00:00" fixdate="2016-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs complaints in hbase-client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.MultiRowRangeFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.LongComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FuzzyRowFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetryingCallerInterceptorFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MetricsConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HConnectable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15137" opendate="2016-1-20 00:00:00" fixdate="2016-3-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallTimeoutException and CallQueueTooBigException should trigger PFFE</summary>
      <description>If a region server is backed up enough that lots of calls are timing out then we should think about treating a server as failing.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFastFail.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.PreemptiveFastFailException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.ClientExceptionsUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.PreemptiveFastFailInterceptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.FastFailInterceptorContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15139" opendate="2016-1-20 00:00:00" fixdate="2016-1-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Connection manager doesn&amp;#39;t pass client metrics to RpcClient</summary>
      <description></description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="1514" opendate="2009-6-12 00:00:00" fixdate="2009-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hfile inspection tool</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="15146" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t block on Reader threads queueing to a scheduler queue</summary>
      <description>Blocking on the epoll thread is awful. The new rpc scheduler can have lots of different queues. Those queues have different capacity limits. Currently the dispatch method can block trying to add the the blocking queue in any of the schedulers.This causes readers to block, tcp acks are delayed, and everything slows down.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="15147" opendate="2016-1-21 00:00:00" fixdate="2016-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should use Admin.listTableNames() instead of Admin.listTables()</summary>
      <description>It seems that getTableDescriptors() in master checks for A and C permissions while getTableNames() checks for any privilege on the table. The reasoning is explained here: https://issues.apache.org/jira/browse/HBASE-12564?focusedCommentId=14234504&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14234504 We should change the shell command for list to use the getTableNames() version because of this. Otherwise a user having only R or W cannot list the table name. This has been reported from a user here: https://community.hortonworks.com/questions/10742/why-does-a-user-need-create-permission-for-list-co.html#comment-11000. While we are at it, should we revisit the fact that you cannot get a table descriptor if you have only R or W? It seems strange that you cannot even know the CF names of a table that you can read from. I could not find info about the "describe" privileges on SQL databases. However, if there are use cases where Table descriptor might contain sensitive info, the current semantics seems fine. cc apurtell and mbertozzi.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="1515" opendate="2009-6-13 00:00:00" fixdate="2009-6-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address part of config option hbase.regionserver unnecessary</summary>
      <description>We have a configuration option "hbase.regionserver" that specifies address + port the region servers should bind to. I believe all of our users don't require the address part of it and always leave it as 0.0.0.0. Most people rsync their configs to all the machines in the cluster, so anything other than 0.0.0.0 doesn't really make sense.We should change this option into just hbase.regionserver.port, like we have with the master now.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.OOMERegionServer.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15151" opendate="2016-1-21 00:00:00" fixdate="2016-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rely on nightly tests for findbugs compliance on existing branch</summary>
      <description>the "-1" for extant findbugs warnings has confused interpretation of our precommit checks enough that we should switch to non-strict mode.It will still record the number of findbugs warnings present before the patch, but it'll vote "0" rather than calling attention to things via a -1.</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.3,2.0.0,1.2.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="15200" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znode ACL checks should only compare the shortname</summary>
      <description>After HBASE-13768 we check at startup in secure configurations if our znodes have the correct ACLs. However when checking the ACL we compare the Kerberos fullname, which includes the host component. We should only compare the shortname, the principal. Otherwise in a multimaster configuration we will unnecessarily reset ACLs whenever any master running on a host other than the one that initialized the ACLs makes the check. You can imagine this happening multiple times in a rolling restart scenario.</description>
      <version>1.2.0,1.0.3,1.1.3,0.98.17,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="15201" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase-spark to hbase assembly</summary>
      <description>hbase-spark currently is missing from hbase assembly.We should add it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15203" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce garbage created by path.toString() during Checksum verification</summary>
      <description>When we try to read a block we do checksum verification for which we need the file name in which the block belongs to. So we do Path.toString() every time. This seems to create around 163MB of char[] that is garbage collected in a simple scan run. It is also visible in writes but the impact is lesser. In overall write/read profile the top 2 factors are byte[] and char[]. This toString() can easily be avoided and reduce its share from the total. To make it more precise in 1 min of profiling, among the 1.8G of garbage created by StringBuilder.toString - this path.toString() was contributing around 3.5%. After the patch this is totally not there.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.ChecksumUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15206" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey testSplitDaughtersNotInMeta test</summary>
      <description>Run into the following failure with hbase 1.0.0.Stacktracejava.lang.AssertionError: nullat org.junit.Assert.fail(Assert.java:86)at org.junit.Assert.assertTrue(Assert.java:41)at org.junit.Assert.assertNotNull(Assert.java:712)at org.junit.Assert.assertNotNull(Assert.java:722)at org.apache.hadoop.hbase.util.TestHBaseFsck.testSplitDaughtersNotInMeta(TestHBaseFsck.java:1723)From the log, the ntp issue caused clock skew and it woke up CatalogJanitor earlier. The CatalogJanitor cleaned up the parent region. It could happen with master branch as well. The fix is to disable CatalogJanitor to make sure this will not happen.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
    </fixedFiles>
  </bug>
  <bug id="15212" opendate="2016-2-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RPCServer should enforce max request size</summary>
      <description>A TODO from HBASE-15177 was that we are not protecting the RPCServer in case an RPC request with a very large size is received. This might cause the server to go OOM because we are allocating the RPC serialization into a BB. Instead we should reject the RPC and close the connection.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.AbstractTestIPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15222" opendate="2016-2-5 00:00:00" fixdate="2016-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use less contended classes for metrics</summary>
      <description>Running the benchmarks now, but it looks like the results are pretty extreme. The locking in our histograms is pretty extreme.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.AgeSnapshot.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.metrics.TestBaseSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableTimeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableSizeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableRangeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MetricsExecutorImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MetricMutableQuantiles.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.thrift.MetricsThriftServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.rest.MetricsRESTSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSinkSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.wal.MetricsWALSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsSnapshotSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterFilesystemSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.balancer.MetricsBalancerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.metrics2.MetricHistogram.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseSource.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFastLongHistogram.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.FastLongHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="15255" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pointer to linkedin blog on putting jvm logs on fast disk</summary>
      <description>Add pointer to linked in blog: https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-trafficIIRC, tsdb says to do similar.Also add into perf section note on native crc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15259" opendate="2016-2-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALEdits under replay will also be replicated</summary>
      <description>I need to verify this but seeing the codetry { // We are about to append this edit; update the region-scoped sequence number. Do it // here inside this single appending/writing thread. Events are ordered on the ringbuffer // so region sequenceids will also be in order. regionSequenceId = entry.stampRegionSequenceId(); // Edits are empty, there is nothing to append. Maybe empty when we are looking for a // region sequence id only, a region edit/sequence id that is not associated with an actual // edit. It has to go through all the rigmarole to be sure we have the right ordering. if (entry.getEdit().isEmpty()) { return; } // Coprocessor hook. if (!coprocessorHost.preWALWrite(entry.getHRegionInfo(), entry.getKey(), entry.getEdit())) { if (entry.getEdit().isReplay()) { // Set replication scope null so that this won't be replicated entry.getKey().setScopes(null); } } if (!listeners.isEmpty()) { for (WALActionsListener i: listeners) { // TODO: Why does listener take a table description and CPs take a regioninfo? Fix. i.visitLogEntryBeforeWrite(entry.getHTableDescriptor(), entry.getKey(), entry.getEdit()); } }When a WALEdit is in replay we set the Logkey to null. But in the visitLogEntryBeforeWrite() we again set the LogKey based on the replication scope associated with the cells. So the previous step of setting null does not work here?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSmallTests.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
    </fixedFiles>
  </bug>
  <bug id="1526" opendate="2009-6-15 00:00:00" fixdate="2009-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapreduce fixup</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.DisabledTestTableMapReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexConfiguration.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.BuildTableIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="15261" opendate="2016-2-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Make Throwable t in DaughterOpener volatile</summary>
      <description>In the region split process, daughter regions are opened in different threads, Throwable t is set in these threads and it is checked in the calling thread. Need to make it volatile so the checking will not miss any exceptions from opening daughter regions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15285" opendate="2016-2-17 00:00:00" fixdate="2016-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forward-port respect for isReturnResult from HBASE-15095</summary>
      <description>This issue is about forward-porting the bug fix done in HBASE-15095 so we respect the isReturnResult properly in append and increment.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Mutation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Increment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Append.java</file>
    </fixedFiles>
  </bug>
  <bug id="15297" opendate="2016-2-20 00:00:00" fixdate="2016-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>error message is wrong when a wrong namspace is specified in grant in hbase shell</summary>
      <description>In HBase shell, specify a non-existing namespace in "grant" command, such ashbase(main):001:0&gt; grant 'a1', 'R', '@aaa' &lt;--- there is no namespace called "aaa"The error message issued is not correctERROR: Unknown namespace a1!a1 is the user name, not the namespace.The following error message would be betterERROR: Unknown namespace aaa!orCan't find a namespace: aaa</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.security.rb</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15298" opendate="2016-2-20 00:00:00" fixdate="2016-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing or wrong asciidoc anchors in the reference guide</summary>
      <description>There are some missing or wrong asciidoc anchors in the reference guide.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ycsb.adoc</file>
      <file type="M">src.main.asciidoc..chapters.unit.testing.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.preface.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.compression.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.contributing.to.documentation.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15302" opendate="2016-2-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reenable the other tests disabled by HBASE-14678</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="15306" opendate="2016-2-22 00:00:00" fixdate="2016-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make RPC call queue length dynamically configurable</summary>
      <description>Sometimes it's useful to be able to modify call queue size on prod cluster without having to do a rolling restart. Here setting fixed hard limit on call queue size and soft limit which is dynamically-updatable.https://reviews.facebook.net/D54579 for review, will test more.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="1532" opendate="2009-6-17 00:00:00" fixdate="2009-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UI Visibility into ZooKeeper</summary>
      <description>Add ZooKeeper information/administration to UI.Discussion showed particular interest in a tree-viewer application, something like ZOOKEEPER-418.There was talk between Lars/JimK about how often the viewer should update its data.See HBASE-1329 for more information.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15325" opendate="2016-2-25 00:00:00" fixdate="2016-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ResultScanner allowing partial result will miss the rest of the row if the region is moved between two rpc requests</summary>
      <description>HBASE-11544 allow scan rpc return partial of a row to reduce memory usage for one rpc request. And client can setAllowPartial or setBatch to get several cells in a row instead of the whole row.However, the status of the scanner is saved on server and we need this to get the next part if there is a partial result before. If we move the region to another RS, client will get a NotServingRegionException and open a new scanner to the new RS which will be regarded as a new scan from the end of this row. So the rest cells of the row of last result will be missing.</description>
      <version>1.2.0,1.1.3</version>
      <fixedVersion>1.3.0,1.2.1,1.1.5,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellComparator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="15354" opendate="2016-2-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use same criteria for clearing meta cache for all operations</summary>
      <description>Currently we do not clear/update meta cache for some special exceptions if the operation went through AsyncProcess#submit like HTable#put calls. But, we clear meta cache without checking for these special exceptions in case of other operations like gets, deletes etc because they directly go through the RpcRetryingCaller#callWithRetries instead of the AsyncProcess.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMetaCache.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionAdminServiceCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15358" opendate="2016-2-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>canEnforceTimeLimitFromScope should use timeScope instead of sizeScope</summary>
      <description>A small but maybe critical bug</description>
      <version>1.2.0,1.3.0,1.1.3,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,1.1.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScannerContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15376" opendate="2016-3-2 00:00:00" fixdate="2016-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScanNext metric is size-based while every other per-operation metric is time based</summary>
      <description>We have per-operation metrics for Get, Mutate, Delete, Increment, and ScanNext. The metrics are emitted like: "Get_num_ops" : 4837505, "Get_min" : 0, "Get_max" : 296, "Get_mean" : 0.2934618155433431, "Get_median" : 0.0, "Get_75th_percentile" : 0.0, "Get_95th_percentile" : 1.0, "Get_99th_percentile" : 1.0,... "ScanNext_num_ops" : 194705, "ScanNext_min" : 0, "ScanNext_max" : 18441, "ScanNext_mean" : 7468.274651395701, "ScanNext_median" : 583.0, "ScanNext_75th_percentile" : 583.0, "ScanNext_95th_percentile" : 13481.0, "ScanNext_99th_percentile" : 13481.0,The problem is that all of Get,Mutate,Delete,Increment,Append,Replay are time based tracking how long the operation ran, while ScanNext is tracking returned response sizes (returned cell-sizes to be exact). Obviously, this is very confusing and you would only know this subtlety if you read the metrics collection code. Not sure how useful is the ScanNext metric as it is today. We can deprecate it, and introduce a time based one to keep track of scan request latencies. ps. Shamelessly using the parent jira (since these seem relavant).</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15377" opendate="2016-3-2 00:00:00" fixdate="2016-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Per-RS Get metric is time based, per-region metric is size-based</summary>
      <description>We have metrics for Get operations at the region server level and region level. "Get_num_ops" : 4837505, "Get_min" : 0, "Get_max" : 296, "Get_mean" : 0.2934618155433431, "Get_median" : 0.0, "Get_75th_percentile" : 0.0, "Get_95th_percentile" : 1.0, "Get_99th_percentile" : 1.0,and "Namespace_hbase_table_meta_region_1588230740_metric_get_num_ops" : 103, "Namespace_hbase_table_meta_region_1588230740_metric_get_min" : 450, "Namespace_hbase_table_meta_region_1588230740_metric_get_max" : 470, "Namespace_hbase_table_meta_region_1588230740_metric_get_mean" : 450.19417475728153, "Namespace_hbase_table_meta_region_1588230740_metric_get_median" : 460.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_75th_percentile" : 470.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_95th_percentile" : 470.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_99th_percentile" : 470.0,The problem is that the report values for the region server shows the latency, versus the reported values for the region shows the response sizes. There is no way of telling this without reading the source code. I think we should deprecate response size histograms in favor of latency histograms. See also HBASE-15376.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15406" opendate="2016-3-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Split / merge switch left disabled after early termination of hbck</summary>
      <description>This was what I did on cluster with 1.4.0-SNAPSHOT built Thursday:Run 'hbase hbck -disableSplitAndMerge' on gateway node of the clusterTerminate hbck earlyEnter hbase shell where I observed:hbase(main):001:0&gt; splitormerge_enabled 'SPLIT'false0 row(s) in 0.3280 secondshbase(main):002:0&gt; splitormerge_enabled 'MERGE'false0 row(s) in 0.0070 secondsExpectation is that the split / merge switches should be restored to default value after hbck exits.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSplitOrMergeStatus.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.SplitOrMergeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15407" opendate="2016-3-7 00:00:00" fixdate="2016-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SASL support for fan out OutputStream</summary>
      <description>Otherwise we can not use it in secure environment.Should be a netty handler, but seehttps://github.com/netty/netty/issues/1966I do not think it will be available in the near future, so we need to do it by ourselves.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFanOutOneBlockAsyncDFSOutput.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FanOutOneBlockAsyncDFSOutputHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FanOutOneBlockAsyncDFSOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="15422" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Avoid double yield</summary>
      <description>ServerCrashProcedure is using a combination of isYieldBeforeExecuteFromState() and ProcedureYieldException, which may end up in yielding twice. (ServerCrashProcedure is the only user of yield)</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestYieldProcedures.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="15441" opendate="2016-3-10 00:00:00" fixdate="2016-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix WAL splitting when region has moved multiple times</summary>
      <description>Currently WAL splitting is broken when a region has been opened multiple times in recent minutes.Region open and region close write event markers to the wal. These markers should have the sequence id in them. However it is currently getting 1. That means that if a region has moved multiple times in the last few mins then multiple split log workers will try and create the recovered edits file for sequence id 1. One of the workers will fail and on failing they will delete the recovered edits. Causing all split wal attempts to fail.We need to: make sure that close get the correct sequence id for open. Filter all region events from recovered editsIt appears that the close event with a sequence id of one is coming from region warm up.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15451" opendate="2016-3-13 00:00:00" fixdate="2016-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary wait in MVCC</summary>
      <description>Currently the return value of MVCC#complete indicates whether readPoint already advanced over write number of the given WriteEntry:return readPoint.get() &gt;= writeEntry.getWriteNumber();While in MVCC#checkAndWait we never take usage of this but always call waitForRead which will acquire and release lock on readWaiters and cause additional context switch. This JIRA will improve this logic and remove the unnecessary wait.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15460" opendate="2016-3-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix infer issues in hbase-common</summary>
      <description></description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.test.LoadTestKVGenerator.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JVM.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.DynamicClassLoader.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Base64.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.SingleByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.HFileBlockDefaultDecodingContext.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="15461" opendate="2016-3-14 00:00:00" fixdate="2016-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ref guide has bad links to blogs originally posted on cloudera website</summary>
      <description>The ref guide section on "Secure Client Access to Apache HBase" starts with a link to a blog post from Matteo, but the link is broken.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.other.info.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15463" opendate="2016-3-14 00:00:00" fixdate="2016-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region normalizer should check whether split/merge is enabled</summary>
      <description>HBASE-15128 added switch for disabling split / merge.When split / merge switch is turned off, region normalizer should not perform split / merge operation, respectively.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.RegionNormalizer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="15464" opendate="2016-3-15 00:00:00" fixdate="2016-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flush / Compaction metrics revisited</summary>
      <description>We can add a couple of metrics related to flushes and compactions: flush memstore and output file size histogram: This will allow seeing whether we are flushing too early due to memory pressure, too many regions, etc. Tracking flush memstore size vs output file size is useful in understanding the block encoding compression benefits. total flushed output bytes: This will allow to monitor the IO / throughput from flushers. You can use this to set num flushers, flush throttle, etc. smallCompactionQueueLength / large...: This is tracked, but not emitted anymore due to a bug. compaction time histogram: similar to flush time histogram, how long compactions are taking. compaction input num files / output num files histogram: How many files on average we are compacting. Stripe compaction / date tiered compaction can use the num output files metric. compaction input / output data sizes histogram: How much data on average we are compacting. compaction input / output total bytes: Measure compaction IO / throughput. measure write amplification, enables to set compaction throttle. Breakdown for above for major compactions</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeStoreEngine.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MockRegionServerServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFlushContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15465" opendate="2016-3-15 00:00:00" fixdate="2016-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>userPermission returned by getUserPermission() for the selected namespace does not have namespace set</summary>
      <description>The request sent is with type = Namespace, but the response returned contains Global permissions (that is, the field of namespace is not set)It is in hbase-client/src/main/java/org/apache/hadoop/hbase/protobuf/ProtobufUtil.java, from line 2380, and I made some comments into it/** * A utility used to get permissions for selected namespace. * &lt;p&gt; * It's also called by the shell, in case you want to find references. * * @param protocol the AccessControlService protocol proxy * @param namespace name of the namespace * @throws ServiceException */ public static List&lt;UserPermission&gt; getUserPermissions( AccessControlService.BlockingInterface protocol, byte[] namespace) throws ServiceException { AccessControlProtos.GetUserPermissionsRequest.Builder builder = AccessControlProtos.GetUserPermissionsRequest.newBuilder(); if (namespace != null) { builder.setNamespaceName(ByteStringer.wrap(namespace)); } builder.setType(AccessControlProtos.Permission.Type.Namespace); //builder is set with type = Namespace AccessControlProtos.GetUserPermissionsRequest request = builder.build(); //I printed the request, its type is Namespace, which is correct. AccessControlProtos.GetUserPermissionsResponse response = protocol.getUserPermissions(null, request);/* I printed the response, it contains Global permissions, as below, not a Namespace permission.user_permission { user: "a1" permission { type: Global global_permission { action: READ action: WRITE action: ADMIN action: EXEC action: CREATE } }}AccessControlProtos.GetUserPermissionsRequest has a member called type_ to store the type, but AccessControlProtos.GetUserPermissionsResponse does not.*/ List&lt;UserPermission&gt; perms = new ArrayList&lt;UserPermission&gt;(response.getUserPermissionCount()); for (AccessControlProtos.UserPermission perm: response.getUserPermissionList()) { perms.add(ProtobufUtil.toUserPermission(perm)); // (1) } return perms; }it could be more reasonable to return user permissions with namespace set in getUserPermission() for selected namespace ?</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
    </fixedFiles>
  </bug>
  <bug id="15466" opendate="2016-3-15 00:00:00" fixdate="2016-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>precommit should not run all java goals when given a docs-only patch</summary>
      <description>Right now docs-only patches (those that only impact the top level src/main/site, src/main/asciidoc, or src/main/xslt) run through all of the java related precommit checks, including test4tests and the full unit test suite.Since we know these paths don't require those checks, we should update our personality to skip them. (or fix our project structure to match "the maven way".)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,1.4.4,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15470" opendate="2016-3-16 00:00:00" fixdate="2016-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a setting for Priority queue length</summary>
      <description>Meta can have very different request rates than any other region. We shouldn't use the same call queue length.For example:If a normal request takes 100ms ( long scan or large get )but a call to meta is all in memory so it takes &lt; 1msSo a call queue length of 100 represents multiple seconds of work for normal requests, but less than a second for meta.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="15471" opendate="2016-3-16 00:00:00" fixdate="2016-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add num calls in priority and general queue to RS UI</summary>
      <description>1.2 added the queue size. We should add the number of calls in the queue.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="15473" opendate="2016-3-17 00:00:00" fixdate="2016-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation for the usage of hbase dataframe user api (JSON, Avro, etc)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15479" opendate="2016-3-17 00:00:00" fixdate="2016-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No more garbage or beware of autoboxing</summary>
      <description>Quick journey with JMC in a profile mode revealed very interesting and unexpected heap polluter on a client side. Patch will shortly follow.</description>
      <version>1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,0.98.19,1.1.5,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="15485" opendate="2016-3-18 00:00:00" fixdate="2016-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter.reset() should not be called between batches</summary>
      <description>As discussed in HBASE-15325, now we will resetFilters if partial result not formed, but we should not reset filters when batch limit reached</description>
      <version>1.2.0,1.1.3</version>
      <fixedVersion>1.3.0,1.1.5,1.2.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScannerContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="15519" opendate="2016-3-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add per-user metrics</summary>
      <description>Per-user metrics will be useful in multi-tenant cases where we can emit number of requests, operations, num RPCs etc at the per-user aggregate level per regionserver. We currently have throttles per user, but no way to monitor resource usage per-user. Looking at these metrics, operators can adjust throttles, do capacity planning, etc per-user.</description>
      <version>1.2.0</version>
      <fixedVersion>2.3.0,3.0.0-alpha-3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestLossyCounting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsTableLatencies.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.LossyCounting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MetaTableMetrics.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactory.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsUserSourceImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15551" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make call queue too big exception use servername</summary>
      <description>If the rpc server is listening to something other than the hostname ( 0.0.0.0 for example ) then the exception thrown isn't very helpful. We should make that more helpful.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15554" opendate="2016-3-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StoreFile$Writer.appendGeneralBloomFilter generates extra KV</summary>
      <description>Accounts for 10% memory allocation in compaction thread when BloomFilterType is ROWCOL.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterChunk.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CompoundBloomFilter.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.MurmurHash3.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.MurmurHash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15557" opendate="2016-3-29 00:00:00" fixdate="2016-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guidance on HashTable/SyncTable to the RefGuide</summary>
      <description>The docs for SyncTable are insufficient. Brief description from davelatham HBASE-13639 comment:Sorry for the lack of better documentation, Abhishek Soni. Thanks for bringing it up. I'll try to provide a better explanation. You may have already seen it, but if not, the design doc linked in the description above may also give you some better clues as to how it should be used.Briefly, the feature is intended to start with a pair of tables in remote clusters that are already substantially similar and make them identical by comparing hashes of the data and copying only the diffs instead of having to copy the entire table. So it is targeted at a very specific use case (with some work it could generalize to cover things like CopyTable and VerifyRepliaction but it's not there yet). To use it, you choose one table to be the "source", and the other table is the "target". After the process is complete the target table should end up being identical to the source table.In the source table's cluster, run org.apache.hadoop.hbase.mapreduce.HashTable and pass it the name of the source table and an output directory in HDFS. HashTable will scan the source table, break the data up into row key ranges (default of 8kB per range) and produce a hash of the data for each range.Make the hashes available to the target cluster - I'd recommend using DistCp to copy it across.In the target table's cluster, run org.apache.hadoop.hbase.mapreduce.SyncTable and pass it the directory where you put the hashes, and the names of the source and destination tables. You will likely also need to specify the source table's ZK quorum via the --sourcezkcluster option. SyncTable will then read the hash information, and compute the hashes of the same row ranges for the target table. For any row range where the hash fails to match, it will open a remote scanner to the source table, read the data for that range, and do Puts and Deletes to the target table to update it to match the source.I hope that clarifies it a bit. Let me know if you need a hand. If anyone wants to work on getting some documentation into the book, I can try to write some more but would love a hand on turning it into an actual book patch.</description>
      <version>1.2.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15623" opendate="2016-4-9 00:00:00" fixdate="2016-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update refguide to change hadoop &lt;= 2.3.x from NT to X for hbase-1.2.x</summary>
      <description>This issue is about updating our hadoop supported versions grid in the prerequisites section of refguide. Here is thread proposing this change up on dev list: http://osdir.com/ml/general/2016-04/msg09194.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15666" opendate="2016-4-17 00:00:00" fixdate="2016-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded dependencies for hbase-testing-util</summary>
      <description>Folks that make use of our shaded client but then want to test things using the hbase-testing-util end up getting all of our dependencies again in the test scope.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE.txt</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-it.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15667" opendate="2016-4-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more clarity to Reference Guide related to importing Eclipse Formatter</summary>
      <description>In Hbase Reference Guide: 141.1.1. Code Formattingin procedure bullet point 2: It is not clear what the menu item is. It should be changed to the following:"In Preferences, click Java-&gt;Code Style-&gt;Formatter"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15698" opendate="2016-4-23 00:00:00" fixdate="2016-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment TimeRange not serialized to server</summary>
      <description>Before HBase-1.2, the Increment TimeRange set on the client was serialized over to the server. As of HBase 1.2, this appears to no longer be true, as my preIncrement coprocessor always gets HConstants.LATEST_TIMESTAMP as the value of increment.getTimeRange().getMax() regardless of what the client has specified.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15808" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce potential bulk load intermediate space usage and waste</summary>
      <description>If the bulk load input files do not match the existing region boudaries, the files will be splitted.In the unfornate cases where the files need to be splitted multiple times,the process can consume unnecessary space and can even cause out of space.Here is over-simplified example.Orinal size of input files: consumed space: size --&gt; 300GBAfter a round of splits: consumed space: size + tmpspace1 --&gt; 300GB + 300GBAfter another round of splits: consumded space: size + tmpspace1 + tmpspace2 --&gt; 300GB + 300GB + 300GB..Currently we don't do any cleanup in the process. At least all the intermediate tmpspace (not the last one) can be deleted in the process.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug id="15889" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>String case conversions are locale-sensitive, used without locale</summary>
      <description>Static code analysis is flagging cases of String.toLowerCase and String.toUpperCase being used without Locale. From the API reference:Note: This method is locale sensitive, and may produce unexpected results if used for strings that are intended to be interpreted locale independently. Examples are programming language identifiers, protocol keys, and HTML tags. For instance, "TITLE".toLowerCase() in a Turkish locale returns "t\u0131tle", where '\u0131' is the LATIN SMALL LETTER DOTLESS I character. To obtain correct results for locale insensitive strings, use toLowerCase(Locale.ROOT).Many uses of these functions do appear to be looking up classes, etc. and not dealing with stored data, so I'd think there aren't significant compatibility problems here and specifying the locale is indeed the safer way to go.</description>
      <version>1.2.0</version>
      <fixedVersion>1.4.0,0.98.20,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HThreadedSelectorServerArgs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.LoadTestTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TimedOutTestsListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIPv6NIOServerSocketChannel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerHostname.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.DataBlockEncodingTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScanBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ServerCommandLine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionMover.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.DirectMemoryUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.HBaseSaslRpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.BaseRowProcessor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.filter.GzipFilter.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.StripeCompactionsPerformanceEvaluation.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.RESTApiClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.test.MetricsAssertHelperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceFactoryImpl.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.crypto.KeyStoreKeyProvider.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.MetaTableLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.util.PoolMap.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AsyncRpcChannelImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">hbase-annotations.src.main.java.org.apache.hadoop.hbase.classification.tools.StabilityOptions.java</file>
    </fixedFiles>
  </bug>
  <bug id="15890" opendate="2016-5-25 00:00:00" fixdate="2016-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow thrift to set/unset "cacheBlocks" for Scans</summary>
      <description>Long running scans going through thrift cache everything to the block cache. We need the ability to disable caching for scans going through thrift.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.20,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="15895" opendate="2016-5-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unmaintained jenkins build analysis tool.</summary>
      <description>See HBASE-15889. We don't actually maintain the "buildstats" module any more.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.0.4,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-tools.README.md</file>
      <file type="M">dev-support.jenkins-tools.pom.xml</file>
      <file type="M">dev-support.jenkins-tools.buildstats.src.main.java.org.apache.hadoop.hbase.devtools.buildstats.TestSuite.java</file>
      <file type="M">dev-support.jenkins-tools.buildstats.src.main.java.org.apache.hadoop.hbase.devtools.buildstats.TestResultHistory.java</file>
      <file type="M">dev-support.jenkins-tools.buildstats.src.main.java.org.apache.hadoop.hbase.devtools.buildstats.TestCaseResult.java</file>
      <file type="M">dev-support.jenkins-tools.buildstats.src.main.java.org.apache.hadoop.hbase.devtools.buildstats.HistoryReport.java</file>
      <file type="M">dev-support.jenkins-tools.buildstats.src.main.java.org.apache.hadoop.hbase.devtools.buildstats.BuildResultWithTestCaseDetails.java</file>
      <file type="M">dev-support.jenkins-tools.buildstats.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15925" opendate="2016-5-31 00:00:00" fixdate="2016-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compat-module maven variable not evaluated</summary>
      <description>Looks like we've regressed on HBASE-8488. Have a look at the dependency artifacts list on http://mvnrepository.com/artifact/org.apache.hbase/hbase-testing-util/1.2.1. Notice the direct dependency's artifactId is ${compat.module}.</description>
      <version>1.0.0,1.1.0,1.2.0,1.2.1,1.0.3,1.1.5</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15984" opendate="2016-6-7 00:00:00" fixdate="2016-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Given failure to parse a given WAL that was closed cleanly, replay the WAL.</summary>
      <description>subtask for a general work around for "underlying reader failed / is in a bad state" just for the case where a WAL 1) was closed cleanly and 2) we can tell that our current offset ought not be the end of parseable entries.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,0.98.23,1.2.4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationWALReaderManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15985" opendate="2016-6-7 00:00:00" fixdate="2016-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>clarify promises about edits from replication in ref guide</summary>
      <description>we should make clear in a call out that replication only provides at-least-once delivery and doesn't guarantee ordering so that e.g. folks using increments aren't surprised.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15989" opendate="2016-6-8 00:00:00" fixdate="2016-6-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hbase.online.schema.update.enable</summary>
      <description>Pre discussion on HBASE-15981, the configuration hbase.online.schema.update.enable is an artifact of a bygone era. Lets rip it out.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">hbase-shell.src.test.rsgroup.org.apache.hadoop.hbase.client.rsgroup.TestShellRSGroups.java</file>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.AbstractTestShell.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.alter.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDefaultVisLabelService.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelReplicationWithExpAsString.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEncryptionKeyRotation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestTableLockManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestChangingEncoding.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterDDLOperationHelper.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="16096" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication keeps accumulating znodes</summary>
      <description>If there is an error while creating the replication source on adding the peer, the source if not added to the in memory list of sources but the replication peer is. However, in such a scenario, when you remove the peer, it is deleted from zookeeper successfully but for removing the in memory list of peers, we wait for the corresponding sources to get deleted (which as we said don't exist because of error creating the source). The problem here is the ordering of operations for adding/removing source and peer. Modifying the code to always remove queues from the underlying storage, even if there exists no sources also requires a small refactoring of TableBasedReplicationQueuesImpl to not abort on removeQueues() of an empty queue</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="1615" opendate="2009-7-6 00:00:00" fixdate="2009-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-1597 introduced a bug when compacting after a split</summary>
      <description>The way HBASE-1597 instantiated the CompactionReader ignored whether the Reader it came from was a half reader or not.Need to reimplement CompactionReader.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HalfHFileReader.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16183" opendate="2016-7-6 00:00:00" fixdate="2016-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct errors in example programs of coprocessor in Ref Guide</summary>
      <description>There are some errors in the example programs for coprocessor in Ref Guide. Such as using deprecated APIs, generic...</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16221" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server drops connection on long scans</summary>
      <description>Thrift servers use connection cache and we drop connections after hbase.thrift.connection.max-idletime milliseconds from the last time a connection object was accessed. However, we never update this last accessed time on scan path. By default, this will cause scanners to fail after 10 minutes, if the underlying connection object is not being used along other operation paths (like put).</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ConnectionCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="16296" opendate="2016-7-28 00:00:00" fixdate="2016-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reverse scan performance degrades when using filter lists</summary>
      <description>When a reverse scan is done, the server seems to not know it's done when the scanner cache size matches the number of rows in a PageFilter. See PHOENIX-3121 for how this manifests itself. We have a standalone, pure HBase API reproducer too that I'll attach (courtesy of churromorales and mujtabachohan).</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.6,0.98.21,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="16530" opendate="2016-8-30 00:00:00" fixdate="2016-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce DBE code duplication</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.NoOpDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.RowIndexSeekerV1.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.RowIndexEncoderV1.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.RowIndexCodecV1.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.CopyKeyDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
    </fixedFiles>
  </bug>
  <bug id="16678" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce jobs do not update counters from ScanMetrics</summary>
      <description>Was inspecting a perf issue, where we needed the scanner metrics as counters for a MR job. Turns out that the HBase scan counters are no longer working in 1.0+. I think it got broken via HBASE-13030. These are the counters: HBase Counters BYTES_IN_REMOTE_RESULTS=0 BYTES_IN_RESULTS=280 MILLIS_BETWEEN_NEXTS=11 NOT_SERVING_REGION_EXCEPTION=0 NUM_SCANNER_RESTARTS=0 NUM_SCAN_RESULTS_STALE=0 REGIONS_SCANNED=1 REMOTE_RPC_CALLS=0 REMOTE_RPC_RETRIES=0 RPC_CALLS=3 RPC_RETRIES=0</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16682" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Shell tests failure. NoClassDefFoundError for MiniKdc</summary>
      <description>Stacktracejava.lang.NoClassDefFoundError: org/apache/hadoop/minikdc/MiniKdc at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.minikdc.MiniKdc at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17502" opendate="2017-1-21 00:00:00" fixdate="2017-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document hadoop pre-2.6.1 and Java 1.8 Kerberos problem in our hadoop support matrix</summary>
      <description>Hadoop pre-2.6.1 on JDK 1.8 has problem with Kerberos keytabe relogin.HADOOP-10786 fixed the problem in Hadoop 2.6.1.Let's document it in the Hadoop support matrix.This was brought up in HBase 1.3.0 RC0 voting.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17514" opendate="2017-1-23 00:00:00" fixdate="2017-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Warn when Thrift Server 1 is configured for proxy users but not the HTTP transport</summary>
      <description>The config hbase.thrift.support.proxyuser is ignored if the Thrift Server 1 isn't configured to use an HTTP transport with hbase.regionserver.thrift.http.We should emit a warning if our configs request proxy user support but don't specify that HTTP should be used for the transport.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17516" opendate="2017-1-23 00:00:00" fixdate="2017-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table quota not taking precedence over namespace quota</summary>
      <description>Romil Choksi found a bug in the current patch-set where a more restrictive table quota did not take priority over a less-restrictive namespace quota.Turns out some of the logic to handle this case was incorrect.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17518" opendate="2017-1-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Reference Guide has a syntax error</summary>
      <description>The image of "HFile Version 2 Structure" in Appendix F of HBase Reference Guide (pdf) is missing because of a wrong asciidoc syntax:image:hfilev2.png&amp;#91;HFile Version 2&amp;#93;modified as:image::hfilev2.png&amp;#91;HFile Version 2&amp;#93;it should be a double colon instead of single one</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17520" opendate="2017-1-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isTableEnabled/Disabled/Available methods</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17522" opendate="2017-1-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeExceptions from MemoryMXBean should not take down server process</summary>
      <description>RegionServer died after MemoryMXBean threw an IllegalArgumentException while attempting to create a MemoryUsage object for the heap during construction of the server load.We shouldn't allow failure to get load information to take down the RS.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HeapMemoryManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.util.MemorySizeUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="17707" opendate="2017-2-28 00:00:00" fixdate="2017-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>New More Accurate Table Skew cost function/generator</summary>
      <description>This patch includes new version of the TableSkewCostFunction and a new TableSkewCandidateGenerator.The new TableSkewCostFunction computes table skew by counting the minimal number of region moves required for a given table to perfectly balance the table across the cluster (i.e. as if the regions from that table had been round-robin-ed across the cluster). This number of moves is computer for each table, then normalized to a score between 0-1 by dividing by the number of moves required in the absolute worst case (i.e. the entire table is stored on one server), and stored in an array. The cost function then takes a weighted average of the average and maximum value across all tables. The weights in this average are configurable to allow for certain users to more strongly penalize situations where one table is skewed versus where every table is a little bit skewed. To better spread this value more evenly across the range 0-1, we take the square root of the weighted average to get the final value.The new TableSkewCandidateGenerator generates region moves/swaps to optimize the above TableSkewCostFunction. It first simply tries to move regions until each server has the right number of regions, then it swaps regions around such that each region swap improves table skew across the cluster.We tested the cost function and generator in our production clusters with 100s of TBs of data and 100s of tables across dozens of servers and found both to be very performant and accurate.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancer2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1771" opendate="2009-8-17 00:00:00" fixdate="2009-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE sequentialWrite is 7x slower because of MemStoreFlusher#checkStoreFileCount</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1772" opendate="2009-8-17 00:00:00" fixdate="2009-8-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up the default ZK session timeout from 30seconds to 60seconds</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="18049" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It is not necessary to re-open the region when MOB files cannot be found</summary>
      <description>In HBASE-17712, we try to re-open the region when store files cannot be found. This is useful for store files in a region, but is not necessary when the MOB files cannot be found, because the store files in a region only contain the references to the MOB files and a re-open of a region doesn't help the lost MOB files.In this JIRA, we will directly throw DNRIOE only when the MOB files are not found in MobStoreScanner and ReversedMobStoreScanner. Other logics keep the same.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="18263" opendate="2017-6-23 00:00:00" fixdate="2017-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve NPE in backup Master UI when access to procedures.jsp</summary>
      <description>When accessing procedures.jsp ,the NPE comes in backup master UI:HTTP ERROR 500Problem accessing /procedures.jsp. Reason: INTERNAL_SERVER_ERRORCaused by:java.lang.NullPointerException at org.apache.hadoop.hbase.generated.master.procedures_jsp._jspService(procedures_jsp.java:67) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) at org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:113) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.ClickjackingPreventionFilter.doFilter(ClickjackingPreventionFilter.java:48) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1354) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)In server ,only the active master initialize procedureStore in HMaster. so ,i think it will be better to remove procedures.jsp link in backup Master UI</description>
      <version>1.2.0,2.0.0-alpha-1</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="18327" opendate="2017-7-6 00:00:00" fixdate="2017-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>redo test-patch personality &amp;#39;hadoopcheck&amp;#39; to better account for feature branches</summary>
      <description>right now our 'which hadoop checks do we need' check looks like this: if [[ "${PATCH_BRANCH}" = "master" ]]; then hbase_hadoop2_versions=${HBASE_MASTER_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_MASTER_HADOOP3_VERSIONS} elif [[ ${PATCH_BRANCH} = branch-2* ]]; then hbase_hadoop2_versions=${HBASE_BRANCH2_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_BRANCH2_HADOOP3_VERSIONS} else hbase_hadoop2_versions=${HBASE_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_HADOOP3_VERSIONS} fithe check is basically "if master do this, if like branch-2 do that, otherwise behave like branch-1".we often have feature branches that thus end up being treated like branch-1, even though those branches should all be based off of master. (since we follow a master-first development approach.)we should redo this check so it's "if branch-1 do this, if branch-2 do that, otherwise behave like master"</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18469" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct RegionServer metric of totalRequestCount</summary>
      <description>when i get the metric ,i found this three metric may be have some error as follow : "totalRequestCount" : 17541, "readRequestCount" : 17483, "writeRequestCount" : 1633,</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="18566" opendate="2017-8-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[RSGROUP]Log the client IP/port of the rsgroup admin</summary>
      <description>It is necessary to log the client IP/port of the rsgroup admin command as discussion in HBASE-8754,so that we know who moved the servers from one group to another etc.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockNoopMasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="18577" opendate="2017-8-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded client includes several non-relocated third party dependencies</summary>
      <description>we have some unexpected unrelocated third party dependencies in our shaded artifacts.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18759" opendate="2017-9-5 00:00:00" fixdate="2017-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-shaded-check-invariants failure</summary>
      <description>Here's the error.[ERROR] Found artifact with unexpected contents: '/Users/appy/apache/hbase/hbase-shaded/hbase-shaded-server/target/hbase-shaded-server-3.0.0-SNAPSHOT.jar' Please check the following and either correct the build or update the allowed list with reasoning. about.html org/eclipse/ org/eclipse/jetty/ org/eclipse/jetty/http/ org/eclipse/jetty/http/encoding.properties org/eclipse/jetty/http/GZIPContentDecoder$State.class org/eclipse/jetty/http/HttpFields$Itr.class org/eclipse/jetty/http/HttpParser$HttpHandler.class org/eclipse/jetty/http/HttpScheme.class org/eclipse/jetty/http/HttpURI.class org/eclipse/jetty/http/pathmap/ org/eclipse/jetty/http/pathmap/PathMappings$1.class org/eclipse/jetty/http/pathmap/PathSpecGroup.class org/eclipse/jetty/http/pathmap/PathSpecSet$1.class org/eclipse/jetty/http/pathmap/RegexPathSpec.class org/eclipse/jetty/http/PathMap$MappedEntry.class org/eclipse/jetty/http/QuotedCSV$State.class org/eclipse/jetty/http/QuotedQualityCSV$1.class org/eclipse/jetty/http/DateParser.class org/eclipse/jetty/http/Http1FieldPreEncoder.class org/eclipse/jetty/http/HttpContent$ContentFactory.class org/eclipse/jetty/http/HttpFields$1.class org/eclipse/jetty/http/HttpFields.class org/eclipse/jetty/http/HttpStatus$Code.class org/eclipse/jetty/http/pathmap/UriTemplatePathSpec.class org/eclipse/jetty/http/ResourceHttpContent.class org/eclipse/jetty/http/BadMessageException.class org/eclipse/jetty/http/GZIPContentDecoder$1.class org/eclipse/jetty/http/HttpGenerator$Result.class org/eclipse/jetty/http/HttpParser$State.class org/eclipse/jetty/http/HttpURI$State.class org/eclipse/jetty/http/QuotedCSV.class org/eclipse/jetty/http/CompressedContentFormat.class org/eclipse/jetty/http/HttpField$LongValueHttpField.class org/eclipse/jetty/http/HttpGenerator$2.class org/eclipse/jetty/http/HttpMethod.class org/eclipse/jetty/http/HttpParser$ComplianceHandler.class org/eclipse/jetty/http/HttpParser$RequestHandler.class org/eclipse/jetty/http/HttpStatus.class org/eclipse/jetty/http/MetaData$Response.class org/eclipse/jetty/http/MimeTypes$Type.class org/eclipse/jetty/http/pathmap/PathMappings.class org/eclipse/jetty/http/PathMap$PathSet.class org/eclipse/jetty/http/QuotedQualityCSV.class org/eclipse/jetty/http/CookieCompliance.class org/eclipse/jetty/http/GZIPContentDecoder.class org/eclipse/jetty/http/HttpFields$2.class org/eclipse/jetty/http/HttpGenerator$State.class org/eclipse/jetty/http/HttpHeader.class org/eclipse/jetty/http/HttpParser$IllegalCharacterException.class org/eclipse/jetty/http/HttpTokens$EndOfContent.class org/eclipse/jetty/http/HttpURI$1.class org/eclipse/jetty/http/mime.properties org/eclipse/jetty/http/PreEncodedHttpField.class org/eclipse/jetty/http/DateGenerator$1.class org/eclipse/jetty/http/HostPortHttpField.class org/eclipse/jetty/http/HttpCompliance.class org/eclipse/jetty/http/HttpFieldPreEncoder.class org/eclipse/jetty/http/HttpGenerator$PreparedResponse.class org/eclipse/jetty/http/HttpParser$1.class org/eclipse/jetty/http/HttpParser$ResponseHandler.class org/eclipse/jetty/http/HttpTokens.class org/eclipse/jetty/http/HttpVersion.class org/eclipse/jetty/http/MetaData.class org/eclipse/jetty/http/pathmap/MappedResource.class org/eclipse/jetty/http/pathmap/PathSpec.class org/eclipse/jetty/http/pathmap/PathSpecSet.class org/eclipse/jetty/http/PreEncodedHttpField$1.class org/eclipse/jetty/http/QuotedCSV$1.class org/eclipse/jetty/http/Syntax.class org/eclipse/jetty/http/DateGenerator.class org/eclipse/jetty/http/HttpContent.class org/eclipse/jetty/http/HttpField$IntValueHttpField.class org/eclipse/jetty/http/HttpField.class org/eclipse/jetty/http/HttpGenerator.class org/eclipse/jetty/http/MetaData$Request.class org/eclipse/jetty/http/MimeTypes.class org/eclipse/jetty/http/pathmap/ServletPathSpec$1.class org/eclipse/jetty/http/PathMap.class org/eclipse/jetty/http/DateParser$1.class org/eclipse/jetty/http/HttpCookie.class org/eclipse/jetty/http/HttpGenerator$1.class org/eclipse/jetty/http/HttpHeaderValue.class org/eclipse/jetty/http/HttpParser$CharState.class org/eclipse/jetty/http/HttpParser$FieldState.class org/eclipse/jetty/http/HttpParser.class org/eclipse/jetty/http/pathmap/ServletPathSpec.class org/eclipse/jetty/http/PrecompressedHttpContent.class org/eclipse/jetty/http/useragents org/eclipse/jetty/io/ org/eclipse/jetty/io/AbstractEndPoint.class org/eclipse/jetty/io/ByteBufferPool$Lease.class org/eclipse/jetty/io/ChannelEndPoint$2.class org/eclipse/jetty/io/ChannelEndPoint$4.class org/eclipse/jetty/io/ChannelEndPoint$RunnableTask.class org/eclipse/jetty/io/Connection$UpgradeTo.class org/eclipse/jetty/io/ManagedSelector$EndPointCloser.class org/eclipse/jetty/io/ManagedSelector$Selectable.class org/eclipse/jetty/io/ssl/ org/eclipse/jetty/io/ssl/SslConnection$1.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint.class org/eclipse/jetty/io/WriterOutputStream.class org/eclipse/jetty/io/AbstractConnection$1.class org/eclipse/jetty/io/ArrayByteBufferPool.class org/eclipse/jetty/io/ByteArrayEndPoint.class org/eclipse/jetty/io/Connection$UpgradeFrom.class org/eclipse/jetty/io/EofException.class org/eclipse/jetty/io/ManagedSelector$CreateEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Server.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$WriteCallBack$1.class org/eclipse/jetty/io/AbstractConnection$ReadCallback.class org/eclipse/jetty/io/ByteArrayEndPoint$1.class org/eclipse/jetty/io/Connection$Listener$Adapter.class org/eclipse/jetty/io/FillInterest.class org/eclipse/jetty/io/ManagedSelector$Connect.class org/eclipse/jetty/io/ManagedSelector$DumpKeys.class org/eclipse/jetty/io/ssl/ALPNProcessor.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$WriteCallBack.class org/eclipse/jetty/io/ssl/SslConnection.class org/eclipse/jetty/io/WriteFlusher$FailedState.class org/eclipse/jetty/io/WriteFlusher$PendingState.class org/eclipse/jetty/io/WriteFlusher$WritingState.class org/eclipse/jetty/io/AbstractConnection.class org/eclipse/jetty/io/Connection.class org/eclipse/jetty/io/LeakTrackingByteBufferPool$1.class org/eclipse/jetty/io/ManagedSelector$1.class org/eclipse/jetty/io/ManagedSelector$Acceptor.class org/eclipse/jetty/io/ManagedSelector$SelectorProducer.class org/eclipse/jetty/io/NegotiatingClientConnection.class org/eclipse/jetty/io/NetworkTrafficListener$Adapter.class org/eclipse/jetty/io/QuietException.class org/eclipse/jetty/io/SelectChannelEndPoint.class org/eclipse/jetty/io/SocketChannelEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Server$1.class org/eclipse/jetty/io/AbstractEndPoint$1.class org/eclipse/jetty/io/ByteBufferPool$Bucket.class org/eclipse/jetty/io/ByteBufferPool.class org/eclipse/jetty/io/ChannelEndPoint$1.class org/eclipse/jetty/io/ChannelEndPoint$3.class org/eclipse/jetty/io/ChannelEndPoint$RunnableCloseable.class org/eclipse/jetty/io/ChannelEndPoint.class org/eclipse/jetty/io/LeakTrackingByteBufferPool.class org/eclipse/jetty/io/ManagedSelector$CloseEndPoints.class org/eclipse/jetty/io/ManagedSelector$ConnectTimeout.class org/eclipse/jetty/io/ManagedSelector.class org/eclipse/jetty/io/NetworkTrafficSelectChannelEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Client$1.class org/eclipse/jetty/io/ssl/SslConnection$2.class org/eclipse/jetty/io/ssl/SslHandshakeListener$Event.class org/eclipse/jetty/io/ssl/SslHandshakeListener.class org/eclipse/jetty/io/WriteFlusher$1.class org/eclipse/jetty/io/WriteFlusher$CompletingState.class org/eclipse/jetty/io/WriteFlusher$IdleState.class org/eclipse/jetty/io/WriteFlusher$State.class org/eclipse/jetty/io/WriteFlusher$StateType.class org/eclipse/jetty/io/WriteFlusher.class org/eclipse/jetty/io/AbstractEndPoint$2.class org/eclipse/jetty/io/ClientConnectionFactory.class org/eclipse/jetty/io/Connection$Listener.class org/eclipse/jetty/io/ConnectionStatistics.class org/eclipse/jetty/io/IdleTimeout.class org/eclipse/jetty/io/ManagedSelector$Accept.class org/eclipse/jetty/io/ManagedSelector$NonBlockingAction.class org/eclipse/jetty/io/ssl/SslClientConnectionFactory.class org/eclipse/jetty/io/ssl/SslConnection$4.class org/eclipse/jetty/io/AbstractEndPoint$3.class org/eclipse/jetty/io/EndPoint.class org/eclipse/jetty/io/ManagedSelector$CloseSelector.class org/eclipse/jetty/io/ssl/ALPNProcessor$Client.class org/eclipse/jetty/io/ssl/SslConnection$3.class org/eclipse/jetty/io/ssl/SslConnection$RunnableTask.class org/eclipse/jetty/io/AbstractEndPoint$State.class org/eclipse/jetty/io/IdleTimeout$1.class org/eclipse/jetty/io/MappedByteBufferPool$Tagged.class org/eclipse/jetty/io/MappedByteBufferPool.class org/eclipse/jetty/io/NegotiatingClientConnectionFactory.class org/eclipse/jetty/io/NetworkTrafficListener.class org/eclipse/jetty/io/RuntimeIOException.class org/eclipse/jetty/io/SelectorManager.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$FailWrite.class org/eclipse/jetty/security/ org/eclipse/jetty/security/authentication/ org/eclipse/jetty/security/authentication/DeferredAuthentication$2.class org/eclipse/jetty/security/authentication/DigestAuthenticator$Nonce.class org/eclipse/jetty/security/authentication/LoginAuthenticator.class org/eclipse/jetty/security/Authenticator.class org/eclipse/jetty/security/RunAsToken.class org/eclipse/jetty/security/SecurityHandler.class org/eclipse/jetty/security/AbstractLoginService$RolePrincipal.class org/eclipse/jetty/security/authentication/DeferredAuthentication.class org/eclipse/jetty/security/authentication/DigestAuthenticator.class org/eclipse/jetty/security/authentication/SpnegoAuthenticator.class org/eclipse/jetty/security/DefaultUserIdentity.class org/eclipse/jetty/security/JDBCLoginService$JDBCUserPrincipal.class org/eclipse/jetty/security/RoleRunAsToken.class org/eclipse/jetty/security/SecurityHandler$NotChecked.class org/eclipse/jetty/security/ServerAuthException.class org/eclipse/jetty/security/AbstractLoginService.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormResponse.class org/eclipse/jetty/security/authentication/SessionAuthentication.class org/eclipse/jetty/security/DefaultIdentityService.class org/eclipse/jetty/security/IdentityService.class org/eclipse/jetty/security/RoleInfo.class org/eclipse/jetty/security/UserStore.class org/eclipse/jetty/security/AbstractLoginService$UserPrincipal.class org/eclipse/jetty/security/authentication/DigestAuthenticator$Digest.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormAuthentication.class org/eclipse/jetty/security/authentication/LoginCallback.class org/eclipse/jetty/security/Authenticator$Factory.class org/eclipse/jetty/security/DefaultAuthenticatorFactory.class org/eclipse/jetty/security/LoginService.class org/eclipse/jetty/security/SecurityHandler$1.class org/eclipse/jetty/security/SpnegoUserIdentity.class org/eclipse/jetty/security/AbstractUserAuthentication.class org/eclipse/jetty/security/authentication/FormAuthenticator.class org/eclipse/jetty/security/HashLoginService.class org/eclipse/jetty/security/PropertyUserStore$UserListener.class org/eclipse/jetty/security/PropertyUserStore.class org/eclipse/jetty/security/UserAuthentication.class org/eclipse/jetty/security/authentication/BasicAuthenticator.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormRequest.class org/eclipse/jetty/security/authentication/LoginCallbackImpl.class org/eclipse/jetty/security/ConstraintMapping.class org/eclipse/jetty/security/JDBCLoginService.class org/eclipse/jetty/security/SpnegoLoginService.class org/eclipse/jetty/security/UserDataConstraint.class org/eclipse/jetty/security/authentication/ClientCertAuthenticator.class org/eclipse/jetty/security/Authenticator$AuthConfiguration.class org/eclipse/jetty/security/ConstraintSecurityHandler.class org/eclipse/jetty/security/SecurityHandler$3.class org/eclipse/jetty/security/authentication/DeferredAuthentication$1.class org/eclipse/jetty/security/ConstraintAware.class org/eclipse/jetty/security/SecurityHandler$2.class org/eclipse/jetty/security/SpnegoUserPrincipal.class org/glassfish/ org/glassfish/jsp/ org/glassfish/jsp/api/ org/glassfish/jsp/api/ByteWriter.class org/glassfish/jsp/api/JspProbeEmitter.class org/glassfish/jsp/api/ResourceInjector.class com/ com/sun/ com/sun/el/ com/sun/el/ExpressionFactoryImpl.class com/sun/el/lang/ com/sun/el/lang/ELArithmetic$BigDecimalDelegate.class com/sun/el/lang/ELArithmetic$BigIntegerDelegate.class com/sun/el/lang/ELArithmetic$DoubleDelegate.class com/sun/el/lang/ELArithmetic$LongDelegate.class com/sun/el/lang/ELArithmetic.class com/sun/el/lang/ELSupport.class com/sun/el/lang/EvaluationContext.class com/sun/el/lang/ExpressionBuilder$1.class com/sun/el/lang/ExpressionBuilder$NodeSoftReference.class com/sun/el/lang/ExpressionBuilder$SoftConcurrentHashMap.class com/sun/el/lang/ExpressionBuilder.class com/sun/el/lang/FunctionMapperFactory.class com/sun/el/lang/FunctionMapperImpl$Function.class com/sun/el/lang/FunctionMapperImpl.class com/sun/el/lang/VariableMapperFactory.class com/sun/el/lang/VariableMapperImpl.class com/sun/el/Messages.properties com/sun/el/MethodExpressionImpl.class com/sun/el/MethodExpressionLiteral.class com/sun/el/parser/ com/sun/el/parser/ArithmeticNode.class com/sun/el/parser/AstAnd.class com/sun/el/parser/AstAssign.class com/sun/el/parser/AstBracketSuffix.class com/sun/el/parser/AstChoice.class com/sun/el/parser/AstCompositeExpression.class com/sun/el/parser/AstConcat.class com/sun/el/parser/AstDeferredExpression.class com/sun/el/parser/AstDiv.class com/sun/el/parser/AstDotSuffix.class com/sun/el/parser/AstDynamicExpression.class com/sun/el/parser/AstEmpty.class com/sun/el/parser/AstEqual.class com/sun/el/parser/AstFalse.class com/sun/el/parser/AstFloatingPoint.class com/sun/el/parser/AstFunction.class com/sun/el/parser/AstGreaterThan.class com/sun/el/parser/AstGreaterThanEqual.class com/sun/el/parser/AstIdentifier.class com/sun/el/parser/AstInteger.class com/sun/el/parser/AstLambdaExpression.class com/sun/el/parser/AstLambdaParameters.class com/sun/el/parser/AstLessThan.class com/sun/el/parser/AstLessThanEqual.class com/sun/el/parser/AstListData.class com/sun/el/parser/AstLiteralExpression.class com/sun/el/parser/AstMapData.class com/sun/el/parser/AstMapEntry.class com/sun/el/parser/AstMethodArguments.class com/sun/el/parser/AstMinus.class com/sun/el/parser/AstMod.class com/sun/el/parser/AstMult.class com/sun/el/parser/AstNegative.class com/sun/el/parser/AstNot.class com/sun/el/parser/AstNotEqual.class com/sun/el/parser/AstNull.class com/sun/el/parser/AstOr.class com/sun/el/parser/AstPlus.class com/sun/el/parser/AstSemiColon.class com/sun/el/parser/AstString.class com/sun/el/parser/AstTrue.class com/sun/el/parser/AstValue$Target.class com/sun/el/parser/AstValue.class com/sun/el/parser/BooleanNode.class com/sun/el/parser/ELParser$1.class com/sun/el/parser/ELParser$JJCalls.class com/sun/el/parser/ELParser$LookaheadSuccess.class com/sun/el/parser/ELParser.class com/sun/el/parser/ELParserConstants.class com/sun/el/parser/ELParserTokenManager.class com/sun/el/parser/ELParserTreeConstants.class com/sun/el/parser/JJTELParserState.class com/sun/el/parser/Node.class com/sun/el/parser/NodeVisitor.class com/sun/el/parser/ParseException.class com/sun/el/parser/SimpleCharStream.class com/sun/el/parser/SimpleNode.class com/sun/el/parser/Token.class com/sun/el/parser/TokenMgrError.class com/sun/el/stream/ com/sun/el/stream/Operator.class com/sun/el/stream/Optional.class com/sun/el/stream/Stream$1$1.class com/sun/el/stream/Stream$1.class com/sun/el/stream/Stream$2$1.class com/sun/el/stream/Stream$2.class com/sun/el/stream/Stream$3$1.class com/sun/el/stream/Stream$3.class com/sun/el/stream/Stream$4$1.class com/sun/el/stream/Stream$4.class com/sun/el/stream/Stream$5.class com/sun/el/stream/Stream$6$1.class com/sun/el/stream/Stream$6.class com/sun/el/stream/Stream$7$1.class com/sun/el/stream/Stream$7$2.class com/sun/el/stream/Stream$7.class com/sun/el/stream/Stream$8$1.class com/sun/el/stream/Stream$8$2.class com/sun/el/stream/Stream$8.class com/sun/el/stream/Stream$9$1.class com/sun/el/stream/Stream$9.class com/sun/el/stream/Stream$Iterator0.class com/sun/el/stream/Stream$Iterator1.class com/sun/el/stream/Stream$Iterator2.class com/sun/el/stream/Stream.class com/sun/el/stream/StreamELResolver$1.class com/sun/el/stream/StreamELResolver.class com/sun/el/util/ com/sun/el/util/MessageFactory.class com/sun/el/util/ReflectionUtil$1.class com/sun/el/util/ReflectionUtil$ConstructorWrapper.class com/sun/el/util/ReflectionUtil$MethodWrapper.class com/sun/el/util/ReflectionUtil$Wrapper.class com/sun/el/util/ReflectionUtil.class com/sun/el/ValueExpressionImpl.class com/sun/el/ValueExpressionLiteral.class javax/ javax/el/ javax/el/ArrayELResolver.class javax/el/BeanELResolver$1.class javax/el/BeanELResolver$BeanProperties.class javax/el/BeanELResolver$BeanProperty.class javax/el/BeanELResolver$BPSoftReference.class javax/el/BeanELResolver$SoftConcurrentHashMap.class javax/el/BeanELResolver.class javax/el/BeanNameELResolver.class javax/el/BeanNameResolver.class javax/el/CompositeELResolver$CompositeIterator.class javax/el/CompositeELResolver.class javax/el/ELClass.class javax/el/ELContext.class javax/el/ELContextEvent.class javax/el/ELContextListener.class javax/el/ELException.class javax/el/ELManager.class javax/el/ELProcessor.class javax/el/ELResolver.class javax/el/ELUtil$1.class javax/el/ELUtil$ConstructorWrapper.class javax/el/ELUtil$MethodWrapper.class javax/el/ELUtil$Wrapper.class javax/el/ELUtil.class javax/el/EvaluationListener.class javax/el/Expression.class javax/el/ExpressionFactory.class javax/el/FactoryFinder.class javax/el/FunctionMapper.class javax/el/ImportHandler.class javax/el/LambdaExpression.class javax/el/ListELResolver.class javax/el/MapELResolver.class javax/el/MethodExpression.class javax/el/MethodInfo.class javax/el/MethodNotFoundException.class javax/el/PrivateMessages.properties javax/el/PropertyNotFoundException.class javax/el/PropertyNotWritableException.class javax/el/ResourceBundleELResolver.class javax/el/StandardELContext$1.class javax/el/StandardELContext$DefaultFunctionMapper.class javax/el/StandardELContext$DefaultVariableMapper.class javax/el/StandardELContext$LocalBeanNameResolver.class javax/el/StandardELContext.class javax/el/StaticFieldELResolver.class javax/el/TypeConverter.class javax/el/ValueExpression.class javax/el/ValueReference.class javax/el/VariableMapper.class javax/servlet/ javax/servlet/jsp/ javax/servlet/jsp/el/ javax/servlet/jsp/el/ELException.class javax/servlet/jsp/el/ELParseException.class javax/servlet/jsp/el/Expression.class javax/servlet/jsp/el/ExpressionEvaluator.class javax/servlet/jsp/el/FunctionMapper.class javax/servlet/jsp/el/ImplicitObjectELResolver$1.class javax/servlet/jsp/el/ImplicitObjectELResolver$EnumeratedMap.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$1.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$2.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$3.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$4.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$5.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$6.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$7.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$8.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$9.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects.class javax/servlet/jsp/el/ImplicitObjectELResolver.class javax/servlet/jsp/el/ScopedAttributeELResolver.class javax/servlet/jsp/el/VariableResolver.class javax/servlet/jsp/ErrorData.class javax/servlet/jsp/HttpJspPage.class javax/servlet/jsp/JspApplicationContext.class javax/servlet/jsp/JspContext.class javax/servlet/jsp/JspEngineInfo.class javax/servlet/jsp/JspException.class javax/servlet/jsp/JspFactory.class javax/servlet/jsp/JspPage.class javax/servlet/jsp/JspTagException.class javax/servlet/jsp/JspWriter.class javax/servlet/jsp/PageContext.class javax/servlet/jsp/SkipPageException.class javax/servlet/jsp/tagext/ javax/servlet/jsp/tagext/BodyContent.class javax/servlet/jsp/tagext/BodyTag.class javax/servlet/jsp/tagext/BodyTagSupport.class javax/servlet/jsp/tagext/DynamicAttributes.class javax/servlet/jsp/tagext/FunctionInfo.class javax/servlet/jsp/tagext/IterationTag.class javax/servlet/jsp/tagext/JspFragment.class javax/servlet/jsp/tagext/JspIdConsumer.class javax/servlet/jsp/tagext/JspTag.class javax/servlet/jsp/tagext/PageData.class javax/servlet/jsp/tagext/SimpleTag.class javax/servlet/jsp/tagext/SimpleTagSupport.class javax/servlet/jsp/tagext/Tag.class javax/servlet/jsp/tagext/TagAdapter.class javax/servlet/jsp/tagext/TagAttributeInfo.class javax/servlet/jsp/tagext/TagData.class javax/servlet/jsp/tagext/TagExtraInfo.class javax/servlet/jsp/tagext/TagFileInfo.class javax/servlet/jsp/tagext/TagInfo.class javax/servlet/jsp/tagext/TagLibraryInfo.class javax/servlet/jsp/tagext/TagLibraryValidator.class javax/servlet/jsp/tagext/TagSupport.class javax/servlet/jsp/tagext/TagVariableInfo.class javax/servlet/jsp/tagext/TryCatchFinally.class javax/servlet/jsp/tagext/ValidationMessage.class javax/servlet/jsp/tagext/VariableInfo.class javax/servlet/annotation/ javax/servlet/annotation/HandlesTypes.class javax/servlet/annotation/HttpConstraint.class javax/servlet/annotation/HttpMethodConstraint.class javax/servlet/annotation/MultipartConfig.class javax/servlet/annotation/package.html javax/servlet/annotation/ServletSecurity$EmptyRoleSemantic.class javax/servlet/annotation/ServletSecurity$TransportGuarantee.class javax/servlet/annotation/ServletSecurity.class javax/servlet/annotation/WebFilter.class javax/servlet/annotation/WebInitParam.class javax/servlet/annotation/WebListener.class javax/servlet/annotation/WebServlet.class javax/servlet/AsyncContext.class javax/servlet/AsyncEvent.class javax/servlet/AsyncListener.class javax/servlet/descriptor/ javax/servlet/descriptor/JspConfigDescriptor.class javax/servlet/descriptor/JspPropertyGroupDescriptor.class javax/servlet/descriptor/package.html javax/servlet/descriptor/TaglibDescriptor.class javax/servlet/DispatcherType.class javax/servlet/Filter.class javax/servlet/FilterChain.class javax/servlet/FilterConfig.class javax/servlet/FilterRegistration$Dynamic.class javax/servlet/FilterRegistration.class javax/servlet/GenericServlet.class javax/servlet/http/ javax/servlet/http/Cookie.class javax/servlet/http/HttpServlet.class javax/servlet/http/HttpServletRequest.class javax/servlet/http/HttpServletRequestWrapper.class javax/servlet/http/HttpServletResponse.class javax/servlet/http/HttpServletResponseWrapper.class javax/servlet/http/HttpSession.class javax/servlet/http/HttpSessionActivationListener.class javax/servlet/http/HttpSessionAttributeListener.class javax/servlet/http/HttpSessionBindingEvent.class javax/servlet/http/HttpSessionBindingListener.class javax/servlet/http/HttpSessionContext.class javax/servlet/http/HttpSessionEvent.class javax/servlet/http/HttpSessionIdListener.class javax/servlet/http/HttpSessionListener.class javax/servlet/http/HttpUpgradeHandler.class javax/servlet/http/HttpUtils.class javax/servlet/http/LocalStrings.properties javax/servlet/http/LocalStrings_es.properties javax/servlet/http/LocalStrings_fr.properties javax/servlet/http/LocalStrings_ja.properties javax/servlet/http/NoBodyOutputStream.class javax/servlet/http/NoBodyResponse.class javax/servlet/http/package.html javax/servlet/http/Part.class javax/servlet/http/WebConnection.class javax/servlet/HttpConstraintElement.class javax/servlet/HttpMethodConstraintElement.class javax/servlet/LocalStrings.properties javax/servlet/LocalStrings_fr.properties javax/servlet/LocalStrings_ja.properties javax/servlet/MultipartConfigElement.class javax/servlet/package.html javax/servlet/ReadListener.class javax/servlet/Registration$Dynamic.class javax/servlet/Registration.class javax/servlet/RequestDispatcher.class javax/servlet/Servlet.class javax/servlet/ServletConfig.class javax/servlet/ServletContainerInitializer.class javax/servlet/ServletContext.class javax/servlet/ServletContextAttributeEvent.class javax/servlet/ServletContextAttributeListener.class javax/servlet/ServletContextEvent.class javax/servlet/ServletContextListener.class javax/servlet/ServletException.class javax/servlet/ServletInputStream.class javax/servlet/ServletOutputStream.class javax/servlet/ServletRegistration$Dynamic.class javax/servlet/ServletRegistration.class javax/servlet/ServletRequest.class javax/servlet/ServletRequestAttributeEvent.class javax/servlet/ServletRequestAttributeListener.class javax/servlet/ServletRequestEvent.class javax/servlet/ServletRequestListener.class javax/servlet/ServletRequestWrapper.class javax/servlet/ServletResponse.class javax/servlet/ServletResponseWrapper.class javax/servlet/ServletSecurityElement.class javax/servlet/SessionCookieConfig.class javax/servlet/SessionTrackingMode.class javax/servlet/SingleThreadModel.class javax/servlet/UnavailableException.class javax/servlet/WriteListener.class org/apache/commons/ org/apache/commons/lang/ org/apache/commons/lang/ArrayUtils.class org/apache/commons/lang/BitField.class org/apache/commons/lang/BooleanUtils.class org/apache/commons/lang/builder/ org/apache/commons/lang/builder/CompareToBuilder.class org/apache/commons/lang/builder/EqualsBuilder.class org/apache/commons/lang/builder/HashCodeBuilder.class org/apache/commons/lang/builder/IDKey.class org/apache/commons/lang/builder/ReflectionToStringBuilder.class org/apache/commons/lang/builder/StandardToStringStyle.class org/apache/commons/lang/builder/ToStringBuilder.class org/apache/commons/lang/builder/ToStringStyle$DefaultToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$MultiLineToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$NoFieldNameToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$ShortPrefixToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$SimpleToStringStyle.class org/apache/commons/lang/builder/ToStringStyle.class org/apache/commons/lang/CharEncoding.class org/apache/commons/lang/CharRange$1.class org/apache/commons/lang/CharRange$CharacterIterator.class org/apache/commons/lang/CharRange.class org/apache/commons/lang/CharSet.class org/apache/commons/lang/CharSetUtils.class org/apache/commons/lang/CharUtils.class org/apache/commons/lang/ClassUtils.class org/apache/commons/lang/Entities$ArrayEntityMap.class org/apache/commons/lang/Entities$BinaryEntityMap.class org/apache/commons/lang/Entities$EntityMap.class org/apache/commons/lang/Entities$HashEntityMap.class org/apache/commons/lang/Entities$LookupEntityMap.class org/apache/commons/lang/Entities$MapIntMap.class org/apache/commons/lang/Entities$PrimitiveEntityMap.class org/apache/commons/lang/Entities$TreeEntityMap.class org/apache/commons/lang/Entities.class org/apache/commons/lang/enum/ org/apache/commons/lang/enum/Enum$Entry.class org/apache/commons/lang/enum/Enum.class org/apache/commons/lang/enum/EnumUtils.class org/apache/commons/lang/enum/ValuedEnum.class org/apache/commons/lang/enums/ org/apache/commons/lang/enums/Enum$Entry.class org/apache/commons/lang/enums/Enum.class org/apache/commons/lang/enums/EnumUtils.class org/apache/commons/lang/enums/ValuedEnum.class org/apache/commons/lang/exception/ org/apache/commons/lang/exception/CloneFailedException.class org/apache/commons/lang/exception/ExceptionUtils.class org/apache/commons/lang/exception/Nestable.class org/apache/commons/lang/exception/NestableDelegate.class org/apache/commons/lang/exception/NestableError.class org/apache/commons/lang/exception/NestableException.class org/apache/commons/lang/exception/NestableRuntimeException.class org/apache/commons/lang/IllegalClassException.class org/apache/commons/lang/IncompleteArgumentException.class org/apache/commons/lang/IntHashMap$Entry.class org/apache/commons/lang/IntHashMap.class org/apache/commons/lang/LocaleUtils.class org/apache/commons/lang/math/ org/apache/commons/lang/math/DoubleRange.class org/apache/commons/lang/math/FloatRange.class org/apache/commons/lang/math/Fraction.class org/apache/commons/lang/math/IEEE754rUtils.class org/apache/commons/lang/math/IntRange.class org/apache/commons/lang/math/JVMRandom.class org/apache/commons/lang/math/LongRange.class org/apache/commons/lang/math/NumberRange.class org/apache/commons/lang/math/NumberUtils.class org/apache/commons/lang/math/RandomUtils.class org/apache/commons/lang/math/Range.class org/apache/commons/lang/mutable/ org/apache/commons/lang/mutable/Mutable.class org/apache/commons/lang/mutable/MutableBoolean.class org/apache/commons/lang/mutable/MutableByte.class org/apache/commons/lang/mutable/MutableDouble.class org/apache/commons/lang/mutable/MutableFloat.class org/apache/commons/lang/mutable/MutableInt.class org/apache/commons/lang/mutable/MutableLong.class org/apache/commons/lang/mutable/MutableObject.class org/apache/commons/lang/mutable/MutableShort.class org/apache/commons/lang/NotImplementedException.class org/apache/commons/lang/NullArgumentException.class org/apache/commons/lang/NumberRange.class org/apache/commons/lang/NumberUtils.class org/apache/commons/lang/ObjectUtils$Null.class org/apache/commons/lang/ObjectUtils.class org/apache/commons/lang/RandomStringUtils.class org/apache/commons/lang/reflect/ org/apache/commons/lang/reflect/ConstructorUtils.class org/apache/commons/lang/reflect/FieldUtils.class org/apache/commons/lang/reflect/MemberUtils.class org/apache/commons/lang/reflect/MethodUtils.class org/apache/commons/lang/SerializationException.class org/apache/commons/lang/SerializationUtils.class org/apache/commons/lang/StringEscapeUtils.class org/apache/commons/lang/StringUtils.class org/apache/commons/lang/SystemUtils.class org/apache/commons/lang/text/ org/apache/commons/lang/text/CompositeFormat.class org/apache/commons/lang/text/ExtendedMessageFormat.class org/apache/commons/lang/text/FormatFactory.class org/apache/commons/lang/text/StrBuilder$StrBuilderReader.class org/apache/commons/lang/text/StrBuilder$StrBuilderTokenizer.class org/apache/commons/lang/text/StrBuilder$StrBuilderWriter.class org/apache/commons/lang/text/StrBuilder.class org/apache/commons/lang/text/StrLookup$MapStrLookup.class org/apache/commons/lang/text/StrLookup.class org/apache/commons/lang/text/StrMatcher$CharMatcher.class org/apache/commons/lang/text/StrMatcher$CharSetMatcher.class org/apache/commons/lang/text/StrMatcher$NoMatcher.class org/apache/commons/lang/text/StrMatcher$StringMatcher.class org/apache/commons/lang/text/StrMatcher$TrimMatcher.class org/apache/commons/lang/text/StrMatcher.class org/apache/commons/lang/text/StrSubstitutor.class org/apache/commons/lang/text/StrTokenizer.class org/apache/commons/lang/time/ org/apache/commons/lang/time/DateFormatUtils.class org/apache/commons/lang/time/DateUtils$DateIterator.class org/apache/commons/lang/time/DateUtils.class org/apache/commons/lang/time/DurationFormatUtils$Token.class org/apache/commons/lang/time/DurationFormatUtils.class org/apache/commons/lang/time/FastDateFormat$CharacterLiteral.class org/apache/commons/lang/time/FastDateFormat$NumberRule.class org/apache/commons/lang/time/FastDateFormat$PaddedNumberField.class org/apache/commons/lang/time/FastDateFormat$Pair.class org/apache/commons/lang/time/FastDateFormat$Rule.class org/apache/commons/lang/time/FastDateFormat$StringLiteral.class org/apache/commons/lang/time/FastDateFormat$TextField.class org/apache/commons/lang/time/FastDateFormat$TimeZoneDisplayKey.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNameRule.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNumberRule.class org/apache/commons/lang/time/FastDateFormat$TwelveHourField.class org/apache/commons/lang/time/FastDateFormat$TwentyFourHourField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitMonthField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitNumberField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitYearField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedMonthField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedNumberField.class org/apache/commons/lang/time/FastDateFormat.class org/apache/commons/lang/time/StopWatch.class org/apache/commons/lang/UnhandledException.class org/apache/commons/lang/Validate.class org/apache/commons/lang/WordUtils.class[ERROR] Found artifact with unexpected contents: '/Users/appy/apache/hbase/hbase-shaded/hbase-shaded-client/target/hbase-shaded-client-3.0.0-SNAPSHOT.jar' Please check the following and either correct the build or update the allowed list with reasoning. org/apache/commons/ org/apache/commons/lang/ org/apache/commons/lang/ArrayUtils.class org/apache/commons/lang/BitField.class org/apache/commons/lang/BooleanUtils.class org/apache/commons/lang/builder/ org/apache/commons/lang/builder/CompareToBuilder.class org/apache/commons/lang/builder/EqualsBuilder.class org/apache/commons/lang/builder/HashCodeBuilder.class org/apache/commons/lang/builder/IDKey.class org/apache/commons/lang/builder/ReflectionToStringBuilder.class org/apache/commons/lang/builder/StandardToStringStyle.class org/apache/commons/lang/builder/ToStringBuilder.class org/apache/commons/lang/builder/ToStringStyle$DefaultToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$MultiLineToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$NoFieldNameToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$ShortPrefixToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$SimpleToStringStyle.class org/apache/commons/lang/builder/ToStringStyle.class org/apache/commons/lang/CharEncoding.class org/apache/commons/lang/CharRange$1.class org/apache/commons/lang/CharRange$CharacterIterator.class org/apache/commons/lang/CharRange.class org/apache/commons/lang/CharSet.class org/apache/commons/lang/CharSetUtils.class org/apache/commons/lang/CharUtils.class org/apache/commons/lang/ClassUtils.class org/apache/commons/lang/Entities$ArrayEntityMap.class org/apache/commons/lang/Entities$BinaryEntityMap.class org/apache/commons/lang/Entities$EntityMap.class org/apache/commons/lang/Entities$HashEntityMap.class org/apache/commons/lang/Entities$LookupEntityMap.class org/apache/commons/lang/Entities$MapIntMap.class org/apache/commons/lang/Entities$PrimitiveEntityMap.class org/apache/commons/lang/Entities$TreeEntityMap.class org/apache/commons/lang/Entities.class org/apache/commons/lang/enum/ org/apache/commons/lang/enum/Enum$Entry.class org/apache/commons/lang/enum/Enum.class org/apache/commons/lang/enum/EnumUtils.class org/apache/commons/lang/enum/ValuedEnum.class org/apache/commons/lang/enums/ org/apache/commons/lang/enums/Enum$Entry.class org/apache/commons/lang/enums/Enum.class org/apache/commons/lang/enums/EnumUtils.class org/apache/commons/lang/enums/ValuedEnum.class org/apache/commons/lang/exception/ org/apache/commons/lang/exception/CloneFailedException.class org/apache/commons/lang/exception/ExceptionUtils.class org/apache/commons/lang/exception/Nestable.class org/apache/commons/lang/exception/NestableDelegate.class org/apache/commons/lang/exception/NestableError.class org/apache/commons/lang/exception/NestableException.class org/apache/commons/lang/exception/NestableRuntimeException.class org/apache/commons/lang/IllegalClassException.class org/apache/commons/lang/IncompleteArgumentException.class org/apache/commons/lang/IntHashMap$Entry.class org/apache/commons/lang/IntHashMap.class org/apache/commons/lang/LocaleUtils.class org/apache/commons/lang/math/ org/apache/commons/lang/math/DoubleRange.class org/apache/commons/lang/math/FloatRange.class org/apache/commons/lang/math/Fraction.class org/apache/commons/lang/math/IEEE754rUtils.class org/apache/commons/lang/math/IntRange.class org/apache/commons/lang/math/JVMRandom.class org/apache/commons/lang/math/LongRange.class org/apache/commons/lang/math/NumberRange.class org/apache/commons/lang/math/NumberUtils.class org/apache/commons/lang/math/RandomUtils.class org/apache/commons/lang/math/Range.class org/apache/commons/lang/mutable/ org/apache/commons/lang/mutable/Mutable.class org/apache/commons/lang/mutable/MutableBoolean.class org/apache/commons/lang/mutable/MutableByte.class org/apache/commons/lang/mutable/MutableDouble.class org/apache/commons/lang/mutable/MutableFloat.class org/apache/commons/lang/mutable/MutableInt.class org/apache/commons/lang/mutable/MutableLong.class org/apache/commons/lang/mutable/MutableObject.class org/apache/commons/lang/mutable/MutableShort.class org/apache/commons/lang/NotImplementedException.class org/apache/commons/lang/NullArgumentException.class org/apache/commons/lang/NumberRange.class org/apache/commons/lang/NumberUtils.class org/apache/commons/lang/ObjectUtils$Null.class org/apache/commons/lang/ObjectUtils.class org/apache/commons/lang/RandomStringUtils.class org/apache/commons/lang/reflect/ org/apache/commons/lang/reflect/ConstructorUtils.class org/apache/commons/lang/reflect/FieldUtils.class org/apache/commons/lang/reflect/MemberUtils.class org/apache/commons/lang/reflect/MethodUtils.class org/apache/commons/lang/SerializationException.class org/apache/commons/lang/SerializationUtils.class org/apache/commons/lang/StringEscapeUtils.class org/apache/commons/lang/StringUtils.class org/apache/commons/lang/SystemUtils.class org/apache/commons/lang/text/ org/apache/commons/lang/text/CompositeFormat.class org/apache/commons/lang/text/ExtendedMessageFormat.class org/apache/commons/lang/text/FormatFactory.class org/apache/commons/lang/text/StrBuilder$StrBuilderReader.class org/apache/commons/lang/text/StrBuilder$StrBuilderTokenizer.class org/apache/commons/lang/text/StrBuilder$StrBuilderWriter.class org/apache/commons/lang/text/StrBuilder.class org/apache/commons/lang/text/StrLookup$MapStrLookup.class org/apache/commons/lang/text/StrLookup.class org/apache/commons/lang/text/StrMatcher$CharMatcher.class org/apache/commons/lang/text/StrMatcher$CharSetMatcher.class org/apache/commons/lang/text/StrMatcher$NoMatcher.class org/apache/commons/lang/text/StrMatcher$StringMatcher.class org/apache/commons/lang/text/StrMatcher$TrimMatcher.class org/apache/commons/lang/text/StrMatcher.class org/apache/commons/lang/text/StrSubstitutor.class org/apache/commons/lang/text/StrTokenizer.class org/apache/commons/lang/time/ org/apache/commons/lang/time/DateFormatUtils.class org/apache/commons/lang/time/DateUtils$DateIterator.class org/apache/commons/lang/time/DateUtils.class org/apache/commons/lang/time/DurationFormatUtils$Token.class org/apache/commons/lang/time/DurationFormatUtils.class org/apache/commons/lang/time/FastDateFormat$CharacterLiteral.class org/apache/commons/lang/time/FastDateFormat$NumberRule.class org/apache/commons/lang/time/FastDateFormat$PaddedNumberField.class org/apache/commons/lang/time/FastDateFormat$Pair.class org/apache/commons/lang/time/FastDateFormat$Rule.class org/apache/commons/lang/time/FastDateFormat$StringLiteral.class org/apache/commons/lang/time/FastDateFormat$TextField.class org/apache/commons/lang/time/FastDateFormat$TimeZoneDisplayKey.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNameRule.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNumberRule.class org/apache/commons/lang/time/FastDateFormat$TwelveHourField.class org/apache/commons/lang/time/FastDateFormat$TwentyFourHourField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitMonthField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitNumberField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitYearField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedMonthField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedNumberField.class org/apache/commons/lang/time/FastDateFormat.class org/apache/commons/lang/time/StopWatch.class org/apache/commons/lang/UnhandledException.class org/apache/commons/lang/Validate.class org/apache/commons/lang/WordUtils.class</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19635" opendate="2017-12-27 00:00:00" fixdate="2017-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a thread at RS side to call reportProcedureDone</summary>
      <description>So that we can do some batching and also prevent blocking too many threads when HMaster is temporary done.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionServerStatus.proto</file>
    </fixedFiles>
  </bug>
  <bug id="19636" opendate="2017-12-27 00:00:00" fixdate="2017-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>All rs should already start work with the new peer change when replication peer procedure is finished</summary>
      <description>When replication peer operations use zk, the master will modify zk directly. Then the rs will asynchronous track the zk event to start work with the new peer change. When replication peer operations use procedure, need to make sure this process is synchronous. All rs should already start work with the new peer change when procedure is finished.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestNamespaceReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.ReplicationSourceDummy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManagerZkImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceShipper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSourceShipper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RecoveredReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandlerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.PeerProcedureHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ReplicationSourceService.java</file>
      <file type="M">hbase-replication.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationUtils.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueueInfo.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeerImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeerConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="20066" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region sequence id may go backward after split or merge</summary>
      <description>The problem is that, now we have markers which will be written to WAL but not in store file. For a normal region close, we will write a sequence id file under the region directory, and when opening we will use this as the open sequence id. But for split and merge, we do not copy the sequence id file to the newly generated regions so the sequence id may go backwards since when closing the region we will write flush marker and close marker into WAL...</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.AbstractTestDLS.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
    </fixedFiles>
  </bug>
  <bug id="20068" opendate="2018-2-24 00:00:00" fixdate="2018-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopcheck project health check uses default maven repo instead of yetus managed ones</summary>
      <description>Recently had a precommit run fail hadoop check for all 3 versions with [ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.2:install (default-install) on project hbase-thrift: Failed to install metadata org.apache.hbase:hbase-thrift:3.0.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hbase/hbase-thrift/3.0.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got / (position: END_TAG seen ...&lt;/metadata&gt;\n/... @25:2) -&gt; [Help 1]Looks like maven repo corruption.Also the path /home/jenkins/.m2/repository means that those invocations are using the jenkins user repo, which isn't safe since there are multiple executors. either the plugin isn't using the yetus provided maven repo path or our yetus invocation isn't telling yetus to provide its own maven repo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.4,2.0.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20069" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix existing findbugs errors in hbase-server</summary>
      <description>now that findbugs is running on precommit we have some cleanup to do.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterStatusPublisher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.compaction.MajorCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.nio.TestMultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.MultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="20070" opendate="2018-2-24 00:00:00" fixdate="2018-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>website generation is failing</summary>
      <description>website generation has been failing since Feb 20thChecking out files: 100% (68971/68971), done.Usage: grep [OPTION]... PATTERN [FILE]...Try 'grep --help' for more information.PUSHED is 2 is not yet mentioned in the hbase-site commit log. Assuming we don't have it yet. 2Building HBaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Failure: mvn clean siteBuild step 'Execute shell' marked build as failureThe status email saysBuild status: Still FailingThe HBase website has not been updated to incorporate HBase commit ${CURRENT_HBASE_COMMIT}.Looking at the code where that grep happens, it looks like the env variable CURRENT_HBASE_COMMIT isn't getting set. That comes from some git command. I'm guessing the version of git changed on the build hosts and upended our assumptions.we should fix this to 1) rely on git's porcelain interface, and 2) fail as soon as that git command fails</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-spark-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-resource-bundle.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-build-configuration.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-annotations.pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20587" opendate="2018-5-15 00:00:00" fixdate="2018-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Jackson with shaded thirdparty gson</summary>
      <description>HBASE-20582 got me looking at how we use Jackson. It appears that we moved some JSON code from hbase-server into hbase-common via HBASE-19053. But, there seems to be no good reason why this code should live there and not in hbase-http instead. Keeping Jackson off the user's classpath is a nice goal.FYI appy, mdrob</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestJSONMetricUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALPrettyPrinter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.AgeSnapshot.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-metrics.src.main.java.org.apache.hadoop.hbase.metrics.impl.FastLongHistogram.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.TestPerformanceEvaluation.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.RESTApiClusterManager.java</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.src.main.java.org.apache.hadoop.hbase.http.jmx.JMXJsonServlet.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JSONMetricUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JSONBean.java</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestOperation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.util.JsonMapper.java</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20666" opendate="2018-5-31 00:00:00" fixdate="2018-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unsuccessful table creation leaves entry in hbase:rsgroup table</summary>
      <description>If a table creation fails in a cluster enabled with rsgroup feature, the table is still listed as part of default rsgroup.To recreate the scenario: Create a namespace (NS) with number of region limit Create table in the NS which satisfies the region limit by pre-splitting Create a new table in the NS which will fail list_rsgroup will show the table being part of default rsgroup and data can be found in hbase:rsgroup tableWould be good to revert the entry when the table creation fails or a script to clean up the metadata.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroups.java</file>
    </fixedFiles>
  </bug>
  <bug id="20909" opendate="2018-7-19 00:00:00" fixdate="2018-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.0 to the download page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21697" opendate="2019-1-8 00:00:00" fixdate="2019-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.2 to the download page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2177" opendate="2010-2-1 00:00:00" fixdate="2010-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timestamping to gc logging options</summary>
      <description>http://forums.sun.com/thread.jspa?threadID=5165451</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22250" opendate="2019-4-16 00:00:00" fixdate="2019-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The same constants used in many places should be placed in constant classes</summary>
      <description>I think we should put these configurations in the HConstants class to avoid the trouble of modifying a lot of places when we modify them later.public static final String MASTER_KRB_PRINCIPAL = "hbase.master.kerberos.principal";public static final String MASTER_KRB_KEYTAB_FILE = "hbase.master.keytab.file";public static final String REGIONSERVER_KRB_PRINCIPAL = "hbase.regionserver.kerberos.principal";public static final String REGIONSERVER_KRB_KEYTAB_FILE = "hbase.regionserver.keytab.file";</description>
      <version>1.2.0,2.0.0,2.1.1,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.HBaseKerberosUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestZKAndFSPermissions.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.SecurityInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="22264" opendate="2019-4-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out jars related to JDK 11 into a folder in /lib</summary>
      <description>UPDATE:Separate out thethe jars related to JDK 11 and add control their addition to the classpath using an environment variable or auto-detection of the jdk version installed.OLD:This is in continuation withHBASE-22249. When compiled with jdk 8 and run on jdk 11, the master branch throws the following exceptionduring an attempt to start the hbase rest server:Exception in thread "main" java.lang.NoClassDefFoundError: javax/annotation/Priority at org.glassfish.jersey.model.internal.ComponentBag.modelFor(ComponentBag.java:483) at org.glassfish.jersey.model.internal.ComponentBag.access$100(ComponentBag.java:89) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:408) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:398) at org.glassfish.jersey.internal.Errors.process(Errors.java:315) at org.glassfish.jersey.internal.Errors.process(Errors.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:228) at org.glassfish.jersey.model.internal.ComponentBag.registerModel(ComponentBag.java:398) at org.glassfish.jersey.model.internal.ComponentBag.register(ComponentBag.java:235) at org.glassfish.jersey.model.internal.CommonConfig.register(CommonConfig.java:420) at org.glassfish.jersey.server.ResourceConfig.register(ResourceConfig.java:425) at org.apache.hadoop.hbase.rest.RESTServer.run(RESTServer.java:245) at org.apache.hadoop.hbase.rest.RESTServer.main(RESTServer.java:421)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="22495" opendate="2019-5-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SyncTable section, explaining from which specific minor versions "doDeletes/doPuts" is available</summary>
      <description>On SyncTable section from ref guide for 1.2, we are mentioning "doDeletes/doPuts", but these were only added by HBASE-20305. Original patch proposed there was for master branch only, and related branch-1 and branch-2 patches were only committed recently, so minimal supported version is 1.4.10, for branch-1 releases, and 2.1.5 for branch-2. Am proposing a patch that mentions which are the minimum versions that support such commands.Also, we should add this whole section to 2.0 and 2.1 Documentations (these are currently missing).Thanksnpopa for pointing this out.</description>
      <version>1.2.0</version>
      <fixedVersion>1.5.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
</bugrepository>
