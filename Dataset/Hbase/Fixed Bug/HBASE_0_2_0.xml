<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="1286" opendate="2009-3-24 00:00:00" fixdate="2009-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift should support next(nbRow) like functionality</summary>
      <description>Currently the java hbase api support calling next(number_of_rows) where as the thrift interface doesn't. We have a patch to get this working internally.</description>
      <version>0.2.0</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2360" opendate="2010-3-22 00:00:00" fixdate="2010-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure we have all the hadoop fixes in our our copy of its rpc</summary>
      <description>Our RPC was different from hadoops because we fixed some bugs &amp;#8211; i.e. that buffers in rpc would keep the shape of the largest ever allocation and not snap back to original size, that rpc would never timeout &amp;#8211; and we used send codes instead of method names. The latter has been removed recently from hbase rpc and the above two fixes have been committed to hadoop (IIUC). So, this issue about reviewing our RPC to see that we have all good fixes that are currently up in hadoop and to look at perhaps using hadoop rpc again, directly, because the reason to have our own has perhaps dissipated.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="281" opendate="2008-1-30 00:00:00" fixdate="2008-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should allow deletions in .META. and -ROOT- tables</summary>
      <description>For administrative and debugging purposes, it would be nice to be able to delete rows from .META. via the shell. The alternative is writing custom java code to do such operations, which is just ridiculous. The reality of HBase's maturity is that from time to time we're going to have to reach into the .META. and ROOT tables to fix things, so I think the shell should be where that happens.Currently, attempting to delete from either table gives a "non-existant table" error.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="426" opendate="2008-2-7 00:00:00" fixdate="2008-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase can&amp;#39;t find remote filesystem</summary>
      <description>If filesystem is remote, e.g. its an Hadoop DFS running "over there", there is no means of pointing hbase at it currently (unless you count copying hadoop-site.xml into hbase/conf). Should be possible to just set a fully qualified hbase.rootdir and that should be sufficient for hbase figuring the fs (needs to be backported to 0.1 too).</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMigrate.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestTimestamp.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestSplit.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestMergeMeta.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestLogRolling.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHStoreFile.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestGet2.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestGet.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestDeleteFamily.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestDeleteAll.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestCompaction.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableMapReduce.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableIndex.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConnectionManager.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="434" opendate="2008-2-10 00:00:00" fixdate="2008-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestTableIndex failed in HBasePatch build #14</summary>
      <description>TestTableIndex failed in HBase-Patch build #14. See http://hudson.zones.apache.org/hudson/job/HBase-Patch/14/testReport/junit.framework.AssertionFailedError at org.apache.hadoop.hbase.MultiRegionTable.makeMultiRegionTable(MultiRegionTable.java:137) at org.apache.hadoop.hbase.mapred.TestTableIndex.setUp(TestTableIndex.java:125)</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4350" opendate="2011-9-8 00:00:00" fixdate="2011-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a Bloom filter bug introduced by HFile v2 and TestMultiColumnScanner that caught it</summary>
      <description>Nicolas pointed out to me that the new unit test TestMultiColumnScanner that I wrote for the multi-column scanner Bloom filter optimization (which we will soon release) did not pass on the open-source trunk, and it bisected down to the HFile v2 commit. I debugged the unit test and found that there was a serious bug in HFile v2 Bloom filter lookup not caught by any of the existing unit tests: Bloom filters were used for "non-Get" Scans, which did not have minimum/maximum row set correctly, and some scan results were not returned.This diff is the unit test that helped catch the problem and a one-line fix for the bug.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="436" opendate="2008-2-10 00:00:00" fixdate="2008-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>website</summary>
      <description>Make the hbase website. Base it on hadoop forrest templates.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docs.src.documentation.content.xdocs.tabs.xml</file>
      <file type="M">build.xml</file>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">docs.linkmap.html</file>
      <file type="M">docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug id="440" opendate="2008-2-12 00:00:00" fixdate="2008-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add optional log roll interval so that log files are garbage collected</summary>
      <description>While optional cache flushes will increase the sequence id in the region server's log file, if the region server is basically idle, the log will not get rolled and consequently old log files will not get garbage collected.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="444" opendate="2008-2-13 00:00:00" fixdate="2008-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase is very slow at determining table is not present</summary>
      <description>If I misspell a table name, it takes a very long time for hbase to determine that the table doesn't exist, because there are many levels of retries. This often causes timeouts, which then obscure the true cause of the problem.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4450" opendate="2011-9-21 00:00:00" fixdate="2011-9-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>test for number of blocks read: to serve as baseline for expected blocks read and for catching regressions</summary>
      <description>Add a simple test for number of blocks read. The tests intent is to serve as baseline for expected blocks read and for catching regressions. As optimizations for HBase-4433 or Hbase-4434 are committed, the test would need to be updated to adjust the counts for expected blocks read in various cases.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="446" opendate="2008-2-14 00:00:00" fixdate="2008-2-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fully qualified hbase.rootdir doesn&amp;#39;t work</summary>
      <description>Jim was setting up cluster w/ new hbase. Setting fully qualified hbase.rootdir was failing. Complaint was that the filesystems didn't match &amp;#8211; i.e. the hdfs of the fully qualified hbase.rootdir didn't jibe w/ the default hadoop file:///.Fix needs to be backported.The problem was that because the hadoop config files were not found (because they are in a different directory and not on the classpath) then fs.get(conf) returns file:///</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4461" opendate="2011-9-22 00:00:00" fixdate="2011-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose getRowOrBefore via Thrift</summary>
      <description>In order for fat Thrift-based clients to locate region locations they need to utilize the getRowOrBefore method.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4463" opendate="2011-9-22 00:00:00" fixdate="2011-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run more aggressive compactions during off peak hours</summary>
      <description>The number of iops on the disk and the top of the rack bandwidth utilization at off peak hours is much lower than at peak hours depending on the application usage pattern. We can utilize this knowledge to improve the performance of the HBase cluster by increasing the compact selection ratio to a much larger value during off-peak hours than otherwise - increasing hbase.hstore.compaction.ratio (1.2 default) to hbase.hstore.compaction.ratio.offpeak (5 default). This will help reduce the average number of files per store.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSelection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="452" opendate="2008-2-15 00:00:00" fixdate="2008-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"region offline" should throw IOException, not IllegalStateException</summary>
      <description>It would be nice if I could wrap my HTable.get calls in try {} catch (IOException e). But that doesn't work, since I also need to catch IllegalStateException. I think that any time there is something wrong with hbase, hbase calls should throw an IOException (or subclass thereof). Things like IllegalStateException should be reserved for programmer error.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="462" opendate="2008-2-23 00:00:00" fixdate="2008-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update migration tool</summary>
      <description>HBASE-2 is really an incompatible change as it changes the format of region server log file names.Update Migration tool so that it ensures there are no unrecovered region server log files.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestScannerAPI.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestBloomFilters.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.StaticTestEnvironment.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4620" opendate="2011-10-19 00:00:00" fixdate="2011-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>I broke the build when I submitted HBASE-3581 (Send length of the rpc response)</summary>
      <description>Thanks to Ted, Ram and Gao for figuring my messup.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ResponseFlag.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="465" opendate="2008-2-24 00:00:00" fixdate="2008-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc for all public declarations</summary>
      <description>Just as client, master and region server were refactored in subissues of HBASE-75, create subtasks for fixing javadoc for all public declarations. Some are missing, some are just incorrect. See the hadoop code review checklist: http://wiki.apache.org/hadoop/CodeReviewChecklist</description>
      <version>0.2.0</version>
      <fixedVersion>0.18.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.LeaseException.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestKeying.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestClassMigration.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.transactional.TestTransactionalHLogManager.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHStoreFile.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestBloomFilters.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableIndex.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestWhileMatchRowFilter.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestStopRowFilter.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRowFilterSet.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRowFilterAfterWrite.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRegExpRowFilter.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestPageRowFilter.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestInclusiveStopRowFilter.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.DFSAbort.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestListTables.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.java.org.onelab.filter.RetouchedBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.Key.java</file>
      <file type="M">src.java.org.onelab.filter.Filter.java</file>
      <file type="M">src.java.org.onelab.filter.DynamicBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.CountingBloomFilter.java</file>
      <file type="M">src.java.org.onelab.filter.BloomFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.ipc.HBaseClient.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.VersionInfo.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.SoftSortedMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.Dispatcher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalHLogManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.CleanOldTransactionsChore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLogKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLogEdit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HAbstractScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Flusher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.ChangedReadersObserver.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.BeforeThisStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RootScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionServerOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessRegionClose.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ModifyColumn.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.MetaScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.MetaRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.DeleteColumn.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.Leases.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Scanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.RegionHistorian.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestTable.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.CommitUnsuccessfulException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.LocalTransactionLogger.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionLogger.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.UnknownTransactionException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ColumnNameParseException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.DoNotRetryIOException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.ColumnValueFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.InclusiveStopRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.PageRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RowFilterSet.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.StopRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.WhileMatchRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMsg.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionLocation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerAddress.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.BatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.BlockFSInputStream.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.Cell.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseMapWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.ImmutableBytesWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.RowResult.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HbaseRPC.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
    </fixedFiles>
  </bug>
  <bug id="4661" opendate="2011-10-25 00:00:00" fixdate="2011-12-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to export the list of files for a some or all column families for a given region</summary>
      <description>Should be able to query the regionservers to figure out the list of files in one/some/all column families for a given regions to determine which files to copy for a backup.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
    </fixedFiles>
  </bug>
  <bug id="4720" opendate="2011-11-1 00:00:00" fixdate="2011-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement atomic update operations (checkAndPut, checkAndDelete) for REST client/server</summary>
      <description>I have several large application/HBase clusters where an application node will occasionally need to talk to HBase from a different cluster. In order to help ensure some of my consistency guarantees I have a sentinel table that is updated atomically as users interact with the system. This works quite well for the "regular" hbase client but the REST client does not implement the checkAndPut and checkAndDelete operations. This exposes the application to some race conditions that have to be worked around. It would be ideal if the same checkAndPut/checkAndDelete operations could be supported by the REST client.</description>
      <version>None</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.RootResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndPutTableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndPutRowResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndDeleteTableResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.CheckAndDeleteRowResource.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="4721" opendate="2011-11-1 00:00:00" fixdate="2011-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain Delete Markers after Major Compaction</summary>
      <description>There is a need to provide long TTLs for delete markers. This is useful when replicating hbase logs from one cluster to another. The receiving cluster shouldn't compact away the delete markers because the affected key-values might still be on the way.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="477" opendate="2008-2-29 00:00:00" fixdate="2008-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for an HBASE_CLASSPATH</summary>
      <description>We have mention of HBASE_CLASSPATH in hbase-env.sh but its not actually read anywhere. Make it work like HADOOP_CLASSPATH. See classpath discussion on this page, http://wiki.apache.org/hadoop/Hbase/Jython, for a use case.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="478" opendate="2008-2-29 00:00:00" fixdate="2008-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>offlining of table does not run reliably</summary>
      <description>I have a table of 4 regions made w/ PE. I cannot reliably offline it. I'm using 'disable TestTable' and have traced it to ensure its not a problem in hql. What I see is that one region will get the offlined mark or maybe two.. but never all.Jim in IRC suggested that if we did the .TABLE. catalog table, offlining the entry there might be more reliable than trying to offline all regions in a table.</description>
      <version>0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableDelete.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RetryableMetaOperation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessRegionClose.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ModifyColumn.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ChangeTableState.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="480" opendate="2008-2-29 00:00:00" fixdate="2008-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to manually merge two regions</summary>
      <description>hbase-471 needs a tool to merge two regions that have same start key. This tool may be of use elsewhere making repairs.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="490" opendate="2008-3-4 00:00:00" fixdate="2008-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doubly-assigned .META.; master uses one and clients another</summary>
      <description>Internal cluster has two .META.,,1 regions up (Its possible for a region to be added twice to the unassigned map if meta scans run close together). Worse is that the master is working with one .META. but when clients come in, they're being give the other. Makes for odd results.Made it a blocker. Still trying to track down how master doesn't see subsequent update of .META. info in ROOT.....</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RootScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.MetaScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="4920" opendate="2011-12-1 00:00:00" fixdate="2011-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We need a mascot, a totem</summary>
      <description>We need a totem for our t-shirt that is yet to be printed. O'Reilly owns the Clyesdale. We need something else.We could have a fluffy little duck that quacks 'hbase!' when you squeeze it and we could order boxes of them from some off-shore sweatshop that subcontracts to a contractor who employs child labor only.....Or we could have an Orca (Big!, Fast!, Killer!, and in a poem that Marcy from Salesforce showed me, that was a bit too spiritual for me to be seen quoting here, it had the Orca as the 'Guardian of the Cosmic Memory': i.e. in translation, bigdata).</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.site.site.vm</file>
      <file type="M">NOTICE.txt</file>
      <file type="M">src.main.site.resources.css.site.css</file>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.jumping-orca.rotated.12percent.png</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.hbase.logo.small.png</file>
    </fixedFiles>
  </bug>
  <bug id="5000" opendate="2011-12-9 00:00:00" fixdate="2011-12-9 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Speed up simultaneous reads of a block when block caching is turned off</summary>
      <description>With block caching, when one client starts reading a block and another one comes around asking for the same block, the second client waits for the first one to finish reading and returns the block from cache. This is achieved by locking on the block offset using IdLock, a "sparse lock" primitive allowing to lock on arbitrary long numbers. However, in case there is no block caching, there is no reason to wait for other clients that are reading the same block. One challenge optimizing this that we don't necessary have accurate information about whether other HFile API clients interested in the block would cache it.Setting priority as minor, as it is very unusual to turn off block caching.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="505" opendate="2008-3-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region assignments should never time out so long as the region server reports that it is processing the open request</summary>
      <description>Currently, when the master assigns a region to a region server, it extends the reassignment timeout when the region server reports that it is processing the open. This only happens once, and so if the region takes a long time to come on line due to a large set of transactions in the redo log or because the initial compaction takes a long time, the master will assign the region to another server when the reassignment timeout occurs.Assigning a region to multiple region servers can easily corrupt the region. For example:region server 1 is processing the redo log creating a new mapfile. It takes more than one interval to do so so the master assigns the region to region server 2. region server 2 starts processing the redo log creating essentially the same mapFile as region server 1, but with a different name. region server 2 can fail to open the region if region server 1 deletes the old log file or if it tries to open the new mapFile that region server 1 is creating.region server 1 can fail to open the region if it tries to open the mapFile that region server 2 is creating.Often region server 1 eventually succeeds and reports to the master that it has finished opening the region, but the master tells it to close that region because it has assigned it to another server. Region server 2 often fails to open the region, because the old log file has been deleted, or it fails to process the new map file created by region server 1.Proposed solution:During the open process the region server should send a MSG_PROCESS_OPEN with each heartbeat until the region is opened (when it sends MSG_REGION_OPEN). The master will extend the reassignment timeout with each MSG_PROCESS_OPEN it receives and will not assign the region to another server so long as it continues to receive heart beat messages from the region server processing the open.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5050" opendate="2011-12-16 00:00:00" fixdate="2011-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[rest] SPNEGO-based authentication</summary>
      <description>Currently the REST gateway can authenticate to a HBase cluster using a preconfigured principal. This provides a limited form of secure operation where one or more gateways can be deployed with distinct principals granting appropriate levels of privilege, but the service ports must be protected through network ACLs. This is at best a stopgap.SPNEGO is the standard mechanism for Kerberos authentication over HTTP. Enhance the REST gateway such that it provides this option, and issues requests to the HBase cluster with the established context.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="510" opendate="2008-3-13 00:00:00" fixdate="2008-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HConnectionManger.listTables returns empty list if exception (though there may be many tables present)</summary>
      <description>Its a problem because commonly a check for existence will get list of current tables.Yesterday saw problem when .META. went off line. A piece of client code was asking for list of tables when .META. was offline, it was getting back an empty list because listTables do while was seeing 'org.apache.hadoop.hbase.NotServingRegionException: .META.,,1'Problem is the do while in HCM.listTables goes as long as startRow does not equal LAST_ROW but startRow is initialized with EMPTY_START_ROW which is equal to LAST_ROW.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="511" opendate="2008-3-13 00:00:00" fixdate="2008-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do exponential backoff in clients on NSRE, WRE, ISE, etc.</summary>
      <description>Jim Firby suggestion is that we do expotential backoff retrying locating regions, committing edits, etc.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="515" opendate="2008-3-14 00:00:00" fixdate="2008-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>At least double default timeouts between regionserver and master</summary>
      <description>501 added logging of how long we sleep at the end of the HRegionServer main run method before we send heartback back to the master. Last night during upload I saw this:2008-03-13 00:03:50,884 WARN org.apache.hadoop.hbase.util.Sleeper: We slept ten times longer than scheduled: 3000Above log has since been improved but its saying that we slept &gt; 30 seconds, the default timeout on master/regionserver communications (When the lease expires, master starts giving regions to someone else and when this regionserver reports in, its told to close all its regions).Server was under load from other processes but still...upload rate was not that rabid.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="523" opendate="2008-3-17 00:00:00" fixdate="2008-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>package-level javadoc should have example client</summary>
      <description>Package-level javadoc should have example client or at least point at the FAQ that ties to package release.For example, the BatchUpdate example code appears in the FAQ sample code, but it is missing in the javadoc.It will be better to tie the example code snippet with the new methods that replaces the deprecated methods which will also be tied to release in the javadoc.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5230" opendate="2012-1-19 00:00:00" fixdate="2012-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure compactions do not cache-on-write data blocks</summary>
      <description>Create a unit test for HBASE-3976 (making sure we don't cache data blocks on write during compactions even if cache-on-write is enabled generally enabled). This is because we have very different implementations of HBASE-3976 without HBASE-4422 CacheConfig (on top of 89-fb, created by Liyin) and with CacheConfig (presumably it's there but not sure if it even works, since the patch in HBASE-3976 may not have been committed). We need to create a unit test to verify that we don't cache data blocks on write during compactions, and resolve HBASE-3976 so that this new unit test does not fail.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.SchemaMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
    </fixedFiles>
  </bug>
  <bug id="525" opendate="2008-3-17 00:00:00" fixdate="2008-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTable.getRow(Text) does not work</summary>
      <description>Updated from SVN to find that Hbase.getRow(Text) always return empty map.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="528" opendate="2008-3-18 00:00:00" fixdate="2008-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>table &amp;#39;does not exist&amp;#39; when it does</summary>
      <description>This one I've seen a few times. In hql, I do show tables and it shows my table. I then try to do a select against the table and hql reports table does not exist. Digging, whats happening is that the getClosest facility is failing to find the first table region in the .META. table. I hacked up a region reading tool &amp;#8211; attached (for 0.1 branch) &amp;#8211; and tried it against but a copy and the actual instance of the region and it could do the getClosest fine. I'm pretty sure I restarted the HRS and when it came up again, the master had given it again the .META. and again was failing to find the first region in the table (Looked around in server logs and it seemed 'healthy').</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestGet2.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="531" opendate="2008-3-19 00:00:00" fixdate="2008-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port HBASE-483 to trunk</summary>
      <description>Once HBASE-483 is committed to branch 0.1, port it to trunk.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMigrate.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.Cell.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5382" opendate="2012-2-10 00:00:00" fixdate="2012-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test that we always cache index and bloom blocks</summary>
      <description>This is a unit test that should have been part of HBASE-4683 but was not committed. The original test was reviewed as part of https://reviews.facebook.net/D807. Submitting unit test as a separate JIRA and patch, and extending the scope of the test to also handle the case when block cache is enabled for the column family. The new review is at https://reviews.facebook.net/D1695.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="5384" opendate="2012-2-10 00:00:00" fixdate="2012-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up heap used by hadoopqa</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="541" opendate="2008-3-23 00:00:00" fixdate="2008-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop jars used in HBase/lib are not compatible with Hadoop-trunk</summary>
      <description>The hadoop jars included in the HBase tree under /lib are not compatible with hadoop-core trunk.Apparently there have been a couple of revisions to the Hadoop RPC protocol so an HBase built with the included jars will not run against a hadoop trunk cluster.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.hadoop-0.17.0-dev.2008-03-04.15-19-00-test.jar</file>
      <file type="M">lib.hadoop-0.17.0-dev.2008-03-04.15-19-00-core.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5430" opendate="2012-2-18 00:00:00" fixdate="2012-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix licenses in 0.92.1 -- RAT plugin won&amp;#39;t pass</summary>
      <description>Use the -Drelease profile to see we are missing 30 or so license. Fix.</description>
      <version>None</version>
      <fixedVersion>0.92.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="544" opendate="2008-3-24 00:00:00" fixdate="2008-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Purge startUpdate usage from internal code and test cases</summary>
      <description>We have batch updates now. Nothing internal should be using the deprecated startUpdate method.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TimestampTestBase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestGet2.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestGet.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestDeleteFamily.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestDeleteAll.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.DisabledTestScanner2.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestMultipleUpdates.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestHTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.TableHandler.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.InsertCommand.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.DeleteCommand.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="5443" opendate="2012-2-22 00:00:00" fixdate="2012-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add PB-based calls to HRegionInterface</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestHTableUtil.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.protobuf.Admin.proto</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.SortedCopyOnWriteSet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ParseConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.TimeRange.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ExecRPCInvoker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Invocation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.WritableRpcEngine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.generated.RegionAdminProtos.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.generated.RegionClientProtos.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Leases.java</file>
      <file type="M">src.main.protobuf.hbase.proto</file>
      <file type="M">src.main.protobuf.RegionAdmin.proto</file>
      <file type="M">src.main.protobuf.RegionClient.proto</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.catalog.TestCatalogTracker.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.catalog.TestMetaReaderEditorNoCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.HConnectionTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.OOMERegionServer.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.java</file>
      <file type="M">security.src.main.java.org.apache.hadoop.hbase.ipc.SecureRpcEngine.java</file>
      <file type="M">security.src.main.java.org.apache.hadoop.hbase.ipc.SecureServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.RpcEngine.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.AdminProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.ClientProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.protobuf.ResponseConverter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="5448" opendate="2012-2-22 00:00:00" fixdate="2012-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for dynamic coprocessor endpoints with PB-based RPC</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.java</file>
      <file type="M">hbase-server.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-server.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.ExecRPCInvoker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CoprocessorProtocol.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.coprocessor.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.coprocessor.ExecResult.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.coprocessor.Exec.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.coprocessor.Batch.java</file>
    </fixedFiles>
  </bug>
  <bug id="5449" opendate="2012-2-22 00:00:00" fixdate="2012-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for wire-compatible security functionality</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestTablePermissions.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TablePermission.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="548" opendate="2008-3-27 00:00:00" fixdate="2008-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to online single region</summary>
      <description>A sequence of events put a region offline in the middle of an online table. I was unable to get the region backon by running 'enable table'. Here is a little bit of code that I ran to bring the region back online and get the table running again. This issue is about adding it to hbase tools (A new package named 'tools'?). public static void main(String[] args) throws IOException { HBaseConfiguration c = new HBaseConfiguration(); c.set("hbase.master", args[0]); HTable t = new HTable(c, new Text(".META.")); Text row = new Text(args[1]); byte [] cell = t.get(row, new Text("info:regioninfo")); HRegionInfo info = Writables.getHRegionInfo(cell); LOG.info(info); long id = t.startUpdate(row); info.setOffline(false); t.put(id, COL_REGIONINFO, Writables.getBytes(info)); t.delete(id, COL_SERVER); t.delete(id, COL_STARTCODE); t.commit(id); }</description>
      <version>0.1.0,0.1.1,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="554" opendate="2008-3-30 00:00:00" fixdate="2008-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>filters generate StackOverflowException</summary>
      <description>Below is from list.You're doing nothing wrong.The filters as written recurse until they find a match. If long stretches between matching rows, then you will get a StackOverflowError. Filters need to be changed. Thanks for pointing this out. Can you do without them for the moment until we get a chance to fix it?St.AckDavid Alves wrote:&gt; Hi St.Ack and all&gt; &gt; The error always occurs when trying to see if there are more rows to&gt; process.&gt; Yes I'm using a filter(RegExpRowFilter) to select only the rows (any&gt; row key) that match a specific value in one of the columns.&gt; Then I obtain the scanner just test the hasNext method, close the&gt; scanner and return.&gt; Am I doing something wrong?&gt; Still StackOverflowError is not supposed to happen right?&gt;&gt; Regards&gt; David Alves&gt; On Thu, 2008-03-27 at 12:36 -0700, stack wrote:&gt;&gt; You are using a filter? If so, tell us more about it.&gt;&gt; St.Ack&gt;&gt;&gt;&gt; David Alves wrote:&gt;&gt;&gt; Hi guys &gt;&gt;&gt;&gt;&gt;&gt; I 'm using HBase to keep data that is later indexed.&gt;&gt;&gt; The data is indexed in chunks so the cycle is get XXXX records index&gt;&gt;&gt; them check for more records etc...&gt;&gt;&gt; When I tryed the candidate-2 instead of the old 0.16.0 (which I&gt;&gt;&gt; switched to do to the regionservers becoming unresponsive) I got the&gt;&gt;&gt; error in the end of this email well into an indexing job.&gt;&gt;&gt; So you have any idea why? Am I doing something wrong?&gt;&gt;&gt;&gt;&gt;&gt; David Alves&gt;&gt;&gt;&gt;&gt;&gt; java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException:&gt;&gt;&gt; java.io.IOException: java.lang.StackOverflowError&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at java.io.DataInputStream.readLong(DataInputStream.java:399)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.readChunk(DFSClient.java:735)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.read(DFSClient.java:658)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.readBuffer(DFSClient.java:1130)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.read(DFSClient.java:1166)&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at org.apache.hadoop.io.DataOutputBuffer&gt;&gt;&gt; $Buffer.write(DataOutputBuffer.java:56)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1829)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1729)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1775)&gt;&gt;&gt; at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:461)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $StoreFileScanner.getNext(HStore.java:2350)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.hbase.HAbstractScanner.next(HAbstractScanner.java:256)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $HStoreScanner.next(HStore.java:2561)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1807)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; ...</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="558" opendate="2008-4-2 00:00:00" fixdate="2008-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output hbase+hadoop+jvm version as well as java opts, ulimit, into master/regionserver log on startup</summary>
      <description></description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="5641" opendate="2012-3-27 00:00:00" fixdate="2012-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>decayingSampleTick1 prevents HBase from shutting down.</summary>
      <description>I think this is the problem. It creates a non-daemon thread. private static final ScheduledExecutorService TICK_SERVICE = Executors.newScheduledThreadPool(1, Threads.getNamedThreadFactory("decayingSampleTick"));</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.histogram.ExponentiallyDecayingSample.java</file>
    </fixedFiles>
  </bug>
  <bug id="569" opendate="2008-4-8 00:00:00" fixdate="2008-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DemoClient.php</summary>
      <description>Adding DemoClient.php implementation</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.README.txt</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="573" opendate="2008-4-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not read hadoop-*.xml for dfs configuration after moving out hadoop/contrib</summary>
      <description>When HBase was in hadoop/contrib, the hbase script set both HADOOP_CONF_DIRand HBASE_CONF_DIR to CLASSPATH, so that dfs's configuration can be loadedcorrectly. However, when moved out hadoop/contrib, it only sets HBASE_CONF_DIR.I can think of several possible solutions:1) set HADOOP_CONF_DIR in hbase-env.sh, then add HADOOP_CONF_DIR to CLASSPATH as before2) Instruct user to create links for hadoop-*.xml if they want to customize some dfs settings.3) If only a small set of dfs confs are related to dfs's client, maybe they can be set via hbase-site.xml, then hbase sets these for us when create a FileSystem obj.Please see the thread "# of dfs replications when using hbase" on hbase-user@.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="574" opendate="2008-4-11 00:00:00" fixdate="2008-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not load hadoop native libs</summary>
      <description>After moving out from hadoop/contrib, the standalone release does not include hadoop native libs in hbase/lib/native while it still includes hadoop-core.jar. I think they should be included as well to improve speed for compression and decompression.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="589" opendate="2008-4-17 00:00:00" fixdate="2008-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to deprecated methods in Hadoop once hadoop-0.17.0 is released</summary>
      <description>A number of methods in Hadoop have been deprecated for release 0.17.0. Once 0.17.0 is released, use preferred alternate.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMigrate.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHStoreFile.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.TestTableIndex.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MapFilePerformanceEvaluation.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseClusterTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.DisabledTestScanner2.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableDelete.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.DeleteColumn.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.BuildTableIndex.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.ColumnValueFilter.java</file>
      <file type="M">lib.hadoop-0.17.0-dev-2008.04.04-13.34.00-test.jar</file>
      <file type="M">lib.hadoop-0.17.0-dev-2008.04.04-13.34.00-core.jar</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="614" opendate="2008-5-5 00:00:00" fixdate="2008-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retiring regions is not used; exploit or remove</summary>
      <description>There is a little dance around region close where a region first gets moved into the retiring queue. The idea, IIRC, was that regions in retiring could serve reads while close was going about its business. Meant that region was online that bit longer.This feature is not used any more &amp;#8211; regions are added to retiring but gets do not bother to look in retiring. We should either remove retiring cocept altogether or else make use of it again.</description>
      <version>0.1.2,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="615" opendate="2008-5-6 00:00:00" fixdate="2008-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region balancer oscillates during cluster startup</summary>
      <description>When starting a cluster with four region servers and a large table (49 regions) (+root +meta) = 51 total regions, the region balancer oscillates for a very long time and does not seem to reach a steady state.Additionally, for whatever reason, it seems reluctant to assign regions to the first of four region servers, which may be the root cause. In my test, the first server had 10 regions assigned, the second and fourth had 13 regions assigned, and the master would continually assign and deassign 2 regions to the third server, which oscillated between 13 and 15 regions. If it assigned the two fluctuating regions to the first server, it would achieve the best balance possible: 12, 13, 13, 13.After 20 minutes, it had not stopped oscillating. An application trying to work against this cluster would run very slowly as it would be continually re-finding the two regions in flux.When the table was being created, regions were nicely balanced. On restart, however, it just would not settle down.Perhaps the balancer should set a target number of regions for each server which when the server achieved +/- 1 regions, the rebalancer would not try to change unless the number of regions changed.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6150" opendate="2012-6-1 00:00:00" fixdate="2012-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove empty files causing rat check fail</summary>
      <description>Set of empty files found by Jesse.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.UniformSample.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.Snapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.Sample.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.ExponentiallyDecayingSample.java</file>
    </fixedFiles>
  </bug>
  <bug id="619" opendate="2008-5-7 00:00:00" fixdate="2008-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix &amp;#39;logs&amp;#39; link in UI</summary>
      <description>Clicking on the 'local logs' link in UI gives 404</description>
      <version>0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="620" opendate="2008-5-7 00:00:00" fixdate="2008-5-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testmergetool failing in branch and trunk since hbase-618 went in</summary>
      <description>The hbase-618 fix revealed that testmergetool depends on compactions running.</description>
      <version>0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="624" opendate="2008-5-13 00:00:00" fixdate="2008-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master will shut down if number of active region servers is zero even if shutdown was not requested</summary>
      <description>The master will initiate shutdown if the number of active region servers goes to zero, even if shutdown has not been requested.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6272" opendate="2012-6-26 00:00:00" fixdate="2012-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In-memory region state is inconsistent</summary>
      <description>AssignmentManger stores region state related information in several places: regionsInTransition, regions (region info to server name map), and servers (server name to region info set map). However the access to these places is not coordinated properly. It leads to inconsistent in-memory region state information. Sometimes, some region could even be offline, and not in transition.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestDrainingServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestOpenedRegionHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMXBean.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.Mocking.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.UnknownRegionException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.OnlineRegions.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.NotifiableConcurrentSkipListMap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MXBeanImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterDumpServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DeleteTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.BulkReOpen.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ClusterStatus.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="675" opendate="2008-6-10 00:00:00" fixdate="2008-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Report correct server hosting a table split for assignment to for MR Jobs</summary>
      <description>Currently we return a null String array to the MR framework to use a random node for MR job assignment.class: org.apache.hadoop.hbase.mapred.tableSplitfunction getLocations()We should be able to query the meta now for the current host name of the server hosting the region in question.This will help with scaling as there will be less cross server communication removing bandwidth as a bottleneck.The side effect of fixing this will help from overloading region servers with lots of MR clients all pulling from the same region server while theres work local for them to do.</description>
      <version>0.2.0</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6864" opendate="2012-9-22 00:00:00" fixdate="2012-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Online snapshots scaffolding</summary>
      <description>Basic scaffolding for taking a snapshot of an offline table. This includes the basic work on both the regionserver and master to support (but not implement) timestamp and globally consistent snapshots.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.TakeSnapshotUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.MasterSnapshotVerifier.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="6865" opendate="2012-9-22 00:00:00" fixdate="2012-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Snapshot File Cleaners</summary>
      <description>HFile and HLog cleaners are necessary for fully implementing snapshots, but can be broken out into their own piece really cleanly. The HFile cleaner is necessary for both timestamp and globally consistent snapshots, but the HLog cleaner is necessary for globally consistent and offline snapshots. Putting everything together in one patch isn't too much overload as there is a lot of overlap between the cleaners.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.server.snapshot.TakeSnapshotUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="6940" opendate="2012-10-3 00:00:00" fixdate="2012-10-3 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Enable GC logging by default</summary>
      <description>I think we should enable gc by default. Its pretty frictionless apparently and could help in the case where folks are getting off the ground.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
    </fixedFiles>
  </bug>
  <bug id="697" opendate="2008-6-18 00:00:00" fixdate="2008-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>thrift idl needs update/edit to match new 0.2 API (and to fix bugs)</summary>
      <description>Talking w/ Bryan, moving this out of the way of the 0.2.0 release.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.README.txt</file>
      <file type="M">src.examples.thrift.DemoClient.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.package.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.ScanEntry.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.RegionDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.NotFound.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Constants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">lib.libthrift-r746.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6990" opendate="2012-10-13 00:00:00" fixdate="2012-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pretty print TTL</summary>
      <description>I've seen a lot of users getting confused by the TTL configuration and I think that if we just pretty printed it it would solve most of the issues. For example, let's say a user wanted to set a TTL of 90 days. That would be 7776000. But let's say that it was typo'd to 77760000 instead, it gives you 900 days!So when we print the TTL we could do something like "x days, x hours, x minutes, x seconds (real_ttl_value)". This would also help people when they use ms instead of seconds as they would see really big values in there.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.96.3,0.98.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="703" opendate="2008-6-25 00:00:00" fixdate="2008-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid regions listed by regionserver.jsp</summary>
      <description>The region list displayed by regionserver.jsp contains regions that have ceased existence due to splits.Example:Region Name Encoded Name Start Key End Key...maxentriestest,acacdk,1214292085212 732557990 acacdk maxentriestest,acacdk,1214297936860 1583424516 acacdk acqtzkmaxentriestest,acacdk,1214293855954 1509492302 acacdk adhlxwmaxentriestest,acqtzk,1214297936862 1120286366 acqtzk adhlxwmaxentriestest,adhlxw,1214293855955 400707061 adhlxw maxentriestest,adhlxw,1214299372674 2060549477 adhlxw aelrxomaxentriestest,adhlxw,1214297324386 336026175 adhlxw afpxzsmaxentriestest,aelrxo,1214299372674 1352588233 aelrxo afpxzsmaxentriestest,afpxzs,1214297324387 1235754353 afpxzs</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="704" opendate="2008-6-25 00:00:00" fixdate="2008-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update new shell docs and commands on help menu</summary>
      <description>From the help screen on the new shell[root@s2 hbase]# hbase shellHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Version: 0.2.0-dev, r670701, Wed Jun 25 02:27:19 CDT 2008hbase(main):001:0&gt; create 't1' {NAME =&gt; 'f1', VERSIONS =&gt; 5}SyntaxError: (hbase):2: , unexpected tLCURLYThe help menu gives the above example on creating table in hbase but it does not work!If we release this for the new shell examples need to be more clear. I have not been able to create a table using the new shell yet,Also might be worth adding the old shell back in and remove it after we release 2.0 or at lease until we work out the bugs in new client. If it would not require much updating to keep it current with the new api.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hirb.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="716" opendate="2008-6-27 00:00:00" fixdate="2008-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestGet2.testGetClosestBefore fails with hadoop-0.17.1</summary>
      <description>TestGet2.testGetClosestBefore fails with hadoop-0.17.1After the rows are flushed to a MapFile, we get no result when we try to find the closest row before 038. We find 035, but that is deleted. So we advance, the next record is 040 which is after 038 and we give up. This results in a null result being passed back to the test which then dies with an NPE because it expects that getClosestRowBefore should find row 030.It appears that there is no logic to back up from a candidate row if the candidate came before the desired key but is deleted. We should find the row before.I'm guessing that this is failing because hadoop-0.17.1 incorporates HADOOP-3472 (MapFile.Reader getClosest() function returns incorrect results when before is true)</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7171" opendate="2012-11-16 00:00:00" fixdate="2012-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initial web UI for region/memstore/storefiles details</summary>
      <description>Click on a region in UI and get a listing of hfiles in HDFS and summary of memstore content; click on an HFile and see its content</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="718" opendate="2008-6-30 00:00:00" fixdate="2008-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase shell help info</summary>
      <description>these lines below are in the alter table help section should not be using the words "create a table" should be something like "To alter a table to add columns 'xx', 'yy', 'zz' using all defaults" keep the confusion out of it. To create a table with an 'f1', 'f2', and 'f3' using all defaults: hbase&gt; alter 't1', {NAME =&gt; 'f1'}, {NAME =&gt; 'f2'}, {NAME =&gt; 'f3'}</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="7181" opendate="2012-11-18 00:00:00" fixdate="2012-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge documentation improvement</summary>
      <description>There is one small issue on the documentation regarding the merge. The class name is not correct. Also, it might be usefull to give an example of the region format.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7183" opendate="2012-11-19 00:00:00" fixdate="2012-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>print WARN message if hbase.replication.sizeOfLogQueue is too big</summary>
      <description>A metric hbase.replication.sizeOfLogQueue may become big when replication is delaying.It would be useful if HBase prints WARN log which tells hbase.replication.sizeOfLogQueue is too big.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="7190" opendate="2012-11-19 00:00:00" fixdate="2012-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to hbck to check only meta and assignment</summary>
      <description>Currently, hbck loads region info from HDFS for each run. It may take some time if there are many regions.We need an option to not check HDFS, i.e. just checking meta and assignment.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="729" opendate="2008-7-8 00:00:00" fixdate="2008-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>client region/metadata cache should have a public method for invalidating entries</summary>
      <description>While writing a testcase for HBASE-62, I observed that table metadata is cached as part of the region information cached client side. This cached region information (and therefore table metadata) is not directly invalidated by disable/enable table, so to get up to date metadata the client may have to use a scanner over .META. directly using the meta visitor. Ideally other client code &amp;#8211; for example the support for HBASE-62 &amp;#8211; should be able to invalidate entries as necessary, so then the next HTable.getTableDescriptor() would go to meta to return up to date information instead of incorrectly reusing outdated information from the cache.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestHTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="735" opendate="2008-7-9 00:00:00" fixdate="2008-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase shell doesn&amp;#39;t trap CTRL-C signal</summary>
      <description>From withing the hbase shell, when there's a IO problem, the hbase client code tries to recover automatically but sometimes we know what's going on and all we want is to cancel the operation by pressing CTRL-C but the shell doesn't catch it and we need to either wait for the operation to timeout or close the terminal and open another one.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="7350" opendate="2012-12-13 00:00:00" fixdate="2012-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey tests make CI unreliable</summary>
      <description>Most PreCommit and PostCommit builds are failing these days. Keeping an eye on usual suspects reveals them to be mostly longer-running tests. Either the tests need to me made more rigorous or the erroneous Jenkins configuration needs addressed.The usual suspects: org.apache.hadoop.hbase.client.TestMultiParallel org.apache.hadoop.hbase.TestDrainingServer org.apache.hadoop.hbase.regionserver.TestSplitTransaction org.apache.hadoop.hbase.client.TestMultiParallel.testFlushCommitsNoAbort org.apache.hadoop.hbase.replication.TestReplication org.apache.hadoop.hbase.util.TestHBaseFsck</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="737" opendate="2008-7-10 00:00:00" fixdate="2008-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanner: every cell in a row has the same timestamp</summary>
      <description>A row can have multiple cells, and each cell can have a different timestamp. The get command in the shell demonstrates that cells are being stored with different timestamps:hbase(main):008:0&gt; get 'table1', 'row2' COLUMN CELL fam1:letters timestamp=1215707612949, value=def fam1:numbers timestamp=1215707629064, value=123 fam2:letters timestamp=1215711498969, value=abc 3 row(s) in 0.0100 secondsHowever, using the scanners to retrieve these cells shows that they all have the same timestamp:hbase(main):009:0&gt; scan 'table1' ROW COLUMN+CELL row2 column=fam1:letters, timestamp=1215711498969, value=def row2 column=fam1:numbers, timestamp=1215711498969, value=123 row2 column=fam2:letters, timestamp=1215711498969, value=abc 3 row(s) in 0.0600 secondsThe scanners are losing timestamp information somewhere along the line.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TimestampTestBase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestScannerAPI.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestBloomFilters.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestSplit.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestGet2.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRowFilterSet.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRowFilterOnMultipleFamilies.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRowFilterAfterWrite.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.filter.TestRegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.InternalScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HAbstractScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Flusher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.WhileMatchRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.StopRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RowFilterSet.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RowFilterInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.PageRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.ColumnValueFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHRegionInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="739" opendate="2008-7-10 00:00:00" fixdate="2008-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseAdmin.createTable() using old HTableDescription doesn&amp;#39;t work</summary>
      <description>The following test case (see below) illustrate what used to work in branch 0.1 and that doesn't anymore. testTruncateInTrunk() shows how I got it to work again. I get this error now when trying the old code but using trunk:java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at com.openplaces.test.fixture.FixtureLoader.truncateHbaseTable(FixtureLoader.java:105) at com.openplaces.test.fixture.FixtureLoader.loadHbaseFixtures(FixtureLoader.java:63) at com.openplaces.test.fixture.TestCaseWithFixtures.hbaseFixtures(TestCaseWithFixtures.java:34) at com.openplaces.test.isolated.TestSearchSRFIEF.setUp(TestSearchSRFIEF.java:37) at junit.framework.TestCase.runBare(TestCase.java:125) at junit.framework.TestResult$1.protect(TestResult.java:106) at junit.framework.TestResult.runProtected(TestResult.java:124) at junit.framework.TestResult.run(TestResult.java:109) at junit.framework.TestCase.run(TestCase.java:118) at junit.framework.TestSuite.runTest(TestSuite.java:208) at junit.framework.TestSuite.run(TestSuite.java:203) at org.eclipse.jdt.internal.junit.runner.junit3.JUnit3TestReference.run(JUnit3TestReference.java:130) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:460) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:673) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:386) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:196)Caused by: java.net.SocketTimeoutException: timed out waiting for rpc response at org.apache.hadoop.ipc.Client.call(Client.java:559) at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.invoke(HbaseRPC.java:211) at $Proxy5.createTable(Unknown Source) at org.apache.hadoop.hbase.client.HBaseAdmin.createTableAsync(HBaseAdmin.java:184) at org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:144) at com.openplaces.util.hbaserecord.connectionadapters.HbaseAdapter.truncateTable(HbaseAdapter.java:502) at com.openplaces.util.hbaserecord.Base$Singleton.truncate(Base.java:609) ... 21 moreimport java.io.IOException;import java.util.Collection;import org.apache.hadoop.hbase.HBaseConfiguration;import org.apache.hadoop.hbase.HColumnDescriptor;import org.apache.hadoop.hbase.HTableDescriptor;import org.apache.hadoop.hbase.client.HBaseAdmin;import org.apache.hadoop.hbase.client.HTable;import junit.framework.TestCase;@SuppressWarnings("deprecation")public class TestTruncate extends TestCase { public void testTruncateInBranch_0_1() throws IOException{ HTable table = new HTable("mytable"); HBaseAdmin admin = new HBaseAdmin(new HBaseConfiguration()); HTableDescriptor tableDesc = table.getMetadata(); admin.deleteTable(table.getTableName()); admin.createTable(tableDesc); } public void testTruncateInTrunk() throws IOException{ HTable table = new HTable("mytable"); HBaseAdmin admin = new HBaseAdmin(new HBaseConfiguration()); Collection&lt;HColumnDescriptor&gt; families = table.getMetadata().getFamilies(); HTableDescriptor tableDesc = new HTableDescriptor(table.getTableName()); for(HColumnDescriptor family : families){ tableDesc.addFamily(family); } admin.deleteTable(table.getTableName()); admin.createTable(tableDesc); }}</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="742" opendate="2008-7-11 00:00:00" fixdate="2008-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column length limit is not enforced</summary>
      <description>HColumnDescriptor provides for a limit on column value length but it is not enforced in 0.1.3 or 0.2.0 other than in the REST and Thrift APIs. (I thought it was enforced in some earlier revision but cannot find it).Enforcement on the client side would be less complicated than doing it on the server side.</description>
      <version>0.1.3,0.2.0</version>
      <fixedVersion>0.2.0,0.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="743" opendate="2008-7-11 00:00:00" fixdate="2008-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bin/hbase migrate upgrade fails when redo logs exists</summary>
      <description>I migrated several hbase-0.1.3 instances to hbase trunk and even if I stop hbase-0.1.3 cleanup it leaves redo logs on hdfs. The problems is that when migrating the data with hbase-trunk it fails because it finds these redo-logs and quit with a error message saying that we should reinstall the old hbase and shut it down cleanly and that in theory it erases the redo logs. The work around has been to delete the redo logs manually... which is bad.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="745" opendate="2008-7-15 00:00:00" fixdate="2008-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scaling of one regionserver, improving memory and cpu usage</summary>
      <description>after weeks testing hbase 0.1.3 and hadoop(0.16.4, 0.17.1), i found there are many works to do, before a particular regionserver can handle data about 100G, or even more. i'd share my opions here with stack, and other developers.first, the easiest way improving scalability of regionserver is upgrading hardware, use 64bit os and 8G memory for the regionserver process, and speed up disk io. besides hardware, following are software bottlenecks i found in regionserver:1. as data increasing, compaction was eating cpu(with io) times, the total compaction time is basicly linear relative to whole data size, even worse, sometimes square relavtive to that size.2. memory usage are depends on opened mapfiles3. network connection are depends on opened mapfiles, see HADOOP-2341 and HBASE-24.</description>
      <version>0.1.3,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.util.TestMetaUtils.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="746" opendate="2008-7-15 00:00:00" fixdate="2008-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batching row mutations via thrift</summary>
      <description>It would be nice to be able to send to thrift a whole bunch of rows and mutations to be applied to them. This will be very useful when doing a large initial dump to HBase as doing the serialization for each row separately is expensive.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
    </fixedFiles>
  </bug>
  <bug id="747" opendate="2008-7-15 00:00:00" fixdate="2008-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a simple way to do batch updates of many rows</summary>
      <description>Add a simple to do batch updates of many rows as described in HBASE-48.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="748" opendate="2008-7-16 00:00:00" fixdate="2008-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an efficient way to batch update many rows</summary>
      <description>HBASE-747 introduced a simple way to batch update many rows. The goal of this issue is to have an enhanced version that will send many rows in a single RPC to each region server. To do this, the client code will have to figure which rows goes to which server, group them accordingly and then send them.</description>
      <version>0.1.3,0.2.0</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.BatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7480" opendate="2013-1-2 00:00:00" fixdate="2013-1-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explicit message for not allowed snapshot on meta tables</summary>
      <description>taking a snapshot of ROOT or .META. now results in something like this:Illegal first character &lt;46&gt; at 0. User-space table names can only start with 'word characters': i.e. [a-zA-Z_0-9]changing the message in something more human readable to inform that meta table are not supported</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="7484" opendate="2013-1-3 00:00:00" fixdate="2013-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Restore with schema changes</summary>
      <description>The restore code in the offline branch doesn't handle the schema change.I think that I've lost it during the various rebase, the Handler restore the schema, but the restoreRegion() method in the helper handle just the "same schema" case.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="754" opendate="2008-7-18 00:00:00" fixdate="2008-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The JRuby shell documentation is wrong in "get" and "put"</summary>
      <description>In the shell documentation we can read: hbase&gt; get 't1', 'r1', {TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4}when in fact there are no facility for this. It will work only because it uses getRow(row, ts).Alsohbase&gt; put 't1', 'r1', 'c1', ts1does not work because the 'value' is missing.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="762" opendate="2008-7-22 00:00:00" fixdate="2008-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteFamily takes timestamp, should only take row and family. Javadoc describes both cases but only implements the timestamp case.</summary>
      <description>The three version of deleteFamily in client.HTable (Text, String, byte[]) have varying descriptions about whether they take timestamps or not.public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family, long timestamp) throws IOException Delete all cells for a row with matching column family with timestamps less than or equal to timestamp. These will become:public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family) throws IOException Delete all cells for a row with matching column family at all timestamps.Per Jean-Daniel's comment, deleteAll should then not permit families. I'm unsure whether this is currently allowed or not, but the documentation must be updated either way.Will post patch after more thorough testing.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7641" opendate="2013-1-21 00:00:00" fixdate="2013-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port HBASE-6669 &amp;#39;Add BigDecimalColumnInterpreter for doing aggregations using AggregationClient&amp;#39; to trunk</summary>
      <description>ColumnInterpreter implementation in trunk is different from that in 0.94This issue ports BigDecimalColumnInterpreter to trunk</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="768" opendate="2008-7-23 00:00:00" fixdate="2008-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Migration] This message &amp;#39;java.io.IOException: Install 0.1.x of hbase and run its migration first&amp;#39; is useless</summary>
      <description>You'll see above message after you've committed to a new version of hadoop. You won't be able to go back.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7680" opendate="2013-1-26 00:00:00" fixdate="2013-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement compaction policy for stripe compactions</summary>
      <description>Bringing into 0.95.2 so gets some consideration</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeStoreFileManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreFileManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreConfigInformation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug id="7681" opendate="2013-1-26 00:00:00" fixdate="2013-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address some recent random test failures</summary>
      <description>I've seen many unspecific test failures recently that cannot be reproduced locally even when running these test is a loop for a very long time.Many of these test one way or the other make assumption w.r.t. wall clock time. While I cannot fix that, an option to increase some of these timeout a bit.This issue is to remind me to do that.</description>
      <version>None</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServerCmdLine.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNodeHealthCheckChore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitLogWorker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="7770" opendate="2013-2-5 00:00:00" fixdate="2013-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>minor integration test framework fixes</summary>
      <description>made FileSystem on HBaseTestingUtil.createMulti() not expect mini cluster added check if server is not running before deciding to restore a server</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.DistributedHBaseCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="783" opendate="2008-7-28 00:00:00" fixdate="2008-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For single row, single family retrieval, getRow() works half as fast as getScanner().next()</summary>
      <description>We have a very typical use-case of wanting to retrieve all columns under a single row, single family.Benchmarking regularly reproduces the same result. Using HTable.getScanner().next() to get the RowResult works twice as fast as HTable.getRow().This is related to HBASE-631 by jdcryans which added this functionality to getRow().</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="787" opendate="2008-7-30 00:00:00" fixdate="2008-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgresql to HBase table replication example</summary>
      <description>It is useful to have an easy way to replicate data from Postgresql tables to a HBase tables.I made a simple python tool which does this, called hbrep.hbrep is a tool for replicating data from postgresql tables to hbase tables.Dependancies: python 2.4 hbase 0.2.0 skytools 2.1.7 postgresqlIt has two main functions. bootstrap, which bootstraps all the data from specified columns of a table play, which processes incoming insert, update and delete events and applies them to hbase.Example usage:install triggers: ./hbrep.py hbrep.ini install schema1.table1 schema2.table2now that future updates are queuing, bootstrap the tables. ./hbrep.py hbrep.ini bootstrap schema1.table1 schema2.table2start pgq ticker (this is part of skytools, it manages event queues and sends the events to registered consumers). pgqadm.py pgq.ini tickerplay our queue consumer to replicate events ./hbrep.py hbrep.ini play schema1.table1 schema2.table2more details in the readme.feedback and improvements appreciated.</description>
      <version>0.2.0</version>
      <fixedVersion>0.18.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.io.RowResult.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseMapWritable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="789" opendate="2008-7-31 00:00:00" fixdate="2008-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add clover coverage report targets</summary>
      <description>Clover has an open source license for projects under ASF. We can use that to generate coverage report.</description>
      <version>0.2.0</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="79" opendate="2008-1-30 00:00:00" fixdate="2008-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] When HBase needs to be migrated, it should display a message on stdout, not just in the logs</summary>
      <description>When you upgrade your HBase code version, there is occasionally the need to migrate the underlying data store to a new version. However, if you are unaware of this need, then you'll be very confused by what happens when you restart HBase. Using start-hbase.sh, you get messages indicating that the master and regionservers started as expected. However, in reality, it will have tried to start and failed due to a version mismatch. This information is displayed in the logs, but you won't know that until you go log diving.Instead, let's have the start-hbase.sh script do a check to see if the version number is wrong itself. That way, if it fails, it can write messages about startup failure to the console instead of to the logs. This will make new admins much happier.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7902" opendate="2013-2-22 00:00:00" fixdate="2013-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deletes may be removed during minor compaction, in non-standard compaction schemes [rename enums]</summary>
      <description>Deletes are only removed during major compaction now. However, in presence of file ordering, deletes can be removed during minor compaction too, as long as there's no file that is not being compacted that is older than the files that are.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorScanPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.NoOpScanPolicyObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.HFileReadWriteTest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanType.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactionPolicy.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.ZooKeeperScanPolicyObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7904" opendate="2013-2-22 00:00:00" fixdate="2013-4-22 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>Make mapreduce jobs pass based on 2.0.4-alpha</summary>
      <description>2.0.3-alpha has been released.We should upgrade the dependency.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="7905" opendate="2013-2-22 00:00:00" fixdate="2013-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add passing of optional cell blocks over rpc</summary>
      <description>Make it so we can pass Cells/data w/o having to bury it all in protobuf to get it over the wire.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServerCmdLine.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestJoinedScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestClusterId.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.protobuf.TestProtobufUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestProtoBufRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestDelayedRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWithScanLimits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiParallel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSideWithCoprocessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientScannerRPCTimeout.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift.IncrementCoalescer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.ZKProcedureMemberRpcs.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MutationSerialization.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.KeyValueSerialization.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.ProtobufRpcServerEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MultiRowMutationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.RPC.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.MultiRowMutation.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RPCProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MultiRowMutation.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.ProtobufRpcClientEngine.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RowMutations.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="791" opendate="2008-8-1 00:00:00" fixdate="2008-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RowCount doesn&amp;#39;t work</summary>
      <description>From Yair Even-Zoharlooked at the code in the 0.2.0 and the args&amp;#91;0&amp;#93; is used twice c.set("hbase.master", args&amp;#91;0&amp;#93;);And // First arg is the output directory. c.setOutputPath(new Path(args&amp;#91;0&amp;#93;));Was anybody able to use this class?In fact it does not work and there is also a NPE that gets thrown.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7930" opendate="2013-2-25 00:00:00" fixdate="2013-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbck should provide an option to fix .META. rows with empty REGIONINFO_QUALIFIER</summary>
      <description>Today when master and HBCK are reporting empty REGIONINFO_QUALIFIER .META. rows, we need to manually delete them from the .META. region. We need to enhance hbck to do that automatically.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="7932" opendate="2013-2-25 00:00:00" fixdate="2013-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do the necessary plumbing for the region locations in META table and send the info to the RegionServers</summary>
      <description>To be specific, this jira is to do the following during createTable:1. Favored node calculation2. Persisting of calculation in .META.3. Letting RegionServers know of the favored nodes for new regions4. Addition of a balancer that will honor favored nodes but it is currently a noop.</description>
      <version>None</version>
      <fixedVersion>0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.LoadBalancerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="7933" opendate="2013-2-25 00:00:00" fixdate="2013-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in TableLockManager</summary>
      <description>We are getting NPE in TableLockManager sometimes in tests.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="801" opendate="2008-8-7 00:00:00" fixdate="2008-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] When a table haven&amp;#39;t disable, shell could response in a "user friendly" way.</summary>
      <description>Currently, 0.2.0's Hbase shell will throw raw exception if user execute a 'delete' command before 'disable' the table. Compare to 0.1.2's shell script, we would like to have a more friendly message instead of dumping the exception.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
    </fixedFiles>
  </bug>
  <bug id="8050" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small fix to book/hbase.tests.html</summary>
      <description>Writing Tests section appears to have a formatting mistake. "Categories and execution time" looks like it was intended to be a sibling heading to "General Rules" and "Sleeps in tests".</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8051" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>0.95 build failing on site goal: &amp;#39;failed to get report for org.apache.maven.plugins:maven-project-info-reports-plugin: Could not find goal &amp;#39;dependency-info&amp;#39;&amp;#39;</summary>
      <description>I cannot reproduce locally using same mvn. Let me try upgrading our report plugin. Apparently 'dependency-info' is a new target since 2.5 and our version is 2.4 going by http://maven.apache.org/plugins/maven-project-info-reports-plugin/ (I can't find an explicity invocation of 'dependency-info')</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8054" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-7797 Use consistent package name dregs</summary>
      <description>I didn't get it all over in HBASE-7797. Elliott just noticed.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.PrefixTreeTestConstants.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.vint.TestVLongTool.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.vint.TestVIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.vint.TestFIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.number.RandomNumberUtils.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.number.NumberFormatter.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.comparator.ByteArrayComparator.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.util.bytes.TestByteRange.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.timestamp.TestTimestampEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.timestamp.TestTimestampData.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestampDataRepeats.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestampDataNumbers.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.timestamp.data.TestTimestampDataBasic.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.TestRowEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.TestRowData.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.TestPrefixTreeSearcher.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataUrlsExample.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataUrls.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataTrivial.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataSingleQualifier.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataSimple.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataSearcherRowMiss.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataRandomKeyValues.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataQualifierByteOrdering.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataNumberStrings.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataNub.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataMultiFamilies.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataExerciseFInts.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataEmpty.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataDifferentTimestamps.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataDeeper.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.data.TestRowDataComplexQualifiers.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.row.BaseTestRowData.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.DataBlockEncoding.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.ArraySearcherPool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.column.ColumnNodeReader.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.column.ColumnReader.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.DecoderFactory.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.PrefixTreeArrayReversibleScanner.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.PrefixTreeArrayScanner.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.PrefixTreeCell.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.row.RowNodeReader.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.timestamp.MvccVersionDecoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.decode.timestamp.TimestampDecoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.column.ColumnNodeWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.column.ColumnSectionWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.EncoderFactory.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.EncoderPool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.other.CellTypeEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.other.LongEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.PrefixTreeEncoder.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.row.RowNodeWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.row.RowSectionWriter.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.ThreadLocalEncoderPool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.tokenize.TokenDepthComparator.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.tokenize.Tokenizer.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerNode.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerRowSearchPosition.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.encode.tokenize.TokenizerRowSearchResult.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.PrefixTreeBlockMeta.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.PrefixTreeCodec.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.scanner.CellScannerPosition.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.scanner.CellSearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.codec.prefixtree.scanner.ReversibleCellScanner.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.byterange.ByteRangeSet.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.byterange.impl.ByteRangeHashSet.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.byterange.impl.ByteRangeTreeSet.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.vint.UFIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.vint.UVIntTool.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hbase.util.vint.UVLongTool.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.keyvalue.TestKeyValueTool.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.blockmeta.TestBlockMeta.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.builder.data.TestTokenizerDataBasic.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.builder.data.TestTokenizerDataEdgeCase.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.builder.TestTokenizer.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.builder.TestTokenizerData.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.builder.TestTreeDepth.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.column.data.TestColumnDataRandom.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.column.data.TestColumnDataSimple.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.column.TestColumnBuilder.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hbase.codec.prefixtree.column.TestColumnData.java</file>
    </fixedFiles>
  </bug>
  <bug id="806" opendate="2008-8-8 00:00:00" fixdate="2008-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change HbaseMapWritable and RowResult to implement SortedMap instead of Map</summary>
      <description>HbaseMapWritable and RowResult currently implement Map. However, it would be trivial (and highly useful) for them to implement SortedMap since HbaseMapWritable already uses a TreeMap for the map.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="810" opendate="2008-8-9 00:00:00" fixdate="2008-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent temporary deadlocks when, during a scan with write operations, the region splits</summary>
      <description>HBASE-804 was not about the good problem, this one is. Anyone that iterates through the results of a scanner and that rewrites data back into the row at each iteration will hit a UnknownScannerException if a split occurs. See the stack in the referred jira. Timeline :Split occurs, acquires a write lock and waits for scanners to finishThe scanner in the custom code iterates and writes data until the write is blocked by the lockdeadlockThe scanner timeouts thus the region splits but the USE will be thrown when next() is calledInside a Map, the task will simply be retried when the first one fails. Elsewhere, it becomes more complicated.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="811" opendate="2008-8-10 00:00:00" fixdate="2008-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTD is not fully copyable</summary>
      <description>Part of my HBASE-62 patch was not applied.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="813" opendate="2008-8-10 00:00:00" fixdate="2008-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a row counter in the new shell</summary>
      <description>Asked for in the thread "Any equivalence of HQL "select count..." in 0.2 shell?". Add a row counter in the new shell.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
      <file type="M">bin.HBase.rb</file>
      <file type="M">bin.Formatter.rb</file>
    </fixedFiles>
  </bug>
  <bug id="8162" opendate="2013-3-21 00:00:00" fixdate="2013-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix import of hbase-default.xml into refguide; broke</summary>
      <description>We are importing html version of the transformed hbase-default.xml; our styl'ing of the xml has broken.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8165" opendate="2013-3-21 00:00:00" fixdate="2013-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move to Hadoop 2.1.0-beta from 2.0.x-alpha (WAS: Update our protobuf to 2.5 from 2.4.1)</summary>
      <description>Update to new 2.5 pb. Some speedups and a new PARSER idiom that bypasses making a builder.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.CellProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.WALProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AggregateProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ComparatorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ErrorHandlingProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FilterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FSProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HFileProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.LoadBalancerProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterMonitorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MultiRowMutation.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProcessorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RPCProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.Tracing.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterAdmin.proto</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.CellMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.VersionMessage.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.protobuf.generated.ColumnAggregationProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.protobuf.generated.IncrementCounterProcessorTestProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.protobuf.generated.PingProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.protobuf.generated.TestDelayedRpcProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.protobuf.generated.TestProtos.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.protobuf.generated.TestRpcServiceProtos.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.protobuf.README.txt</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.HBaseCommonTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="819" opendate="2008-8-11 00:00:00" fixdate="2008-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove DOS-style ^M carriage returns from all code where found</summary>
      <description>There are a few files that contain DOS-style carriage returns. This is leading to issues when applying patches.The presence of these may also be causing a snowball effect as some IDEs/editors may see one and attempt to apply that LF/CR format to all lines or files.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8191" opendate="2013-3-23 00:00:00" fixdate="2013-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation is not giving the right class name for offline merges.</summary>
      <description>Existing:$ bin/hbase org.apache.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;Should be:$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="830" opendate="2008-8-14 00:00:00" fixdate="2008-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debugging HCM.locateRegionInMeta is painful</summary>
      <description>I've been debugging a case where a bunch of reduces were hanging for no apparent reason and then get killed because they did not do anything for 600 seconds. I figured that it's because we are stuck in a very long waiting time due to retry backoffs. public static int RETRY_BACKOFF[] = { 1, 1, 1, 1, 2, 4, 8, 16, 32, 64 };That means we wait 10 sec, 10 sec, 10, 10, ... then 640 sec. That's a long time, do we really need that much time to finally be warned that there's a bug in HBase? Also, the places where we get this:LOG.debug("reloading table servers because: " + t.getMessage());should be more verbose. I my logs these are caused by a table not found but the only thing I see is "reloading table servers because: tableName".</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="831" opendate="2008-8-14 00:00:00" fixdate="2008-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>committing BatchUpdate with no row should complain</summary>
      <description>Running this code:BatchUpdate update = new BatchUpdate();update.put(key, value);table.commit(update);Down in getRegionServer, this triggers an NPE because the row is null (which I saw because I was running in a debugger); this NPE gets retried somewhere in the bowels of IPC. Instead, we should either remove the zero-arg BatchUpdate constructor, or have table.commit throw a runtimeexception if the row is null.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="832" opendate="2008-8-14 00:00:00" fixdate="2008-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem with row keys beginnig with characters &lt; than &amp;#39;,&amp;#39; and the region location cache</summary>
      <description>We currently have a problem the way we design .META. row keys. When user table row keys begin with characters lesser than ',' like a '$', any operation will fail when: A client has a certain set of regions in cache One region with the faulty row key splits The client receives a request for a row in the split regionThe reason is that it will first get a NSRE then it will try to locate a region using the passed row key. For example: Row in META: entities,,1216750777411Row passed: entities,$-94f9386f-e235-4cbd-aacc-37210a870991,99999999999999The passed row is lesser then the row in .META.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8321" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log split worker should heartbeat to avoid timeout when the hlog is under recovery</summary>
      <description>Currently, hlog splitter could spend quite sometime to split a log in case any HDFS issue and recoverLease/retry opening is needed. If distributed log split manager times out the log worker, other log worker to take over will run into the same issue.Ideally, we should not need a timeout monitor. Since we have a timeout monitor for DSL now, the worker should heartbeat to avoid wrong/unneeded timeouts.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSMapRUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSHDFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
    </fixedFiles>
  </bug>
  <bug id="8322" opendate="2013-4-10 00:00:00" fixdate="2013-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable hbase checksums by default</summary>
      <description>Double checksumming issues in HBase level checksums(HBASE-5074) has been fixed in HBASE-6868. However, that patch also disables hbase checksums by default. I think we should re-enable it by default, and document the interaction with shortcircuit reads.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.performance.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8410" opendate="2013-4-23 00:00:00" fixdate="2013-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Basic quota support for namespaces</summary>
      <description>This task involves creating an observer which provides basic quota support to namespaces in terms of (1) number of tables and (2) number of regions. The quota support can be enabled by setting:&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.namespace.NamespaceController&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.namespace.NamespaceController&lt;/value&gt;&lt;/property&gt;in the hbase-site.xml.To add quotas to namespace, while creating namespace properties need to be added.Examples:1. namespace_create 'ns1', {'hbase.namespace.quota.maxregion'=&gt;'10'}2. 1. namespace_create 'ns2', {'hbase.namespace.quota.maxtables'=&gt;'2'}, {'hbase.namespace.quota.maxregion'=&gt;'5'}The quotas can be modified/added to namespace at any point of time.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DeleteTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8470" opendate="2013-4-30 00:00:00" fixdate="2013-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data file used by TestReference should be excluded from Apache Rat check</summary>
      <description>The following was discovered by Matteo:hbase-server/src/test/data/a6a6562b777440fd9c34885428f5cb61.21e75333ada3d5bafb34bb918f29576c is used by TestReference to verify the format compatibility.We should exclude this file from Apache Rat check.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8560" opendate="2013-5-16 00:00:00" fixdate="2013-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMasterShutdown failing in trunk 0.95/trunk -- "Unable to get data of znode /hbase/meta-region-server because node does not exist (not an error)"</summary>
      <description>I'm looking at this too... jeffreyz you are too?</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="8581" opendate="2013-5-20 00:00:00" fixdate="2013-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rpc refactor dropped passing the operation timeout through to the rpcclient</summary>
      <description>jd was poking and noticed that we were not passing the operation timeout down as rpc timeout as we used to in 0.94. Let me fix.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientNoCluster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
    </fixedFiles>
  </bug>
  <bug id="8611" opendate="2013-5-24 00:00:00" fixdate="2013-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve test coverage in pkg org.apache.hadoop.hbase.mapred</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="865" opendate="2008-9-3 00:00:00" fixdate="2008-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc warnings</summary>
      <description>There are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.2,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8652" opendate="2013-5-29 00:00:00" fixdate="2013-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of compacting KVs is not reset at the end of compaction</summary>
      <description>Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).In DefaultCompactor#compact(), we have this at the beginning: this.progress = new CompactionProgress(fd.maxKeyCount);But progress.totalCompactingKVs is not reset at the end of compact().</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8826" opendate="2013-6-28 00:00:00" fixdate="2013-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-8695 is covered in Thrift 2</summary>
      <description>HBASE-8695 is about using the config file, make sure Thrift 2 is doing the same.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="883" opendate="2008-9-12 00:00:00" fixdate="2008-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secondary Indexes</summary>
      <description>I'm working on a secondary index impl. The basic idea is to maintain a separate table per index.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.package.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecification.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexNotFoundException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
    </fixedFiles>
  </bug>
  <bug id="8832" opendate="2013-6-28 00:00:00" fixdate="2013-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-4658 is supported by Thrift 2</summary>
      <description>HBASE-4658 adds support for "attributes" for certain operations. Make sure Thrift 2 supports them where ever available in the native API.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
    </fixedFiles>
  </bug>
  <bug id="889" opendate="2008-9-18 00:00:00" fixdate="2008-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.</summary>
      <description>The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
