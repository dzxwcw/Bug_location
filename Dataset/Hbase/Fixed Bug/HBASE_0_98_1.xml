<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="1020" opendate="2008-11-22 00:00:00" fixdate="2008-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver OOME handler should dump vital stats</summary>
      <description>On OOME the regionserver should dump into the log some vital stats: Number of regions Number of store files Estimated item count and size of memcache(s) Estimated item count and size of store file indexesAssumes the reserve can be released upon OOME to allow the additional actions.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10252" opendate="2013-12-29 00:00:00" fixdate="2013-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t write back to WAL/memstore when Increment amount is zero (mostly for query rather than update intention)</summary>
      <description>When user calls Increment by providing amount=0, we don't write the original value to WAL or memstore : adding 0 yields a 'new' value just with the same value as the original one.1. user provides 0 amount for query rather than for update, this fix is ok; this intention is the most possible case;2. user provides 0 amount for an update, this fix is also ok : no need to touch back-end value if that value isn't changed;3. either case we both return correct value, and keep subsequent query results correct : if the 0 amount Increment is the first update, the query is the same for retrieving a 0 value or retrieving nothing;</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestDurability.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="1027" opendate="2008-11-24 00:00:00" fixdate="2008-12-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make global flusher check work with percentages rather than hard code memory sizes.</summary>
      <description>Currently defaults are 512M for upperbound and 256 for the lowerbound. Comes of HBASE-1023.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestGlobalMemcacheLimit.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.MemcacheFlusher.java</file>
    </fixedFiles>
  </bug>
  <bug id="10729" opendate="2014-3-12 00:00:00" fixdate="2014-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable table doesn&amp;#39;t balance out replicas evenly if the replicas were unassigned earlier</summary>
      <description>Enable table doesn't assign out replicas keeping availability in mind, if the replicas were unassigned before the table was disabled. For example, when a snapshot is restored and then the table is enabled, the replicas are unevenly assigned. The reason for this is that the the enable table invokes randomAssign that assigns one region at a time. Since the master doesn't have any information about the unassigned replicas, the calls to randomAssign can't do any availability checks.</description>
      <version>None</version>
      <fixedVersion>0.99.0,hbase-10070</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="10823" opendate="2014-3-25 00:00:00" fixdate="2014-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve LATEST_TIMESTAMP to current server time before scanning for ACLs</summary>
      <description>Storing values with timestamps in the future is probably bad practice and can lead to surprises. If cells with timestamps in the future have ACLs, permissions from those ACLs will incorrectly be considered for authorizing the pending mutation. For sure that will be surprising.We should be able to avoid this case by resolving LATEST_TIMESTAMP to the current server time when creating the internal scanner for finding ACLs in the covered cell set. Documenting a todo item from a discussion between anoop.hbase and myself.</description>
      <version>0.98.1</version>
      <fixedVersion>0.99.0,0.98.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCellACLWithMultipleVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCellACLs.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug id="10824" opendate="2014-3-25 00:00:00" fixdate="2014-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance detection of protobuf generated code in line length check</summary>
      <description>In review of HBASE-5175, Anoop found that long line detection wasn't very effective for protobuf generated code.Here is one example:-1 lineLengths. The patch introduces the following lines longer than 100:+ private DoubleMsg(boolean noInit){ this.unknownFields = com.google.protobuf.UnknownFieldSet.getDefaultInstance(); }This is current filter: lines=`cat $PATCH_DIR/patch | grep "^+" | grep -v "^@@" | grep -v "^+++" | grep -v "import" | grep -v "hbase.protobuf.generated" | awk -v len="$MAX_LINE_LENGTH_PATCH" 'length ($0) &gt; len' | head -n 10`'com.google.protobuf.' should be considered as well.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="10842" opendate="2014-3-26 00:00:00" fixdate="2014-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some loggers not declared static final</summary>
      <description>In a few of source files, the logger is defined as private final Log LOG = LogFactory.getLog(MyClass.class);This should be changed to static final.One question is about the following declaration:private final Log LOG = LogFactory.getLog(this.getClass());In this form, the logger can be shared by derived classes. But one will get NPE when logging in methods that are invoked inside the constructors.</description>
      <version>0.98.1,0.96.1.1</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSizeCalculator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.OfflineCallback.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableOutputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug id="10883" opendate="2014-4-1 00:00:00" fixdate="2014-5-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restrict the universe of labels and authorizations</summary>
      <description>Currently we allow any string as visibility label or request authorization. However as seen on HBASE-10878, we accept for authorizations strings that would not work if provided as labels in visibility expressions. We should throw an exception at least in cases where someone tries to define or use a label or authorization including visibility expression operators '&amp;', '|', '!', '(', ')'.</description>
      <version>0.98.1</version>
      <fixedVersion>0.99.0,0.98.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestScan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityLabelsValidator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.visibility.Authorizations.java</file>
    </fixedFiles>
  </bug>
  <bug id="10885" opendate="2014-4-1 00:00:00" fixdate="2014-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support visibility expressions on Deletes</summary>
      <description>Accumulo can specify visibility expressions for delete markers. During compaction the cells covered by the tombstone are determined in part by matching the visibility expression. This is useful for the use case of data set coalescing, where entries from multiple data sets carrying different labels are combined into one common large table. Later, a subset of entries can be conveniently removed using visibility expressions.Currently doing the same in HBase would only be possible with a custom coprocessor. Otherwise, a Delete will affect all cells covered by the tombstone regardless of any visibility expression scoping. This is correct behavior in that no data spill is possible, but certainly could be surprising, and is only meant to be transitional. We decided not to support visibility expressions on Deletes to control the complexity of the initial implementation.</description>
      <version>0.98.1</version>
      <fixedVersion>0.99.0,0.98.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTSVWithVisibilityLabels.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DeleteTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LabelExpander.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.TagType.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="10886" opendate="2014-4-1 00:00:00" fixdate="2014-9-1 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>add htrace-zipkin to the runtime dependencies again</summary>
      <description>Once htrace-zipkin was removed from depencencies in HBASE-9700. Because all of the depencencies of htrace-zipkin is bundled with HBase now, it is good to add it for the ease of use.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.tracing.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10888" opendate="2014-4-1 00:00:00" fixdate="2014-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable distributed log replay as default</summary>
      <description>Enable 'distributed log replay' by default. Depends on hfilev3 being enabled.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.LogReplayHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.DeadServer.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
    </fixedFiles>
  </bug>
  <bug id="10903" opendate="2014-4-3 00:00:00" fixdate="2014-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-10740 regression; cannot pass commands for zk to run</summary>
      <description>We can't do this:./bin/hbase zkcli deleteall /hbase/rs/c2022.halxg.cloudera.com,16020,1396502726715after upgrade to 3.4.6 zookeeper. Works if I put back 3.4.5.See below where only difference is the zk jar:[stack@c2022 hbase-0.99.0-SNAPSHOT]$ ~/bin/java/bin/java -cp ~/hbase-0.96.1.1-hadoop2/lib/zookeeper-3.4.5.jar:lib/slf4j-log4j12-1.6.4.jar:lib/slf4j-api-1.6.4.jar:lib/log4j-1.2.17.jar org.apache.zookeeper.ZooKeeperMain -server c2020:2181 ls "/hbase/rs"Connecting to c2020:2181log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.WATCHER::WatchedEvent state:SyncConnected type:None path:null[c2020.halxg.cloudera.com,16020,1396482186194, c2021.halxg.cloudera.com,16020,1396499398203, c2023.halxg.cloudera.com,16020,1396498834473, c2025.halxg.cloudera.com,16020,1396482188110, c2022.halxg.cloudera.com,16020,1396502726715, c2024.halxg.cloudera.com,16020,1396482188280][stack@c2022 hbase-0.99.0-SNAPSHOT]$ ~/bin/java/bin/java -cp lib/zookeeper-3.4.6.jar:lib/slf4j-log4j12-1.6.4.jar:lib/slf4j-api-1.6.4.jar:lib/log4j-1.2.17.jar org.apache.zookeeper.ZooKeeperMain -server c2020:2181 ls "/hbase/rs"Connecting to c2020:2181log4j:WARN No appenders could be found for logger (org.apache.zookeeper.ZooKeeper).log4j:WARN Please initialize the log4j system properly.log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.</description>
      <version>0.98.1,0.99.0</version>
      <fixedVersion>0.99.0,0.98.2,0.96.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperMainServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="10948" opendate="2014-4-9 00:00:00" fixdate="2014-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase table file &amp;#39;x&amp;#39; mode</summary>
      <description>The hbase table files currently all have 'x' mode in there:$hadoop fs -ls -R /hbase/data/default/TestTable/drwxr-xr-x - hbase biadmin 0 2014-04-08 20:53 /hbase/data/default/TestTable/.tabledesc-rw-r--r-- 1 hbase biadmin 313 2014-04-08 20:53 /hbase/data/default/TestTable/.tabledesc/.tableinfo.0000000001drwxr-xr-x - hbase biadmin 0 2014-04-08 20:53 /hbase/data/default/TestTable/724c8c3075da516b450ce4826327ce64-rwxr-xr-x 1 hbase biadmin 68 2014-04-08 20:53 /hbase/data/default/TestTable/724c8c3075da516b450ce4826327ce64/.regioninfodrwxr-xr-x - hbase biadmin 0 2014-04-08 21:54 /hbase/data/default/TestTable/724c8c3075da516b450ce4826327ce64/info-rwxr-xr-x 1 hbase biadmin 272958577 2014-04-08 20:53 /hbase/data/default/TestTable/724c8c3075da516b450ce4826327ce64/info/7138e61cbcd24538b64726db13dab48e-rwxr-xr-x 1 hbase biadmin 108603714 2014-04-08 20:53 /hbase/data/default/TestTable/724c8c3075da516b450ce4826327ce64/info/9ce233fcdfde49679797d13f28e26129drwxr-xr-x - hbase biadmin 0 2014-04-08 20:53 /hbase/data/default/TestTable/b5350c581363f786e49ff6f32e71f564-rwxr-xr-x 1 hbase biadmin 68 2014-04-08 20:53 /hbase/data/default/TestTable/b5350c581363f786e49ff6f32e71f564/.regioninfodrwxr-xr-x - hbase biadmin 0 2014-04-08 21:54 /hbase/data/default/TestTable/b5350c581363f786e49ff6f32e71f564/info-rwxr-xr-x 1 hbase biadmin 33800049 2014-04-08 21:54 /hbase/data/default/TestTable/b5350c581363f786e49ff6f32e71f564/info/576190de431341b9a02280654e3deb58-rwxr-xr-x 1 hbase biadmin 108650474 2014-04-08 20:53 /hbase/data/default/TestTable/b5350c581363f786e49ff6f32e71f564/info/7c54098fb62a4ef29aab0f5658b25260If the user does not specify 'hbase.data.umask', we use the fs default:FsPermission.getDefault()Instead we should use FsPermission.getFileDefault().</description>
      <version>0.96.2,0.98.1</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="10951" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use PBKDF2 to generate test encryption keys in the shell</summary>
      <description>We provide some support in the shell for setting the column family data encryption key, which enables some simple testing when kicking the tires. (CF data key management should be done using the Java API.) Despite the very modest goal there might be an objection to using a hash instead of a key derivation function, so just go ahead and do that.</description>
      <version>0.98.1,0.99.0</version>
      <fixedVersion>0.99.0,0.98.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.crypto.Encryption.java</file>
    </fixedFiles>
  </bug>
  <bug id="10952" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[REST] Let the user turn off block caching if desired</summary>
      <description>After HBASE-10884 the REST gateway will use scanner defaults with respect to block caching. Add support for a query parameter for hinting blocks for the query should not be cached. Enable block caching by default.</description>
      <version>0.98.1,0.99.0</version>
      <fixedVersion>0.99.0,0.98.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rest.model.TestScannerModel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.rest.XMLSchema.xsd</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.ScannerMessage.proto</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.ScannerResultGenerator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RowResultGenerator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.ResultGenerator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.MultiRowResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug id="10955" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK leaves the region in masters in-memory RegionStates if region hdfs dir is lost</summary>
      <description>One of our tests removes the hdfs directory for the region, and invokes HBCK to fix the issue. This test fails flakily because the region is removed from meta and unassigned, but the region is not offlined from the masters in-memory. This affects further LB runs and disable table, etc. In case of inMeta &amp;&amp; !inHdfs &amp;&amp; isDeployed, we should not just close the region from RS, but call master.unassign().</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="10956" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop-2 dependency to 2.4.0</summary>
      <description>Hadoop 2.4.0 has been released:http://search-hadoop.com/m/LgpTk2YKhUfThis JIRA is to upgrade hadoop-2 dependency to 2.4.0</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="10958" opendate="2014-4-10 00:00:00" fixdate="2014-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[dataloss] Bulk loading with seqids can prevent some log entries from being replayed</summary>
      <description>We found an issue with bulk loads causing data loss when assigning sequence ids (HBASE-6630) that is triggered when replaying recovered edits. We're nicknaming this issue Blindspot.The problem is that the sequence id given to a bulk loaded file is higher than those of the edits in the region's memstore. When replaying recovered edits, the rule to skip some of them is that they have to be lower than the highest sequence id. In other words, the edits that have a sequence id lower than the highest one in the store files should have also been flushed. This is not the case with bulk loaded files since we now have an HFile with a sequence id higher than unflushed edits.The log recovery code takes this into account by simply skipping the bulk loaded files, but this "bulk loaded status" is lost on compaction. The edits in the logs that have a sequence id lower than the bulk loaded file that got compacted are put in a blind spot and are skipped during replay.Here's the easiest way to recreate this issue: Create an empty table Put one row in it (let's say it gets seqid 1) Bulk load one file (it gets seqid 2). I used ImporTsv and set hbase.mapreduce.bulkload.assign.sequenceNumbers. Bulk load a second file the same way (it gets seqid 3). Major compact the table (the new file has seqid 3 and isn't considered bulk loaded). Kill the region server that holds the table's region. Scan the table once the region is made available again. The first row, at seqid 1, will be missing since the HFile with seqid 3 makes us believe that everything that came before it was flushed.</description>
      <version>0.96.2,0.98.1,0.94.18</version>
      <fixedVersion>0.99.0,0.98.2,0.96.3,0.94.20</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="11038" opendate="2014-4-19 00:00:00" fixdate="2014-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filtered scans can bypass metrics collection</summary>
      <description>In RegionScannerImpl#nextRaw, after a batch of results are retrieved, delegates to the filter regarding continuation of the scan. If filterAllRemaining returns true, the method exits immediately, without calling MetricsRegion#updateNextScan.</description>
      <version>0.96.2,0.98.1,0.99.0</version>
      <fixedVersion>0.99.0,0.98.2,0.96.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="11047" opendate="2014-4-21 00:00:00" fixdate="2014-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove TimeoutMontior</summary>
      <description>With HBASE-8002, the TimeoutMonitor is disabled by default. Lately, we haven't see much problem of region assignments during integration testing with CM. I was thinking it may be time to remove the timeout monitor now?</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestGlobalMemStoreSize.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRollingRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestOpenedRegionHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterRestartAfterDisablingTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.UnAssignCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKAssign.java</file>
    </fixedFiles>
  </bug>
  <bug id="1105" opendate="2009-1-1 00:00:00" fixdate="2009-1-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove duplicated code in HCM, add javadoc to RegionState, etc.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="11052" opendate="2014-4-23 00:00:00" fixdate="2014-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sending random data crashes thrift service</summary>
      <description>Upstream thrift library has a know issue (THRIFT-601) causing the thrift server to crash with an Out-of-Memory Error when bogus requests are sent.This reproduces when a very large request size is sent in the request header, making the thrift server to allocate a large memory segment leading to OOM.LoadBalancer health checks are the first "candidate" for bogus requestsThrift developers admit this is a known issue with TBinaryProtocol and their recommandation is to use TCompactProtocol/TFramedTransport but this requires all thrift clients to be updated (might not be feasible atm)So we need a fix similar to CASSANDRA-475.</description>
      <version>0.98.1,1.0.0,0.94.18</version>
      <fixedVersion>0.99.0,0.94.21,0.98.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11081" opendate="2014-4-25 00:00:00" fixdate="2014-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk Master won&amp;#39;t start; looking for Constructor that takes conf only</summary>
      <description>Committing the Consensus Infra, we broke starting master. Small fix so constructMaster passes in a ConsensusProvider.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="11096" opendate="2014-4-29 00:00:00" fixdate="2014-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stop method of Master and RegionServer coprocessor is not invoked</summary>
      <description>the stop method of coprocessor specified by "hbase.coprocessor.master.classes" and "hbase.coprocessor.regionserver.classes" is not invoked.If coprocessor allocates OS resources, it could cause master/regionserver resource leak or hang during exit.</description>
      <version>0.96.2,0.98.1,0.94.19</version>
      <fixedVersion>0.99.0,0.98.3,0.94.21</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
    </fixedFiles>
  </bug>
  <bug id="11102" opendate="2014-4-30 00:00:00" fixdate="2014-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document JDK versions supported by each release</summary>
      <description>We can make use of a JDK version x HBase version matrix to explain which JDK version is supported and required. 0.94, 0.96, and 0.98 releases all support JDK6 and JDK7. For 1.0, there is a discussion thread to decide whether to drop JDK6 support. There has been some work to support JDK8. We can also document that.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.getting.started.xml</file>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="11171" opendate="2014-5-14 00:00:00" fixdate="2014-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>More doc improvements on block cache options</summary>
      <description>I have more doc improvement (no code changes for sure this time). Follow on from HBASE-11098.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.DirectMemoryUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SlabCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.DoubleBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferArray.java</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.package-info.java</file>
    </fixedFiles>
  </bug>
  <bug id="11237" opendate="2014-5-22 00:00:00" fixdate="2014-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bulk load initiated by user other than hbase fails</summary>
      <description>Running TestLoadIncrementalHFiles and TestHFileOutputFormat as a properly kinit'd HBase superuser who isn't "hbase" began to fail last month after a patch to fix HBASE-10902 was committed to trunk.</description>
      <version>0.98.1</version>
      <fixedVersion>0.99.0,0.98.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingKeyRange.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.java</file>
    </fixedFiles>
  </bug>
  <bug id="11629" opendate="2014-7-31 00:00:00" fixdate="2014-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operational concerns for Replication should call out ZooKeeper</summary>
      <description>Our design invariants state that ZooKeeper data is safe to delete. However, replication only stores its data in zookeeper. This can lead to operators accidentally disabling their replication set up while attempting to recover from an unrelated issue by clearing the zk state.We should update the operational concerns section on replication to call out that the /hbase/replication tree should not be deleted. We should probably also add a warning to the set up steps.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
      <file type="M">src.main.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12983" opendate="2015-2-7 00:00:00" fixdate="2015-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book mentions hadoop.ssl.enabled when it should be hbase.ssl.enabled</summary>
      <description>In the HBase book we say the following:A default HBase install uses insecure HTTP connections for web UIs for the master and region servers. To enable secure HTTP (HTTPS) connections instead, set hadoop.ssl.enabled to true in hbase-site.xml. This does not change the port used by the Web UI. To change the port for the web UI for a given HBase component, configure that port’s setting in hbase-site.xml. These settings are:The property should be hbase.ssl.enabled instead.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="9346" opendate="2013-8-26 00:00:00" fixdate="2013-12-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK should provide an option to check if regions boundaries are the same in META and in stores.</summary>
      <description>If META don't have the same region boundaries as the stores files, writes and read might go to the wrong place. We need to provide a way to check that withing HBCK.</description>
      <version>0.94.14,0.98.1,0.99.0,0.96.1.1</version>
      <fixedVersion>0.98.0,0.94.16,0.96.2,0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
