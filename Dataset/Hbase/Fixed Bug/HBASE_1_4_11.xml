<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="22618" opendate="2019-6-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>added the possibility to load custom cost functions</summary>
      <description>Hi,We wouls like to open the discussion about bringing the possibility to have regions deployed on Heterogeneous deployment, i.e Hbase cluster running different kind of hardware.Why? Cloud deployments means that we may not be able to have the same hardware throughout the years Some tables may need special requirements such as SSD whereas others should be using hard-drives  in our usecase(single table, dedicated HBase and Hadoop tuned for our usecase, good key distribution), the number of regions per RS was the real limit for us.Our usecaseWe found out that in our usecase(single table, dedicated HBase and Hadoop tuned for our usecase, good key distribution), the number of regions per RS was the real limit for us.Over the years, due to historical reasons and also the need to benchmark new machines, we ended-up with differents groups of hardware: some servers can handle only 180 regions, whereas the biggest can handle more than 900. Because of such a difference, we had to disable the LoadBalancing to avoid the roundRobinAssigmnent. We developed some internal tooling which are responsible for load balancing regions across RegionServers. That was 1.5 year ago.Our Proof-of-conceptWe did work on a Proof-of-concept here, and some early tests here, here, and here. We wrote the balancer for our use-case, which means that: there is one table there is no region-replica good key dispersion there is no regions on masterA rule file is loaded before balancing. It contains lines of rules. A rule is composed of a regexp for hostname, and a limit. For example, we could have: rs&amp;#91;0-9&amp;#93; 200rs1&amp;#91;0-9&amp;#93; 50 RegionServers with hostname matching the first rules will have a limit of 200, and the others 50. If there's no match, a default is set.Thanks to the rule, we have two informations: the max number of regions for this cluster, and the rules for each servers. HeterogeneousBalancer will try to balance regions according to their capacity.Let's take an example. Let's say that we have 20 RS: 10 RS, named through rs0 to rs9 loaded with 60 regions each, and each can handle 200 regions. 10 RS, named through rs10 to rs19 loaded with 60 regions each, and each can support 50 regions.Based on the following rules: rs&amp;#91;0-9&amp;#93; 200rs1&amp;#91;0-9&amp;#93; 50 The second group is overloaded, whereas the first group has plenty of space.We know that we can handle at maximum 2500 regions (200*10 + 50*10) and we have currently 1200 regions (60*20). HeterogeneousBalancer will understand that the cluster is full at 48.0% (1200/2500). Based on this information, we will then try to put all the RegionServers to ~48% of load according to the rules. In this case, it will move regions from the second group to the first.The balancer will: compute how many regions needs to be moved. In our example, by moving 36 regions on rs10, we could go from 120.0% to 46.0% select regions with lowest data-locality try to find an appropriate RS for the region. We will take the lowest available RS.Other implementations and ideasClay Baenziger proposed this idea on the dev ML:Could it work to have the stochastic load balancer use pluggable cost functions instead of this static list of cost functions? Then, could this type of a load balancer be implemented simply as a new cost function which folks could choose to load and mix with the others?I think this could be an interesting way to include user-functions in the mix. As you know your hardawre and the pattern access, you can easily know which metrics is important for balancing, for us, it will only be the number of regions, but we could mix-it with the incoming writes! bhupendra.jain proposed also the ideas of "labels" Internally, we are also having discussion to develop similar solution. In our approach, We were also thinking of adding "RS Label" Feature similar to Hadoop Node Label feature. Each RS can have a label to denote its capabilities / resources . When user create table, there can be extra attributes with its descriptor. The balancer can decide to host region of table based on RS label and these attributes further.   With RS label feature, Balancer can be more intelligent.  Example tables with high read load needs more cache backed by SSDs , So such table regions should be hosted on RS having SSDs ... I love the idea, but I think Clay's idea is better for a better and faster first set of commits on the subject! What do you think?</description>
      <version>3.0.0-alpha-1,2.2.0,2.2.1,2.1.6,1.4.11,2.1.7</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2262" opendate="2010-2-24 00:00:00" fixdate="2010-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZKW.ensureExists should check for existence</summary>
      <description>The fact that ZKW.ensureExists relies on KeeperException.NodeExistsException creates a lot of chatter in HBase and Zookeeper logs and also confuses users. We should use ZooKeeper.exists instead.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22689" opendate="2019-7-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Line break for fix version in documentation</summary>
      <description>The section describing the policy for the fix version in JIRA is missing line breaks.</description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="23184" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HeapAllocation in WebUI is not accurate</summary>
      <description>HeapAllocation in WebUI is always 0, the same reason as HBASE-22663</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ByteBuffAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23202" opendate="2019-10-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExportSnapshot (import) will fail if copying files to root directory takes longer than cleaner TTL</summary>
      <description>HBASE-17330 removed the checking of the snapshot .tmp directory when determining which files are candidates for deletes. It appears that in the latest branches, this isn't an issue for taking a snapshot as it checks whether a snapshot is in progress via the SnapshotManager.However, when using the ExportSnapshot tool to import a snapshot into a cluster, it will first copy the snapshot manifest into /.snapshot/.tmp/&lt;snapshot&gt; &amp;#91;1&amp;#93;, copies the files, and then renames the snapshot manifest to the final snapshot directory. If the copyFiles job takes longer than the cleaner TTL, the ExportSnapshot job will fail because HFiles will get deleted before the snapshot is committed to the final directory. The ExportSnapshot tool already has a functionality to skipTmp and write the manifest directly to the final location. However, this has unintended consequences such as the snapshot appearing to the user before it is usable. So it looks like we will have to bring back the tmp directory check to avoid this situation.&amp;#91;1&amp;#93; https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java#L1029</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.1,1.4.11,2.1.7</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCacheWithDifferentWorkingDir.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="23207" opendate="2019-10-23 00:00:00" fixdate="2019-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log a region open journal</summary>
      <description>Like HBASE-22828, but for region opening.Also, tweak the calls to enableStatusJournal to pass through 'true' as parameter to include the current status in the journal, for slightly more context.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,1.3.7,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="23227" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson-databind to 2.9.10.1 to avoid recent CVEs</summary>
      <description>Several net new CVEs were raised against jackson-databind 2.9.10.CVE-2019-16942CVE-2019-169432.9.10.1 is released, which I believe addresses these two CVEs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,2.1.8,2.2.3</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23228" opendate="2019-10-29 00:00:00" fixdate="2019-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for jdk8 specific modules on branch-1 in precommit/nightly testing</summary>
      <description>At least 1 feature backport is waiting on proper handling of jdk8 activated modules for our yetus personality (HBASE-22114 tinylfu).Implement the general handling here so that we don't have to worry about pushes to the PR branch overwriting it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2327" opendate="2010-3-15 00:00:00" fixdate="2010-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[EC2] Allocate elastic IP addresses for ZK and master nodes</summary>
      <description>Amazon EC2 supports Elastic IP Addresses to implement the effect of having a static IP address for public servers running on EC2. Up on hbase-users@ there was some recent discussion, confirmed, that when an EC2 instance queries the external DNS name of an elastic IP address, EC2 DNS returns the internal IP address of the instance to which the elastic IP address is bound, so it is safe to use elastic IPs for the ZK and master nodes. We gain the ability to do transparent replacement of one instance, e.g. failed, with another without incurring any additional cost. Update launch-hbase-zookeeper and launch-hbase-master to allocate elastic IPs: $ ec2-allocate-address ADDRESS 1.1.1.1and then assign the elastic IP address to the appropriate instance(s):$ ec2-associate-address -i i-11111111 1.1.1.1ADDRESS 1.1.1.1 i-11111111and then get the external DNS name to use when performing substitutions on master and slave configs:$ ec2-describe-instances i-11111111 | egrep ^INSTANCE | cut -f4ec2-1-1-1-1.compute-1.amazonaws.comWhen shutting down the cluster, just release the elastic IPs after terminating the instances:ec2-release-address 1.1.1.1...NOTE: AWS accounts default to a limit of 5 Elastic IP addresses but most will run with 1 master and 3 or 1 ZK instances. And, the ZK ensemble can be shared. A follow up issue can address making scripts to launch replacements for failed instances transparently.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.ec2.bin.terminate-hbase-cluster</file>
      <file type="M">contrib.ec2.bin.launch-hbase-zookeeper</file>
      <file type="M">contrib.ec2.bin.launch-hbase-slaves</file>
      <file type="M">contrib.ec2.bin.launch-hbase-master</file>
      <file type="M">contrib.ec2.bin.image.create-hbase-image-remote</file>
      <file type="M">contrib.ec2.bin.hbase-ec2-env.sh</file>
      <file type="M">contrib.ec2.bin.create-hbase-image</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23289" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update links to Hadoop wiki in code and book</summary>
      <description>Seems Hadoop has moved their wiki, so now links throughout our book are broken. We've found and fixed a couple one-offs, but we should do a sweep and clean up the rest.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.metrics.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.package-info.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="23829" opendate="2020-2-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get `-PrunSmallTests` passing on JDK11</summary>
      <description>Start with the small tests, shaking out issues identified by the harness. So far it seems like -Dhadoop.profile=3.0 and -Dhadoop-three.version=3.3.0-SNAPSHOT maybe be required.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.log.TestLogLevel.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFutureUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
