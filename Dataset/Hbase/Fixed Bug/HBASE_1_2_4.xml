<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="16973" opendate="2016-10-31 00:00:00" fixdate="2016-1-31 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Revisiting default value for hbase.client.scanner.caching</summary>
      <description>We are observing below logs for a long-running scan:2016-10-30 08:51:41,692 WARN [B.defaultRpcServer.handler=50,queue=12,port=16020] ipc.RpcServer:(responseTooSlow-LongProcessTime): {"processingtimems":24329,"call":"Scan(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ScanRequest)","client":"11.251.157.108:50415","scandetails":"table: ae_product_image region: ae_product_image,494:,1476872321454.33171a04a683c4404717c43ea4eb8978.","param":"scanner_id: 5333521 number_of_rows: 2147483647close_scanner: false next_call_seq: 8 client_handles_partials: true client_handles_heartbeats: true","starttimems":1477788677363,"queuetimems":0,"class":"HRegionServer","responsesize":818,"method":"Scan"}From which we found the "number_of_rows" is as big as Integer.MAX_VALUEAnd we also observed a long filter list on the customized scan. After checking application code we confirmed that there's no Scan.setCaching or hbase.client.scanner.caching setting on client side, so it turns out using the default value the caching for Scan will be Integer.MAX_VALUE, which is really a big surprise.After checking code and commit history, I found it's HBASE-11544 which changes HConstants.DEFAULT_HBASE_CLIENT_SCANNER_CACHING from 100 to Integer.MAX_VALUE, and from the release note there I could see below notation:Scan caching default has been changed to Integer.Max_Value This value works together with the new maxResultSize value from HBASE-12976 (defaults to 2MB) Results returned from server on basis of size rather than number of rows Provides better use of network since row size varies amongst tablesAnd I'm afraid this lacks of consideration of the case of scan with filters, which may involve many rows but only return with a small result.What's more, we still have below comment/code in Scan.java /* * -1 means no caching */ private int caching = -1;But actually the implementation does not follow (instead of no caching, we are caching Integer.MAX_VALUE...).So here I'd like to bring up two points:1. Change back the default value of HConstants.DEFAULT_HBASE_CLIENT_SCANNER_CACHING to some small value like 1282. Reenforce the semantic of "no caching"</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16985" opendate="2016-11-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestClusterId failed due to wrong hbase rootdir</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/4253/testReport/org.apache.hadoop.hbase.regionserver/TestClusterId/testClusterId/java.io.IOException: Shutting down at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:230) at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:409) at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:227) at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;(MiniHBaseCluster.java:96) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1071) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1037) at org.apache.hadoop.hbase.regionserver.TestClusterId.testClusterId(TestClusterId.java:85)The cluster can not start up because there are no active master. The active master can not finish initialing because the hbase:namespace region can not be assign. In TestClusterId unit test, TEST_UTIL.startMiniHBaseCluster set new hbase root dir. But the regionserver thread which stared first used a different hbase root dir. If assign hbase:namespace region to this regionserver, the region can not be assigned because there are no tableinfo on wrong hbase root dir.When regionserver report to master, it will get back some new config. But the FSTableDescriptors has been initialed so it's root dir didn't changed.if (LOG.isDebugEnabled()) { LOG.info("Config from master: " + key + "=" + value);} I thought FSTableDescriptors need update the rootdir when regionserver get report from master.The master branch has same problem, too. But the balancer always assign hbase:namesapce region to master. So this unit test can passed on master branch.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.4.0,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16986" opendate="2016-11-1 00:00:00" fixdate="2016-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note on how scanner caching has changed since 0.98 to refguid</summary>
      <description>Add note on how scanner caching config changed from 0.98 to the refguide (see parent issue for discussion but basics are we used to have default of 100 but not have unlimited as default)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17058" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower epsilon used for jitter verification from HBASE-15324</summary>
      <description>The current epsilon used is 1E-6 and its too big it might overflow the desiredMaxFileSize. A trivial fix is to lower the epsilon to 2^-52 or even 2^-53. An option to consider too is just to shift the jitter to always decrement hbase.hregion.max.filesize (MAX_FILESIZE) instead of increase the size of the region and having to deal with the round off.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="17074" opendate="2016-11-11 00:00:00" fixdate="2016-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit job always fails because of OOM</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/4434/artifact/patchprocess/patch-unit-hbase-server.txtException in thread "Thread-2369" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:596) at java.lang.StringBuffer.append(StringBuffer.java:367) at java.io.BufferedReader.readLine(BufferedReader.java:370) at java.io.BufferedReader.readLine(BufferedReader.java:389) at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:66)Exception in thread "Thread-2357" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2365" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.filter.TestFuzzyRowFilterEndToEndRunning org.apache.hadoop.hbase.filter.TestFilterListOrOperatorWithBlkCntException in thread "Thread-2383" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2397" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2401" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.TestHBaseTestingUtilityException in thread "Thread-2407" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2411" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2413" java.lang.OutOfMemoryError: Java heap spaceThe OOM happens in the surefire plugin when reading the stdout or stderr of the running test...</description>
      <version>1.3.0,1.4.0,1.1.7,0.98.23,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="17112" opendate="2016-11-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent setting timestamp of delta operations the same as previous value&amp;#39;s</summary>
      <description>In delta operations, Increment and Append. We will read current value first and then write the new whole result into WAL as the type of Put with current timestamp. If the previous ts is larger than current ts, we will use the previous ts.If we have two Puts with same TS, we will ignore the Put with lower sequence id. It is not friendly with versioning. And for replication we will drop sequence id while writing to peer cluster so in the slave we don't know what the order they are being written. If the pushing is disordered, the result will be wrong.We can set the new ts to previous+1 if the previous is not less than now.</description>
      <version>1.1.7,0.98.23,1.2.4</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="17114" opendate="2016-11-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to set special retry pause when encountering CallQueueTooBigException</summary>
      <description>As titled, after HBASE-15146 we will throw CallQueueTooBigException instead of dead-wait. This is good for performance for most cases but might cause a side-effect that if too many clients connect to the busy RS, that the retry requests may come over and over again and RS never got the chance for recovering, and the issue will become especially critical when the target region is META.So here in this JIRA we propose to add a new property in name of hbase.client.pause.cqtbe to make it possible to set a special-longer pause for CallQueueTooBigException, and by default it will use the setting of hbase.client.pause</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRequestFutureImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="17127" opendate="2016-11-18 00:00:00" fixdate="2016-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locate region should fail fast if underlying Connection already closed</summary>
      <description>Currently if try to locate region when underlying connection is closed, we will retry and wait at least 10s for each round until exhausted (refer to the catch clause of RpcRetryingCallerImpl#callWithRetries and RegionServerCallable#sleep for more details), which is unnecessary and time-costing.The issue is caused by user incorrectly manipulating connection, which shows the disadvantage of force user to handle connection life cycle and proves the necessity to support auto-managed connection as we did before, as indicated in HBASE-17009In this JIRA we will make it fail fast in the above case.</description>
      <version>1.1.7,1.2.4</version>
      <fixedVersion>1.4.0,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientNoCluster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="17149" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Fix nonce submission to avoid unnecessary calling coprocessor multiple times</summary>
      <description>instead of having all the logic in submitProcedure(), split in registerNonce() + submitProcedure().In this case we can avoid calling the coprocessor twice and having a clean submit logic knowing that there will only be one submission.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCoprocessorWhitelistMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestEnableTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDispatchMergingRegionsProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestAddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterSchemaServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterSchema.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureRecovery.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTableDDLProcedureBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="17160" opendate="2016-11-22 00:00:00" fixdate="2016-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undo unnecessary inter-module dependency; spark to hbase-it and hbase-it to shell</summary>
      <description>Very minor untangling.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="17162" opendate="2016-11-22 00:00:00" fixdate="2016-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid unconditional call to getXXXArray() in write path</summary>
      <description>Still some calls left. Patch will address these areas.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MutableSegment.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterImpl.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Increment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
    </fixedFiles>
  </bug>
  <bug id="17207" opendate="2016-11-30 00:00:00" fixdate="2016-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrays.asList() with too few arguments</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RackManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputSaslHelper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MultiAction.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="17210" opendate="2016-11-30 00:00:00" fixdate="2016-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set timeout on trying rowlock according to client&amp;#39;s RPC timeout</summary>
      <description>Now when we want to get a row lock, the timeout is fixed and default is 30s. But the requests from client have different RPC timeout setting. We can use the client's deadline to set timeout on tryLock.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17256" opendate="2016-12-5 00:00:00" fixdate="2016-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rpc handler monitoring will be removed when the task queue is full</summary>
      <description>The RPC handlers monitoring will displayed in the Web UI when the cluster has just started. As time goes on, RPC handlers disappeared .We have the RPC handlers listed as tasks and stored in CircularFifoBuffer. CircularFifoBuffer is a first in first out buffer with a fixed size that replaces its oldest element if full.When we refresh the page, completed tasks will be removed from CircularFifoBuffer.Not refresh the page for a long time, the oldest element (RPC handlers) will be replaced by new tasks.</description>
      <version>0.98.23,1.2.4,2.0.0</version>
      <fixedVersion>1.4.0,0.98.24,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17318" opendate="2016-12-14 00:00:00" fixdate="2016-12-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment does not add new column if the increment amount is zero at first time writing</summary>
      <description>When the data written for the first time is 0, no new columns are added.Iterate the input columns and update existing values if they were found, otherwise add new column initialized to the increment amount.Does not add new column if the increment amount is zero at first time writting.It is necessary to add a new column at the first write to 0. If not, the result of using the phoenix is NULL.</description>
      <version>0.98.23,1.2.4,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestDurability.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestIncrementsFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="1732" opendate="2009-8-1 00:00:00" fixdate="2009-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flag to disable regionserver restart</summary>
      <description>Add a flag so on zk session expiration, regionserver goes down and doesn't try restarting.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="17431" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Incorrect precheck condition in RoundRobinPool#get()</summary>
      <description>Here is related code: public R get() { if (super.size() &lt; maxSize) { return null; } nextResource %= super.size();Since super.size() is involved in modulo operation after the check, it seems the check should compare against 0 instead of maxSize.Looks like a copy-paste error from put() method.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.util.PoolMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="17434" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New Synchronization Scheme for Compaction Pipeline</summary>
      <description>A new copyOnWrite synchronization scheme is introduced for the compaction pipeline.The new scheme is better since it removes the lock from getSegments() which is invoked in every get and scan operation, and it reduces the number of LinkedList objects that are created at runtime, thus can reduce GC (not by much, but still...).In addition, it fixes the method getTailSize() in compaction pipeline. This method creates a MemstoreSize object which comprises the data size and the overhead size of the segment and needs to be atomic.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactionPipeline.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17435" opendate="2017-1-6 00:00:00" fixdate="2017-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call to preCommitStoreFile() hook encounters SaslException in secure deployment</summary>
      <description>romil.choksi was testing bulk load in secure cluster where LoadIncrementalHFiles failed.Looking at region server log, we saw the following: at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next(ClientSmallReversedScanner.java:185) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta(ConnectionManager.java:1257) at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1163) at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations(RpcRetryingCallerWithReadReplicas.java:300) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:156) at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call(ScannerCallableWithReplicas.java:60) at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:200) at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:327) at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:302) at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:167) at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:162) at org.apache.hadoop.hbase.client.HTable.getScanner(HTable.java:794) at org.apache.hadoop.hbase.backup.impl.BackupSystemTable.getBackupContexts(BackupSystemTable.java:540) at org.apache.hadoop.hbase.backup.impl.BackupSystemTable.getBackupHistory(BackupSystemTable.java:517) at org.apache.hadoop.hbase.backup.impl.BackupSystemTable.getTablesForBackupType(BackupSystemTable.java:589) at org.apache.hadoop.hbase.backup.BackupObserver.preCommitStoreFile(BackupObserver.java:89) at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$61.call(RegionCoprocessorHost.java:1494) at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost$RegionOperation.call(RegionCoprocessorHost.java:1660) at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.execOperation(RegionCoprocessorHost.java:1734) at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.execOperation(RegionCoprocessorHost.java:1692) at org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.preCommitStoreFile(RegionCoprocessorHost.java:1490) at org.apache.hadoop.hbase.regionserver.HRegion.bulkLoadHFiles(HRegion.java:5512) at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run(SecureBulkLoadEndpoint.java:293) at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint$1.run(SecureBulkLoadEndpoint.java:276) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:356) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1704) at org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.secureBulkLoadHFiles(SecureBulkLoadEndpoint.java:276) at org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos$SecureBulkLoadService.callMethod(SecureBulkLoadProtos.java:4721)...Caused by: java.lang.RuntimeException: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'. at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$1.run(RpcClientImpl.java:679) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.handleSaslConnectionFailure(RpcClientImpl.java:637) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupIOstreams(RpcClientImpl.java:745) ... 16 moreCaused by: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Attempt to obtain new INITIATE credentials failed! (null))] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212) at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:179) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.setupSaslConnection(RpcClientImpl.java:611) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection.access$600(RpcClientImpl.java:156) at org.apache.hadoop.hbase.ipc.RpcClientImpl$Connection$2.run(RpcClientImpl.java:737)The cause was that in the coprocessor hook, security Token for accessing hbase was missing, leading to the above SaslException.This JIRA adds support for the case where the observer hook needs the HBase token/credential since the hook is executed under the RPC request user context.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17437" opendate="2017-1-6 00:00:00" fixdate="2017-2-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying a WAL directory outside of the root directory</summary>
      <description>Currently, the WAL and the StoreFiles need to be on the same FileSystem. Some FileSystems (such as Amazon S3) don’t support append or consistent writes. These two properties are imperative for the WAL in order to avoid loss of writes. However, StoreFiles don’t necessarily need the same consistency guarantees (since writes are cached locally and if writes fail, they can always be replayed from the WAL).This JIRA aims to allow users to configure a log directory (for WALs) that is outside of the root directory or even in a different FileSystem. The default value will still put the log directory under the root directory.</description>
      <version>1.2.4</version>
      <fixedVersion>1.4.0,1.3.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALFactory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.IOTestProvider.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestFSHLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestFSWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureWalLease.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALRecordReader.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestWALObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.FSHLogProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DisabledWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AsyncFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AbstractFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterWalManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.WALLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.fs.HFileSystem.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALPerformanceEvaluation.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALLoaderPerformanceEvaluation.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1744" opendate="2009-8-4 00:00:00" fixdate="2009-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server to match the new java api.</summary>
      <description>This mutateRows, etc.. is a little confusing compared to the new cleaner java client.Thinking of ways to make a thrift client that is just as elegant. something like:void put(1:Bytes table, 2:TPut put) throws (1:IOError io)with:struct TColumn { 1:Bytes family, 2:Bytes qualifier, 3:i64 timestamp}struct TPut { 1:Bytes row, 2:map&lt;TColumn, Bytes&gt; values}This creates more verbose rpc than if the columns in TPut were just map&lt;Bytes, map&lt;Bytes, Bytes&gt;&gt;, but that is harder to fit timestamps into and still be intuitive from say python.Presumably the goal of a thrift gateway is to be easy first.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="17542" opendate="2017-1-26 00:00:00" fixdate="2017-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move backup system table into separate namespace</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupSystemTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupRestoreConstants.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupHFileCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17568" opendate="2017-1-31 00:00:00" fixdate="2017-3-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expire region reports in the HMaster</summary>
      <description>(From a TODO)The RegionServers send reports of sizes from to the Master so the Master can compute the "size" of each Table.Presently, the Master has no way to expire these reports. Thus, reports for tables that are deleted would be retained in memory. Retain the time at which the report was received by the master, and then use that to determine when to age it off.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="17611" opendate="2017-2-8 00:00:00" fixdate="2017-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift 2 per-call latency metrics are capped at ~ 2 seconds</summary>
      <description>Thrift 2 latency metrics are measured in nanoseconds. However, the duration used for per-method latencies is cast to an int, meaning the values are capped at 2.147 seconds. Let's use a long instead.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="17614" opendate="2017-2-9 00:00:00" fixdate="2017-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Backup/Restore into separate module</summary>
      <description>Move all the backup code into separate hbase-backup module.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupDeleteWithFailures.java</file>
      <file type="M">hbase-backup..DS.Store</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestSystemTableSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestRestoreBoundaryTests.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestRepairAfterFailedDelete.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestRemoteRestore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestRemoteBackup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackupWithFailures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackupWithBulkLoad.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackupMergeWithFailures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackupDeleteTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestFullRestore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestFullBackupWithFailures.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestFullBackupSetRestoreSet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestFullBackupSet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestFullBackup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupSystemTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupStatusProgress.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupShowHistory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupRepair.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupMultipleDeletes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupDescribe.java</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupClientFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupCopyJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupDriver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupMergeJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupRestoreConstants.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupRestoreFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.BackupTableInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HBackupFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupAdminImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupCommands.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupManifest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupSystemTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.FullTableBackupClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.IncrementalTableBackupClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.RestoreTablesClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.impl.TableBackupClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.LogUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupCopyJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.mapreduce.MapReduceBackupMergeJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.mapreduce.MapReduceHFileSplitterJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.mapreduce.MapReduceRestoreJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.master.BackupLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.master.LogRollMasterProcedureManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.regionserver.LogRollBackupSubprocedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.regionserver.LogRollBackupSubprocedurePool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.regionserver.LogRollRegionServerProcedureManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.RestoreDriver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.RestoreJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.RestoreRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.util.BackupSet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.util.BackupUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.util.RestoreTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.master.TestBackupLogCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupBoundaryTests.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupCommandLineTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupDelete.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestBackupDeleteRestore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17673" opendate="2017-2-21 00:00:00" fixdate="2017-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Monitored RPC Handler not shown in the WebUI</summary>
      <description>This issue has been fixed once in HBASE-14674. But, I noticed that almost all RS in our production environment still have this problem. Strange thing is that newly started servers seems do not affected. Digging for a while, then I realize the CircularFifoBuffer introduced by HBASE-10312 is the root cause. The RPC hander's monitoredTask only create once, if the server is flooded with tasks, RPC monitoredTask can be purged by CircularFifoBuffer, and then never visible in WebUI.So my solution is creating a list for RPC monitoredTask separately. It is OK to do so since the RPC handlers remain in a fixed number. It won't increase or decrease during the lifetime of the server.</description>
      <version>3.0.0-alpha-1,1.2.4,1.1.8,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.monitoring.TestTaskMonitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17691" opendate="2017-2-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ScanMetrics support for async scan</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRawAsyncTableScan.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableScan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScanResultConsumer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ReversedScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawScanResultConsumer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableResultScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncScanSingleRegionRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17718" opendate="2017-3-2 00:00:00" fixdate="2017-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Difference between RS&amp;#39;s servername and its ephemeral node cause SSH stop working</summary>
      <description>After HBASE-9593, RS put up an ephemeral node in ZK before reporting for duty. But if the hosts config (/etc/hosts) is different between master and RS, RS's serverName can be different from the one stored the ephemeral zk node. The email metioned in HBASE-13753 (http://mail-archives.apache.org/mod_mbox/hbase-user/201505.mbox/%3CCANZDn9ueFEEuZMx=pZdmtLsdGLyZz=rrm1N6EQvLswYc1z-H=g@mail.gmail.com%3E) is exactly what happened in our production env. But what the email didn't point out is that the difference between serverName in RS and zk node can cause SSH stop to work. as we can see from the code in RegionServerTracker @Override public void nodeDeleted(String path) { if (path.startsWith(watcher.rsZNode)) { String serverName = ZKUtil.getNodeName(path); LOG.info("RegionServer ephemeral node deleted, processing expiration [" + serverName + "]"); ServerName sn = ServerName.parseServerName(serverName); if (!serverManager.isServerOnline(sn)) { LOG.warn(serverName.toString() + " is not online or isn't known to the master."+ "The latter could be caused by a DNS misconfiguration."); return; } remove(sn); this.serverManager.expireServer(sn); } }The server will not be processed by SSH/ServerCrashProcedure. The regions on this server will not been assigned again until master restart or failover.I know HBASE-9593 was to fix the issue if RS report to duty and crashed before it can put up a zk node. It is a very rare case(And controllable， just fix the bug making rs to crash). But The issue I metioned can happened more often(and uncontrollable, can't be fixed in HBase, due to DNS, hosts config, etc.) and have more severe consequence.So here I offer some solutions to discuss:1. Revert HBASE-9593 from all branches, Andrew Purtell has reverted it in branch-0.982. Abort RS if master return a different name, otherwise SSH can't work properly3. Master accepts whatever servername reported by RS and don't change it.4.correct the zk node if master return another name( idea from Ted Yu)</description>
      <version>1.2.4,1.1.8,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRSKilledWhenInitializing.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.DrainingServerTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="17798" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RpcServer.Listener.Reader can abort due to CancelledKeyException</summary>
      <description>In our production cluster(0.98), some of the requests were unacceptable because RpcServer.Listener.Reader were aborted.getReader() will return the next reader to deal with request.The implementation of getReader() as below：RpcServer.java // The method that will return the next reader to work with // Simplistic implementation of round robin for now Reader getReader() { currentReader = (currentReader + 1) % readers.length; return readers[currentReader]; }If one of the readers abort, then it will lead to fall on the reader's request will never be dealt with.Why does RpcServer.Listener.Reader abort?We add the debug log to get it.After a while, we got the following exception:2017-03-10 08:05:13,247 ERROR [RpcServer.reader=3,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: unexpectedly error in Reader(Throwable)java.nio.channels.CancelledKeyException at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87) at java.nio.channels.SelectionKey.isReadable(SelectionKey.java:289) at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:592) at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:566) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)So, when deal with the request in reader, we should handle CanceledKeyException.----------versions 1.x and 2.0 will log and retrun when dealing with the InterruptedException in Reader#doRunLoop after HBASE-10521. It will lead to the same problem.</description>
      <version>1.3.0,1.2.4,0.98.24,2.0.0</version>
      <fixedVersion>1.4.0,1.3.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1800" opendate="2009-8-27 00:00:00" fixdate="2009-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too many ZK connections</summary>
      <description>Currently we open tons of new connections to Zookeeper, like every time we instantiate a new HTable. There is a maximum number of client connections as described here: &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.maxClientCnxns&lt;/name&gt; &lt;value&gt;30&lt;/value&gt; &lt;description&gt;Property from ZooKeeper's config zoo.cfg. Limit on number of concurrent connections (at the socket level) that a single client, identified by IP address, may make to a single member of the ZooKeeper ensemble. Set high to avoid zk connection issues running standalone and pseudo-distributed. &lt;/description&gt; &lt;/property&gt;If you hit that max number, ZK will just refuse your connections. Suppose you have 4 maps running on a server hosting a RS, you may actually lose your connection in the RS and eventually hit a session timeout. Maybe we should singleton ZKW?</description>
      <version>None</version>
      <fixedVersion>0.20.0,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
