<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="20153" opendate="2018-3-8 00:00:00" fixdate="2018-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable error-prone analysis in precommit</summary>
      <description>We've done a lot of work to get rid of the error-prone errors, we should make sure they stay out. Let's enable errorProne profile and analysis in precommit.busbey - I tried figuring out how to pass flags (-PerrorProne to the mvn compile precommit check but was unable to unravel that thread. Any help is appreciated.</description>
      <version>None</version>
      <fixedVersion>1.4.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20566" opendate="2018-5-10 00:00:00" fixdate="2018-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creating a system table after enabling rsgroup feature puts region into RIT</summary>
      <description>Steps to reproduce Enable rsgroup feature Enable quota feature which created hbase::quota table quota table region will be marked as RIT since the rsgroup for the table is not known2018-05-10 14:33:32,392 INFO [ProcedureExecutorThread-0] zookeeper.ZKTableStateManager: Moving table hbase:quota state from null to ENABLING2018-05-10 14:33:32,397 WARN [ProcedureExecutorThread-0] rsgroup.RSGroupBasedLoadBalancer: Group for table hbase:quota is null2018-05-10 14:33:32,398 WARN [ProcedureExecutorThread-0] master.RegionStates: Failed to open/close 89490cd5e00ea8948af413a1df65091a on null, set to FAILED_OPEN2018-05-10 14:33:32,398 INFO [ProcedureExecutorThread-0] master.RegionStates: Transition {89490cd5e00ea8948af413a1df65091a state=OFFLINE, ts=1525977212397, server=null} to {89490cd5e00ea8948af413a1df65091a state=FAILED_OPEN, ts=1525977212398, server=null}2018-05-10 14:33:32,398 INFO [ProcedureExecutorThread-0] zookeeper.ZKTableStateManager: Moving table hbase:quota state from ENABLING to ENABLED Reason for this issue:  system table creation doesn't move the table to the appropriate rs group to which system namespace is assigned to. Need to execute logic similar to what is done in the RSGroupAdminEndpoint for post table creation for user table creation.Work Around Assigning the system table to default rsgroup (or to the rsgroup to which the system namespace has been assigned). Manually assigning the region in RIT from the system table  </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroups.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupInfoManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="20567" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass both old and new descriptors to pre/post hooks of modify operations for table and namespace</summary>
      <description>In postModify* hooks like postModifyX(..., Descriptor newDesc), there's no way of getting the old descriptor which was there before modification happened.Having both old and new descriptors will make the hooks more useful.We felt the need when we wanted to audit certain events but there was no way of deducing them by just seeing 'after-state' of modification.To keep the method signatures consistent, i have modified both pre and post hooks with new arguments which are well named and commented.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20772" opendate="2018-6-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Controlled shutdown fills Master log with the disturbing message "No matching procedure found for rit=OPEN, location=ZZZZ, table=YYYYY, region=XXXX transition to CLOSED</summary>
      <description>I just saw this and was disturbed but this is a controlled shutdown. Change message so it doesn't freak out the poor operator</description>
      <version>2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="20778" opendate="2018-6-22 00:00:00" fixdate="2018-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it so WALPE runs on DFS</summary>
      <description>WALPE is broke for running on DFS. The old issue is the cause HBASE-9908 (making stuff work on windows) though it went in a long time ago.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.WALPerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug id="20841" opendate="2018-7-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note about the limitations when running WAL against the recent versions of hadoop</summary>
      <description>AsyncFSWAL may easily be broken when upgrading the DFSClient, so we introduced a fallback logic in HBASE-20839. And also, WAL can not be written into a directory with EC enabled, but the API for creating a non-EC file in EC directory is not available in hadoop-2.8-, see HBASE-19369 for more details.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20843" opendate="2018-7-4 00:00:00" fixdate="2018-7-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add release manager for 2.1 in ref guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20860" opendate="2018-7-9 00:00:00" fixdate="2018-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merged region&amp;#39;s RIT state may not be cleaned after master restart</summary>
      <description>In MergeTableRegionsProcedure, we issue UnassignProcedures to offline regions to merge. But if we restart master just after MergeTableRegionsProcedure finished these two UnassignProcedure and before it can delete their meta entries. The new master will found these two region is CLOSED but no procedures are attached to them. They will be regard as RIT regions and nobody will clean the RIT state for them later.A quick way to resolve this stuck situation in the production env is restarting master again, since the meta entries are deleted in MergeTableRegionsProcedure. Here, I offer a fix for this problem.</description>
      <version>3.0.0-alpha-1,2.1.0,2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
    </fixedFiles>
  </bug>
  <bug id="20893" opendate="2018-7-16 00:00:00" fixdate="2018-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data loss if splitting region while ServerCrashProcedure executing</summary>
      <description>Similar case as HBASE-20878.</description>
      <version>3.0.0-alpha-1,2.1.0,2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMergeTableRegionsWhileRSCrash.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="20937" opendate="2018-7-25 00:00:00" fixdate="2018-1-25 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Update the support matrix in our ref guide about the recent hadoop releases</summary>
      <description>Copied from HBASE-20538 "Upgrade our hadoop versions to 2.7.7 and 3.0.3" and from HBASE-20970Apache9 Duo Zhang added a comment - 6 days agoI think we should mark 2.7.x before 2.7.7 as 'Not Supported' due to the JDK issue? We only support jdk8 for 2.0 or above.Apache9 Duo Zhang added a comment - 6 days ago - Restricted to CommittersAnd IIRC there is a security issue before 2.7.7 so it is important to drop the support and let users upgrade to 2.7.7?Apache9 Duo Zhang added a comment - 6 days agoAlso upgrade hadoop3 dependency to 3.0.3 which contains HADOOP-15473.How does this issue relate to HBASE-20937?Do we support 3.0.3 now?Actually no. We do not support any 3.0.x releases due to the YARN issue YARN-7190. But at least it could let us add back the ignored security UT. There is still no production ready 3.1.x release...</description>
      <version>None</version>
      <fixedVersion>3.0.0-beta-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20977" opendate="2018-7-30 00:00:00" fixdate="2018-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t use the word "Snapshot" when defining "HBase Snapshots"</summary>
      <description>From http://hbase.apache.org/book.html#ops.snapshotsHBase Snapshots allow you to take a snapshot of a table without too much impact on Region ServersWe should change this to not use the word "snapshot" when defining what HBase Snapshots are. It's confusing enough to English-as-a-first-language individuals; I imagine it's even more cyclical to ESL individuals.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20979" opendate="2018-7-30 00:00:00" fixdate="2018-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test reporting should specify what JSON it needs and handle HTTP errors</summary>
      <description>Current flaky test report should be including the tree= parameter in its Jenkins API calls (see https://support.cloudbees.com/hc/en-us/articles/217911388-Best-Practice-For-Using-Jenkins-REST-API).Also should provide some info on failure so that when jobs change or go away we don't get blank failures.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.report-flakies.py</file>
    </fixedFiles>
  </bug>
  <bug id="20981" opendate="2018-7-30 00:00:00" fixdate="2018-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollback stateCount accounting thrown-off when exception out of rollbackState</summary>
      <description>Found by might allan163 over in HBASE-20893. Quoting Allan:But, there is truly a bug here, @Override protected void rollback(final TEnvironment env) throws IOException, InterruptedException { if (isEofState()) stateCount--; try { updateTimestamp(); rollbackState(env, getCurrentState()); stateCount--; } finally { updateTimestamp(); } }We need to decrease the stateCount when rolling back, so we can rollback for the previous state correctly. But. since a exception is thrown, the decrease for stateCount never happen. So ProcedureExecutor will continue to rollback for only one state(the one throw a exception) until the end of the execution stack.</description>
      <version>2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.1.1,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestYieldProcedures.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestStateMachineProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="20986" opendate="2018-7-31 00:00:00" fixdate="2018-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate the config of block size when we do log splitting and write Hlog</summary>
      <description>Since the block size of recovered edits and hlog are the same right now, if we set a large value to block size, name node may not able to assign enough space when we do log splitting. But set a large value to hlog block size can help reduce the number of region server asking for a new block. Thus I think separate the config of block size is necessary.</description>
      <version>3.0.0-alpha-1,2.1.0,2.0.1,2.2.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.FSHLogProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2103" opendate="2010-1-9 00:00:00" fixdate="2010-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[EC2] pull version from build</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.contrib.ec2.bin.image.create-hbase-image-remote</file>
      <file type="M">src.contrib.ec2.bin.hbase-ec2-env.sh</file>
      <file type="M">src.contrib.build-contrib.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21072" opendate="2018-8-19 00:00:00" fixdate="2018-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block out HBCK1 in hbase2</summary>
      <description>busbey left a note in the parent issue that I only just read which has a prescription for how we might block hbck1 from running against an hbase-2.x (hbck1 could damage a hbase-2....Its disabled in hbase-2 but an errant hbck1 from an hbase-1.x install might run).Here is quote from parent issue:I was idly thinking about how to stop HBase v1 HBCK. Thanks to HBASE-11405, we know that all HBase 1.y.z hbck instances should refuse to run if there's a lock file at '/hbase/hbase-hbck.lock' (given defaults). How about HBase v2 places that file permanently in place and replace the contents (usually just an IP address) with a note about how you must not run HBase v1 HBCK against the cluster?There is also the below:We could pick another location for locking on HBase version 2 and start building in a version check of some kind?... to which I'd answer, lets see. hbck2 is a different beast. It asks the master to do stuff. It doesn't do it itself, as hbck1 did. So no need of a lock/version.</description>
      <version>2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="21073" opendate="2018-8-19 00:00:00" fixdate="2018-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Maintenance mode" master</summary>
      <description>Make it so we can bring up a Master in "maintenance mode". This is parse of master wal procs but not taking on regionservers. It would be in a state where "repair" Procedures could run; e.g. a Procedure that could recover meta by looking for meta WALs, splitting them, dropping recovered.edits, and even making it so meta is readable. See parent issue for why needed (disaster recovery).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.MasterMaintenanceModeTracker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21082" opendate="2018-8-21 00:00:00" fixdate="2018-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reimplement assign/unassign related procedure metrics</summary>
      <description>As after HBASE-20881, we only have one TRSP procedure to handle all the assign/unassign/move/reopen operations, we need to reimplement(redefine?) the metrics for these procedures.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRegionBypass.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="21084" opendate="2018-8-21 00:00:00" fixdate="2018-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>When cloning a snapshot including a split parent region, the split parent region of the cloned table will be online</summary>
      <description>Investigating HBASE-21015, I found another issue. It seems like after HBASE-20881, the split parent region of the cloned table will be online when cloning a snapshot including a split parent region.Steps to reproduce are as follows, which is the same as the steps in HBASE-21015:1. Create a tablecreate "test", "cf"2. Put some data into the table(0...2000).each{|i| put "test", "row#{i}", "cf:col", "val"}3. Split the tablesplit "test"4. Take a snapshot of the table before CatalogJanitor cleans up the split parent regionsnapshot "test", "snap"5. Clone the snapshotclone_snapshot "snap", "cloned_table"After following the above steps, the split parent region of the cloned table will be online.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestCloneSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionReplicaUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21107" opendate="2018-8-23 00:00:00" fixdate="2018-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a metrics for netty direct memory</summary>
      <description>This is separated from HBASE-20913 as I realized that netty direct memory usage needs to show up in metrics instead of the web page.</description>
      <version>2.0.1</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.DirectMemoryUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="21179" opendate="2018-9-10 00:00:00" fixdate="2018-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the number of actions in responseTooSlow log</summary>
      <description>responseTooSlow2018-09-10 16:13:53,022 WARN &amp;#91;B.DefaultRpcServer.handler=209,queue=29,port=60020&amp;#93; ipc.RpcServer: (responseTooSlow): {"processingtimems":321262,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"127.0.0.1:56149","param":"region= tsdb,\\x00\\x00.[\\x89\\x1F\\xB0\\x00\\x00\\x01\\x00\\x01Y\\x00\\x00\\x02\\x00\\x00x04,1536133210446.7c752de470bd5558a001117b123a5db5., for 1 actions and 1st row key=\\x00\\x00.[\\x96x16p","starttimems":1536566911759,"queuetimems":0,"class":"HRegionServer","responsesize":2,"method":"Multi"}The responseTooSlow log is printed when the processing time of a request exceeds the specified threshold. The number of actions and the contents of the first rowkey in the request will be included in the log.However, the number of actions is inaccurate, and it is actually the number of regions that the request needs to visit.Just like the logs above, users may be mistaken for using 321262ms to process an action, which is incredible, so we need to fix it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,1.2.8,2.2.0,1.4.8,2.1.1,2.0.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21502" opendate="2018-11-20 00:00:00" fixdate="2018-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SyncTable section on RefGuide once HBASE-20586 is committed</summary>
      <description>SyncTable refguide section currently mentions limitation to run it on different kerberos realm. HBASE-20586 is ongoing to resolve this problem. This jira is to make sure RefGuide is updated accordingly once HBASE-20586 is resolved.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21705" opendate="2019-1-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should treat meta table specially for some methods in AsyncAdmin</summary>
      <description>For example, tableExists, isTableEnabled, isTableDisabled...For now, we will go to the meta table directly but obviously, meta table does not contain the record for itself...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi3.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="21976" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deal with RetryImmediatelyException for batching request</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  <bug id="21977" opendate="2019-3-1 00:00:00" fixdate="2019-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip replay WAL and update seqid when open regions restored from snapshot</summary>
      <description>TableSnapshotScanner restore a snapshot and then open the restored regions. When open these regions, we can skip replay WAL and update seqid.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreSnapshotHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ClientSideRegionScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="21978" opendate="2019-3-2 00:00:00" fixdate="2019-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should close AsyncRegistry if we fail to get cluster id when creating AsyncConnection</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22077" opendate="2019-3-21 00:00:00" fixdate="2019-3-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose sleep time as a command line argument of IntergationTestBackupRestore</summary>
      <description>Extend command line arguments of IntergationTestBackupRestore with a sleep time of chaos monkey options to be able to setup policy of region server restarts more granularly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBackupRestore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2208" opendate="2010-2-10 00:00:00" fixdate="2010-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableServers # processBatchOfRows - converts from List to [ ] - Expensive copy</summary>
      <description>With autoFlush to false and a large write buffer on HTable, when we write bulk puts - TableServer # processBatchOfRows , convert the input (List) to an [ ] , before sending down the wire. With a write buffer as large as 20 MB , that becomes an expensive copy when we do - list.toArray(new T[ ] ). May be - should we change the wire protocol to support List as well , and then revisit this to prevent the bulk copy ?Batch b = new Batch(this) { @Override int doCall(final List&lt;Row&gt; currentList, final byte [] row, final byte [] tableName) throws IOException, RuntimeException { *final Put [] puts = currentList.toArray(PUT_ARRAY_TYPE);* return getRegionServerWithRetries(new ServerCallable&lt;Integer&gt;(this.c, tableName, row) { public Integer call() throws IOException { return server.put(location.getRegionInfo().getRegionName(), puts); } }); }</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24051" opendate="2020-3-26 00:00:00" fixdate="2020-1-26 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>When the data directory is set to EC,the amount of storefiles reaches a relatively high level,heap Occupancy Percent is above heap occupancy alarm watermark,resulting in frequent gc and eventual failure</summary>
      <description>When the data directory is set to EC, during the continuous load process, regionserver is found down ,to view the log:regionserver.HeapMemoryManager: heapOccupancyPercent 0.91630864 is now below the heap occupancy alarm watermark (0.95)regionserver.HeapMemoryManager: heapOccupancyPercent 0.969546 is above heap occupancy alarm watermark (0.95)&amp;#91;JvmPauseMonitor&amp;#93; util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1254ms GC pool 'G1 Young Generation' had collection(s): count=2 time=115ms GC pool 'G1 Old Generation' had collection(s): count=1 time=1487msIn particular, GC pauses occurred 283 times in half an hour.The memory analysis found that most of the memory footprint was generated by HeepByteBuffer,and DFSStripedInputStream produces the HeepByteBufferSee HDFS-14308，but after the fix, hbase did not return to normal.</description>
      <version>2.0.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestFSDataInputStreamWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="24055" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make AsyncFSWAL can run on EC cluster</summary>
      <description>We need to enable replicate when creating the file.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHBaseWalOnEC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.asyncfs.TestFanOutOneBlockAsyncDFSOutput.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="24056" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the layout of our proto files in hbase-protocol-shaded module</summary>
      <description>Now it is only a big flat directory contains all the proto files, I think it will be better to have several sub directories to hold the proto files to different modules, such as rsgroup, rest, etc.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.TableInfoMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.VisibilityLabels.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.VersionMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Tracing.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.TooSlowLog.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.test.rpc.service.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.TestProcedure.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.test.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.TableSchemaMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.TableListMessage.proto</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.RowCountEndpoint.java</file>
      <file type="M">hbase-protocol-shaded.README.txt</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Aggregate.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Authentication.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.BucketCacheEntry.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.BulkDelete.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.CellMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.CellSetMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ClusterId.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ColumnAggregationNullResponseProtocol.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ColumnAggregationProtocol.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ColumnAggregationWithErrorsProtocol.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ColumnSchemaMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Comparator.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.DummyRegionServerEndpoint.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Encryption.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ErrorHandling.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Examples.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Export.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Filter.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.FS.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.HFile.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.IncrementCounterProcessor.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.LoadBalancer.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.LockService.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MultiRowMutation.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.NamespacePropertiesMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.NamespacesMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.PingProtocol.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Procedure.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Quota.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RefreshHFiles.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionNormalizer.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionServerStatus.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Replication.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RowProcessor.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RPC.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RSGroup.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RSGroupAdmin.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ScannerMessage.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Snapshot.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.SnapshotCleanup.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.StorageClusterStatusMessage.proto</file>
    </fixedFiles>
  </bug>
</bugrepository>
