<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="2217" opendate="2010-2-12 00:00:00" fixdate="2010-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>VM OPTS for shell only</summary>
      <description>Over in hbase-2177 Ryan goes on how enabling gc logging, it shows on stdout when you fire the shell:so one problem with this is the irb then logs all GC to stdout, which is ugly. I do something like this in my scripts:export HBASE_OPTS=""export HBASE_LOG_DIR=&lt;somewhere&gt;export SERVER_GC_OPTS="$HBASE_OPTS -verbose:gc -XX:+PrintGCApplicationStoppedTime -XX:+PrintGCDateStamps -XX:+PrintGCDetails -Xloggc:$HBASE_HOME/logs/gc-hbase.log"export JMX_OPTS="-Dcom.sun.management.jmxremote.authenticate=true -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.password.file=$HBASE_HOME/conf/jmxremote.password -Dcom.sun.management.jmxremote"export HBASE_MASTER_OPTS="$SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/logs/gc-master.log"export HBASE_REGIONSERVER_OPTS="$SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-hbase.log -Dcom.sun.management.jmxremote.port=10102 $JMX_OPTS"export HBASE_THRIFT_OPTS="-Xmx1000m $SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-hbase-thrift.log -Dcom.sun.management.jmxremote.port=10103 $JMX_OPTS"export HBASE_ZOOKEEPER_OPTS="-Xmx1000m $SERVER_GC_OPTS -Xloggc:$HBASE_LOG_DIR/gc-zk.log -Dcom.sun.management.jmxremote.port=10104 $JMX_OPTS"now you get remote JMX with logging to whatever directory (we have to log to our large data partition since logs... can be big). Also the shell doesnt log GC to stdout, and you can get separate GC logs for hmaster, hrs, thrift, zookeeper.Need to make an OPTS for the shell to use.... or do the above.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.96.3,0.98.5,0.94.22</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="2221" opendate="2010-2-12 00:00:00" fixdate="2010-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR to copy a table</summary>
      <description>As discussed in HBASE-2197, we need a way to copy a table from one cluster to another. This requires creating the job itself and modifying TOF.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.Driver.java</file>
      <file type="M">src.contrib.mdc.replication.src.test.org.apache.hadoop.hbase.replication.TestReplication.java</file>
      <file type="M">src.contrib.mdc.replication.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22250" opendate="2019-4-16 00:00:00" fixdate="2019-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The same constants used in many places should be placed in constant classes</summary>
      <description>I think we should put these configurations in the HConstants class to avoid the trouble of modifying a lot of places when we modify them later.public static final String MASTER_KRB_PRINCIPAL = "hbase.master.kerberos.principal";public static final String MASTER_KRB_KEYTAB_FILE = "hbase.master.keytab.file";public static final String REGIONSERVER_KRB_PRINCIPAL = "hbase.regionserver.kerberos.principal";public static final String REGIONSERVER_KRB_KEYTAB_FILE = "hbase.regionserver.keytab.file";</description>
      <version>1.2.0,2.0.0,2.1.1,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.HBaseKerberosUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestZKAndFSPermissions.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.SecurityInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="22285" opendate="2019-4-22 00:00:00" fixdate="2019-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>A normalizer which merges very small size regions with adjacent regions.(MergeToNormalize)</summary>
      <description>There are scenarios where we have seen around 5% of total regions with a size of 0 bytes and another 5-6 % regions with size in a few bytes. These kinds of regions increase with time considering we have TTL over the rows. After exploring the option of RegionNormalizer and doing some quick runs we found that that is not suitable considering it also splits the regions and merges to normalize. What we really want is to split as per Split policy and merge very small regions with adjacent regions to make sure we reduce 0-byte regions.We can plugin this normalizer using the property "hbase.master.normalizer.class"</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22312" opendate="2019-4-25 00:00:00" fixdate="2019-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop 3 profile for hbase-shaded-mapreduce should like mapreduce as a provided dependency</summary>
      <description>the hadoop 3 profile currently misses declaring a provided dependency on the core mapreduce client module. that means we pick it up as a compile dependency from the hbase-mapreduce module, which means we include things in the shaded jar that we don't need to. (and expressly aren't supposed to include because they're supposed to come from Hadoop at runtime).</description>
      <version>2.1.0,2.2.0,2.1.1,2.1.2,2.1.3,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22395" opendate="2019-5-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document RC voting guidelines in ref guide</summary>
      <description>Document all necessary and suggested steps to vote on a release. There are only a handful necessary checks a PMC member must do on everyrelease, and all of them relate to packaging, LICENSE and NOTICE files, andlicense auditing, which can be accomplished by running the RAT tool, byattempting to compile from source (unit tests optional), and through manualinspection of LICENSE and NOTICE files in the source distribution andembedded in a sample of the binaries. This entire process should take youless than 15 minutes, from my experience. This is the baseline.Any individual PMCer may opt to do more than the baseline, but it isoptional. Personally I would also read the compatibility report, and thenrun the unit test suite in the background and come back to it when finishedto complete the voting task. In my opinion now that is the baseline tasksany HBase PMC voter should take. Beyond that, at least for my releases, youcan read the vote email to find the additional functional and performancechecks I might have done and factor that in to your voting confidence. Youcan also run them yourselves, but is totally optional.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22442" opendate="2019-5-18 00:00:00" fixdate="2019-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly build is failing with hadoop 3.x</summary>
      <description>[ERROR] Found artifact with unexpected contents: '/testptch/hbase/hbase-shaded/hbase-shaded-client/target/hbase-shaded-client-2.2.1-SNAPSHOT.jar' Please check the following and either correct the build or update the allowed list with reasoning. javax/ javax/servlet/ javax/servlet/annotation/ javax/servlet/annotation/HandlesTypes.class javax/servlet/annotation/HttpConstraint.class javax/servlet/annotation/HttpMethodConstraint.class javax/servlet/annotation/MultipartConfig.class javax/servlet/annotation/package.html javax/servlet/annotation/ServletSecurity$EmptyRoleSemantic.class javax/servlet/annotation/ServletSecurity$TransportGuarantee.class javax/servlet/annotation/ServletSecurity.class javax/servlet/annotation/WebFilter.class javax/servlet/annotation/WebInitParam.class javax/servlet/annotation/WebListener.class javax/servlet/annotation/WebServlet.class javax/servlet/AsyncContext.class javax/servlet/AsyncEvent.class javax/servlet/AsyncListener.class javax/servlet/descriptor/ javax/servlet/descriptor/JspConfigDescriptor.class javax/servlet/descriptor/JspPropertyGroupDescriptor.class javax/servlet/descriptor/package.html javax/servlet/descriptor/TaglibDescriptor.class javax/servlet/DispatcherType.class javax/servlet/Filter.class javax/servlet/FilterChain.class javax/servlet/FilterConfig.class javax/servlet/FilterRegistration$Dynamic.class javax/servlet/FilterRegistration.class javax/servlet/GenericServlet.class javax/servlet/http/ javax/servlet/http/Cookie.class javax/servlet/http/HttpServlet.class javax/servlet/http/HttpServletRequest.class javax/servlet/http/HttpServletRequestWrapper.class javax/servlet/http/HttpServletResponse.class javax/servlet/http/HttpServletResponseWrapper.class javax/servlet/http/HttpSession.class javax/servlet/http/HttpSessionActivationListener.class javax/servlet/http/HttpSessionAttributeListener.class javax/servlet/http/HttpSessionBindingEvent.class javax/servlet/http/HttpSessionBindingListener.class javax/servlet/http/HttpSessionContext.class javax/servlet/http/HttpSessionEvent.class javax/servlet/http/HttpSessionIdListener.class javax/servlet/http/HttpSessionListener.class javax/servlet/http/HttpUpgradeHandler.class javax/servlet/http/HttpUtils.class javax/servlet/http/LocalStrings.properties javax/servlet/http/LocalStrings_es.properties javax/servlet/http/LocalStrings_fr.properties javax/servlet/http/LocalStrings_ja.properties javax/servlet/http/NoBodyOutputStream.class javax/servlet/http/NoBodyResponse.class javax/servlet/http/package.html javax/servlet/http/Part.class javax/servlet/http/WebConnection.class javax/servlet/HttpConstraintElement.class javax/servlet/HttpMethodConstraintElement.class javax/servlet/LocalStrings.properties javax/servlet/LocalStrings_fr.properties javax/servlet/LocalStrings_ja.properties javax/servlet/MultipartConfigElement.class javax/servlet/package.html javax/servlet/ReadListener.class javax/servlet/Registration$Dynamic.class javax/servlet/Registration.class javax/servlet/RequestDispatcher.class javax/servlet/Servlet.class javax/servlet/ServletConfig.class javax/servlet/ServletContainerInitializer.class javax/servlet/ServletContext.class javax/servlet/ServletContextAttributeEvent.class javax/servlet/ServletContextAttributeListener.class javax/servlet/ServletContextEvent.class javax/servlet/ServletContextListener.class javax/servlet/ServletException.class javax/servlet/ServletInputStream.class javax/servlet/ServletOutputStream.class javax/servlet/ServletRegistration$Dynamic.class javax/servlet/ServletRegistration.class javax/servlet/ServletRequest.class javax/servlet/ServletRequestAttributeEvent.class javax/servlet/ServletRequestAttributeListener.class javax/servlet/ServletRequestEvent.class javax/servlet/ServletRequestListener.class javax/servlet/ServletRequestWrapper.class javax/servlet/ServletResponse.class javax/servlet/ServletResponseWrapper.class javax/servlet/ServletSecurityElement.class javax/servlet/SessionCookieConfig.class javax/servlet/SessionTrackingMode.class javax/servlet/SingleThreadModel.class javax/servlet/UnavailableException.class javax/servlet/WriteListener.class com/ com/sun/ com/sun/jersey/ com/sun/jersey/api/ com/sun/jersey/api/core/ com/sun/jersey/api/core/servlet/ com/sun/jersey/api/core/servlet/WebAppResourceConfig.class com/sun/jersey/server/ com/sun/jersey/server/impl/ com/sun/jersey/server/impl/cdi/ com/sun/jersey/server/impl/cdi/AbstractBean.class com/sun/jersey/server/impl/cdi/AnnotatedCallableImpl.class com/sun/jersey/server/impl/cdi/AnnotatedConstructorImpl.class com/sun/jersey/server/impl/cdi/AnnotatedFieldImpl.class com/sun/jersey/server/impl/cdi/AnnotatedImpl.class com/sun/jersey/server/impl/cdi/AnnotatedMemberImpl.class com/sun/jersey/server/impl/cdi/AnnotatedMethodImpl.class com/sun/jersey/server/impl/cdi/AnnotatedParameterImpl.class com/sun/jersey/server/impl/cdi/AnnotatedTypeImpl.class com/sun/jersey/server/impl/cdi/BeanGenerator$1.class com/sun/jersey/server/impl/cdi/BeanGenerator.class com/sun/jersey/server/impl/cdi/CDIComponentProviderFactory$1.class com/sun/jersey/server/impl/cdi/CDIComponentProviderFactory$2.class com/sun/jersey/server/impl/cdi/CDIComponentProviderFactory$ComponentProviderDestroyable.class com/sun/jersey/server/impl/cdi/CDIComponentProviderFactory.class com/sun/jersey/server/impl/cdi/CDIComponentProviderFactoryInitializer.class com/sun/jersey/server/impl/cdi/CDIExtension$1.class com/sun/jersey/server/impl/cdi/CDIExtension$2.class com/sun/jersey/server/impl/cdi/CDIExtension$3.class com/sun/jersey/server/impl/cdi/CDIExtension$ContextAnnotationLiteral.class com/sun/jersey/server/impl/cdi/CDIExtension$InjectAnnotationLiteral.class com/sun/jersey/server/impl/cdi/CDIExtension$JNDIContextDiver.class com/sun/jersey/server/impl/cdi/CDIExtension$ParameterBean.class com/sun/jersey/server/impl/cdi/CDIExtension$PatchInformation.class com/sun/jersey/server/impl/cdi/CDIExtension$PredefinedBean.class com/sun/jersey/server/impl/cdi/CDIExtension$SyntheticQualifierAnnotationImpl.class com/sun/jersey/server/impl/cdi/CDIExtension.class com/sun/jersey/server/impl/cdi/DiscoveredParameter.class com/sun/jersey/server/impl/cdi/InitializedLater.class com/sun/jersey/server/impl/cdi/ProviderBasedBean.class com/sun/jersey/server/impl/cdi/SyntheticQualifier.class com/sun/jersey/server/impl/cdi/Utils.class com/sun/jersey/server/impl/container/ com/sun/jersey/server/impl/container/servlet/ com/sun/jersey/server/impl/container/servlet/Include.class com/sun/jersey/server/impl/container/servlet/JSPTemplateProcessor.class com/sun/jersey/server/impl/container/servlet/JerseyServletContainerInitializer.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$1.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$AbstractPerSession.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$PerSesson.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$PerSessonInstantiated.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$PerSessonProxied.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory$SessionMap.class com/sun/jersey/server/impl/container/servlet/PerSessionFactory.class com/sun/jersey/server/impl/container/servlet/RequestDispatcherWrapper.class com/sun/jersey/server/impl/container/servlet/ServletAdaptor$1$1.class com/sun/jersey/server/impl/container/servlet/ServletAdaptor$1.class com/sun/jersey/server/impl/container/servlet/ServletAdaptor.class com/sun/jersey/server/impl/container/servlet/Wrapper.class com/sun/jersey/server/impl/ejb/ com/sun/jersey/server/impl/ejb/EJBComponentProviderFactory$EJBManagedComponentProvider.class com/sun/jersey/server/impl/ejb/EJBComponentProviderFactory.class com/sun/jersey/server/impl/ejb/EJBComponentProviderFactoryInitilizer.class com/sun/jersey/server/impl/ejb/EJBExceptionMapper.class com/sun/jersey/server/impl/ejb/EJBInjectionInterceptor$1.class com/sun/jersey/server/impl/ejb/EJBInjectionInterceptor.class com/sun/jersey/server/impl/ejb/EJBRequestDispatcherProvider$1.class com/sun/jersey/server/impl/ejb/EJBRequestDispatcherProvider.class com/sun/jersey/server/impl/managedbeans/ com/sun/jersey/server/impl/managedbeans/ManagedBeanComponentProviderFactory$ManagedBeanComponentProvider.class com/sun/jersey/server/impl/managedbeans/ManagedBeanComponentProviderFactory.class com/sun/jersey/server/impl/managedbeans/ManagedBeanComponentProviderFactoryInitilizer.class com/sun/jersey/spi/ com/sun/jersey/spi/container/ com/sun/jersey/spi/container/servlet/ com/sun/jersey/spi/container/servlet/PerSession.class com/sun/jersey/spi/container/servlet/ServletContainer$ContextInjectableProvider.class com/sun/jersey/spi/container/servlet/ServletContainer$InternalWebComponent.class com/sun/jersey/spi/container/servlet/ServletContainer.class com/sun/jersey/spi/container/servlet/WebComponent$1.class com/sun/jersey/spi/container/servlet/WebComponent$2.class com/sun/jersey/spi/container/servlet/WebComponent$3.class com/sun/jersey/spi/container/servlet/WebComponent$4.class com/sun/jersey/spi/container/servlet/WebComponent$ContextInjectableProvider.class com/sun/jersey/spi/container/servlet/WebComponent$Writer.class com/sun/jersey/spi/container/servlet/WebComponent.class com/sun/jersey/spi/container/servlet/WebConfig$ConfigType.class com/sun/jersey/spi/container/servlet/WebConfig.class com/sun/jersey/spi/container/servlet/WebFilterConfig.class com/sun/jersey/spi/container/servlet/WebServletConfig.class com/sun/jersey/spi/scanning/ com/sun/jersey/spi/scanning/servlet/ com/sun/jersey/spi/scanning/servlet/WebAppResourcesScanner$1.class com/sun/jersey/spi/scanning/servlet/WebAppResourcesScanner$2.class com/sun/jersey/spi/scanning/servlet/WebAppResourcesScanner.class[ERROR] Command execution failed.org.apache.commons.exec.ExecuteException: Process exited with an error: 1 (Exit value: 1) at org.apache.commons.exec.DefaultExecutor.executeInternal (DefaultExecutor.java:404) at org.apache.commons.exec.DefaultExecutor.execute (DefaultExecutor.java:166) at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:804) at org.codehaus.mojo.exec.ExecMojo.executeCommandLine (ExecMojo.java:751) at org.codehaus.mojo.exec.ExecMojo.execute (ExecMojo.java:313) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:208) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:154) at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:146) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305) at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192) at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105) at org.apache.maven.cli.MavenCli.execute (MavenCli.java:954) at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:288) at org.apache.maven.cli.MavenCli.main (MavenCli.java:192) at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke (Method.java:498) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:356)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22443" opendate="2019-5-18 00:00:00" fixdate="2019-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase-vote script details to documentation</summary>
      <description>In HBASE-21963 taklwu provided hbase-vote.sh which helps with verification of new release. Adding it to the documentation will help anyone who would like to understand the verification process.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22456" opendate="2019-5-22 00:00:00" fixdate="2019-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Polish TestSplitTransitionOnCluster</summary>
      <description>Remove the compaction state check in TestSplitTransitionOnCluster.testMasterRestartAtRegionSplitPendingCatalogJanitor, as region.compact is a synchronous call, we will only return when the compaction is finished. And not sure why we do not wait for the split to finish in this test, before getting the daughter regions...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="22462" opendate="2019-5-23 00:00:00" fixdate="2019-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should run a &amp;#39;mvn install&amp;#39; at the end of hadoop check in pre commit job</summary>
      <description>Now for branch-2.x, we will build with hadoop 3.x in the hadoop check stage, so in later unit check, if we run from a sub module, then the other hbase modules we depend on will depend on hadoop 3.x while the module we build will depend on 2.x, this will cause the following problem2019-05-23 04:47:41,156 WARN [RS_CLOSE_META-regionserver/b001f91a596c:0-0] handler.AssignRegionHandler(157): Fatal error occurred while opening region hbase:meta,,1.1588230740, aborting...java.lang.IncompatibleClassChangeError: Found class org.apache.hadoop.hdfs.protocol.HdfsFileStatus, but interface was expected at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.createOutput(FanOutOneBlockAsyncDFSOutputHelper.java:496) at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.access$400(FanOutOneBlockAsyncDFSOutputHelper.java:116) at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$8.doCall(FanOutOneBlockAsyncDFSOutputHelper.java:576) at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper$8.doCall(FanOutOneBlockAsyncDFSOutputHelper.java:571) at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81) at org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.createOutput(FanOutOneBlockAsyncDFSOutputHelper.java:584) at org.apache.hadoop.hbase.io.asyncfs.AsyncFSOutputHelper.createOutput(AsyncFSOutputHelper.java:51) at org.apache.hadoop.hbase.regionserver.wal.AsyncProtobufLogWriter.initOutput(AsyncProtobufLogWriter.java:169) at org.apache.hadoop.hbase.regionserver.wal.AbstractProtobufLogWriter.init(AbstractProtobufLogWriter.java:166) at org.apache.hadoop.hbase.wal.AsyncFSWALProvider.createAsyncWriter(AsyncFSWALProvider.java:105) at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createAsyncWriter(AsyncFSWAL.java:664) at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:670) at org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.createWriterInstance(AsyncFSWAL.java:128) at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:832) at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.rollWriter(AbstractFSWAL.java:538) at org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.init(AbstractFSWAL.java:479) at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:156) at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.getWAL(AbstractFSWALProvider.java:61) at org.apache.hadoop.hbase.wal.WALFactory.getWAL(WALFactory.java:293) at org.apache.hadoop.hbase.regionserver.HRegionServer.getWAL(HRegionServer.java:2170) at org.apache.hadoop.hbase.regionserver.handler.AssignRegionHandler.process(AssignRegionHandler.java:133) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22463" opendate="2019-5-23 00:00:00" fixdate="2019-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some paths in HFileScannerImpl did not consider block#release which will exhaust the ByteBuffAllocator</summary>
      <description>When I debug the issue HBASE-22422, I observed that the ByteBuffAllocator#usedBufCount will was always increasing and all direct ByteBuffers would be exhausted, which lead to may heap allocation happen. The comment here &amp;#91;1&amp;#93; is also related to this problem.Check the code path, the HFileScannerImpl is the biggest suspect, so create issue to address this.1. https://issues.apache.org/jira/browse/HBASE-22387?focusedCommentId=16838446&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16838446</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestTinyLfuBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCachedBlockQueue.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.CacheTestUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestByteBufferIOEngine.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestBucketCacheRefCnt.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.TinyLfuBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheableDeserializer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.Cacheable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.SharedMemoryMmapIOEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.FileIOEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.ExclusiveMemoryMmapIOEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.ByteBufferIOEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketEntry.java</file>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="22478" opendate="2019-5-27 00:00:00" fixdate="2019-5-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add jackson dependency for hbase-http module</summary>
      <description>We use Configuration.dumpConfiguration method in ConfServlet, which will reference jackson.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-http.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22501" opendate="2019-5-30 00:00:00" fixdate="2019-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the hadoop support matrix in the ref guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22529" opendate="2019-6-3 00:00:00" fixdate="2019-12-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sanity check for in-memory compaction policy</summary>
      <description>Currently, if a column family is altered with an invalid in-memory compaction policy via Admin API, the regions for the table get stuck indefinitely.Failed open of region=######java.io.IOException: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hbase.MemoryCompactionPolicy.INVALID at org.apache.hadoop.hbase.regionserver.HRegion.initializeStores(HRegion.java:1081) at org.apache.hadoop.hbase.regionserver.HRegion.initializeRegionInternals(HRegion.java:942) at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:898) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:7241) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:7200) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:7172) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:7130) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:7081) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:283) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:108) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:104) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.hbase.MemoryCompactionPolicy.INVALID at java.lang.Enum.valueOf(Enum.java:238) at org.apache.hadoop.hbase.MemoryCompactionPolicy.valueOf(MemoryCompactionPolicy.java:26) at org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder$ModifyableColumnFamilyDescriptor.lambda$getInMemoryCompaction$4(ColumnFamilyDescriptorBuilder.java:869) at org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder$ModifyableColumnFamilyDescriptor.lambda$getStringOrDefault$0(ColumnFamilyDescriptorBuilder.java:703) at org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder$ModifyableColumnFamilyDescriptor.getOrDefault(ColumnFamilyDescriptorBuilder.java:711) at org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder$ModifyableColumnFamilyDescriptor.getStringOrDefault(ColumnFamilyDescriptorBuilder.java:703) at org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder$ModifyableColumnFamilyDescriptor.getInMemoryCompaction(ColumnFamilyDescriptorBuilder.java:868) at org.apache.hadoop.hbase.regionserver.HStore.getMemstore(HStore.java:350) at org.apache.hadoop.hbase.regionserver.HStore.&lt;init&gt;(HStore.java:278) at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:5728) at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:1045) at org.apache.hadoop.hbase.regionserver.HRegion$1.call(HRegion.java:1042) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) ... 3 moreThis patch adds a sanity check for the value so that such alter operation should not proceed.</description>
      <version>3.0.0-alpha-1,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestIllegalTableDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.TableDescriptorChecker.java</file>
    </fixedFiles>
  </bug>
  <bug id="2253" opendate="2010-2-23 00:00:00" fixdate="2010-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show Block cache hit ratio for requests where cacheBlocks=true</summary>
      <description>We've been disabling block caching for MR jobs and other big scans. It seems to improve cache performance, but it's difficult to measure the hit ratio because even scans that don't cache blocks still request blocks from the cache (which is good), and those requests affect the hit/miss stats, which makes it difficult to see how the queries where cacheBlocks=true are performing.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.SimpleBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22530" opendate="2019-6-3 00:00:00" fixdate="2019-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The metrics of store files count of region are returned to clients incorrectly</summary>
      <description>There is a mistake on mapping metrics; the count of stores is mapped to the count of store files. So clients cannot retreive the metrics correctly.</description>
      <version>2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestRegionMetrics.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionMetricsBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="22612" opendate="2019-6-20 00:00:00" fixdate="2019-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address the final overview reviewing comments of HBASE-21879</summary>
      <description>I've created a big PR(https://github.com/apache/hbase/pull/320) for HBASE-21879, and Apache9 left some minor issues. Will address those comment here.If others have some comment about that PR, will also address in this PR.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.RefCnt.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileContext.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ByteBufferListOutputStream.java</file>
    </fixedFiles>
  </bug>
  <bug id="22617" opendate="2019-6-22 00:00:00" fixdate="2019-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Recovered WAL directories not getting cleaned up</summary>
      <description>While colocating the recovered edits directory with hbase.wal.dir, BASE_NAMESPACE_DIR got missed. This results in recovered edits being put in a separate directory rather than the default region directory even if the hbase.wal.dir is not overridden. Eg. if data is stored in /hbase/data/namespace/table1, recovered edits are put in  /hbase/namespace/table1. This also messes up the regular cleaner chores which never operate on this new directory and these directories will never be deleted, even for split parents or dropped tables. We should change the default back to have the base namespace directory in path.</description>
      <version>1.3.3,2.2.0,1.4.8,2.1.1,1.4.9,2.1.2,1.4.10,2.1.3,1.3.4,2.1.4,2.1.5,1.3.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRecoveredEdits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.GCRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CommonFSUtils.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.util.BackupUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
