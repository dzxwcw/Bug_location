<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="12751" opendate="2014-12-23 00:00:00" fixdate="2014-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow RowLock to be reader writer</summary>
      <description>Right now every write operation grabs a row lock. This is to prevent values from changing during a read modify write operation (increment or check and put). However it limits parallelism in several different scenarios.If there are several puts to the same row but different columns or stores then this is very limiting.If there are puts to the same column then mvcc number should ensure a consistent ordering. So locking is not needed.However locking for check and put or increment is still needed.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.FaultyFSLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.WALPerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALReaderOnSecureWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALFactory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestSecureWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestDefaultWALProviderWithHLogKey.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestDefaultWALProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.NamespacesInstanceModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.NamespacesModel.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MultiVersionConcurrencyControl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSWALEntry.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HashedBytes.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DisabledWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverStacking.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestWALObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHLogRecordReader.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALRecordReader.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBulkLoad.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionReplayEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeepDeletes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMultiVersionConcurrencyControl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMultiVersionConcurrencyControlBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPerColumnFamilyFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicaFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFileRefresherChore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWALLockup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestFSHLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollingNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationWALReaderManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsWithDistributedLogReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIOFencing.java</file>
    </fixedFiles>
  </bug>
  <bug id="13930" opendate="2015-6-18 00:00:00" fixdate="2015-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Findbugs packages from shaded jars</summary>
      <description>Looking at 1.1.1RC0 shaded artifacts, looks like classes from find bugs are under the edu prefix and are not shaded. We should exclude find bugs from the shaded builds, and/or shade shade the edu prefix as well.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="13973" opendate="2015-6-25 00:00:00" fixdate="2015-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update documentation for 10070 Phase 2 changes</summary>
      <description>HBASE-10513 added documentation for 10070. Let's update it with the Phase 2 changes.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14058" opendate="2015-7-10 00:00:00" fixdate="2015-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stabilizing default heap memory tuner</summary>
      <description>The memory tuner works well in general cases but when we have a work load that is both read heavy as well as write heavy the tuner does too many tuning. We should try to control the number of tuner operation and stabilize it. The main problem was that the tuner thinks it is in steady state even if it sees just one neutral tuner period thus does too many tuning operations and too many reverts that too with large step sizes(step size was set to maximum even after one neutral period). So to stop this I have thought of these steps:1) The division created by μ + δ/2 and μ - δ/2 is too small. Statistically ~62% periods will lie outside this range, which means 62% of the data points are considered either high or low which is too much. Use μ + δ*0.8 and μ - δ*0.8 instead. On expectations it will decrease number of tuner operations per 100 periods from 19 to just 10. If we use δ/2 then 31% of data values will be considered to be high and 31% will be considered to be low (2*0.31 * 0.31 = 0.19), on the other hand if we use δ*0.8 then 22% will be low and 22% will be high(2*0.22*0.22 ~ 0.10).2) Defining proper steady state by looking at past few periods(it is equal to hbase.regionserver.heapmemory.autotuner.lookup.periods) rather than just last tuner operation. We say tuner is in steady state when last few tuner periods were NEUTRAL. We keep decreasing step size unless it is extremely low. Then leave system in that state for some time.3) Rather then decreasing step size only while reverting, decrease the magnitude of step size whenever we are trying to revert tuning done in last few periods(sum the changes of last few periods and compare to current step) rather than just looking at last period. When its magnitude gets too low then make tuner steps NEUTRAL(no operation). This will cause step size to continuously decrease unless we reach steady state. After that tuning process will restart (tuner step size rests again when we reach steady state).4) The tuning done in last few periods will be decaying sum of past tuner steps with sign. This parameter will be positive for increase in memstore and negative for increase in block cache. Rather than using arithmetic mean we use this to give more priority to recent tuner steps.Please see the attachments. One represents the size of memstore(green) and size of block cache(blue) adjusted by tuner without these modification and other with the above modifications. The x-axis is time axis and y-axis is the fraction of heap memory available to memstore and block cache at that time(it always sums up to 80%). I configured min/max ranges for both components to 0.1 and 0.7 respectively(so in the plots the y-axis min and max is 0.1 and 0.7). In both cases the tuner tries to distribute memory by giving ~15% to memstore and ~65% to block cache. But the modified one does it much more smoothly.I got these results from YCSB test. The test was doing approximately 5000 inserts and 500 reads per second (for one region server). The results can be further fine tuned and number of tuner operation can be reduced with these changes in configuration.For more fine tuning:a) lower max step size (suggested = 4%)b) lower min step size ( default if also fine )To further decrease frequency of tuning operations:c) increase the number of lookup periods ( in the tests it was just 10, default is 60 )d) increase tuner period ( in the tests it was just 20 secs, default is 60secs)I used smaller tuner period/ number of look up periods to get more data points.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultHeapMemoryTuner.java</file>
    </fixedFiles>
  </bug>
  <bug id="14077" opendate="2015-7-14 00:00:00" fixdate="2015-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add package to hbase-protocol protobuf files.</summary>
      <description>c++ generated code is currently in the default namespace. That's bad practice; so lets fix it</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.MapReduce.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.VisibilityLabels.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Tracing.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Snapshot.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.SecureBulkLoad.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RPC.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RowProcessor.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.RegionServerStatus.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Quota.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Procedure.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.MultiRowMutation.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.generated.BulkDeleteProtos.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.generated.ExampleProtos.java</file>
      <file type="M">hbase-examples.src.main.protobuf.BulkDelete.proto</file>
      <file type="M">hbase-examples.src.main.protobuf.Examples.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AccessControlProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AggregateProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.CellProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterIdProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClusterStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ComparatorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.EncryptionProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ErrorHandlingProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FilterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FSProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HFileProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.LoadBalancerProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MapReduceProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProcedureProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MultiRowMutationProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ProcedureProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RegionServerStatusProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RowProcessorProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.RPCProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.SecureBulkLoadProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.TracingProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.VisibilityLabelsProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.WALProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Aggregate.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Authentication.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Cell.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterId.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Comparator.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Encryption.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.ErrorHandling.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.Filter.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.FS.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.HFile.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.LoadBalancer.proto</file>
    </fixedFiles>
  </bug>
  <bug id="14086" opendate="2015-7-15 00:00:00" fixdate="2015-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14087" opendate="2015-7-15 00:00:00" fixdate="2015-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14092" opendate="2015-7-15 00:00:00" fixdate="2015-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbck should run without locks by default and only disable the balancer when necessary</summary>
      <description>HBCK is sometimes used as a way to check the health of the cluster. When doing that it's not necessary to turn off the balancer. As such it's not needed to lock other runs of hbck out.We should add the --no-lock and --no-balancer command line flags.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="14097" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log link to client scan troubleshooting section when scanner exceptions happen.</summary>
      <description>As per description.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="14098" opendate="2015-7-16 00:00:00" fixdate="2015-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow dropping caches behind compactions</summary>
      <description></description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestReversibleScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMobStoreCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.compactions.TestPartitionedMobCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.MobFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="14100" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix high priority findbugs warnings</summary>
      <description>See here:https://builds.apache.org/job/HBase-TRUNK/6654/findbugsResult/HIGH/We have 6 high priority findbugs warnings. A high priority findbugs warning is usually a bug.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14102" opendate="2015-7-16 00:00:00" fixdate="2015-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thank you to our thanks page for vectorportal.com</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="14162" opendate="2015-7-28 00:00:00" fixdate="2015-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixing maven target for regenerating thrift classes fails against 0.9.2</summary>
      <description>HBASE-14045 updated the thrift version, but our enforcer rule is still checking 0.9.0.$ git checkout masterSwitched to branch 'master'Your branch is up-to-date with 'origin/master'.$ mvn compile -Pcompile-thrift -DskipTests[INFO] Scanning for projects...... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Building HBase - Thrift 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce) @ hbase-thrift ---[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-thrift-version) @ hbase-thrift ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireProperty failed with message:--[FATAL] ==========================================================================================[FATAL] HBase Thrift requires the thrift generator version 0.9.0.[FATAL] Setting it to something else needs to be reviewed for wire and behavior compatibility.[FATAL] ==========================================================================================--[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] HBase .............................................. SUCCESS [ 2.897 s][INFO] HBase - Checkstyle ................................. SUCCESS [ 0.554 s][INFO] HBase - Annotations ................................ SUCCESS [ 0.940 s][INFO] HBase - Protocol ................................... SUCCESS [ 15.454 s][INFO] HBase - Common ..................................... SUCCESS [ 8.984 s][INFO] HBase - Procedure .................................. SUCCESS [ 1.982 s][INFO] HBase - Client ..................................... SUCCESS [ 6.805 s][INFO] HBase - Hadoop Compatibility ....................... SUCCESS [ 0.202 s][INFO] HBase - Hadoop Two Compatibility ................... SUCCESS [ 1.393 s][INFO] HBase - Prefix Tree ................................ SUCCESS [ 1.233 s][INFO] HBase - Server ..................................... SUCCESS [ 13.841 s][INFO] HBase - Testing Util ............................... SUCCESS [ 2.979 s][INFO] HBase - Thrift ..................................... FAILURE [ 0.234 s][INFO] HBase - Shell ...................................... SKIPPED[INFO] HBase - Integration Tests .......................... SKIPPED[INFO] HBase - Examples ................................... SKIPPED[INFO] HBase - Rest ....................................... SKIPPED[INFO] HBase - Assembly ................................... SKIPPED[INFO] HBase - Shaded ..................................... SKIPPED[INFO] HBase - Shaded - Client ............................ SKIPPED[INFO] HBase - Shaded - Server ............................ SKIPPED[INFO] Apache HBase - Spark ............................... SKIPPED[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 01:00 min[INFO] Finished at: 2015-07-28T12:36:15-05:00[INFO] Final Memory: 84M/1038M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.3.1:enforce (enforce-thrift-version) on project hbase-thrift: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :hbase-thrift</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14201" opendate="2015-8-10 00:00:00" fixdate="2015-8-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbck should not take a lock unless fixing errors</summary>
      <description>By default, hbck is run in a read-only checker mode. In this case, it issensible to let others run. By default, the balancer is left alone,which may cause spurious errors, but cannot leave the balancer in a badstate. It is dangerous to leave the balancer by accident, so it is onlyever enabled after fixing, it will never be forced off because ofracing.When hbck is run in fixer mode, it must take an exclusive lock anddisable the balancer, or all havoc will break loose.If you want to stop hbck from running in parallel, the -exclusive flagwill create the lock file. If you want to force -disableBalancer, thatoption is available too. This makes more semantic sense than -noLock and-noSwitchBalancer, respectively.This task is related to HBASE-14092.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.HbckTestingUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="14212" opendate="2015-8-11 00:00:00" fixdate="2015-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add IT test for procedure-v2-based namespace DDL</summary>
      <description>Integration test for proc-v2-based table DDLs was created in HBASE-12439 during HBASE 1.1 release. With HBASE-13212, proc-v2-based namespace DDLs are introduced. We need to enhanced the IT from HBASE-12429 to include namespace DDLs.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.java</file>
    </fixedFiles>
  </bug>
  <bug id="14249" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14250" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14338" opendate="2015-8-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14340" opendate="2015-8-30 00:00:00" fixdate="2015-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14433" opendate="2015-9-14 00:00:00" fixdate="2015-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set down the client executor core thread count from 256 in tests</summary>
      <description>HBASE-10449 upped our core count from 0 to 256 (max is 256). Looking in a recent test run core dump, I see up to 256 threads per client and all are idle. At a minimum it makes it hard reading test thread dumps. Trying to learn more about why we went a core of 256 over in HBASE-10449. Meantime will try setting down configs for test.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-client.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  <bug id="14471" opendate="2015-9-23 00:00:00" fixdate="2015-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift - HTTP Error 413 full HEAD if using kerberos authentication</summary>
      <description>When trying to access a Thrift sever that is kerberized, a HTTP 413 full HEAD error is received. In that case, tcpdump shows http header size exceeded 4k.This seems related to the issue outlined in @HADOOP-8816.The default header size limit is 4k, follow the fix for @HADOOP-8816, propose to increase the header size limit to 64k.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftHttpServer.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="14489" opendate="2015-9-25 00:00:00" fixdate="2015-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>postScannerFilterRow consumes a lot of CPU</summary>
      <description>During an unrelated test I found that when scanning a tall table with CQ only and filtering most results at the server, 50% of time is spend in postScannerFilterRow, even though the coprocessor does nothing in that hook.We need to find a way not to call this hook when not needed, or to question why we have this hook at all.I think ram_krish added the hook (or maybe anoop.hbase). I am also not sure whether Phoenix uses this hook (giacomotaylor?)</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
    </fixedFiles>
  </bug>
  <bug id="1449" opendate="2009-5-26 00:00:00" fixdate="2009-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to latest ZooKeeper trunk</summary>
      <description>The latest ZooKeeper trunk has many improvements and is much more clean. Since we are working on integrating better with ZooKeeper (e.g. shell and JNI) we should run off their latest and greatest.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.zookeeper.HQuorumPeerTest.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">lib.zookeeper-3.1.0-hbase-1241.jar</file>
    </fixedFiles>
  </bug>
  <bug id="14597" opendate="2015-10-13 00:00:00" fixdate="2015-10-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Groups cache in multi-threaded env</summary>
      <description>UGI doesn't hash based on the user as expected so since we have lots of ugi potentially created the cache doesn't do it's job.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestUser.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.UserProvider.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
    </fixedFiles>
  </bug>
  <bug id="14690" opendate="2015-10-24 00:00:00" fixdate="2015-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix css so there&amp;#39;s no left/right scroll bar</summary>
      <description>2 em of extra padding needs to be reomved.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
    </fixedFiles>
  </bug>
  <bug id="14693" opendate="2015-10-24 00:00:00" fixdate="2015-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add client-side metrics for received pushback signals</summary>
      <description>HBASE-12911 added client side metrics. HBASE-5162 added a mechanism for sending advisory backpressure signals to clients when the server is heavily loaded, and HBASE-12702 and subtasks backported this to all active branches. Add client-side metrics for received pushback signal.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.16,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientPushback.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MetricsConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="14695" opendate="2015-10-25 00:00:00" fixdate="2015-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some easy HTML warnings</summary>
      <description>There are a few links to top-level pages but missing the trailing /, and a few please where we link to pages like replication.html even though we have a single-page book now. There are also a few broken links in the APIdocs, probably due to code we have specifically filtered out of the Javadoc:#------------------------------------------------------------# ERROR 4 files had broken links#------------------------------------------------------------/devapidocs/org/apache/hadoop/hbase/wal/NamespaceGroupingStrategy.htmlhad 1 broken link /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/org/apache/hadoop/hbase/wal/package-tree.htmlhad 1 broken link /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/overview-tree.htmlhad 2 broken links /devapidocs/org/apache/hadoop/hbase/io/encoding/BufferedDataBlockEncoder.SeekerState.html /devapidocs/org/apache/hadoop/hbase/wal/RegionGroupingProvider.RegionGroupingStrategy.html/devapidocs/serialized-form.htmlhad 26 broken links /devapidocs/src-html/com/google/protobuf/DescriptorProtos.DescriptorProto.ExtensionRange.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.DescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumValueDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.EnumValueOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FieldDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FieldOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileDescriptorSet.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.FileOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MessageOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MethodDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.MethodOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.ServiceDescriptorProto.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.ServiceOptions.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.SourceCodeInfo.Location.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.SourceCodeInfo.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.UninterpretedOption.NamePart.html /devapidocs/src-html/com/google/protobuf/DescriptorProtos.UninterpretedOption.html /devapidocs/src-html/com/google/protobuf/GeneratedMessage.html /devapidocs/src-html/com/google/protobuf/GeneratedMessageLite.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/ColumnFilter$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/HBaseContext$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/RowKeyFilter$.html /devapidocs/src-html/org/apache/hadoop/hbase/spark/SchemaQualifierDefinition$.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSMapRUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.package-info.java</file>
    </fixedFiles>
  </bug>
  <bug id="1476" opendate="2009-6-2 00:00:00" fixdate="2009-5-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>scaling compaction with multiple threads</summary>
      <description>Was thinking we should build in support to be able to handle more then one thread for compactions this will allow us to keep up with compactions when we get to the point where we store Tb's of data per node and may regionsMaybe a configurable setting to set how many threads a region server can use for compactions.With compression turned on my compactions are limited by cpu speed with multi cores then it would be nice to be able to scale compactions to 2 or more cores.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14765" opendate="2015-11-4 00:00:00" fixdate="2015-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snappy profile</summary>
      <description>Snappy is provided by hadoop and has been for, a long while.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14766" opendate="2015-11-5 00:00:00" fixdate="2015-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In WALEntryFilter, cell.getFamily() needs to be replaced with the new low-cost implementation</summary>
      <description>Cell's getFamily() gets an array copy of the cell's family, while in the filter function, it just needs to peek into the family and do a compare. Replace Bytes.toString(cell.getFamily())with Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength())</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.TableCfWALEntryFilter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14781" opendate="2015-11-6 00:00:00" fixdate="2015-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn per cf flushing on for ITBLL by default</summary>
      <description></description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  <bug id="14849" opendate="2015-11-19 00:00:00" fixdate="2015-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug id="14928" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="14942" opendate="2015-12-7 00:00:00" fixdate="2015-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow turning off BoundedByteBufferPool</summary>
      <description>The G1 does a great job of compacting, there's no reason to use the BoundedByteBufferPool when the JVM can it for us. So we should allow turning this off for people who are running new jvm's where the G1 is working well.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="14946" opendate="2015-12-8 00:00:00" fixdate="2015-12-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow multi&amp;#39;s to over run the max result size.</summary>
      <description>If a user puts a list of tons of different gets into a table we will then send them along in a multi. The server un-wraps each get in the multi. While no single get may be over the size limit the total might be.We should protect the server from this. We should batch up on the server side so each RPC is smaller.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ProcedurePrepareLatch.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RetryImmediatelyException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MultiActionResultTooLarge.java</file>
    </fixedFiles>
  </bug>
  <bug id="14949" opendate="2015-12-8 00:00:00" fixdate="2015-2-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve name conflict when splitting if there are duplicated WAL entries</summary>
      <description>The AsyncFSHLog introduced in HBASE-14790 may write same WAL entries to different WAL files. WAL entry itself is idempotent so replay is not a problem but the intermediate file name and final name when splitting is constructed using the lowest or highest sequence id of the WAL entries written, so it is possible that different WAL files will have same intermediate or final file name when splitting. In the currentm implementation, this will cause split fail or data loss. We need to solve this.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="14969" opendate="2015-12-13 00:00:00" fixdate="2015-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add throughput controller for flush</summary>
      <description>In HBASE-8329 we added a throughput controller for compaction, to avoid spike caused by huge IO pressure like network/disk overflow. However, even with this control on, we are still observing disk utils near 100%, and by analysis we think this is caused by flush, especially when we increase the setting of hbase.hstore.flusher.countIn this JIRA, we propose to add throughput control feature for flush, as a supplement of HBASE-8329 to better control IO pressure.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIOFencing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeStoreEngine.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionReplayEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHMobStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactionWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MockRegionServerServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverScannerOpenHook.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StripeStoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultStoreEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactionTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.NoLimitCompactionThroughputController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputControllerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionThroughputController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.DefaultMobStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="14978" opendate="2015-12-15 00:00:00" fixdate="2015-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow Multi to retain too many blocks</summary>
      <description>Scans and Multi's have limits on the total size of cells that can be returned. However if those requests are not all pointing at the same blocks then the KeyValues can keep alive a lot more data than their size.Take the following example:A multi with a list of 10000 gets to a fat row. Each column being returned in in a different block. Each column is small 32 bytes or so.So the total cell size will be 32 * 10000 = ~320kb. However if each block is 128k then total retained heap size will be almost 2gigs.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="14983" opendate="2015-12-15 00:00:00" fixdate="2015-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metrics for per block type hit/miss ratios</summary>
      <description>Missing a root index block is worse than missing a data block. We should know the difference</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCombinedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CombinedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheKey.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="14984" opendate="2015-12-15 00:00:00" fixdate="2015-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow memcached block cache to set optimze to false</summary>
      <description>In order to keep latency consistent it might not be good to allow the spy memcached client to optimize.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="15005" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use value array in computing block length for 1.2 and 1.3</summary>
      <description>Follow on to HBASE-14978</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
    </fixedFiles>
  </bug>
  <bug id="15007" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update Hadoop support matrix to list 2.6.1+ as supported</summary>
      <description>The Hadoop community responded very well to our request for more maintenance releases and have now put out 2.6.1 - 2.6.3. The first of those included the fix for our catastrophic failure under HDFS encryption.We should update the book to point out those versions are fine.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15015" opendate="2015-12-18 00:00:00" fixdate="2015-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checktyle plugin shouldn&amp;#39;t check Jamon-generated Java classes</summary>
      <description></description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15036" opendate="2015-12-23 00:00:00" fixdate="2015-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase Spark documentation to include bulk load with thin records</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15106" opendate="2016-1-14 00:00:00" fixdate="2016-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Procedure Queue pass Procedure for better debuggability</summary>
      <description>Changes the various acquire/release methods to take the Procedure as argument.That allows better debuggability. (The patch it is just a refactor, it does not introduce any new thing)https://reviews.apache.org/r/42271/</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="15124" opendate="2016-1-17 00:00:00" fixdate="2016-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the new &amp;#39;normalization&amp;#39; feature in refguid</summary>
      <description>A nice new feature is coming in to 1.2.0, normalization. A small bit of doc on it in refguide would help.Should define what normalization is.Should say a sentence or two on how it works and when.Throw in the output of shell commands.A paragraph or so. I can help.Marking critical against 1.2.0. Not a blocker.</description>
      <version>1.3.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15222" opendate="2016-2-5 00:00:00" fixdate="2016-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use less contended classes for metrics</summary>
      <description>Running the benchmarks now, but it looks like the results are pretty extreme. The locking in our histograms is pretty extreme.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCacheStats.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.AgeSnapshot.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheViewTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.metrics.TestBaseSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableTimeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableSizeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableRangeHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MutableHistogram.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MetricsExecutorImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.MetricMutableQuantiles.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.thrift.MetricsThriftServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.rest.MetricsRESTSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSourceSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationSinkSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsReplicationGlobalSourceSource.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.wal.MetricsWALSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsSnapshotSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterFilesystemSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.balancer.MetricsBalancerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.metrics2.MetricHistogram.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseSource.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFastLongHistogram.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.FastLongHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="15255" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pointer to linkedin blog on putting jvm logs on fast disk</summary>
      <description>Add pointer to linked in blog: https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-trafficIIRC, tsdb says to do similar.Also add into perf section note on native crc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15259" opendate="2016-2-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALEdits under replay will also be replicated</summary>
      <description>I need to verify this but seeing the codetry { // We are about to append this edit; update the region-scoped sequence number. Do it // here inside this single appending/writing thread. Events are ordered on the ringbuffer // so region sequenceids will also be in order. regionSequenceId = entry.stampRegionSequenceId(); // Edits are empty, there is nothing to append. Maybe empty when we are looking for a // region sequence id only, a region edit/sequence id that is not associated with an actual // edit. It has to go through all the rigmarole to be sure we have the right ordering. if (entry.getEdit().isEmpty()) { return; } // Coprocessor hook. if (!coprocessorHost.preWALWrite(entry.getHRegionInfo(), entry.getKey(), entry.getEdit())) { if (entry.getEdit().isReplay()) { // Set replication scope null so that this won't be replicated entry.getKey().setScopes(null); } } if (!listeners.isEmpty()) { for (WALActionsListener i: listeners) { // TODO: Why does listener take a table description and CPs take a regioninfo? Fix. i.visitLogEntryBeforeWrite(entry.getHTableDescriptor(), entry.getKey(), entry.getEdit()); } }When a WALEdit is in replay we set the Logkey to null. But in the visitLogEntryBeforeWrite() we again set the LogKey based on the replication scope associated with the cells. So the previous step of setting null does not work here?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSmallTests.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
    </fixedFiles>
  </bug>
  <bug id="1526" opendate="2009-6-15 00:00:00" fixdate="2009-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapreduce fixup</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.DisabledTestTableMapReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexConfiguration.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.BuildTableIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="15261" opendate="2016-2-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Make Throwable t in DaughterOpener volatile</summary>
      <description>In the region split process, daughter regions are opened in different threads, Throwable t is set in these threads and it is checked in the calling thread. Need to make it volatile so the checking will not miss any exceptions from opening daughter regions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="1528" opendate="2009-6-15 00:00:00" fixdate="2009-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure scanners work across memcache snapshot</summary>
      <description>We have hole in scanning where if a snapshot, we'll stop seeing in-memory results.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MultiRegionTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestClient.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="1532" opendate="2009-6-17 00:00:00" fixdate="2009-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UI Visibility into ZooKeeper</summary>
      <description>Add ZooKeeper information/administration to UI.Discussion showed particular interest in a tree-viewer application, something like ZOOKEEPER-418.There was talk between Lars/JimK about how often the viewer should update its data.See HBASE-1329 for more information.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15344" opendate="2016-2-26 00:00:00" fixdate="2016-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.3 to prereq tables in ref guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15346" opendate="2016-2-26 00:00:00" fixdate="2016-2-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.3 RM to docs</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15348" opendate="2016-2-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix tests broken by recent metrics re-work</summary>
      <description>Counts are appoximate and go away. We should re-work the tests or test utils to make them work now.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestStochasticBalancerJmxMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="15349" opendate="2016-2-26 00:00:00" fixdate="2016-7-26 01:00:00" resolution="Implemented">
    <buginformation>
      <summary>Update surefire version to 2.19.1</summary>
      <description>So that new properties like surefire.excludesFile and includesFile can be used to easily exclude/include flaky tests.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15354" opendate="2016-2-26 00:00:00" fixdate="2016-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use same criteria for clearing meta cache for all operations</summary>
      <description>Currently we do not clear/update meta cache for some special exceptions if the operation went through AsyncProcess#submit like HTable#put calls. But, we clear meta cache without checking for these special exceptions in case of other operations like gets, deletes etc because they directly go through the RpcRetryingCaller#callWithRetries instead of the AsyncProcess.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMetaCache.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionAdminServiceCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="15358" opendate="2016-2-29 00:00:00" fixdate="2016-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>canEnforceTimeLimitFromScope should use timeScope instead of sizeScope</summary>
      <description>A small but maybe critical bug</description>
      <version>1.2.0,1.3.0,1.1.3,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,1.1.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScannerContext.java</file>
    </fixedFiles>
  </bug>
  <bug id="15371" opendate="2016-3-1 00:00:00" fixdate="2016-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Completed support parent-child procedure</summary>
      <description>In Procedure-V2 Phase 1 (HBASE-14336), some infrastructure of supporting child procedure exists. However, there is no need in Phase 1 (master DDL) to have multi-level procedures. This JIRA implements adding child procedures to procedure execution list and execute them before parent procedure can make further progress.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureRecovery.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="15376" opendate="2016-3-2 00:00:00" fixdate="2016-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScanNext metric is size-based while every other per-operation metric is time based</summary>
      <description>We have per-operation metrics for Get, Mutate, Delete, Increment, and ScanNext. The metrics are emitted like: "Get_num_ops" : 4837505, "Get_min" : 0, "Get_max" : 296, "Get_mean" : 0.2934618155433431, "Get_median" : 0.0, "Get_75th_percentile" : 0.0, "Get_95th_percentile" : 1.0, "Get_99th_percentile" : 1.0,... "ScanNext_num_ops" : 194705, "ScanNext_min" : 0, "ScanNext_max" : 18441, "ScanNext_mean" : 7468.274651395701, "ScanNext_median" : 583.0, "ScanNext_75th_percentile" : 583.0, "ScanNext_95th_percentile" : 13481.0, "ScanNext_99th_percentile" : 13481.0,The problem is that all of Get,Mutate,Delete,Increment,Append,Replay are time based tracking how long the operation ran, while ScanNext is tracking returned response sizes (returned cell-sizes to be exact). Obviously, this is very confusing and you would only know this subtlety if you read the metrics collection code. Not sure how useful is the ScanNext metric as it is today. We can deprecate it, and introduce a time based one to keep track of scan request latencies. ps. Shamelessly using the parent jira (since these seem relavant).</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15377" opendate="2016-3-2 00:00:00" fixdate="2016-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Per-RS Get metric is time based, per-region metric is size-based</summary>
      <description>We have metrics for Get operations at the region server level and region level. "Get_num_ops" : 4837505, "Get_min" : 0, "Get_max" : 296, "Get_mean" : 0.2934618155433431, "Get_median" : 0.0, "Get_75th_percentile" : 0.0, "Get_95th_percentile" : 1.0, "Get_99th_percentile" : 1.0,and "Namespace_hbase_table_meta_region_1588230740_metric_get_num_ops" : 103, "Namespace_hbase_table_meta_region_1588230740_metric_get_min" : 450, "Namespace_hbase_table_meta_region_1588230740_metric_get_max" : 470, "Namespace_hbase_table_meta_region_1588230740_metric_get_mean" : 450.19417475728153, "Namespace_hbase_table_meta_region_1588230740_metric_get_median" : 460.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_75th_percentile" : 470.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_95th_percentile" : 470.0, "Namespace_hbase_table_meta_region_1588230740_metric_get_99th_percentile" : 470.0,The problem is that the report values for the region server shows the latency, versus the reported values for the region shows the response sizes. There is no way of telling this without reading the source code. I think we should deprecate response size histograms in favor of latency histograms. See also HBASE-15376.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15397" opendate="2016-3-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create bulk load replication znode(hfile-refs) in ZK replication queue by default</summary>
      <description>Create bulk load replication znode(hfile-refs) in ZK replication queue by default same as hbase replication znode. Otherwise the problem what happens is currently replication admin directly operates on ZK without routing through HM/RS. So suppose if a user enables the replication for bulk loaded data in server but fails to do the same in the client configurations then add peer will not add hfile-refs znode, resulting in replication failure for bulk loaded data.So after fixing this the behavior will be same as mutation replication.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationStateZKBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15424" opendate="2016-3-8 00:00:00" fixdate="2016-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add bulk load hfile-refs for replication in ZK after the event is appended in the WAL</summary>
      <description>Currenlty hfile-refs znode used for tracking the bulk loaded data replication is added first and then the bulk load event in appended in the WAL. So this may lead to a issue where the znode is added in ZK but append to WAL is failed(due to some probelm in DN), so this znode will be left in ZK as it is and will not allow hfile to get deleted from archive directory.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.WALPerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestMetricsWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DisabledWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALActionsListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.MetricsWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
    </fixedFiles>
  </bug>
  <bug id="15425" opendate="2016-3-8 00:00:00" fixdate="2016-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failing to write bulk load event marker in the WAL is ignored</summary>
      <description>During LoadIncrementalHFiles process if we fail to write the bulk load event marker in the WAL, it is ignored. So this will lead to data mismatch issue in source and peer cluster in case of bulk loaded data replication scenario.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="15429" opendate="2016-3-9 00:00:00" fixdate="2016-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a split policy for busy regions</summary>
      <description>Currently, all region split policies are based on size. However, in certain cases, it is a wise choice to make a split decision based on number of requests to the region and split busy regions.A crude metric is that if a region blocks writes often and throws RegionTooBusyExceoption, it's probably a good idea to split it.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1543" opendate="2009-6-18 00:00:00" fixdate="2009-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unnecessary toString during scanning costs us some CPU</summary>
      <description>Profiling, I see an unused toString &amp;#8211; left over from debugging is my guess &amp;#8211; is cost us 13% of measured CPU profiling (whatever that means).</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15482" opendate="2016-3-18 00:00:00" fixdate="2016-12-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to skip calculating block locations for SnapshotInputFormat</summary>
      <description>When a MR job is reading from SnapshotInputFormat, it needs to calculate the splits based on the block locations in order to get best locality. However, this process may take a long time for large snapshots. In some setup, the computing layer, Spark, Hive or Presto could run out side of HBase cluster. In these scenarios, the block locality doesn't matter. Therefore, it will be great to have an option to skip calculating the block locations for every job. That will super useful for the Hive/Presto/Spark connectors.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.mapred.TestTableSnapshotInputFormat.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableSnapshotInputFormat.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatTestBase.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormatImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15504" opendate="2016-3-21 00:00:00" fixdate="2016-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Balancer in 1.3 not moving regions off overloaded regionserver</summary>
      <description>We pushed 1.3 to a couple of clusters. In some cases the regions were assigned VERY un-evenly and the regions would not move after that.We ended up with one rs getting thousands of regions and most servers getting 0. Running balancer would do nothing. The balancer would say that it couldn't find a solution with less than the current cost.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15515" opendate="2016-3-22 00:00:00" fixdate="2016-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve LocalityBasedCandidateGenerator in Balancer</summary>
      <description>There are some problems which need to fix.1. LocalityBasedCandidateGenerator.getLowestLocalityRegionOnServer should skip empty region.2. When use LocalityBasedCandidateGenerator to generate Cluster.Action, it should add random operation instead of pickLowestLocalityServer(cluster). Because the search function may stuck here if it always generate the same Cluster.Action.3. getLeastLoadedTopServerForRegion should get least loaded server which have better locality than current server.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15518" opendate="2016-3-22 00:00:00" fixdate="2016-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Per-Table metrics back</summary>
      <description>We used to have per-table metrics, but it was removed in some restructuring. We have per-region metrics, and per-regionserver metrics, but nothing in between. For majority of users, per-region is too granular, they are mostly interested in table level aggregates. This is especially useful in multi-tenant cases where a table's disk usage, number of requests, etc can be made much more visible. In this jira, we'll add the basic infrastructure to add a single (or a few) per-table metrics. Than we can improve on that by adding remaining metrics from the region server level. The plan is to NOT aggregate per-table metrics at master for now. Just aggregation of per-region metrics at the per-table level for every regionserver.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15525" opendate="2016-3-24 00:00:00" fixdate="2016-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>OutOfMemory could occur when using BoundedByteBufferPool during RPC bursts</summary>
      <description>After HBASE-13819 the system some times run out of direct memory whenever there is some network congestion or some client side issues. This was because of pending RPCs in the RPCServer$Connection.responseQueue and since all the responses in this queue hold a buffer for cellblock from BoundedByteBufferPool this could takeup a lot of memory if the BoundedByteBufferPool's moving average settles down towards a higher valueSee the discussion here HBASE-13819-comment</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BufferChain.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.ByteBufferOutputStream.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.IPCUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15551" opendate="2016-3-29 00:00:00" fixdate="2016-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make call queue too big exception use servername</summary>
      <description>If the rpc server is listening to something other than the hostname ( 0.0.0.0 for example ) then the exception thrown isn't very helpful. We should make that more helpful.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="15554" opendate="2016-3-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StoreFile$Writer.appendGeneralBloomFilter generates extra KV</summary>
      <description>Accounts for 10% memory allocation in compaction thread when BloomFilterType is ROWCOL.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterChunk.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CompoundBloomFilterWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CompoundBloomFilter.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.MurmurHash3.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.MurmurHash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15623" opendate="2016-4-9 00:00:00" fixdate="2016-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update refguide to change hadoop &lt;= 2.3.x from NT to X for hbase-1.2.x</summary>
      <description>This issue is about updating our hadoop supported versions grid in the prerequisites section of refguide. Here is thread proposing this change up on dev list: http://osdir.com/ml/general/2016-04/msg09194.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15637" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TSHA Thrift-2 server should allow limiting call queue size</summary>
      <description>Right now seems like thrift-2 hsha server always create unbounded queue, which could lead to OOM)</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="1564" opendate="2009-6-22 00:00:00" fixdate="2009-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>in UI make host addresses all look the same -- not IP sometimes and host at others</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.table.jsp</file>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="15640" opendate="2016-4-12 00:00:00" fixdate="2016-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>L1 cache doesn&amp;#39;t give fair warning that it is showing partial stats only when it hits limit</summary>
      <description>I was baffled looking at L1 Cache because it was stuck at 100k blocks and only 'loading' 3G of data according to the UI. When I looked in log, all the numbers looked write in the Cache summary reported in log.Turns out our buckecache UI template will cut off calculating stats at a limit --which is probably what we want &amp;#8211; but the fact that it has done this should be more plain.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.19,1.2.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="15641" opendate="2016-4-13 00:00:00" fixdate="2016-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell "alter" should do a single modifyTable operation</summary>
      <description>When performing an "alter" on multiple column families in a table, then shell will perform a separate Admin.modifyColumn() call for each column family being modified, with all of the table regions being bulk-reopened each time. It would be much better to simply apply all the changes to the table descriptor, then do a single call to Admin.modifyTable().</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="15664" opendate="2016-4-17 00:00:00" fixdate="2016-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Long.MAX_VALUE instead of HConstants.FOREVER in CompactionPolicy</summary>
      <description>The TTL per CF is in seconds, we will convert it to milliseconds when construct HStore. And if it is HConstants.FOREVER, we will set it to Long.MAX_VALUE.HStore.java public static long determineTTLFromFamily(final HColumnDescriptor family) { // HCD.getTimeToLive returns ttl in seconds. Convert to milliseconds. long ttl = family.getTimeToLive(); if (ttl == HConstants.FOREVER) { // Default is unlimited ttl. ttl = Long.MAX_VALUE; } else if (ttl == -1) { ttl = Long.MAX_VALUE; } else { // Second -&gt; ms adjust for user data ttl *= 1000; } return ttl; }</description>
      <version>1.3.0,0.98.19,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="15669" opendate="2016-4-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HFile size is not considered correctly in a replication request</summary>
      <description>In a single replication request from source cluster a RS can send either at most replication.source.size.capacity size of data or replication.source.nb.capacity entries. The size is calculated by considering the cells size in each entry which will get calculated wrongly in case of bulk loaded data replication, in this case we need to consider the size of hfiles not cell.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.WALProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15698" opendate="2016-4-23 00:00:00" fixdate="2016-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment TimeRange not serialized to server</summary>
      <description>Before HBase-1.2, the Increment TimeRange set on the client was serialized over to the server. As of HBase 1.2, this appears to no longer be true, as my preIncrement coprocessor always gets HConstants.LATEST_TIMESTAMP as the value of increment.getTimeRange().getMax() regardless of what the client has specified.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15703" opendate="2016-4-25 00:00:00" fixdate="2016-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deadline scheduler needs to return to the client info about skipped calls, not just drop them</summary>
      <description>In AdaptiveLifoCodelCallQueue we drop the calls when we think we're overloaded, we should instead return CallDroppedException to the cleent or something.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.ClientExceptionsUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.PreemptiveFastFailInterceptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CallQueueTooBigException.java</file>
    </fixedFiles>
  </bug>
  <bug id="15707" opendate="2016-4-25 00:00:00" fixdate="2016-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ImportTSV bulk output does not support tags with hfile.format.version=3</summary>
      <description>Running the following command:hbase hbase org.apache.hadoop.hbase.mapreduce.ImportTsv \ -Dhfile.format.version=3 \ -Dmapreduce.map.combine.minspills=1 \ -Dimporttsv.separator=, \ -Dimporttsv.skip.bad.lines=false \ -Dimporttsv.columns="HBASE_ROW_KEY,cf1:a,HBASE_CELL_TTL" \ -Dimporttsv.bulk.output=/tmp/testttl/output/1 \ testttl \ /tmp/testttl/input The content of input is like:row1,data1,00000060 row2,data2,00000660 row3,data3,00000060 row4,data4,00000660When running hfile tool with the output hfile, there is no ttl tag.</description>
      <version>1.3.0,1.0.4,1.4.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,0.98.20,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  <bug id="15714" opendate="2016-4-26 00:00:00" fixdate="2016-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We are calling checkRow() twice in doMiniBatchMutation()</summary>
      <description>In multi() -&gt; doMiniBatchMutation() code path, we end up calling checkRow() twice, once from checkBatchOp() and once from getRowLock(). See anoop.hbase's comments at https://issues.apache.org/jira/browse/HBASE-15600?focusedCommentId=15257636&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15257636.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="15801" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle for all branches</summary>
      <description>We should use the same checkstyle for all branches.</description>
      <version>1.3.0,1.2.1,1.0.3,0.98.19,1.4.0,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.0.4,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15811" opendate="2016-5-10 00:00:00" fixdate="2016-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Get after batch Put does not fetch all Cells</summary>
      <description>A big batch put followed by a batch get does not always return all Cells put. See attached test program by Robert Farr that reproduces the issue. It seems to be an issue to do with a cluster of more than one machine. Running against a single machine does not have the problem (though the single machine may have many regions). Robert was unable to make his program fail with a single machine only.I reproduced what Robert was seeing running his program. I was also unable to make a single machine fail. In a batch of 1000 puts, I see one to three Gets fail. I noticed too that if I wait a second after a fail and then re-get, the Get succeeds.</description>
      <version>1.0.0,1.3.0,1.2.1,0.98.19</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug id="15828" opendate="2016-5-14 00:00:00" fixdate="2016-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix extant findbug</summary>
      <description></description>
      <version>1.3.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Future.java</file>
    </fixedFiles>
  </bug>
  <bug id="15844" opendate="2016-5-17 00:00:00" fixdate="2016-7-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>We should respect hfile.block.index.cacheonwrite when write intermediate index Block</summary>
      <description>BlockIndexWriter#writeIntermediateBlock if (cacheConf != null) { HFileBlock blockForCaching = blockWriter.getBlockForCaching(cacheConf); cacheConf.getBlockCache().cacheBlock(new BlockCacheKey(nameForCaching, beginOffset, true, blockForCaching.getBlockType()), blockForCaching); }The if condition should be ?if (cacheConf != null &amp;&amp; cacheConf.shouldCacheIndexesOnWrite())</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="15864" opendate="2016-5-19 00:00:00" fixdate="2016-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reuse the testing helper to wait regions in transition</summary>
      <description>There are a bunch of unit test that do the same loop to wait for region in transitions. get rid of the duplicate code and call the helpers.</description>
      <version>1.3.0,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildOverlap.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.TestOfflineMetaRebuildHole.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.handler.TestTableDeleteFamilyHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
    </fixedFiles>
  </bug>
  <bug id="15882" opendate="2016-5-23 00:00:00" fixdate="2016-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to yetus precommit 0.3.0</summary>
      <description>Now that Yetus 0.3.0 is out, we should update our precommit builds so we can use docker again.Most of the changes to our personality should be covered in the updated hbase example that ships with 0.3.0. we'll have to forward port the flakey test work.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15883" opendate="2016-5-23 00:00:00" fixdate="2016-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding WAL files and tracking offsets in HBase.</summary>
      <description>Implemented simple offset tracking for WAL files inside of an HBase table along with Unit Testing. Have not implemented queue claiming yet</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestReplicationHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="15888" opendate="2016-5-25 00:00:00" fixdate="2016-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend HBASE-12769 for bulk load data replication</summary>
      <description>Include hfile-refs queue check and fix also in hbck -fixReplication command</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.ReplicationChecker.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15919" opendate="2016-5-31 00:00:00" fixdate="2016-5-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document @Rule vs @ClassRule. Also clarify timeout limits are on TestCase.</summary>
      <description>Our timeout for tests is not clear in the refguide. Our @Rule based CategoryBased timeout is for each individual test when the timeout it seems is for the whole testcase... all the tests that make up the test class. This issue is about cleaning up any abiguity and promoting the new change added over in HBASE-15915 by @appy for a @ClassRuleCleanup refguide on what timeout applys to.Add section on how to add timeouts to tests.See HBASE-15915 tail for some notes on what to add to doc.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15948" opendate="2016-6-2 00:00:00" fixdate="2016-6-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port "HADOOP-9956 RPC listener inefficiently assigns connections to readers"</summary>
      <description>Esteban noticed we were missing this upstream issue. Seems to make no difference in profiling but here is the patch anyways.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.AbstractTestIPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SimpleRpcSchedulerFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15950" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix memstore size estimates to be more tighter</summary>
      <description>While testing something else, I was loading a region with a lot of data. Writing 30M cells in 1M rows, with 1 byte values. The memstore size turned out to be estimated as 4.5GB, while with the JFR profiling I can see that we are using 2.8GB for all the objects in the memstore (KV + KV byte[] + CSLM.Node + CSLM.Index). This obviously means that there is room in the write cache that we are not effectively using.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWalAndCompactingMemStoreFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  <bug id="15952" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bulk load data replication is not working when RS user does not have permission on hfile-refs node</summary>
      <description>In our recent testing in secure cluster we found that when a RS user does not have permission on hfile-refs znode then RS was failing to replicate the bulk loaded data as the hfile-refs znode is created by hbase client and RS user may not have permission to it.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestReplicationHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesHBaseImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15954" opendate="2016-6-3 00:00:00" fixdate="2016-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST server should log requests with TRACE instead of DEBUG</summary>
      <description>One of the surprising findings of one of the HBaseCon presentations from nitinverma, pravinmittal, maxluk was that REST server is logging every request in DEBUG which is enabled by default. This obviously causes out-of-the-box experience to be pretty bad in terms of throughput unless DEBUG logging is turned off. We should bring REST server to be on par with the RS level log conventions. Individual requests to be only logged at the TRACE level.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ConnectionCache.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.VersionResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.StorageClusterVersionResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.StorageClusterStatusResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.SchemaResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.ScannerInstanceResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RootResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RegionsResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.ProtobufStreamingUtil.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.NamespacesResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.NamespacesInstanceResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.MultiRowResource.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.filter.RestCsrfPreventionFilter.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.filter.AuthFilter.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.Client.java</file>
    </fixedFiles>
  </bug>
  <bug id="15971" opendate="2016-6-6 00:00:00" fixdate="2016-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression: Random Read/WorkloadC slower in 1.x than 0.98</summary>
      <description>branch-1 is slower than 0.98 doing YCSB random read/workloadC. It seems to be doing about 1/2 the throughput of 0.98.In branch-1, we have low handler occupancy compared to 0.98. Hacking in reader thread occupancy metric, is about the same in both. In parent issue, hacking out the scheduler, I am able to get branch-1 to go 3x faster so will dig in here.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16023" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fastpath for the FIFO rpcscheduler</summary>
      <description>This is an idea copied from kudu where we skip queuing a request if there is a handler ready to go; we just do a direct handoff from reader to handler.Makes for close to a %20 improvement in random read workloadc testing moving the bottleneck to HBASE-15716 and to returning the results.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoWithFastPathBalancedQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16026" opendate="2016-6-14 00:00:00" fixdate="2016-6-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master UI should display status of additional ZK switches</summary>
      <description>Currently in the warning section we show warnings for bad JVM version, master not initialized OR catalog janitor disabled, and balancer being disabled.We should also have status for split / merge switches (so that if someone ran hbck, aborted and it switches are in the bad state we can see that) and possibly normalizer.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="16035" opendate="2016-6-15 00:00:00" fixdate="2016-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested AutoCloseables might not all get closed</summary>
      <description>Subtle problem in HBASE-15891:try (A myA = new A(new B()))An exception thrown between B starting to open an A finishing initialization may not result in B being closed. A safer syntax would be:try(B myB = new B(); A myA = newA(myB))</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.log.LogLevel.java</file>
    </fixedFiles>
  </bug>
  <bug id="16056" opendate="2016-6-17 00:00:00" fixdate="2016-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - fix master crash for FileNotFound</summary>
      <description>syuanjiang and tedyu reported a backup master not able to start with FileNotFound during proc-v2 lease recovery. (another restart should have solved the problem)FileNotFoundException: File does not exist: /hbase/MasterProcWALs/state-000001.lognamenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease(FSNamesystem.java:2877) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.recoverLease(NameNodeRpcServer.java:753) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.recoverLease(ClientNamenodeProtocolServerSideTranslatorPB.java:671) this may happen when the other master is still active (e.g. GC) and tries to remove files while the other master tries to become active. This operation is retryable so the code should able to handle that.</description>
      <version>1.3.0,1.2.1,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16068" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - use consts for conf properties in tests</summary>
      <description>replace the hardcoded properties string conf.set("foo.key", v) in the tests with the use of the configuration property constants that we already have</description>
      <version>1.3.0,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureReplayOrder.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="16069" opendate="2016-6-20 00:00:00" fixdate="2016-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo "trapsparently" in item 3 of chapter 87.2 of Reference Guide</summary>
      <description>In Chapter 87.2. Coprocessor Implementation Overview...3. Call the coprocessor from your client-side code. HBase handles the coprocessor trapsparently....Correct "trapsparently" into "transparently"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16083" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix table based replication related configs</summary>
      <description>Small style changes to make the new Table Based Replication configs match the other HBase configs</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationTableBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateHBaseImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestTableBasedReplicationSourceManagerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16085" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add on metric for failed compactions</summary>
      <description>Failing compactions doesn't stop the server so things can go un-noticed for quite a while if there are no metrics.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="16087" opendate="2016-6-22 00:00:00" fixdate="2016-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication shouldn&amp;#39;t start on a master if if only hosts system tables</summary>
      <description>System tables aren't replicated so we shouldn't start up a replication master if there are no user tables on the master.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16089" opendate="2016-6-22 00:00:00" fixdate="2016-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add on FastPath for CoDel</summary>
      <description>If this is all that awesome, so we should have it on CoDel too.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoWithFastPathBalancedQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="16095" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add priority to TableDescriptor and priority region open thread pool</summary>
      <description>This is in the similar area with HBASE-15816, and also required with the current secondary indexing for Phoenix. The problem with P secondary indexes is that data table regions depend on index regions to be able to make progress. Possible distributed deadlocks can be prevented via custom RpcScheduler + RpcController configuration via HBASE-11048 and PHOENIX-938. However, region opening also has the same deadlock situation, because data region open has to replay the WAL edits to the index regions. There is only 1 thread pool to open regions with 3 workers by default. So if the cluster is recovering / restarting from scratch, the deadlock happens because some index regions cannot be opened due to them being in the same queue waiting for data regions to open (which waits for RPC'ing to index regions which is not open). This is reproduced in almost all Phoenix secondary index clusters (mutable table w/o transactions) that we see. The proposal is to have a "high priority" region opening thread pool, and have the HTD carry the relative priority of a table. This maybe useful for other "framework" level tables from Phoenix, Tephra, Trafodian, etc if they want some specific tables to become online faster. As a follow up patch, we can also take a look at how this priority information can be used by the rpc scheduler on the server side or rpc controller on the client side, so that we do not have to set priorities manually per-operation.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.21,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.ExecutorType.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.executor.EventType.java</file>
    </fixedFiles>
  </bug>
  <bug id="16096" opendate="2016-6-23 00:00:00" fixdate="2016-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication keeps accumulating znodes</summary>
      <description>If there is an error while creating the replication source on adding the peer, the source if not added to the in memory list of sources but the replication peer is. However, in such a scenario, when you remove the peer, it is deleted from zookeeper successfully but for removing the in memory list of peers, we wait for the corresponding sources to get deleted (which as we said don't exist because of error creating the source). The problem here is the ordering of operations for adding/removing source and peer. Modifying the code to always remove queues from the underlying storage, even if there exists no sources also requires a small refactoring of TableBasedReplicationQueuesImpl to not abort on removeQueues() of an empty queue</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="16135" opendate="2016-6-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PeerClusterZnode under rs of removed peer may never be deleted</summary>
      <description>One of our cluster run out of space recently, and we found that the .oldlogs directory had almost the same size as the data directory.Finally we found the problem is that, we removed a peer abort 3 months ago, but there are still some replication queue znode under some rs nodes. This prevents the deletion of .oldlogs.</description>
      <version>1.3.0,1.4.0,1.1.5,1.2.2,0.98.20,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,0.98.21,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestTableBasedReplicationSourceManagerImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManagerZkImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16144" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication queue&amp;#39;s lock will live forever if RS acquiring the lock has died prematurely</summary>
      <description>In default, we will use multi operation when we claimQueues from ZK. But if we set hbase.zookeeper.useMulti=false, we will add a lock first, then copy nodes, finally clean old queue and the lock. However, if the RS acquiring the lock crash before claimQueues done, the lock will always be there and other RS can never claim the queue.</description>
      <version>1.3.0,1.4.0,1.2.2,0.98.20,1.1.6,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.ReplicationZKLockCleanerChore.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16177" opendate="2016-7-5 00:00:00" fixdate="2016-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In dev mode thrift server can&amp;#39;t be run</summary>
      <description>Error: Could not find or load main class org.apache.hadoop.hbase.thrift2.ThriftServer</description>
      <version>1.3.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="1621" opendate="2009-7-7 00:00:00" fixdate="2009-1-7 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>merge tool should work on online cluster</summary>
      <description>taking down the entire cluster to merge 2 regions is a pain, i dont see why the table or regions specifically couldnt be taken offline, then merged then brought back up.this might need a new API to the regionservers so they can take direction from not just the master.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16211" opendate="2016-7-11 00:00:00" fixdate="2016-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMXCacheBuster restarting the metrics system might cause tests to hang</summary>
      <description>JMXCacheBuster restarts the metrics system. In Phoenix we are manually injecting a sink to the metric system which gets lost when we restart the metric system. See PHOENIX-3062</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.impl.JmxCacheBuster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16221" opendate="2016-7-13 00:00:00" fixdate="2016-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server drops connection on long scans</summary>
      <description>Thrift servers use connection cache and we drop connections after hbase.thrift.connection.max-idletime milliseconds from the last time a connection object was accessed. However, we never update this last accessed time on scan path. By default, this will cause scanners to fail after 10 minutes, if the underlying connection object is not being used along other operation paths (like put).</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ConnectionCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="16231" opendate="2016-7-14 00:00:00" fixdate="2016-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integration tests should support client keytab login for secure clusters</summary>
      <description>Integration tests currently rely on an external kerberos login for secure clusters. Elsewhere we use AuthUtil to login and refresh the credentials in a background thread. We should do the same here.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="16266" opendate="2016-7-21 00:00:00" fixdate="2016-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not throw ScannerTimeoutException when catch UnknownScannerException</summary>
      <description>Now in scanner we have heartbeat to prevent timeout. The time blocked on ResultScanner.next() may much longer than scanner timeout. So it is no need any more to throw ScannerTimeoutException when server throws UnknownScannerException, we can just reset the scanner like NotServingRegionException</description>
      <version>1.3.0,1.4.0,1.2.2,1.1.6,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="16285" opendate="2016-7-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop RPC requests if it must be considered as timeout at client</summary>
      <description>After HBASE-15593, we have a timeout param in header of RPC requests. We can use it in more scenes.A straightforward scene is to drop requests if it has waited so long in RPC queue and has been dropped by client. Even if we handle this request and send the response back, it will not be used any more. And client may have sent a retry. In an extreme case, if the server is slow, all requests may be timeout or queue-full-exception because we should handle previous requests which have been dropped by client and many resources at server are wasted.</description>
      <version>1.3.0,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServerInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="1633" opendate="2009-7-9 00:00:00" fixdate="2009-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t delete in TRUNK shell; makes it hard doing admin repairs</summary>
      <description>Because shell uses old API, it runs into the "Can't add deletes to a BatchUpdate" issue. Add new API to do shell delete and deleteAll. Just a few lines.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="16336" opendate="2016-8-2 00:00:00" fixdate="2016-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Removing peers seems to be leaving spare queues</summary>
      <description>I have been running IntegrationTestReplication repeatedly with the backported Replication Table changes. Every other iteration of the test fails with, but these queues should have been deleted when we removed the peers. I believe this may be related to HBASE-16096, HBASE-16208, or HBASE-16081.16/08/02 08:36:07 ERROR util.AbstractHBaseTool: Error running command-line toolorg.apache.hadoop.hbase.replication.ReplicationException: undeleted queue for peerId: TestPeer, replicator: hbase4124.ash2.facebook.com,16020,1470150251042, queueId: TestPeer at org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.checkQueuesDeleted(ReplicationPeersZKImpl.java:544) at org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.addPeer(ReplicationPeersZKImpl.java:127) at org.apache.hadoop.hbase.client.replication.ReplicationAdmin.addPeer(ReplicationAdmin.java:200) at org.apache.hadoop.hbase.test.IntegrationTestReplication$VerifyReplicationLoop.setupTablesAndReplication(IntegrationTestReplication.java:239) at org.apache.hadoop.hbase.test.IntegrationTestReplication$VerifyReplicationLoop.run(IntegrationTestReplication.java:325) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.hbase.test.IntegrationTestReplication.runTestFromCommandLine(IntegrationTestReplication.java:418) at org.apache.hadoop.hbase.IntegrationTestBase.doWork(IntegrationTestBase.java:134) at org.apache.hadoop.hbase.util.AbstractHBaseTool.run(AbstractHBaseTool.java:112) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.hbase.test.IntegrationTestReplication.main(IntegrationTestReplication.java:424)</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.ReplicationChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16338" opendate="2016-8-2 00:00:00" fixdate="2016-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jackson to 2.y</summary>
      <description>Our jackson dependency is from ~3 years ago. Update to the jackson 2.y line, using 2.7.0+.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.taskmonitor.rb</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">dev-support.hbase-personality.sh</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.util.JsonMapper.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestOperation.java</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.RESTApiClusterManager.java</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.TestPerformanceEvaluation.java</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.CellModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.ColumnSchemaModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.NamespacesModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.RowModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.TableSchemaModel.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.ProtobufStreamingUtil.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableScanResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.HBaseRESTTestingUtility.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestColumnSchemaModel.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.RowResourceBase.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestDeleteRow.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestMultiRowResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestNamespacesInstanceResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestSchemaResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestTableScan.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestVersionResource.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.AgeSnapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketAllocator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JSONBean.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JSONMetricUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALPrettyPrinter.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.processMaster.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.processRS.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.processRS.jsp</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestBlockCacheReporting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestJSONMetricUtil.java</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16341" opendate="2016-8-2 00:00:00" fixdate="2016-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing bit on "Regression: Random Read/WorkloadC slower in 1.x than 0.98"</summary>
      <description>larsgeorge found a missing bit in HBASE-15971 "Regression: Random Read/WorkloadC slower in 1.x than 0.98" Let me fix here. Let me quote the man:BTW, in constructor we do this``` String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY, CALL_QUEUE_TYPE_FIFO_CONF_VALUE);```(edited)[8:19] but in `onConfigurationChange()` we do``` String callQueueType = conf.get(CALL_QUEUE_TYPE_CONF_KEY, CALL_QUEUE_TYPE_DEADLINE_CONF_VALUE);```(edited)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug id="16347" opendate="2016-8-3 00:00:00" fixdate="2016-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unevaluated expressions in book</summary>
      <description>Have a look at the quickstart guide, step two$ tar xzvf hbase-&lt;?eval ${project.version}?&gt;-bin.tar.gz$ cd hbase-&lt;?eval ${project.version}?&gt;/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16450" opendate="2016-8-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell tool to dump replication queues</summary>
      <description>Currently there is no way to dump list of the configured queues and the replication queues when replication is enabled. Unfortunately the HBase master only offers an option to dump the whole content of the znodes but not details on the queues being processed on each RS.</description>
      <version>1.3.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,0.98.24,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16451" opendate="2016-8-19 00:00:00" fixdate="2016-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Test WAL protobuf entry size limit</summary>
      <description>Add a test to make sure that we are able to read/write procedures with a big "data" size.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.util.ByteSlot.java</file>
    </fixedFiles>
  </bug>
  <bug id="16474" opendate="2016-8-23 00:00:00" fixdate="2016-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dfs.support.append related code and documentation</summary>
      <description>dfs.support.append not needed anymore in Hadoop-2.0+.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALFactory.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIOFencing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelsReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabelReplicationWithExpAsString.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationWithTags.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestPerTableCFReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSink.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestProtobufLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterTransitions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedureFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.fs.TestBlockReorder.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestWALObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16499" opendate="2016-8-25 00:00:00" fixdate="2016-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>slow replication for small HBase clusters</summary>
      <description>For small clusters 10-20 nodes we recently observed that replication is progressing very slowly when we do bulk writes and there is lot of lag accumulation on AgeOfLastShipped / SizeOfLogQueue. From the logs we observed that the number of threads used for shipping wal edits in parallel comes from the following equation in HBaseInterClusterReplicationEndpointint n = Math.min(Math.min(this.maxThreads, entries.size()/100+1), replicationSinkMgr.getSinks().size());... for (int i=0; i&lt;n; i++) { entryLists.add(new ArrayList&lt;HLog.Entry&gt;(entries.size()/n+1)); &lt;-- batch size }... for (int i=0; i&lt;entryLists.size(); i++) { ..... // RuntimeExceptions encountered here bubble up and are handled in ReplicationSource pool.submit(createReplicator(entryLists.get(i), i)); &lt;-- concurrency futures++; } }maxThreads is fixed &amp; configurable and since we are taking min of the three values n gets decided based replicationSinkMgr.getSinks().size() when we have enough edits to replicatereplicationSinkMgr.getSinks().size() is decided based on int numSinks = (int) Math.ceil(slaveAddresses.size() * ratio);where ratio is this.ratio = conf.getFloat("replication.source.ratio", DEFAULT_REPLICATION_SOURCE_RATIO);Currently DEFAULT_REPLICATION_SOURCE_RATIO is set to 10% so for small clusters of size 10-20 RegionServers the value we get for numSinks and hence n is very small like 1 or 2. This substantially reduces the pool concurrency used for shipping wal edits in parallel effectively slowing down replication for small clusters and causing lot of lag accumulation in AgeOfLastShipped. Sometimes it takes tens of hours to clear off the entire replication queue even after the client has finished writing on the source side. We are running tests by varying replication.source.ratio and have seen multi-fold improvement in total replication time (will update the results here). I wanted to propose here that we should increase the default value for replication.source.ratio also so that we have sufficient concurrency even for small clusters. We figured it out after lot of iterations and debugging so probably slightly higher default will save the trouble.</description>
      <version>None</version>
      <fixedVersion>1.5.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSinkManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSinkManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="16522" opendate="2016-8-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Cache system user and avoid IOException</summary>
      <description>We can cache the system user and avoid the IOException that we have to carry around when we create procedures</description>
      <version>1.3.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DispatchMergingRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.Superusers.java</file>
    </fixedFiles>
  </bug>
  <bug id="16535" opendate="2016-8-31 00:00:00" fixdate="2016-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use regex to exclude generated classes for findbugs</summary>
      <description>As I tried in HBASE-16526, &lt;Match&gt; &lt;Package name="org.apache.hadoop.hbase.ipc.protobuf.generated"/&gt; &lt;/Match&gt;This does not work.So I propose that we can use regex to match the class name to exclude the generated classes.</description>
      <version>1.3.0,1.4.0,1.1.6,0.98.21,1.2.3,2.0.0</version>
      <fixedVersion>1.3.0,0.98.22,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16554" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Recover &amp;#39;updated&amp;#39; part of WAL tracker if trailer is corrupted.</summary>
      <description>If the last wal was closed cleanly, the global tracker will be the last wal tracker (no rebuild needed)if the last wal does not have a tracker (corrupted/master-killed). on load() we will rebuild the global tracker.To compute quickly which files should be deleted, we also want the tracker of each file.if the wal was closed properly and has a tracker we are good, if not we need to rebuild the tracker for that file.each file tracker contains a bitmap about what is in the wal (the updated bitmap), which is easy to compute just by reading each entry of the wal.The 'deleted' bitmap keeps track of the "running procedures" up to that wal, however, it's not required by WAL cleaner, so we don't bother recovering it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormatReader.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormat.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFile.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="16746" opendate="2016-10-1 00:00:00" fixdate="2016-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The log of “region close” should differ from “region move”</summary>
      <description>If we disable some tables, we will see the following in region server log.Close 90ed2fe1748644c6faecdec3651335d4, moving to nullThe message “moving to null” is a bit confusing.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="16811" opendate="2016-10-11 00:00:00" fixdate="2016-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove mob sweep job</summary>
      <description>Discussed here: http://mail-archives.apache.org/mod_mbox/hbase-dev/201610.mbox/%3CCAAjhxro%3Dt62K44dV2wUtq1hqYLogZ45M3oeNOFZPcnwcSY4_DQ%40mail.gmail.com%3E</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.mapreduce.TestMobSweepReducer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.mapreduce.TestMobSweepMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.mapreduce.TestMobSweepJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepReducer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepMapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepJobNodeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.Sweeper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.MobFilePathHashPartitioner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.MemStoreWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="16812" opendate="2016-10-12 00:00:00" fixdate="2016-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up the locks in MOB</summary>
      <description>Clean up the locks in MOB. Retain all the delete markers in mob-enabled columns until they are expired. Remove the locks from major compaction of mob-enabled columns.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMobStoreCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.TestMobFileName.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MobCompactionStoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.MobUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.DefaultMobStoreCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.compactions.PartitionedMobCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="16831" opendate="2016-10-13 00:00:00" fixdate="2016-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Remove org.apache.hadoop.hbase.zookeeper.lock</summary>
      <description>HBASE-16786 removes all uses of zk locks. Can delete the zk lock implementation now.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.lock.TestZKInterProcessReadWriteLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessWriteLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadWriteLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessReadLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.InterProcessReadWriteLock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.InterProcessLock.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.ZooKeeperProtos.java</file>
    </fixedFiles>
  </bug>
  <bug id="16886" opendate="2016-10-20 00:00:00" fixdate="2016-12-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-client: scanner with reversed=true and small=true gets no result</summary>
      <description>Assume HBase have four regions (-oo, b), [b, c), [c, d), [d,+oo) , and all rowKeys are located in region [d, +oo). using a Reversed Small Scanner will get no result.Attached file show this failed test case.</description>
      <version>1.3.0,1.4.0,1.2.3,1.1.7,0.98.23,2.0.0</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientSmallReversedScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallReversedScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="16960" opendate="2016-10-28 00:00:00" fixdate="2016-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionServer hang when aborting</summary>
      <description>We see regionserver hang when aborting several times and cause all regions on this regionserver out of service and then all affected applications stop works.</description>
      <version>1.3.0,1.4.0,1.2.3,1.1.7,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWALLockup.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SyncFuture.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
    </fixedFiles>
  </bug>
  <bug id="16985" opendate="2016-11-1 00:00:00" fixdate="2016-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestClusterId failed due to wrong hbase rootdir</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/4253/testReport/org.apache.hadoop.hbase.regionserver/TestClusterId/testClusterId/java.io.IOException: Shutting down at org.apache.hadoop.hbase.util.JVMClusterUtil.startup(JVMClusterUtil.java:230) at org.apache.hadoop.hbase.LocalHBaseCluster.startup(LocalHBaseCluster.java:409) at org.apache.hadoop.hbase.MiniHBaseCluster.init(MiniHBaseCluster.java:227) at org.apache.hadoop.hbase.MiniHBaseCluster.&lt;init&gt;(MiniHBaseCluster.java:96) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1071) at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniHBaseCluster(HBaseTestingUtility.java:1037) at org.apache.hadoop.hbase.regionserver.TestClusterId.testClusterId(TestClusterId.java:85)The cluster can not start up because there are no active master. The active master can not finish initialing because the hbase:namespace region can not be assign. In TestClusterId unit test, TEST_UTIL.startMiniHBaseCluster set new hbase root dir. But the regionserver thread which stared first used a different hbase root dir. If assign hbase:namespace region to this regionserver, the region can not be assigned because there are no tableinfo on wrong hbase root dir.When regionserver report to master, it will get back some new config. But the FSTableDescriptors has been initialed so it's root dir didn't changed.if (LOG.isDebugEnabled()) { LOG.info("Config from master: " + key + "=" + value);} I thought FSTableDescriptors need update the rootdir when regionserver get report from master.The master branch has same problem, too. But the balancer always assign hbase:namesapce region to master. So this unit test can passed on master branch.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.4.0,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16986" opendate="2016-11-1 00:00:00" fixdate="2016-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note on how scanner caching has changed since 0.98 to refguid</summary>
      <description>Add note on how scanner caching config changed from 0.98 to the refguide (see parent issue for discussion but basics are we used to have default of 100 but not have unlimited as default)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17017" opendate="2016-11-4 00:00:00" fixdate="2016-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the current per-region latency histogram metrics</summary>
      <description></description>
      <version>1.3.0,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17032" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallQueueTooBigException and CallDroppedException should not be triggering PFFE</summary>
      <description>Back in HBASE-15137 we made it so that CQTBE causes preemptive fast fail exception on the client. It seems those 2 load control mechanists don't exactly align here. Server throws CallQueueTooBigException, CallDroppedException (from deadline scheduler) when it feels overloaded. Client should accept that behavior and retry. When servers sheds the load, and client also bails out, the load shedding bubbles up too high and high level impact on the client applications seems worse with PFFE turned on then without.</description>
      <version>1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFastFail.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.PreemptiveFastFailInterceptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17033" opendate="2016-11-5 00:00:00" fixdate="2016-11-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LogRoller makes a lot of allocations unnecessarily</summary>
      <description>I was looking at the other allocations for HBASE-17017. Seems that log roller thread allocates 200MB for ~7% of the TLAB space. This is a lot of allocations. I think the reason is this: while (true) { if (this.safePointAttainedLatch.await(1, TimeUnit.NANOSECONDS)) { break; } if (syncFuture.isThrowable()) { throw new FailedSyncBeforeLogCloseException(syncFuture.getThrowable()); } }This busy wait is causing a lot allocations because the thread is added to the waiting list.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17058" opendate="2016-11-10 00:00:00" fixdate="2016-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower epsilon used for jitter verification from HBASE-15324</summary>
      <description>The current epsilon used is 1E-6 and its too big it might overflow the desiredMaxFileSize. A trivial fix is to lower the epsilon to 2^-52 or even 2^-53. An option to consider too is just to shift the jitter to always decrement hbase.hregion.max.filesize (MAX_FILESIZE) instead of increase the size of the region and having to deal with the round off.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="17074" opendate="2016-11-11 00:00:00" fixdate="2016-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit job always fails because of OOM</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/4434/artifact/patchprocess/patch-unit-hbase-server.txtException in thread "Thread-2369" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:596) at java.lang.StringBuffer.append(StringBuffer.java:367) at java.io.BufferedReader.readLine(BufferedReader.java:370) at java.io.BufferedReader.readLine(BufferedReader.java:389) at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:66)Exception in thread "Thread-2357" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2365" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.filter.TestFuzzyRowFilterEndToEndRunning org.apache.hadoop.hbase.filter.TestFilterListOrOperatorWithBlkCntException in thread "Thread-2383" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2397" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2401" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.TestHBaseTestingUtilityException in thread "Thread-2407" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2411" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2413" java.lang.OutOfMemoryError: Java heap spaceThe OOM happens in the surefire plugin when reading the stdout or stderr of the running test...</description>
      <version>1.3.0,1.4.0,1.1.7,0.98.23,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="17149" opendate="2016-11-21 00:00:00" fixdate="2016-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Fix nonce submission to avoid unnecessary calling coprocessor multiple times</summary>
      <description>instead of having all the logic in submitProcedure(), split in registerNonce() + submitProcedure().In this case we can avoid calling the coprocessor twice and having a clean submit logic knowing that there will only be one submission.</description>
      <version>1.3.0,1.4.0,1.1.7,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCoprocessorWhitelistMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestProcedureAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestEnableTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDispatchMergingRegionsProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDisableTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestAddColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterSchemaServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterSchema.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureRecovery.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTableDDLProcedureBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestCloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="17290" opendate="2016-12-11 00:00:00" fixdate="2016-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential loss of data for replication of bulk loaded hfiles</summary>
      <description>Currently the support for replication of bulk loaded hfiles relies on bulk load marker written in the WAL.The move of bulk loaded hfile(s) (into region directory) may succeed but the write of bulk load marker may fail.This means that although bulk loaded hfile is being served in source cluster, the replication wouldn't happen.Normally operator is supposed to retry the bulk load. But relying on human retry is not robust solution.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.ReplicationSourceDummy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestReplicationHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.TableBasedReplicationQueuesImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
    </fixedFiles>
  </bug>
  <bug id="17328" opendate="2016-12-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Properly dispose of looped replication peers</summary>
      <description>When adding a looped replication peer (clusterId == peerClusterId), the following code terminates the replication source thread, but since the source manager still holds a reference, WALs continue to get enqueued, and never get cleaned because they're stuck in the queue, leading to an unsustainable buildup. Furthermore, the replication statistics thread will continue to print statistics for the terminated source.if (clusterId.equals(peerClusterId) &amp;&amp; !replicationEndpoint.canReplicateToSameCluster()) { this.terminate("ClusterId " + clusterId + " is replicating to itself: peerClusterId " + peerClusterId + " which is not allowed by ReplicationEndpoint:" + replicationEndpoint.getClass().getName(), null, false); }</description>
      <version>1.3.0,1.4.0,0.98.23,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,0.98.24,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="17390" opendate="2016-12-29 00:00:00" fixdate="2016-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Online update of configuration for all servers leaves out masters</summary>
      <description>Looking at the admin API and this method public void updateConfiguration() throws IOException { for (ServerName server : this.getClusterStatus().getServers()) { updateConfiguration(server); } }you can see that it calls getServers() which only returns the region servers.What is missing is also calling on getMaster() and getBackupMasters() to also send them a signal.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestUpdateConfiguration.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="17424" opendate="2017-1-4 00:00:00" fixdate="2017-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protect REST client against malicious XML responses.</summary>
      <description>If, by some means, an unsuspecting REST server client would get a malformed response from the REST server, it could result in the client performing some unintended action from the XML parsing.We should disable these extra options on the XML parser to prevent the possibility.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="17514" opendate="2017-1-23 00:00:00" fixdate="2017-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Warn when Thrift Server 1 is configured for proxy users but not the HTTP transport</summary>
      <description>The config hbase.thrift.support.proxyuser is ignored if the Thrift Server 1 isn't configured to use an HTTP transport with hbase.regionserver.thrift.http.We should emit a warning if our configs request proxy user support but don't specify that HTTP should be used for the transport.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17516" opendate="2017-1-23 00:00:00" fixdate="2017-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table quota not taking precedence over namespace quota</summary>
      <description>Romil Choksi found a bug in the current patch-set where a more restrictive table quota did not take priority over a less-restrictive namespace quota.Turns out some of the logic to handle this case was incorrect.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17518" opendate="2017-1-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Reference Guide has a syntax error</summary>
      <description>The image of "HFile Version 2 Structure" in Appendix F of HBase Reference Guide (pdf) is missing because of a wrong asciidoc syntax:image:hfilev2.png&amp;#91;HFile Version 2&amp;#93;modified as:image::hfilev2.png&amp;#91;HFile Version 2&amp;#93;it should be a double colon instead of single one</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17520" opendate="2017-1-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isTableEnabled/Disabled/Available methods</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17522" opendate="2017-1-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeExceptions from MemoryMXBean should not take down server process</summary>
      <description>RegionServer died after MemoryMXBean threw an IllegalArgumentException while attempting to create a MemoryUsage object for the heap during construction of the server load.We shouldn't allow failure to get load information to take down the RS.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HeapMemoryManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.util.MemorySizeUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="17587" opendate="2017-2-2 00:00:00" fixdate="2017-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not Rethrow DoNotRetryIOException as UnknownScannerException</summary>
      <description>HBase commit https://github.com/apache/hbase/commit/94ade6a514e9935a8c283befb31f29cd8d3a2045 broke co-processors (such as Phoenix) throwing DoNotRetryIOExceptions when scanning a table. This change rethrows them as UnknownScannerExceptions which the HBase client will retry on. This is unintended behavior since co-processors should be able to throw DoNotRetryIOExceptions back to the client. This came up through a phoenix IT when trying to upgrade to HBase 1.3.0 https://github.com/apache/phoenix/pull/230</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="1768" opendate="2009-8-14 00:00:00" fixdate="2009-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST server has upper limit of 5k PUT</summary>
      <description>This is getting in way of our uploading images to hbase.Below is what we see when big img to put.$ curl -v -T /tmp/y.row http://localhost:12041/api/jimk/row/x?column=misc:stack_testing:* About to connect() to localhost port 12041* Trying 127.0.0.1... connected* Connected to localhost (127.0.0.1) port 12041&gt; PUT /api/jimk/row/x?column=misc:stack_testing: HTTP/1.1&gt; User-Agent: curl/7.15.5 (x86_64-redhat-linux-gnu) libcurl/7.15.5 OpenSSL/0.9.8b zlib/1.2.3 libidn/0.6.5&gt; Host: localhost:12041&gt; Accept: */*&gt; Content-Length: 229886&gt; Expect: 100-continue&gt;&lt; HTTP/1.1 100 ContinueHTTP/1.1 500 Internal Server Error&lt; Content-Type: text/xml; charset=iso-8859-1&lt; Transfer-Encoding: chunked&lt; Server: Jetty(6.1.14)* Connection #0 to host localhost left intact* Closing connection #0&lt;status&gt;&lt;code&gt;500&lt;/code&gt;&lt;message&gt;org.apache.hadoop.hbase.rest.exception.HBaseRestException: XML document structures must start and end within the same entity.&lt;/message&gt;&lt;error&gt;true&lt;/error&gt;&lt;/status&gt;[stack@aa0-007-2 tmp]$</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.Dispatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="17691" opendate="2017-2-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ScanMetrics support for async scan</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRawAsyncTableScan.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableScan.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScanResultConsumer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ReversedScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawScanResultConsumer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableResultScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncScanSingleRegionRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17729" opendate="2017-3-3 00:00:00" fixdate="2017-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing shortcuts for some useful HCD options</summary>
      <description>Missing: CACHE_INDEX_ON_WRITE CACHE_BLOOMS_ON_WRITE EVICT_BLOCKS_ON_CLOSE CACHE_DATA_IN_L1It's possible to set them with syntax like { CONFIGURATION = { &lt;option&gt; = &lt;value&gt; } }but let's save some keystrokes.</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="1773" opendate="2009-8-18 00:00:00" fixdate="2009-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken tests (setWriteBuffer now throws IOE)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestPut.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
    </fixedFiles>
  </bug>
  <bug id="17730" opendate="2017-3-4 00:00:00" fixdate="2017-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] Migration to 2.0 for coprocessors</summary>
      <description>Jiras breaking coprocessor compatibility should be marked with component ' Coprocessor', and label 'incompatible'.Close to releasing 2.0, we should go through all such jiras and write down steps for migrating coprocessor easily.The idea is, it might be very hard to fix coprocessor breakages by reverse engineering errors, but will be easier we suggest easiest way to fix breakages resulting from each individual incompatible change.For eg. HBASE-17312 is incompatible change. It'll result in 100s of errors because BaseXXXObserver classes are gone and will probably result in a lot of confusion, but if we explicitly mention the fix which is just one line change - replace "Foo extends BaseXXXObserver" with "Foo implements XXXObserver" - it makes it very easy.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">dev-support.design-docs.Coprocessor.Design.Improvements-Use.composition.instead.of.inheritance-HBASE-17732.adoc</file>
      <file type="M">dev-support.design-docs.Coprocessor.Design.Improvements-Use.composition.instead.of.inheritance-HBASE-17732-2017.09.27.pdf</file>
    </fixedFiles>
  </bug>
  <bug id="17798" opendate="2017-3-17 00:00:00" fixdate="2017-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RpcServer.Listener.Reader can abort due to CancelledKeyException</summary>
      <description>In our production cluster(0.98), some of the requests were unacceptable because RpcServer.Listener.Reader were aborted.getReader() will return the next reader to deal with request.The implementation of getReader() as below：RpcServer.java // The method that will return the next reader to work with // Simplistic implementation of round robin for now Reader getReader() { currentReader = (currentReader + 1) % readers.length; return readers[currentReader]; }If one of the readers abort, then it will lead to fall on the reader's request will never be dealt with.Why does RpcServer.Listener.Reader abort?We add the debug log to get it.After a while, we got the following exception:2017-03-10 08:05:13,247 ERROR [RpcServer.reader=3,port=60020] ipc.RpcServer: RpcServer.listener,port=60020: unexpectedly error in Reader(Throwable)java.nio.channels.CancelledKeyException at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:73) at sun.nio.ch.SelectionKeyImpl.readyOps(SelectionKeyImpl.java:87) at java.nio.channels.SelectionKey.isReadable(SelectionKey.java:289) at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.doRunLoop(RpcServer.java:592) at org.apache.hadoop.hbase.ipc.RpcServer$Listener$Reader.run(RpcServer.java:566) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)So, when deal with the request in reader, we should handle CanceledKeyException.----------versions 1.x and 2.0 will log and retrun when dealing with the InterruptedException in Reader#doRunLoop after HBASE-10521. It will lead to the same problem.</description>
      <version>1.3.0,1.2.4,0.98.24,2.0.0</version>
      <fixedVersion>1.4.0,1.3.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="17887" opendate="2017-4-6 00:00:00" fixdate="2017-5-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Row-level consistency is broken for read</summary>
      <description>The scanner of latest memstore may be lost if we make quick flushes. The following step may help explain this issue. put data_A (seq id = 10, active store data_A and snapshots is empty) snapshot of 1st flush (active is empty and snapshot stores data_A) put data_B (seq id = 11, active store data_B and snapshot store data_A) create user scanner (read point = 11, so It should see the data_B) commit of 1st flush clear snapshot ((hfile_A has data_A, active store data_B, and snapshot is empty) update the reader (the user scanner receives the hfile_A) snapshot of 2st flush (active is empty and snapshot store data_B) commit of 2st flush clear snapshot (hfile_A has data_A, hfile_B has data_B, active is empty, and snapshot is empty) – this is critical piece. update the reader (haven't happen) user scanner update the kv scanners (it creates scanner of hfile_A but nothing of memstore) user see the older data A – wrong result</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWideScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ChangedReadersObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18049" opendate="2017-5-15 00:00:00" fixdate="2017-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>It is not necessary to re-open the region when MOB files cannot be found</summary>
      <description>In HBASE-17712, we try to re-open the region when store files cannot be found. This is useful for store files in a region, but is not necessary when the MOB files cannot be found, because the store files in a region only contain the references to the MOB files and a re-open of a region doesn't help the lost MOB files.In this JIRA, we will directly throw DNRIOE only when the MOB files are not found in MobStoreScanner and ReversedMobStoreScanner. Other logics keep the same.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="18054" opendate="2017-5-16 00:00:00" fixdate="2017-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>log when we add/remove failed servers in client</summary>
      <description>Currently we log if a server is in the failed server list when we go to connect to it, but we don't log anything about when the server got into the list.This means we have to search the log for errors involving the same server name that (hopefully) managed to get into the log within FAILED_SERVER_EXPIRY_KEY milliseconds earlier (default 2 seconds).</description>
      <version>1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestHBaseClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.NettyRpcConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.FailedServers.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.BlockingRpcConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="18075" opendate="2017-5-18 00:00:00" fixdate="2017-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support namespaces and tables with non-latin alphabetical characters</summary>
      <description>On the heels of HBASE-18067, it would be nice to support namespaces and tables with names that fall outside of Latin alphabetical characters and numbers.Our current regex for allowable characters is approximately [a-zA-Z0-9]+.It would be nice to replace a-zA-Z with Java's \p{IsAlphabetic} which will naturally restrict the unicode character space down to just those that are part of the alphabet for each script (e.g. latin, cyrillic, greek).Technically, our possible scope of allowable characters is, best as I can tell, only limited by the limitations of ZooKeeper itself https://zookeeper.apache.org/doc/r3.4.10/zookeeperProgrammers.html#ch_zkDataModel (as both table and namespace are created as znodes).</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HFileLink.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.TableName.java</file>
    </fixedFiles>
  </bug>
  <bug id="18076" opendate="2017-5-19 00:00:00" fixdate="2017-5-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky dashboard improvement: Add status markers to show trends of failure/success</summary>
      <description>Adds those colored status markers:</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.report-flakies.py</file>
      <file type="M">dev-support.flaky-dashboard-template.html</file>
    </fixedFiles>
  </bug>
  <bug id="18461" opendate="2017-7-27 00:00:00" fixdate="2017-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build broken If the username contains a backslash</summary>
      <description>The character '\' is a special character and needs to be escaped when used as part of a String.If the name contains '\', we need to escape it,otherwise the generated file Version.java failed to compile.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.saveVersion.sh</file>
    </fixedFiles>
  </bug>
  <bug id="18469" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct RegionServer metric of totalRequestCount</summary>
      <description>when i get the metric ,i found this three metric may be have some error as follow : "totalRequestCount" : 17541, "readRequestCount" : 17483, "writeRequestCount" : 1633,</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="18471" opendate="2017-7-28 00:00:00" fixdate="2017-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The DeleteFamily cell is skipped when StoreScanner seeks to next column</summary>
      <description>The qualifier of a deleted row (with keep deleted cells true) re-appears after re-inserting the same row multiple times (with different timestamp) with an empty qualifier.Scenario: Put row with family and qualifier (timestamp 1). Delete entire row (timestamp 2). Put same row again with family without qualifier (timestamp 3).A scan (latest version) returns the row with family without qualifier, version 3 (which is correct). Put the same row again with family without qualifier (timestamp 4).A scan (latest version) returns multiple rows: the row with family without qualifier, version 4 (which is correct). the row with family with qualifier, version 1 (which is wrong).There is a test scenario attached.output:&lt;LOG&gt; 13:42:53,952 &amp;#91;main&amp;#93; client.HBaseAdmin - Started disable of test_dml&lt;LOG&gt; 13:42:55,801 &amp;#91;main&amp;#93; client.HBaseAdmin - Disabled test_dml&lt;LOG&gt; 13:42:57,256 &amp;#91;main&amp;#93; client.HBaseAdmin - Deleted test_dml&lt;LOG&gt; 13:42:58,592 &amp;#91;main&amp;#93; client.HBaseAdmin - Created test_dmlPut row: 'myRow' with family: 'myFamily' with qualifier: 'myQualifier' with timestamp: '1'Scan printout =&gt; Row: 'myRow', Timestamp: '1', Family: 'myFamily', Qualifier: 'myQualifier', Value: 'myValue'Delete row: 'myRow'Scan printout =&gt;Put row: 'myRow' with family: 'myFamily' with qualifier: 'null' with timestamp: '3'Scan printout =&gt; Row: 'myRow', Timestamp: '3', Family: 'myFamily', Qualifier: '', Value: 'myValue'Put row: 'myRow' with family: 'myFamily' with qualifier: 'null' with timestamp: '4'Scan printout =&gt; Row: 'myRow', Timestamp: '4', Family: 'myFamily', Qualifier: '', Value: 'myValue' Row: 'myRow', Timestamp: '1', Family: 'myFamily', Qualifier: 'myQualifier', Value: 'myValue'</description>
      <version>3.0.0-alpha-1,1.3.0,1.3.1,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide3.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.querymatcher.ScanQueryMatcher.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="18481" opendate="2017-7-30 00:00:00" fixdate="2017-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The autoFlush flag was not used in PE tool</summary>
      <description>After HBASE-12728, PE used the BufferedMutator for random/sequential write test and the autoFlush flag was not used. So all write test will buffered the write request and send as a batch request when the buffer has filled.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug id="18577" opendate="2017-8-11 00:00:00" fixdate="2017-8-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded client includes several non-relocated third party dependencies</summary>
      <description>we have some unexpected unrelocated third party dependencies in our shaded artifacts.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19227" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly jobs should archive JVM dumpstream files</summary>
      <description>came up on dev@ discussion about some of our current nightly test failures. when surefire fails to launch a test JVM instance, the details go into a file that we currently don't archive:&amp;#91;ERROR&amp;#93; Please refer to dump files (if any exist) &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dump, &amp;#91;date&amp;#93;.dumpstream and &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dumpstream.Add them to the default archive pattern.</description>
      <version>None</version>
      <fixedVersion>1.0.4,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="19228" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly job should gather machine stats.</summary>
      <description>leverage the script added in HBASE-19189 to get machine stats when running nightly</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.gather.machine.environment.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19229" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly script to check source artifact should not do a destructive git operation without opt-in</summary>
      <description>right now we have a "git please destroy all this stuff" command in the check of the source artifact. we shouldn't do this unless the person invoking the script has indicated that's okay (e..g through a cli flag).</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.source-artifact.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20066" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region sequence id may go backward after split or merge</summary>
      <description>The problem is that, now we have markers which will be written to WAL but not in store file. For a normal region close, we will write a sequence id file under the region directory, and when opening we will use this as the open sequence id. But for split and merge, we do not copy the sequence id file to the newly generated regions so the sequence id may go backwards since when closing the region we will write flush marker and close marker into WAL...</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.AbstractTestDLS.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
    </fixedFiles>
  </bug>
  <bug id="20068" opendate="2018-2-24 00:00:00" fixdate="2018-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopcheck project health check uses default maven repo instead of yetus managed ones</summary>
      <description>Recently had a precommit run fail hadoop check for all 3 versions with [ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.2:install (default-install) on project hbase-thrift: Failed to install metadata org.apache.hbase:hbase-thrift:3.0.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hbase/hbase-thrift/3.0.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got / (position: END_TAG seen ...&lt;/metadata&gt;\n/... @25:2) -&gt; [Help 1]Looks like maven repo corruption.Also the path /home/jenkins/.m2/repository means that those invocations are using the jenkins user repo, which isn't safe since there are multiple executors. either the plugin isn't using the yetus provided maven repo path or our yetus invocation isn't telling yetus to provide its own maven repo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.4,2.0.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20069" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix existing findbugs errors in hbase-server</summary>
      <description>now that findbugs is running on precommit we have some cleanup to do.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterStatusPublisher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.compaction.MajorCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.nio.TestMultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.MultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="20070" opendate="2018-2-24 00:00:00" fixdate="2018-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>website generation is failing</summary>
      <description>website generation has been failing since Feb 20thChecking out files: 100% (68971/68971), done.Usage: grep [OPTION]... PATTERN [FILE]...Try 'grep --help' for more information.PUSHED is 2 is not yet mentioned in the hbase-site commit log. Assuming we don't have it yet. 2Building HBaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Failure: mvn clean siteBuild step 'Execute shell' marked build as failureThe status email saysBuild status: Still FailingThe HBase website has not been updated to incorporate HBase commit ${CURRENT_HBASE_COMMIT}.Looking at the code where that grep happens, it looks like the env variable CURRENT_HBASE_COMMIT isn't getting set. That comes from some git command. I'm guessing the version of git changed on the build hosts and upended our assumptions.we should fix this to 1) rely on git's porcelain interface, and 2) fail as soon as that git command fails</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-spark-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-resource-bundle.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-build-configuration.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-annotations.pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20512" opendate="2018-5-1 00:00:00" fixdate="2018-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>document change to running tests on secure clusters</summary>
      <description>We should document the change to authentication handling in HBASE-16231 in the upgrade section of the reference guide.It's surprising to folks that have existing automated testing that's been working on our prior stable release lines. We should give a warning to those updating. The release note is probably suitable for a first pass.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.0.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20704" opendate="2018-6-7 00:00:00" fixdate="2018-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sometimes some compacted storefiles are not archived on region close</summary>
      <description>During region close compacted files which have not yet been archived by the discharger are archived as part of the region closing process. It is important that these files are wholly archived to insure data consistency. ie a storefile containing delete tombstones can be archived while older storefiles containing cells that were supposed to be deleted are left unarchived thereby undeleting those cells. On region close a compacted storefile is skipped from archiving if it has read references (ie open scanners). This behavior is correct for when the discharger chore runs but on region close consistency is of course more important so we should add a special case to ignore any references on the storefile and go ahead and archive it. Attached patch contains a unit test that reproduces the problem and the proposed fix.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.4.8,2.1.1,2.0.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20705" opendate="2018-6-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Having RPC Quota on a table prevents Space quota to be recreated/removed</summary>
      <description>Property hbase.quota.remove.on.table.delete is set to true by default Create a table and set RPC and Space quotahbase(main):022:0&gt; create 't2','cf1'Created table t2Took 0.7420 seconds=&gt; Hbase::Table - t2hbase(main):023:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't2', LIMIT =&gt; '1G', POLICY =&gt; NO_WRITESTook 0.0105 secondshbase(main):024:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; '10M/sec'Took 0.0186 secondshbase(main):025:0&gt; list_quotasTABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINETABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, LIMIT =&gt; 1073741824, VIOLATION_POLICY =&gt; NO_WRITES Drop the table and the Space quota is set to REMOVE =&gt; truehbase(main):026:0&gt; disable 't2'Took 0.4363 secondshbase(main):027:0&gt; drop 't2'Took 0.2344 secondshbase(main):028:0&gt; list_quotasTABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; trueUSER =&gt; u1 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINE Recreate the table and set Space quota back. The Space quota on the table is still set to REMOVE =&gt; truehbase(main):029:0&gt; create 't2','cf1'Created table t2Took 0.7348 seconds=&gt; Hbase::Table - t2hbase(main):031:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't2', LIMIT =&gt; '1G', POLICY =&gt; NO_WRITESTook 0.0088 secondshbase(main):032:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINETABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; true Remove RPC quota and drop the table, the Space Quota is not removedhbase(main):033:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; NONETook 0.0193 secondshbase(main):036:0&gt; disable 't2'Took 0.4305 secondshbase(main):037:0&gt; drop 't2'Took 0.2353 secondshbase(main):038:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t2                               TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; true Deleting the quota entry from hbase:quota seems to be the option to reset it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterQuotasObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20724" opendate="2018-6-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sometimes some compacted storefiles are still opened after region failover</summary>
      <description>It is important that compacted storefiles of a given compaction execution are wholly opened or archived to insure data consistency. ie a storefile containing delete tombstones can be archived while older storefiles containing cells that were supposed to be deleted are left unarchived thereby undeleting those cells.When a server fails compaction markers (in the wal edit) are used to determine which storefiles are compacted and should be excluded during region open (during failover). But the WALs containing compaction markers can be prematurely archived even though there are still compacted storefiles for that particular compaction event that hasn't been archived yet. Thus losing compaction information that needs to be replayed in the event of an RS crash. This is because hlog archiving logic only keeps track of flushed storefiles and not compacted ones.https://issues.apache.org/jira/browse/HBASE-20704?focusedCommentId=16507680&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16507680</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSwitchToStreamRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCleanupCompactedFileOnRegionClose.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotasWithSnapshots.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSnapshotQuotaObserverChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.HFile.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21021" opendate="2018-8-7 00:00:00" fixdate="2018-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Result returned by Append operation should be ordered</summary>
      <description>Problem:The result returned by the append operation should be ordered. Currently, it returns an unordered list, which may cause problems like if the user tries to perform Result.getValue(byte[] family, byte[] qualifier), even if the returned result has a value corresponding to (family, qualifier), the method may return null as it performs a binary search over the  unsorted result (which should have been sorted actually). The result is enumerated by iterating over each entry of tempMemstore hashmap (which will never be ordered) and adding the values (see HRegion.java#L7882). Actual: The returned result is unorderedExpected: Similar to increment op, the returned result should be ordered.</description>
      <version>1.3.0,1.5.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21411" opendate="2018-10-30 00:00:00" fixdate="2018-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to document the snapshot metric data that is shown in HBase Master Web UI</summary>
      <description>We need to add documentation into the Reference Guide for the work that was done in HBASE-15415.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21697" opendate="2019-1-8 00:00:00" fixdate="2019-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.2 to the download page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22264" opendate="2019-4-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out jars related to JDK 11 into a folder in /lib</summary>
      <description>UPDATE:Separate out the the jars related to JDK 11 and add control their addition to the classpath using an environment variable or auto-detection of the jdk version installed.OLD:This is in continuation with HBASE-22249. When compiled with jdk 8 and run on jdk 11, the master branch throws the following exception during an attempt to start the hbase rest server:Exception in thread "main" java.lang.NoClassDefFoundError: javax/annotation/Priority at org.glassfish.jersey.model.internal.ComponentBag.modelFor(ComponentBag.java:483) at org.glassfish.jersey.model.internal.ComponentBag.access$100(ComponentBag.java:89) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:408) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:398) at org.glassfish.jersey.internal.Errors.process(Errors.java:315) at org.glassfish.jersey.internal.Errors.process(Errors.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:228) at org.glassfish.jersey.model.internal.ComponentBag.registerModel(ComponentBag.java:398) at org.glassfish.jersey.model.internal.ComponentBag.register(ComponentBag.java:235) at org.glassfish.jersey.model.internal.CommonConfig.register(CommonConfig.java:420) at org.glassfish.jersey.server.ResourceConfig.register(ResourceConfig.java:425) at org.apache.hadoop.hbase.rest.RESTServer.run(RESTServer.java:245) at org.apache.hadoop.hbase.rest.RESTServer.main(RESTServer.java:421)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="22379" opendate="2019-5-8 00:00:00" fixdate="2019-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Markdown for "Voting on Release Candidates" in book</summary>
      <description>The Markdown in the section "Voting on Release Candidates" of the HBase book seems to be broken. It looks like that there should be a quote, which isn't displayed correctly. Same is true for the formatting of the Maven RAT command.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22943" opendate="2019-8-28 00:00:00" fixdate="2019-9-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Various procedures should not cache log trace level</summary>
      <description>several of the procedures have an idiom where they keep a member variable for if the log is at trace level or not, wrapped in a function so that it can be lazily looked up. This gives us an overhead per call of autoboxing and a function call, instead of just the function call from asking the logging system directly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="22945" opendate="2019-8-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show quota infos in master UI</summary>
      <description>Add a page in master UI to show the following quota infos:if rpc throttle is enabled;if exceed throttle quota is enabled;namespace throtlles;user throttles.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.header.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.ThrottleSettings.java</file>
    </fixedFiles>
  </bug>
  <bug id="22946" opendate="2019-8-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TableNotFound when grant/revoke if AccessController is not loaded</summary>
      <description>When doing grant, revoke..., a TableNotFoundException will occur if AccessController if is not configured.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="23172" opendate="2019-10-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Canary region success count metrics reflect column family successes, not region successes</summary>
      <description>HBase Canary reads once per column family per region. The current "region success count" should actually be "column family success count," which means we need another metric that actually reflects region success count. Additionally, the region read and write latencies only store the latencies of the last column family of the region read. Instead of a map of regions to a single latency value and success value, we should map each region to a list of such values.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0,2.1.5,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestCanaryTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.CanaryTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="7972" opendate="2013-3-1 00:00:00" fixdate="2013-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a configuration for the TCP backlog in the Thrift server</summary>
      <description>Once THRIFT-1868 goes in, we can start letting our users configure the TCP backlog.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
