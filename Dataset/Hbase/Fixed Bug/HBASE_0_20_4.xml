<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="21650" opendate="2018-12-27 00:00:00" fixdate="2018-12-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DDL operation and some other miscellaneous to thrift2</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21682" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support getting from specific replica</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableRegionReplicasGet.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21684" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw DNRIOE when connection or rpc client is closed</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ReadOnlyZKClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.StoppedRpcClientException.java</file>
    </fixedFiles>
  </bug>
  <bug id="21685" opendate="2019-1-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change repository urls to Gitbox</summary>
      <description>Moving to Gitbox is approaching and references to git-wip-us need to be changed to gitbox.Some of the Jenkins jobs are referring to git-wip-us which if going to be locked after the migration. We could move them to github so the build flow will remain intact.Previous discussion on dev@: https://lists.apache.org/thread.html/3496568d6cc002f74f5c3bcce46ed44b7ee9e90d7d53af2c65b6f785@%3Cdev.hbase.apache.org%3E After this notify INFRA to make the change</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5,1.3.4,1.2.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.main.asciidoc..chapters.rpc.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2338" opendate="2010-3-17 00:00:00" fixdate="2010-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>log recovery: deleted items may be resurrected</summary>
      <description>While working on HBASE-2283, noticed that if you do a put followed by a delete, and then crash the RS, and trigger log recovery to happen, then deleted entries may be resurrected. Suprisingly, the issue only affected delete of a specific column. Full row delete didn't run into this issue.&amp;#8212;Code inspection revealed that we might have an issue with timestamps &amp; WAL stuff for delete that come in with "LATEST" timestamp. &amp;#91;Note: The "LATEST" timestamp is syntax sugar/hint to the RS to convert it to "now". &amp;#93;Basically, in:delete(byte [] family, List&lt;KeyValue&gt; kvs, boolean writeToWAL)the "kv.updateLatestStamp(byteNow);" time stamp massaging (from LATEST to now) happens after the WAL log.append() call. So the KeyValue entries written to the HLog do not have the massaged timestamp. On recovery, when these entries are replayed, we add them back to reconstructionCache but don't do anything with timestamps. The above could be the potential source of the problem. But there could be more to the problem than my simple analysis. For instance, we still don't know why full row delete worked fine, but delete of a specific column didn't work ok. Forking this off as a separate issue from HBASE-2283.&amp;#91;Note: Aravind is starting to take a look at this issue.&amp;#93;</description>
      <version>0.20.4</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="23380" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>General Cleanup of FSUtil</summary>
      <description>Clean up JavaDocs Clean up logging and error messages Remove superfluous code Replace static code with library call Do not swallow Interrupted Exceptions Use try-with-resources User multi-Exception catches to reduce code size</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3,2.1.9</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="23381" opendate="2019-12-6 00:00:00" fixdate="2019-1-6 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Improve Logging in HBase Commons Package</summary>
      <description>Based on my observations and suggestions here:https://lists.apache.org/thread.html/71f546c89ecdaa2f25a26bd238e88680ddaad1d3b5c4031338d3533a%40%3Cdev.hbase.apache.org%3E</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.ServerRpcConnection.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.security.provider.example.TestShadeSaslAuthenticationProvider.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.security.provider.example.ShadeSaslServerAuthenticationProvider.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.security.provider.example.ShadeSaslClientAuthenticationProvider.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.NettyHBaseSaslRpcClientHandler.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.AbstractHBaseSaslRpcClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.NettyRpcConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="23383" opendate="2019-12-7 00:00:00" fixdate="2019-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck2] `fixHoles` should queue assignment procedures for any regions its fixing</summary>
      <description>Per comment on HBASE-23321, let's have hbck2 finish the job by queuing an assign procedure for all holes it's trying to patch.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMetaFixer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetaFixer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2340" opendate="2010-3-17 00:00:00" fixdate="2010-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add end-to-end test of sync/flush</summary>
      <description>Add a test to do the following:+ Start a HBase/HDFS cluster (local node is fine). + Use top-level (HTable) level APIs to put items. + Try about single column puts, as well as puts which span multiple columns/multiple column families, etc.+ Then kill one region server.+ Wait for recovery to happen.+ And then check the rows exist.Assigning myself.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2341" opendate="2010-3-17 00:00:00" fixdate="2010-7-17 01:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Suite of test scripts that a.) load a cluster with a verifiable dataset and b.) do random kills of regionserver+datanodes in small cluster</summary>
      <description>We just filed hbase-2340 but discussion up on irc has it that we need something more hardcore than pussy-footing inside a single jvm as hdfs-2340 does. The point was made (tlipcon) that its hard to ensure real recovery working if all is in the one JVM.So, this issue is about scripts that can:+ load a cluster with a dataset that we can 'verify' as in we can tell if it has holes in it, if data has been lost.+ script that does random kill of a random node on some random occasion+ Script that can check cluster for data lossAll above should work while cluster is under load.The above would not sit under junit.This looks like a suite that we'd want to run up in ec2 using Andrew's scripts and our donated aws credits.16:12 &lt; tlipcon&gt; here's my goal: we have a 5 node cluster in the back room. I want to run hbase on that at near full load for a week straight while some process goes around screwing with it16:12 &lt; tlipcon&gt; then I want to verify that I didn't lose a single edit over that week</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.resources.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2456" opendate="2010-4-16 00:00:00" fixdate="2010-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteChangedReaderObserver spitting warnings after HBASE-2248</summary>
      <description>I'm seeing very occasional "Not in set" warnings on region servers under heavy concurrent read/write test after HBASE-2248. Here's a log:http://pastebin.com/1Vm9C7Uf</description>
      <version>0.20.4</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2457" opendate="2010-4-16 00:00:00" fixdate="2010-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS gets stuck compacting region ad infinitum</summary>
      <description>Testing 0.20_pre_durability@934643, I ended up in a state where one region server got stuck compacting a single region over and over again forever. This was with a special config with very low flush threshold in order to stress test flush/compact code.</description>
      <version>0.20.4</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2458" opendate="2010-4-16 00:00:00" fixdate="2010-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client stuck in TreeMap,remove</summary>
      <description>Testing 0.20_pre_durability@934691 my client got permanently stuck with one thread looping inside TreeMap.remove. See attached stack.</description>
      <version>0.20.4</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.SoftValueSortedMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2474" opendate="2010-4-20 00:00:00" fixdate="2010-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in 2248 - mixed version reads (not allowed by spec)</summary>
      <description>While doing a concurrent read/write test, the reader eventually gets a situation where the first column in the result set has the wrong 'value' than the rest of the result set (of 50 columns or so). The test (included) does puts of 50 columns with all the same (Random) value. The reader validates that all values are equal, and fails.</description>
      <version>0.20.4,0.20.5,0.90.0</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2481" opendate="2010-4-23 00:00:00" fixdate="2010-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client is not getting UnknownScannerExceptions; they are being eaten</summary>
      <description>This was reported by mudphone on IRC and confirmed by myself in quick test. If the client takes too long going back to the RS, the RS will throw an UnknownScannerException but it doesn't get back to the client. Instead, the client scan silently ends. Marking this blocker. Its actually in 0.20.4. Thats what I was testing. Mayhaps an RC sinker?</description>
      <version>0.20.4</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2615" opendate="2010-5-26 00:00:00" fixdate="2010-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>M/R on bulk imported tables</summary>
      <description>We are bulk importing using loadtable.rb and running M/R jobs using HBase as input.We're taking the following steps:1a. Load HBase with a M/R job using the normal API. OR1b. Load HBase with bulk import.THEN2a. Using the shell, do a "count" over the table.OR2b. Run a M/R job that scans the whole HBase table (and nothing else).Of the 4 combos, 3 are fine: 1a+2a, 1a+2b, 1b+2a. We're having trouble with 1b+2b. When we run the M/R job, it doesn't seem to read in any records, but there are no explicit errors in either the Hadoop or HBase logs.Any ideas on what might be wrong with the bulk import to cause this problem? We confirmed this problem exists in both hbase-0.20.3 and hbase-0.20.4.We have created dummy data (see attached). This is the test case:After loading the data into HDFS. In hbase shell:create 'tiny', 'values'Execute: {HBASE-HOME}/bin/hbase org.jruby.Main {HBASE-HOME}/bin/loadtable.rb tiny tinytableThen run the simple row counter{HADOOP-HOME}/bin/hadoop jar {HBASE-HOME}/hbase-0.20.x.jar rowcounter tiny valuesNotice that map input records read is always zero. We confirmed that other mapreduce jobs do not execute the map function at all, always returning 0 records.We also ran a major_compaction of all Hbase tables (.META. and .ROOT. as well) but this did not fix the problem.</description>
      <version>0.20.3,0.20.4</version>
      <fixedVersion>0.20.5,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="2636" opendate="2010-6-1 00:00:00" fixdate="2010-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Jetty to 6.1.24</summary>
      <description>Jetty is the servlet container used to host the REST interface and the InfoServers. We are currently pulling version 6.1.14 but the latest version of the server component is 6.1.24. On the Solr list Yonik was suggesting they upgrade. I'll try it out and see what happens.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2638" opendate="2010-6-1 00:00:00" fixdate="2010-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>speed up REST tests</summary>
      <description>In the meantime before HBASE-2564, the REST tests could be a lot faster than they are currently.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestVersionResource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestTableResource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestStatusResource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestSchemaResource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestScannersWithFilters.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2712" opendate="2010-6-11 00:00:00" fixdate="2010-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cached region location that went stale won&amp;#39;t recover if asking for first row</summary>
      <description>Let's say that: A client cached the location of some region, not the first one in the table The RS that was holding it fails The first thing the client does after the failure is trying to reach the first row of that regionThis will never recover, since HCM.deleteCachedLocation doesn't delete if the row we asked for is the first row in a region. This looks a lot like HBASE-1920, but there isn't enough information in that jira to say that it's the same thing.This is a blocker, and it kills 0.20.5 RC2 (sorry).</description>
      <version>0.20.4</version>
      <fixedVersion>0.20.5,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2716" opendate="2010-6-11 00:00:00" fixdate="2010-6-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make HBase&amp;#39;s maven artifacts configurable with -D</summary>
      <description>I want to be able to do stuff like mvn -DskipTests -Dhbase.version=crazy_pants install assembly:assembly</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2771" opendate="2010-6-22 00:00:00" fixdate="2010-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update our hadoop jar to be latest from 0.20-append branch</summary>
      <description>Assigning Ryan.. he's doing work.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2772" opendate="2010-6-22 00:00:00" fixdate="2010-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scan doesn&amp;#39;t recover from region server failure</summary>
      <description>Very simple, if the region server you are scanning from dies for any reason, your scanner will die too because it will not try to get a new lease and will reuse the same id.It doesn't happen to region that moves because we keep them opened until the scanner is closed.It could be affecting branch, I'm just not sure yet.I found this issue with TestReplication from HBASE-2223.</description>
      <version>None</version>
      <fixedVersion>0.20.6,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2907" opendate="2010-8-11 00:00:00" fixdate="2010-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[rest/stargate] Improve error response when trying to create a scanner on a nonexistant table</summary>
      <description>Since 0.20.4, an attempt to create a scanner for a nonexistant table receives a "400 Bad Request" response with no furthur information. Prior to 0.20.4 it would receive a "500 org.apache.hadoop.hbase.TableNotFoundException: &lt;table&gt;" response with a stack trace in the body.Neither of these is ideal - the 400 fails to identify what aspect of the request was bad, and the 500 incorrectly suggests that the error was internal. Ideally the error should be a 400 error with information in the body identifying the nature of the problem.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3117" opendate="2010-10-16 00:00:00" fixdate="2010-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to 0.5 version</summary>
      <description>Thrift 0.5 has been released already and we want to upgrade to at least 0.3 but 0.5 has a lot of improvements so that would be the best.Unfortunately the Java lib has changed so that we'll have to regenerate the current Thrift interface and fix the implementation (byte[] -&gt; ByteBuffer).They also have problems getting Thrift into a Maven repository so we'll need to do our current workaround again unfortunately and upload it to a repository. That would be Ryan's I think?I'll upload an updated thrift jar and a patch for the old Thrift code.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
