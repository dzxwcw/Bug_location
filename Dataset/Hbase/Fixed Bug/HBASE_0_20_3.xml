<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="2057" opendate="2009-12-18 00:00:00" fixdate="2009-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cluster won&amp;#39;t stop</summary>
      <description>It seems that clusters on trunk have some trouble stopping. Even manually deleting the shutdown file in ZK doesn't always help. Investigate.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ZKMasterAddressWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="20571" opendate="2018-5-11 00:00:00" fixdate="2018-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMXJsonServlet generates invalid JSON if it has NaN in metrics</summary>
      <description>/jmx servlet responses invalid JSON, if some metrics are NaN: "l1CacheHitCount" : 0, "l1CacheMissCount" : 0, "l1CacheHitRatio" : NaN, "l1CacheMissRatio" : NaN, "l2CacheHitCount" : 0, "l2CacheMissCount" : 0, "l2CacheHitRatio" : 0.0, "l2CacheMissRatio" : 0.0,NaN is an invalid character sequence in JSON. We should not response NaN in metrics.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,2.0.1,1.4.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheStats.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.JSONBean.java</file>
    </fixedFiles>
  </bug>
  <bug id="20630" opendate="2018-5-23 00:00:00" fixdate="2018-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>B&amp;R: Delete command enhancements</summary>
      <description>Make the command more useable. Currently, user needs to provide list of backup ids to delete. It would be nice to have more convenient options, such as: deleting all backups which are older than XXX days, etc</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.test.java.org.apache.hadoop.hbase.backup.TestBackupDelete.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupCommands.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupRestoreConstants.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2066" opendate="2009-12-22 00:00:00" fixdate="2009-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perf: parallelize puts</summary>
      <description>Right now with large region count tables, the write buffer is not efficient. This is because we issue potentially N RPCs, where N is the # of regions in the table. When N gets large (lets say 1200+) things become sloowwwww.Instead if we batch things up using a different RPC and use thread pools, we could see higher performance!This requires a RPC change...</description>
      <version>0.20.2,0.20.3</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MultiRegionTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseClusterTestCase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="20660" opendate="2018-5-30 00:00:00" fixdate="2018-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reopen regions using ReopenTableRegionsProcedure</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="2109" opendate="2010-1-11 00:00:00" fixdate="2010-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>status &amp;#39;simple&amp;#39; should show total requests per second, also the requests/sec is wrong as is</summary>
      <description>status 'simple' doesnt give us aggregate load, leaving the user to add up numbers by hand. Futhermore, the per-server requests numbers are off, too high by a factor of 3 - they are using the default toString() which assumes a 1 second report rate, when the shipping default is 3 seconds.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="21091" opendate="2018-8-21 00:00:00" fixdate="2018-1-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Hadoop compatibility table</summary>
      <description>https://lists.apache.org/thread.html/7016d322a07e96dccdb071041c37238e43d3df4f93e9515d52ccfafc@%3Cdev.hbase.apache.org%3E covers some discussion around our Hadoop Version Compatibility table. A "leading" suggestion to make this more clear is to use a green/yellow/red (traffic-signal) style marking, instead of using specifics words/phrases (as they're often dependent on the interpretation of the reader).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21095" opendate="2018-8-22 00:00:00" fixdate="2018-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The timeout retry logic for several procedures are broken after master restarts</summary>
      <description>For TRSP, and also RTP in branch-2.0 and branch-2.1, if we fail to assign or unassign a region, we will set the procedure to WAITING_TIMEOUT state, and rely on the ProcedureEvent in RegionStateNode to wake us up later. But after restarting, we do not suspend the ProcedureEvent in RSN, and also do not add the procedure to the ProcedureEvent's suspending queue, so we will hang there forever as no one will wake us up.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestCloseRegionWhileRSCrash.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2146" opendate="2010-1-19 00:00:00" fixdate="2010-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RPC related metrics are missing in 0.20.3 since recent changes</summary>
      <description>Since the recent change to the Metrics setup it seems that the RPC related stats have been completely dropped. See attached files for details.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21460" opendate="2018-11-9 00:00:00" fixdate="2018-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct Document Configurable Bucket Sizes in bucketCache</summary>
      <description>we use the bucket cache(offheap),found the doc was error,the property bucket sizes shoul be "hbase.bucketcache.bucket.sizes"  instead of "hfile.block.cache.sizes"CacheConfig.java /** A comma-delimited array of values for use as bucket sizes. */ public static final String BUCKET_CACHE_BUCKETS_KEY = "hbase.bucketcache.bucket.sizes";the doc was:  HBASE-10641 introduced the ability to configure multiple sizes for the buckets of the BucketCache, in HBase 0.98 and newer. To configurable multiple bucket sizes, configure the new property hfile.block.cache.sizes (instead of hfile.block.cache.size) to a comma-separated list of block sizes, ordered from smallest to largest, with no spaces. The goal is to optimize the bucket sizes based on your data access patterns. The following example configures buckets of size 4096 and 8192. &lt;property&gt; &lt;name&gt;hfile.block.cache.sizes&lt;/name&gt; &lt;value&gt;4096,8192&lt;/value&gt; &lt;/property&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="2156" opendate="2010-1-22 00:00:00" fixdate="2010-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-2037 broke Scan</summary>
      <description>Paul Ambrose wrote to the mailing list about some tests he has that doesn't pass on 0.20.3RC1-2. Looking into the issue it appears that this modification: public Scan addFamily(byte [] family) { familyMap.remove(family);- familyMap.put(family, null);+ familyMap.put(family, EMPTY_NAVIGABLE_SET); return this; }Makes it that when you use addColumn after that you put qualifiers into EMPTY_NAVIGABLE_SET which is static hence shared among all scanners after that like META scanners when calling tableExists.This was introduced by HBASE-2037.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
    </fixedFiles>
  </bug>
  <bug id="21711" opendate="2019-1-13 00:00:00" fixdate="2019-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to git.apache.org/hbase.git</summary>
      <description>With the GitBox migration not only git-wip-us was removed but also git.apache.org/hbase.git is not available anymore. (INFRA-17640)We need to remove all references to this url.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5,1.3.4,1.2.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.hbase.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug id="21714" opendate="2019-1-14 00:00:00" fixdate="2019-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deprecated isTableAvailableWithSplit method in thrift module</summary>
      <description>The one in the Admin interface has already been marked as deprecated but in thrift it is still there.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TReadType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDeleteType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TConsistency.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompareOperator.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21715" opendate="2019-1-14 00:00:00" fixdate="2019-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not throw UnsupportedOperationException in ProcedureFuture.get</summary>
      <description>This is really a bad practice, no one would expected that a Future does not support get, and this can not be detected at compile time. Even though we do not want user to wait for ever, we could set a long timeout, for example, 10 minutes,instead of throwing UnsuportedOperationException. I've already been hurt many times...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="21910" opendate="2019-2-15 00:00:00" fixdate="2019-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The nonce implementation is wrong for AsyncTable</summary>
      <description>We recreate the nonce every time when retry, which is not correct.And it is strange that we even do not have a UT for this...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableNoncedRetry.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="21911" opendate="2019-2-15 00:00:00" fixdate="2019-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move getUserPermissions from regionserver to master</summary>
      <description>Create a sub-task to move acl methods: getUserPermissions from regionserver to master.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.client.ThriftAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ShortCircuitMasterConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="21922" opendate="2019-2-18 00:00:00" fixdate="2019-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>BloomContext#sanityCheck may failed when use ROWPREFIX_DELIMITED bloom filter</summary>
      <description>Assume we use '5' as the delimiter, there are rowkeys: row1 is smaller than row2row1: 12345xxxrow2: 1235xxxxWhen use ROWPREFIX_DELIMITED bloom filter, the rowkey write to bloom filter arerow1's key for bloom filter: 1234row2's key for bloom fitler: 123The row1's key for bloom filter is bigger than row2. Then BloomContext#sanityCheck will failed.private void sanityCheck(Cell cell) throws IOException { if (this.getLastCell() != null) { LOG.debug("Current cell " + cell + ", prevCell = " + this.getLastCell()); if (comparator.compare(cell, this.getLastCell()) &lt;= 0) { throw new IOException("Added a key not lexically larger than" + " previous. Current cell = " + cell + ", prevCell = " + this.getLastCell()); } }}</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.regionserver.BloomType.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeBloomFilterAction.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.util.LoadTestTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RowPrefixDelimiterBloomContext.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekBeforeWithInlineBlocks.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.CreateRandomStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMultiColumnScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRowPrefixBloomFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanWithBloomError.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSeekOptimizations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompareOp.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TConsistency.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDeleteType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TReadType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
    </fixedFiles>
  </bug>
  <bug id="21991" opendate="2019-3-5 00:00:00" fixdate="2019-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix MetaMetrics issues - [Race condition, Faulty remove logic], few improvements</summary>
      <description>Here is a list of the issues related to the MetaMetrics implementation:Bugs: &amp;#91;_Lossy counting for top-k_&amp;#93; Faulty remove logic of non-eligible meters: Under certain conditions, we might end up storing/exposing all the meters rather than top-k-ish MetaMetrics can throw NPE resulting in aborting of the RS because of a Race Condition.Improvements: With high number of regions in the cluster, exposure of metrics for each region blows up the JMX from ~140 Kbs to 100+ Mbs depending on the number of regions. It's better to use lossy counting to maintain top-k for region metrics as well. As the lossy meters do not represent actual counts, I think, it'll be better to rename the meters to include "lossy" in the name. It would be more informative while monitoring the metrics and there would be less confusion regarding actual counts to lossy counts.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestLossyCounting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMetaTableMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.LossyCounting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MetaTableMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="21992" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add release manager for 2.2 in ref guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21995" opendate="2019-3-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a coprocessor to set HDFS ACL for hbase granted user</summary>
      <description>To make hbase granted user have the access to scan table snapshots, use HDFS ACLs to set user read permission over hfiles.The basic implementation is:1. For public directories such as 'data' and 'archive', set other users' permission to '--x' to make everyone have the permission to access the directory.2. For namespace or table directories such as 'data/ns/table', 'archive/ns/table' and '.hbase-snapshot/snapshotName', set user 'r-x' acl and default 'r-x' acl when following operations happen:grant to namespace or table / revoke from namespace or table / snapshot table For more details, please reference the design doc: https://docs.google.com/document/d/1D2iAdbrW5CcKc2SthJBXA1n2tTMTftuVaFtxbOWFuqM/edit#heading=h.uwo33s7kz427</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.PermissionStorage.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21996" opendate="2019-3-5 00:00:00" fixdate="2019-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set locale for javadoc</summary>
      <description>Headers in generated javadoc could have different language based on the user's locale.For example the published 2.1 javadoc's headers are in Chinese while 2.0 is in English.2.0: https://hbase.apache.org/2.0/apidocs/org/apache/hadoop/hbase/client/HTable.html2.1: https://hbase.apache.org/2.1/apidocs/org/apache/hadoop/hbase/client/HTable.html</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21997" opendate="2019-3-5 00:00:00" fixdate="2019-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-rest findbugs ST_WRITE_TO_STATIC_FROM_INSTANCE_METHOD complaint</summary>
      <description>Let me add annotations flagging the problem spot with annotations. It was already flagged 'hack' for test.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21999" opendate="2019-3-6 00:00:00" fixdate="2019-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DEBUG] Exit if git returns empty revision!</summary>
      <description>Let me commit a bit of debug on branch-2.0. Can't figure out how we are getting empty git revision string. Have the build fail if empty....</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-common.src.saveVersion.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22100" opendate="2019-3-24 00:00:00" fixdate="2019-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>False positive for error prone warnings in pre commit job</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16516/artifact/patchprocess/branch-compile-javac-hbase-client.txt[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,42] [UnusedVariable] The parameter 'error' is never read.https://builds.apache.org/job/PreCommit-HBASE-Build/16516/artifact/patchprocess/patch-compile-javac-hbase-client.txt[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,42] [UnusedVariable] The parameter 'error' is never read.[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.And the output is 1 new and 1 fixed, the new one is[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.I think here we should report nothing, as it is just an order change...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22101" opendate="2019-3-25 00:00:00" fixdate="2019-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncAdmin.isTableAvailable should not throw TableNotFoundException</summary>
      <description>Should return false instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="22115" opendate="2019-3-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase RPC aspires to grow an infinite tree of trace scopes; some other places are also unsafe</summary>
      <description>All of those are ClientServices.Multi in this case.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22117" opendate="2019-3-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move hasPermission/checkPermissions from region server to master</summary>
      <description>Create a sub-task to move acl methods: hasPermission/checkPermissions from regionserver to master.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.client.ThriftAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestWithDisabledAuthorization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAccessControlAdminApi.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.AccessControl.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.ShadedAccessControlUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ShortCircuitMasterConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="2212" opendate="2010-2-10 00:00:00" fixdate="2010-2-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor out lucene dependencies from HBase</summary>
      <description>Per an earlier thread on hbase-dev list , and approved by stack as well , need to remove lucene dependencies from HBase and move them elsewhere. Marker jira to revisit this and move them to github .</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.mapreduce.DisabledBecauseVariableSubstTooLargeExceptionTestTableIndex.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexConfiguration.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.BuildTableIndex.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.IndexTableReducer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.IndexRecordWriter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.IndexConfiguration.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.BuildTableIndex.java</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22131" opendate="2019-3-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete the patches in hbase-protocol-shaded module</summary>
      <description>As now we will apply the patch in the hbase-thirdparty repo.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-17239.patch</file>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-17087.patch</file>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-15789.V2.patch</file>
    </fixedFiles>
  </bug>
  <bug id="2219" opendate="2010-2-12 00:00:00" fixdate="2010-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stop using code mapping for method names in the RPC</summary>
      <description>since we use a sorted mapping of method names -&gt; codes and send that over the wire, even trivial changes, such as adding a new call, become wire-incompatible. This means many features which could easily have gone into a minor update must wait for a major update. Eg: 2066, 1845, etc.This will increase on-wire overhead, but the compatibility is worth it I think.</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionalRPC.java</file>
      <file type="M">src.contrib.mdc.replication.src.java.org.apache.hadoop.hbase.regionserver.replication.ReplicationRegionServer.java</file>
      <file type="M">src.contrib.mdc.replication.src.java.org.apache.hadoop.hbase.ipc.ReplicationRPC.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22490" opendate="2019-5-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly client integration test fails with hadoop-3</summary>
      <description>2019-05-28 15:27:10,107 WARN common.Storage: Storage directory /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 does not exist2019-05-28 15:27:10,109 WARN namenode.FSNamesystem: Encountered exception loading fsimageorg.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 is in an inconsistent state: storage directory does not exist or is not accessible. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:376) at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1074) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:706) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:628) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:690) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:919) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:892) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1622) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1305) at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1074) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:949) at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:881) at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:514) at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.start(MiniHadoopClusterManager.java:160) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.run(MiniHadoopClusterManager.java:132) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.main(MiniHadoopClusterManager.java:320) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71) at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144) at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:139) at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:147) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:244) at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.pseudo-distributed-test.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22560" opendate="2019-6-10 00:00:00" fixdate="2019-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Jetty 9.3.latest and Jackson 2.9.latest</summary>
      <description>Worked through some static analysis scans and these two popped up as a result.Upgrades are: Jetty 9.3.25-&gt;9.3.27 Jackson 2.9.2-&gt;2.9.9Not expecting any pain with these, but we'll find out what QA thinks!</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22580" opendate="2019-6-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a table attribute to make user scan snapshot feature configurable for table</summary>
      <description>If a cluster enable user scan snapshot feature, it will work for all tables. Since this feature will make some operations such as grant, revoke, snapshot... slower and some tables don't use scan snaphot to make scan faster. So add a table attribute to make it configurable at table level, in general, the feature is disabled by default, and if someone use feature, must enable the attribute of the specific table firstly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestSnapshotScannerHDFSAclController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.PermissionStorage.java</file>
    </fixedFiles>
  </bug>
  <bug id="22610" opendate="2019-6-20 00:00:00" fixdate="2019-7-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[BucketCache] Rename "hbase.offheapcache.minblocksize"</summary>
      <description>/** * The target block size used by blockcache instances. Defaults to * {@link HConstants#DEFAULT_BLOCKSIZE}. * TODO: this config point is completely wrong, as it's used to determine the * target block size of BlockCache instances. Rename. */ public static final String BLOCKCACHE_BLOCKSIZE_KEY = "hbase.offheapcache.minblocksize";</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22720" opendate="2019-7-22 00:00:00" fixdate="2019-7-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect link for hbase.unittests</summary>
      <description>Link for hbase.unittests link is incorrect in Categories and execution time chapter.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22722" opendate="2019-7-23 00:00:00" fixdate="2019-7-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson databind dependencies to 2.9.9.1</summary>
      <description>Due tohttps://nvd.nist.gov/vuln/detail/CVE-2019-12814https://nvd.nist.gov/vuln/detail/CVE-2019-12384</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22724" opendate="2019-7-23 00:00:00" fixdate="2019-9-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a emoji on the vote table for pre commit result on github</summary>
      <description>zghaobac said that the current vote table on github is not good enough, as the colors are almost the same, it is not easy to find out which line is broken.Since github can not change the color of the text, he suggested that we add a column at the left most with some emojis to better notify the developpers.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,1.3.6,1.4.11,2.1.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23022" opendate="2019-9-14 00:00:00" fixdate="2019-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>download page should use HTTPS for links to archive.a.o</summary>
      <description>Noticed this while working on some automation. We're https everywhere else in this page, just not this link.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2335" opendate="2010-3-17 00:00:00" fixdate="2010-4-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapred package docs don&amp;#39;t say zookeeper jar is a dependency.</summary>
      <description>in:http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapred/package-summary.htmlwe dont say the classpath needs zookeeper-x.y.z.jar - which it does.But this package does say so:http://hadoop.apache.org/hbase/docs/r0.20.3/api/org/apache/hadoop/hbase/mapreduce/package-summary.html</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23350" opendate="2019-11-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make compaction files cacheonWrite configurable based on threshold</summary>
      <description>As per comment from javaman_chen in the parent JIRAhttps://issues.apache.org/jira/browse/HBASE-23066?focusedCommentId=16937361&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16937361This is to introduce a config to identify if the resulting compacted file's blocks should be added to the cache - while writing.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestDateTieredCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheOnWrite.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
    </fixedFiles>
  </bug>
  <bug id="2375" opendate="2010-3-25 00:00:00" fixdate="2010-7-25 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Revisit compaction configuration parameters</summary>
      <description>Currently we will make the decision to split a region when a single StoreFile in a single family exceeds the maximum region size. This issue is about changing the decision to split to be based on the aggregate size of all StoreFiles in a single family (but still not aggregating across families). This would move a check to split after flushes rather than after compactions. This issue should also deal with revisiting our default values for some related configuration parameters.The motivating factor for this change comes from watching the behavior of RegionServers during heavy write scenarios.Today the default behavior goes like this: We fill up regions, and as long as you are not under global RS heap pressure, you will write out 64MB (hbase.hregion.memstore.flush.size) StoreFiles. After we get 3 StoreFiles (hbase.hstore.compactionThreshold) we trigger a compaction on this region. Compaction queues notwithstanding, this will create a 192MB file, not triggering a split based on max region size (hbase.hregion.max.filesize). You'll then flush two more 64MB MemStores and hit the compactionThreshold and trigger a compaction. You end up with 192 + 64 + 64 in a single compaction. This will create a single 320MB and will trigger a split. While you are performing the compaction (which now writes out 64MB more than the split size, so is about 5X slower than the time it takes to do a single flush), you are still taking on additional writes into MemStore. Compaction finishes, decision to split is made, region is closed. The region now has to flush whichever edits made it to MemStore while the compaction ran. This flushing, in our tests, is by far the dominating factor in how long data is unavailable during a split. We measured about 1 second to do the region closing, master assignment, reopening. Flushing could take 5-6 seconds, during which time the region is unavailable. The daughter regions re-open on the same RS. Immediately when the StoreFiles are opened, a compaction is triggered across all of their StoreFiles because they contain references. Since we cannot currently split a split, we need to not hang on to these references for long.This described behavior is really bad because of how often we have to rewrite data onto HDFS. Imports are usually just IO bound as the RS waits to flush and compact. In the above example, the first cell to be inserted into this region ends up being written to HDFS 4 times (initial flush, first compaction w/ no split decision, second compaction w/ split decision, third compaction on daughter region). In addition, we leave a large window where we take on edits (during the second compaction of 320MB) and then must make the region unavailable as we flush it.If we increased the compactionThreshold to be 5 and determined splits based on aggregate size, the behavior becomes: We fill up regions, and as long as you are not under global RS heap pressure, you will write out 64MB (hbase.hregion.memstore.flush.size) StoreFiles. After each MemStore flush, we calculate the aggregate size of all StoreFiles. We can also check the compactionThreshold. For the first three flushes, both would not hit the limit. On the fourth flush, we would see total aggregate size = 256MB and determine to make a split. Decision to split is made, region is closed. This time, the region just has to flush out whichever edits made it to the MemStore during the snapshot/flush of the previous MemStore. So this time window has shrunk by more than 75% as it was the time to write 64MB from memory not 320MB from aggregating 5 hdfs files. This will greatly reduce the time data is unavailable during splits. The daughter regions re-open on the same RS. Immediately when the StoreFiles are opened, a compaction is triggered across all of their StoreFiles because they contain references. This would stay the same.In this example, we only write a given cell twice (instead of 4 times) while drastically reducing data unavailability during splits. On the original flush, and post-split to remove references. The other benefit of post-split compaction (which doesn't change) is that we then get good data locality as the resulting StoreFile will be written to the local DataNode. In another jira, we should deal with opening up one of the daughter regions on a different RS to distribute load better, but that's outside the scope of this one.</description>
      <version>0.20.3</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23751" opendate="2020-1-28 00:00:00" fixdate="2020-1-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move core to hbase-thirdparty 3.2.0</summary>
      <description>Move core to use just-released 3.2.0 hbase-thirdparty. The change has already been through build courtesy of https://github.com/apache/hbase/pull/1086</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23780" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Edit of test classifications</summary>
      <description>Our test classifications have drifted. You can see for yourself running each of the small/medium and large test suites. See test complete times. See how even some large tests should be small and vice versa.The more small tests we can run inside the single JVM, the faster we'll get through the build. Tests that are Medium start their own JVM for each test. Tests that are Medium but only last a second or two are expensive and should be aggregated with other single, short tests to amortize the JVM startup.Anyways, let me edit the test categories and try and clean them up some.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileDataBlockEncoder.java</file>
      <file type="M">hbase-zookeeper.src.test.java.org.apache.hadoop.hbase.zookeeper.TestHQuorumPeer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplitToHFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALReaderOnSecureWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestBoundedRegionGroupingStrategy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadEncoded.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckMOB.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckEncryption.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSVisitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestDefaultEnvironmentEdge.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestCanaryTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestServerSideScanMetricsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestClientClusterMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestCheckTestClasses.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotWhenChoreCleaning.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotManifest.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestRestoreFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestConcurrentFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.token.TestZKSecretWatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestSecureIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestZKPermissionWatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestTablePermissions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestCellACLs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessControlFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationWithTags.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestNamespaceReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplayCompressed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplayBoundedLogWriterCreation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestSecureWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollPeriod.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestAsyncProtobufLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestAsyncLogRollPeriod.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestAsyncFSWALDurability.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.throttle.TestFlushWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.throttle.TestCompactionWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSyncTimeRangeTracker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSwitchToStreamRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStripeStoreFileManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScannerClosure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFileRefresherChore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitWalDataLoss.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestShutdownWhileWALBroken.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSettingTimeoutOnBlockingPoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerRetriableFailure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerReportForDuty.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMove.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestParallelPut.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMobStoreCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinVersions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinorCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsUserAggregate.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsTableAggregate.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStoreLAB.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMajorCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeepDeletes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionFileSystem.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHeapMemoryManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestFSErrorsExposed.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestFlushRegionEntry.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestFailedAppendAndSync.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompoundBloomFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactionArchiveConcurrentClose.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingToCellFlatMapMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestClearRegionBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCacheOnWriteInSchema.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBlocksRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestStripeCompactionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotasWithRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSnapshotQuotaObserverChore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestFailedProcCleanup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure2.store.region.TestWALProcedurePrettyPrinter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure2.store.region.TestRegionProcedureStoreMigration.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure2.store.region.TestRegionProcedureStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure2.store.region.TestHFileProcedurePrettyPrinter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitWALManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestServerCrashProcedureStuck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestServerCrashProcedureCarryingMetaStuck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterRepairMode.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestClusterStatusPublisher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestCatalogJanitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestTableDescriptorModificationFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSCPWithoutMetaWithoutZKCoordinated.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSchedulerQueueDeadLock.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestRestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestDeleteColumnFamilyProcedureFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.locking.TestLockProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancerHeterogeneousCostRules.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestFavoredNodeTableImport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestDefaultLoadBalancer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRegionSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRegionReplicaSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRegionMoveAndAbandon.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRaceBetweenSCPAndDTP.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestMergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestProtoBufRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestNettyIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestHBaseClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestFifoRpcScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestBlockingIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestScannerSelectionUsingTTL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestScannerFromBucketCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileWriterV3WithDataEncoders.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileSeek.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileScannerImplReferenceCount.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdminRpcPriority.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestChoreService.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.types.TestCopyOnWriteMaps.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestAvlUtil.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestByteBufferUtils.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestCommonFSUtils.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestEnvironmentEdgeManager.java</file>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.TestSSLHttpServer.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduceUtil.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.mapred.TestRowCounter.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureEvents.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureRecovery.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureReplayOrder.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureSchedulerConcurrency.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureSkipPersistence.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestStateMachineProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAppendFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncClientPushback.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableScanRenewLease.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableUseMetaReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientOperationInterrupt.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMvccConsistentScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRawAsyncScanCursor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRawAsyncTableScan.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicaWithCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestResultScannerCursor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestResultSizeEstimation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMetaTableMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestNegativeMemStoreSizeWithSlowCoprocessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.favored.TestFavoredNodeAssignmentHelper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterList.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterSerialization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWithScanLimits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestMultipleColumnPrefixFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestParseFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.TestInfoServersACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestEncodedSeekers.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestBucketCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestBucketCacheRefCnt.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestRAMCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.bucket.TestVerifyBucketCacheFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="23818" opendate="2020-2-9 00:00:00" fixdate="2020-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup the remaining RSGroupInfo.getTables call in the code base</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.TestRSGroupShell.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupMajorCompactionTTL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestMigrateRSGroupInfo.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestEnableRSGroups.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.rsgroup.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupMajorCompactionTTL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.rsgroup.IntegrationTestRSGroup.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RSGroupTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="2382" opendate="2010-3-26 00:00:00" fixdate="2010-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t rely on fs.getDefaultReplication() to roll HLogs</summary>
      <description>As I was commenting in HBASE-2234, using fs.getDefaultReplication() to roll HLogs if they lose replicas isn't reliable since that value is client-side and unless HBase is configured with it or has Hadoop's configurations on its classpath, it will do the wrong thing.Dhruba added:Can we use &lt;hlogpath&gt;.getFiletatus().getReplication() instead of fs.getDefaltReplication()? This will will ensure that we look at the repl factor of the precise file we are interested in, rather than what the system-wide default value is.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.javadoc.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23851" opendate="2020-2-15 00:00:00" fixdate="2020-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log networks and bind addresses when multicast publisher/listener enabled</summary>
      <description>Log details on publisher and listeners at DEBUG level so can figure issues when client and server do not match.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterStatusPublisher.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ScheduledChore.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClusterStatusListener.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="2399" opendate="2010-4-1 00:00:00" fixdate="2010-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forced splits only act on the first family in a table</summary>
      <description>While working on a patch for HBASE-2375, I came across a few bugs in the existing code related to splits.If a user triggers a manual split, it flips a forceSplit boolean to true and then triggers a compaction (this is very similar to my current implementation for HBASE-2375). However, the forceSplit boolean is flipped back to false at the beginning of Store.compact(). So the force split only acts on the first family in the table. If that Store is not splittable for some reason (it is empty or has only one row), then the entire region will not be split, regardless of what is in other families.Even if there is data in the first family, the midKey is determined based solely on that family. If it has two rows and the next family has 1M rows, we pick the split key based on the two rows.</description>
      <version>0.20.3</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23992" opendate="2020-3-14 00:00:00" fixdate="2020-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestAdminShell and TestQuotasShell mistakenly broken by parent commit</summary>
      <description>Minor fix...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.quotas.test.rb</file>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
    </fixedFiles>
  </bug>
  <bug id="2417" opendate="2010-4-7 00:00:00" fixdate="2010-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCM.locateRootRegion fails hard on "Connection refused"</summary>
      <description>While running some tests on replication, I saw that our client does something dumb if it tries to contact a dead region server that held the ROOT region in HCM.locateRootRegion. Will post stack trace in a comment.The problem here is that we don't retry at all, the exception will come straight out of HCM like it's the end of the world.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24170" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hadoop-2.0 profile</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client-byo-hadoop.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="24171" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2.1.10 from download page</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24172" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2.1 from the release managers section in ref guide</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="24175" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Flakey Tests] TestSecureExportSnapshot FileNotFoundException</summary>
      <description>Why we writing '/tmp' dir?Error Messageorg.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not existStacktraceorg.apache.hadoop.yarn.exceptions.YarnRuntimeException: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)Caused by: org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)Caused by: java.io.FileNotFoundException: File file:/tmp/hadoop-yarn-jenkins/node-attribute/nodeattribute.mirror.writing does not exist at org.apache.hadoop.hbase.snapshot.TestSecureExportSnapshot.setUpBeforeClass(TestSecureExportSnapshot.java:56)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshotAdjunct.java</file>
    </fixedFiles>
  </bug>
  <bug id="2438" opendate="2010-4-13 00:00:00" fixdate="2010-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addition of a Column Pagination Filter</summary>
      <description>Client applications may need to do pagination, depending on the number of columns returned, it may be more efficient to perform pagination algorithms at the database level (similar to SQL's LIMIT and OFFSET). This will be an additional filter taking two parameters: page pageSizeFor every row, that gets returned, only a subset of columns are returned based on page and pageSizeIf the page / pageSize column goes over the limits, then no results are returned from the filter.A practical example for using a filter like this may be for folks doing Row-based indexing with Hbase.</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="24423" opendate="2020-5-24 00:00:00" fixdate="2020-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to get lock in canSplit because hasReferences will get lock too</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2470" opendate="2010-4-20 00:00:00" fixdate="2010-4-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scan.setTimeRange() support in Shell</summary>
      <description>The shell does not support scans by time range.This is trivial, simply add two more optional values MINTIMESTAMP and MAXTIMESTAMP and if both are set call Scan.setTimeRange(minStamp, maxStamp).</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.ruby.hbase.table.test.rb</file>
      <file type="M">src.main.ruby.hbase.table.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2496" opendate="2010-4-28 00:00:00" fixdate="2010-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less ArrayList churn on the scan path</summary>
      <description>Doing some profiling when testing the scanning speed of 0.20.4, I saw that we are spending a lot of time instantiating ArrayLists when scanning and that we could sometime set the right size of the arrays. I don't expect big improvements for short scans, but people like us who are scanning in batches of 10k could get some nice speedups.</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ColumnCount.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2499" opendate="2010-4-28 00:00:00" fixdate="2010-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race condition when disabling a table leaves regions in transition</summary>
      <description>A lot of people reported that weren't able to add/delete a column because only some of the regions got the modification. I personally thought it was due to the CME bug in the Master, but I'm able to easily reproduce on 0.20.4 on a 1800 regions table.Since 0.20.3, we now call disableTable after every retry to make sure we don't miss any region. This creates a race where while we scan .META. in TableOperation, a region could be reported as closed after we scanned the row. We end up processing it like if it was assigned and we put it back into regionsInTransition. We need to either query .META. before processing each region or make some more check to see if the region was closed.This kills the RC in my book.In the mean time, anyone getting this can restart their HBase and it will pick up the change.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ChangeTableState.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2615" opendate="2010-5-26 00:00:00" fixdate="2010-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>M/R on bulk imported tables</summary>
      <description>We are bulk importing using loadtable.rb and running M/R jobs using HBase as input.We're taking the following steps:1a. Load HBase with a M/R job using the normal API. OR1b. Load HBase with bulk import.THEN2a. Using the shell, do a "count" over the table.OR2b. Run a M/R job that scans the whole HBase table (and nothing else).Of the 4 combos, 3 are fine: 1a+2a, 1a+2b, 1b+2a. We're having trouble with 1b+2b. When we run the M/R job, it doesn't seem to read in any records, but there are no explicit errors in either the Hadoop or HBase logs.Any ideas on what might be wrong with the bulk import to cause this problem? We confirmed this problem exists in both hbase-0.20.3 and hbase-0.20.4.We have created dummy data (see attached). This is the test case:After loading the data into HDFS. In hbase shell:create 'tiny', 'values'Execute: {HBASE-HOME}/bin/hbase org.jruby.Main {HBASE-HOME}/bin/loadtable.rb tiny tinytableThen run the simple row counter{HADOOP-HOME}/bin/hadoop jar {HBASE-HOME}/hbase-0.20.x.jar rowcounter tiny valuesNotice that map input records read is always zero. We confirmed that other mapreduce jobs do not execute the map function at all, always returning 0 records.We also ran a major_compaction of all Hbase tables (.META. and .ROOT. as well) but this did not fix the problem.</description>
      <version>0.20.3,0.20.4</version>
      <fixedVersion>0.20.5,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="2792" opendate="2010-6-25 00:00:00" fixdate="2010-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a better way to chain log cleaners</summary>
      <description>From Stack's review of HBASE-2223:Why this implementation have to know about other implementations? Can't we do a chain of decision classes? Any class can say no? As soon as any decision class says no, we exit the chain.... So in this case, first on the chain would be the ttl decision... then would be this one... and third would be the snapshotting decision. You don't have to do the chain as part of this patch but please open an issue to implement.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestOldLogsCleaner.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.OldLogsCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LogCleanerDelegate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2793" opendate="2010-6-26 00:00:00" fixdate="2010-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to extract a specified list of versions of a column in a single roundtrip</summary>
      <description>In one of the use cases we were looking at, each row contains a single column, but with several versions (e.g., each version representing an event in a log), and we want to be able to extract specific set of versions from the row in a single round-trip.Currently, on a Get, one can retrieve a specific version of a column using setTimeStamp(ts) or a range of versions using setTimeRange(min, max). But not a set of specified versions. It would be useful to add this ability.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2795" opendate="2010-6-26 00:00:00" fixdate="2010-6-26 01:00:00" resolution="Invalid">
    <buginformation>
      <summary>On review HBASE-2707 has problem in that we&amp;#39;ll get stuck in the delay queue and never come out</summary>
      <description>I committed the hbase-2707 patch yesterday but on second thoughts, it has a flaw in that if nothing in the todo queue, we then poll the delayedtodo queue. If we fall into the latter and it has not elements, then we'll never come out; there are no notifyalls going on to wake us up. Patch coming.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestRegionServerOperationQueue.java</file>
    </fixedFiles>
  </bug>
  <bug id="288" opendate="2007-5-21 00:00:00" fixdate="2007-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add in-memory caching of data</summary>
      <description>Bigtable provides two in-memory caches: one for row/column data and one for disk block caches.The size of each cache should be configurable, data should be loaded lazily, and the cache managed by an LRU mechanism.One complication of the block cache is that all data is read through a SequenceFile.Reader which ultimately reads data off of disk via a RPC proxy for ClientProtocol. This would imply that the block caching would have to be pushed down to either the DFSClient or SequenceFile.Reader</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.io.TestBlockFSInputStream.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestToString.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestTimestamp.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestBloomFilters.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.SchemaModificationCommand.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.HQLParser.jj</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.generated.HQLParserTokenManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.generated.HQLParserConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.generated.HQLParser.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.hql.AlterCommand.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">lib.hadoop-0.16.0-test.jar</file>
      <file type="M">lib.hadoop-0.16.0-core.jar</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2907" opendate="2010-8-11 00:00:00" fixdate="2010-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[rest/stargate] Improve error response when trying to create a scanner on a nonexistant table</summary>
      <description>Since 0.20.4, an attempt to create a scanner for a nonexistant table receives a "400 Bad Request" response with no furthur information. Prior to 0.20.4 it would receive a "500 org.apache.hadoop.hbase.TableNotFoundException: &lt;table&gt;" response with a stack trace in the body.Neither of these is ideal - the 400 fails to identify what aspect of the request was bad, and the 500 incorrectly suggests that the error was internal. Ideally the error should be a 400 error with information in the body identifying the nature of the problem.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3117" opendate="2010-10-16 00:00:00" fixdate="2010-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to 0.5 version</summary>
      <description>Thrift 0.5 has been released already and we want to upgrade to at least 0.3 but 0.5 has a lot of improvements so that would be the best.Unfortunately the Java lib has changed so that we'll have to regenerate the current Thrift interface and fix the implementation (byte[] -&gt; ByteBuffer).They also have problems getting Thrift into a Maven repository so we'll need to do our current workaround again unfortunately and upload it to a repository. That would be Ryan's I think?I'll upload an updated thrift jar and a patch for the old Thrift code.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
