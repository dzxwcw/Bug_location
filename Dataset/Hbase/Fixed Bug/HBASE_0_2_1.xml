<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="698" opendate="2008-6-19 00:00:00" fixdate="2008-7-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HLog recovery is not performed after master failure</summary>
      <description>I have a local cluster running, and its logging to&lt;hbase&gt;/log_X.X.X.X_1213228101021_60020/Then I kill both master and regionserver, and restart. Looking throughthe logs I don't see anything about trying to recover from this hlog,it just creates a new hlog alongside the existing one (with a newstartcode). The older hlog seems to be ignored, and the tablescreated in the inital session are all gone.</description>
      <version>0.1.2,0.2.1,0.19.3</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="762" opendate="2008-7-22 00:00:00" fixdate="2008-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteFamily takes timestamp, should only take row and family. Javadoc describes both cases but only implements the timestamp case.</summary>
      <description>The three version of deleteFamily in client.HTable (Text, String, byte[]) have varying descriptions about whether they take timestamps or not.public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family, long timestamp) throws IOException Delete all cells for a row with matching column family with timestamps less than or equal to timestamp. These will become:public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family) throws IOException Delete all cells for a row with matching column family at all timestamps.Per Jean-Daniel's comment, deleteAll should then not permit families. I'm unsure whether this is currently allowed or not, but the documentation must be updated either way.Will post patch after more thorough testing.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8000" opendate="2013-3-5 00:00:00" fixdate="2013-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create integration/perf tests for stripe compactions</summary>
      <description>While writing tests I seem to be finding more errors with edge cases unrelated to what test actually tests compared to what is expected.Integration test will be needed... probably good for perf/compare too.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestIngest.java</file>
    </fixedFiles>
  </bug>
  <bug id="834" opendate="2008-8-15 00:00:00" fixdate="2008-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Major&amp;#39; compactions and upper bound on files we compact at any one time</summary>
      <description>From Billy in HBASE-64, which we closed because it got pulled all over the place:Currently we do compaction on a region when the hbase.hstore.compactionThreshold is reached - default 3I thank we should configure a max number of mapfiles to compact at one time simulator to doing a minor compaction in bigtable. This keep compaction's form getting tied up in one region to long letting other regions get way to many memcache flushes making compaction take longer and longer for each regionIf we did that when a regions updates start to slack off the max number will eventuly include all mapfiles causeing a major compaction on that region. Unlike big table this would leave the master out of the process and letting the region server handle the major compaction when it has time.When doing a minor compaction on a few files I thank we should compact the newest mapfiles first leave the larger/older ones for when we have low updates to a region.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="836" opendate="2008-8-16 00:00:00" fixdate="2008-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update thrift examples to work with changed IDL (HBASE-697)</summary>
      <description>Examples are now out of date since HBASE-697 went in.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.DemoClient.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8611" opendate="2013-5-24 00:00:00" fixdate="2013-10-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve test coverage in pkg org.apache.hadoop.hbase.mapred</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="865" opendate="2008-9-3 00:00:00" fixdate="2008-9-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc warnings</summary>
      <description>There are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.2,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8652" opendate="2013-5-29 00:00:00" fixdate="2013-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of compacting KVs is not reset at the end of compaction</summary>
      <description>Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).In DefaultCompactor#compact(), we have this at the beginning: this.progress = new CompactionProgress(fd.maxKeyCount);But progress.totalCompactingKVs is not reset at the end of compact().</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8680" opendate="2013-6-3 00:00:00" fixdate="2013-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>distributedLogReplay performance regression</summary>
      <description>The JIRA is to check in changes to address performance issues found during my performance testing as following:1) When WALEdits belongs to a region which split/merged later, replay incurs extra waitUntilRegionOnline RPC call2) Using a single shared connection for replaying achieves better performance. Everytime creating a new connection, it incurs 4+ seconds to establish a connection to ZK.3) other small modifications to mitigate excessive logging</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
    </fixedFiles>
  </bug>
  <bug id="871" opendate="2008-9-4 00:00:00" fixdate="2008-9-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Major compaction periodicity should be specifyable at the column family level, not cluster wide</summary>
      <description>jon gray has a table of ten rows and a couple of columns that is constantly being updated. Has max versions of 2. This table is growing fast because all versions written are kept until a major compaction. The way this table is being used is different than that of others. Would be good if he could have major compactions run more often than the default once a day.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="877" opendate="2008-9-8 00:00:00" fixdate="2008-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCM is unable to find table with multiple regions which contains binary</summary>
      <description>HCM can not find the table with exception:org.apache.hadoop.hbase.TableNotFoundException: items at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:508) at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:460) at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:420) at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:130) at HBaseRef.&lt;init&gt;(HBaseRef.java:29) at Import.&lt;init&gt;(Import.java:20) at Import.main(Import.java:26)I have a fix already for this. But the problem re-appeared after some time. I have no recreated it yet, but will post results in the morning.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.BeforeThisStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8770" opendate="2013-6-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deletes and puts with the same ts should be resolved according to mvcc/seqNum</summary>
      <description>This came up during HBASE-8721. Puts with the same ts are resolved by seqNum. It's not clear why deletes with the same ts as a put should always mask the put, rather than also being resolve by seqNum.What do you think?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Brainstorming</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="8826" opendate="2013-6-28 00:00:00" fixdate="2013-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-8695 is covered in Thrift 2</summary>
      <description>HBASE-8695 is about using the config file, make sure Thrift 2 is doing the same.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="883" opendate="2008-9-12 00:00:00" fixdate="2008-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secondary Indexes</summary>
      <description>I'm working on a secondary index impl. The basic idea is to maintain a separate table per index.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.package.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexSpecification.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexNotFoundException.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableAdmin.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
    </fixedFiles>
  </bug>
  <bug id="8832" opendate="2013-6-28 00:00:00" fixdate="2013-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-4658 is supported by Thrift 2</summary>
      <description>HBASE-4658 adds support for "attributes" for certain operations. Make sure Thrift 2 supports them where ever available in the native API.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
    </fixedFiles>
  </bug>
  <bug id="889" opendate="2008-9-18 00:00:00" fixdate="2008-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.</summary>
      <description>The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="891" opendate="2008-9-19 00:00:00" fixdate="2008-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HRS.validateValuesLength throws IOE, gets caught in the retries</summary>
      <description>When HRS.validateValuesLength throws a IOE, it gets caught in the retries because it does not use a DoNotRetryIOException.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.2.2,0.18.1,0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="994" opendate="2008-11-11 00:00:00" fixdate="2008-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IPC interfaces with different versions can cause problems</summary>
      <description>This morning we ran into a situation in which some 0.2.x region servers started up and joined a 0.18.1 cluster. This 'sort of' worked in that the hrs could communicate with the master, but clients could not communicate with the 0.2 region servers because the api version changed (the master wouldn't have liked it if it had deployed root or meta there).Suggestion is that we have a single api version that gets bumped when any of the interfaces changes.</description>
      <version>0.2.1,0.18.0,0.18.1,0.19.0</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HMasterRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
