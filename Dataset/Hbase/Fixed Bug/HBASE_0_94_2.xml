<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="11143" opendate="2014-5-9 00:00:00" fixdate="2014-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve replication metrics</summary>
      <description>We are trying to report on replication lag and find that there is no good single metric to do that.ageOfLastShippedOp is close, but unfortunately it is increased even when there is nothing to ship on a particular RegionServer.I would like discuss a few options here:Add a new metric: replicationQueueTime (or something) with the above meaning. I.e. if we have something to ship we set the age of that last shipped edit, if we fail we increment that last time (just like we do now). But if there is nothing to replicate we set it to current time (and hence that metric is reported to close to 0).Alternatively we could change the meaning of ageOfLastShippedOp to mean to do that. That might lead to surprises, but the current behavior is clearly weird when there is nothing to replicate.Comments? jdcryans, stack.If approach sounds good, I'll make a patch for all branches.Edit: Also adds a new shippedKBs metric to track the amount of data that is shipped via replication.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.94.20,0.98.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.MetricsSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="12186" opendate="2014-10-7 00:00:00" fixdate="2014-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formatting error in Table 8.2. Examples of Visibility Expressions</summary>
      <description>The value of one of the cells of the table got lost somehow during the commit of HBASE-11791. Investigate and fix.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.security.xml</file>
    </fixedFiles>
  </bug>
  <bug id="12189" opendate="2014-10-7 00:00:00" fixdate="2014-10-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix new issues found by coverity static analysis</summary>
      <description>Since the paper about testing and failures, it's probably a good time to start another run of coverity and fix the issues there.</description>
      <version>None</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.MultiHConnection.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreChunkPool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.DefaultMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.UserQuotaState.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaState.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.SimpleLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZKSplitLogManagerCoordination.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.impl.JmxCacheBuster.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.trace.SpanReceiverHost.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="5076" opendate="2011-12-20 00:00:00" fixdate="2011-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell hangs when creating some &amp;#39;illegal&amp;#39; tables.</summary>
      <description>In hbase shell. These commands hang:create 'hbase.version','foo'create 'splitlog','foo'Interestinglycreate 'hbase.id','foo'create existingtablename, 'foo'create '.META.','foo'create '-ROOT-','foo'are properly rejected.We should probably either rename to make the files illegal table names (hbase.version to .hbase.version and splitlog to .splitlog) or we could add more special cases.</description>
      <version>0.92.0,0.94.1,0.94.2,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug id="6373" opendate="2012-7-11 00:00:00" fixdate="2012-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more context information to audit log messages</summary>
      <description>The attached patch adds more information to the audit log messages; namely, it includes the IP address where the request originated, if it's available.The patch is against trunk, but I've tested it against the 0.92 branch. I didn't find any unit test for this code, please let me know if I missed something.</description>
      <version>0.94.2,0.95.2</version>
      <fixedVersion>0.94.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug id="6382" opendate="2012-7-12 00:00:00" fixdate="2012-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Jersey to 1.8 to match Hadoop 1 and 2</summary>
      <description>Upgrade Jersey dependency from 1.4 to 1.8 to match Hadoop dependencies.</description>
      <version>0.90.7,0.92.2,0.94.2,0.95.2</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6419" opendate="2012-7-18 00:00:00" fixdate="2012-9-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PersistentMetricsTimeVaryingRate gets used for non-time-based metrics (part2 of HBASE-6220)</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="6441" opendate="2012-7-22 00:00:00" fixdate="2012-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MasterFS doesn&amp;#39;t set scheme for internal FileSystem</summary>
      <description>FSUtils.getRootDir() just takes a configuration object, which is used to:1) Get the name of the root directory2) Create a filesystem (based on the configured scheme)3) Qualify the root onto the filesystemHowever, the FileSystem from the master filesystem won't generate the correctly qualified root directory under hadoop-2.0 (though it works fine on hadoop-1.0). Seems to be an issue with the configuration parameters.</description>
      <version>0.94.2,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="6445" opendate="2012-7-24 00:00:00" fixdate="2012-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>rat check fails if hs_err_pid26514.log dropped in tests</summary>
      <description>Let test fail because jvm crashed rather than because of rat license complaint</description>
      <version>None</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6476" opendate="2012-7-30 00:00:00" fixdate="2012-12-30 01:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Replace all occurrances of System.currentTimeMillis() with EnvironmentEdge equivalent</summary>
      <description>There are still some areas where System.currentTimeMillis() is used in HBase. In order to make all parts of the code base testable and (potentially) to be able to configure HBase's notion of time, this should be generally be replaced with EnvironmentEdgeManager.currentTimeMillis().How hard would it be to add a maven task that checks for that, so we do not introduce System.currentTimeMillis back in the future?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HMerge.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSHDFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.Canary.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift.TBoundedThreadPoolServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.ScannerResultGenerator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.client.Client.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.metrics.ReplicationSourceMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.metrics.ReplicationSinkMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.RegionTransition.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Threads.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.ProtobufRpcEngine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.DefaultLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.GeneralBulkAssigner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DeleteTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionState.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.MetricsRate.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MemoryBoundedLogMessageBuffer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Leases.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitRequest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="6477" opendate="2012-7-30 00:00:00" fixdate="2012-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use PB filter definitions in RPC</summary>
      <description>Use the filters introduced in HBASE-6454 in the rpc so they are extensible</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.protobuf.Filter.proto</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestSerialization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestSingleColumnValueFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestRandomRowFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestPrefixFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestPageFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestInclusiveStopFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterList.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestColumnPaginationFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScan.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestGet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAttributes.java</file>
      <file type="M">hbase-server.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Append.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Get.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Row.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.RowMutations.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnCountGetFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnPaginationFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnPrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.CompareFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FamilyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FilterBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FilterWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyValueMatchingQualifiersFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FuzzyRowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.KeyOnlyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.MultipleColumnPrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.NullComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.QualifierFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RandomRowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueExcludeFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.TimestampsFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ValueFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.WhileMatchFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.WritableByteArrayComparable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FilterProtos.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlFilter.java</file>
      <file type="M">hbase-server.src.main.protobuf.Client.proto</file>
    </fixedFiles>
  </bug>
  <bug id="6489" opendate="2012-8-1 00:00:00" fixdate="2012-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect TaskTracker logfile name</summary>
      <description>http://hbase.apache.org/book/trouble.log.html"TaskTracker: $HADOOP_HOME/logs/hadoop-&lt;user&gt;jobtracker&lt;hostname&gt;.log"should be "TaskTracker: $HADOOP_HOME/logs/hadoop-&lt;user&gt;tasktracker&lt;hostname&gt;.log"</description>
      <version>0.90.7,0.92.2,0.92.3,0.94.2,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  <bug id="691" opendate="2008-6-16 00:00:00" fixdate="2008-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get* and getScanner are different in how they treat column parameter</summary>
      <description>From the list, cure at xg dot pl there are group of methods "getRow" and group "getScanner" - both get as param array of collumns but in "getRow" methods we have to put it without ":" at the end of column family name, and for "getScanner" the colon is necessary. i think that it will be good to make it identical.</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6926" opendate="2012-10-2 00:00:00" fixdate="2012-10-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup some of the javadoc warnings.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.RegionTransition.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.MasterAdminProtocol.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HBaseIOException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HBaseException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ClusterId.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.metrics2.lib.DynamicMetricsRegistry.java</file>
      <file type="M">hbase-hadoop1-compat.src.main.java.org.apache.hadoop.hbase.metrics.BaseMetricsSourceImpl.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockType.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="6951" opendate="2012-10-4 00:00:00" fixdate="2012-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow the master info server to be started in a read only mode.</summary>
      <description>There are some cases that a user could want a web ui to be accessible but might not want the split and compact functionality to be usable.Allowing the web ui to start in a readOnly mode would be good.</description>
      <version>None</version>
      <fixedVersion>0.92.3,0.94.3,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestInfoServers.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
    </fixedFiles>
  </bug>
  <bug id="6962" opendate="2012-10-9 00:00:00" fixdate="2012-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop 1 dependency to hadoop 1.1</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6979" opendate="2012-10-11 00:00:00" fixdate="2012-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>recovered.edits file should not break distributed log splitting</summary>
      <description>Distributed log splitting fails in creating the recovered.edits folder during upgrade because there is a file called recovered.edits there.Instead of checking if the patch exists, we need to check if it exists and is a path.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="7" opendate="2007-12-17 00:00:00" fixdate="2007-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] Provide a HBase checker and repair tool similar to fsck</summary>
      <description>We need a tool to verify (and repair) HBase much like fsck</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.check.meta.rb</file>
    </fixedFiles>
  </bug>
  <bug id="7027" opendate="2012-10-22 00:00:00" fixdate="2012-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the correct port of info server of region servers</summary>
      <description>Right now the master ui just guesses that the port of the info server will always be the same on all servers. This is not a good assumption setting it for each server is possible, also setting the conf variable to 0 will make the info server choose a port randomly.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="7074" opendate="2012-10-30 00:00:00" fixdate="2012-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document Metrics 2</summary>
      <description>Explain why this work was done. Explain how the data flows from the core classes into hadoop metrics2, and on to jmx. Explain naming metrics.</description>
      <version>None</version>
      <fixedVersion>0.95.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7103" opendate="2012-11-6 00:00:00" fixdate="2012-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to fail split if SPLIT znode is deleted even before the split is completed.</summary>
      <description>This came up after the following mail in dev list'infinite loop of RS_ZK_REGION_SPLIT on .94.2'.The following is the reason for the problemThe following steps happen-&gt; Initially the parent region P1 starts splitting.-&gt; The split is going on normally.-&gt; Another split starts at the same time for the same region P1. (Not sure why this started).-&gt; Rollback happens seeing an already existing node.-&gt; This node gets deleted in rollback and nodeDeleted Event starts.-&gt; In nodeDeleted event the RIT for the region P1 gets deleted.-&gt; Because of this there is no region in RIT.-&gt; Now the first split gets over. Here the problem is we try to transit the node to SPLITTING to SPLIT. But the node even does not exist.But we don take any action on this. We think it is successful.-&gt; Because of this SplitRegionHandler never gets invoked.</description>
      <version>None</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7111" opendate="2012-11-7 00:00:00" fixdate="2012-3-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase zkcli will not start if the zookeeper server chosen to connect to is unavailable</summary>
      <description>there are 3 zookeeper servers in my cluster.s1s2s3after killing s3, i found the hbase zkcli will not start again.it will try to connect to s3 continuely. /11/07 11:01:01 INFO zookeeper.ClientCnxn: Opening socket connection to server s312/11/07 11:01:01 WARN zookeeper.ClientCnxn: Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnectjava.net.ConnectException: Connection refusedfrom the code public String parse(final Configuration c) { // Note that we do not simply grab the property // HConstants.ZOOKEEPER_QUORUM from the HBaseConfiguration because the // user may be using a zoo.cfg file. Properties zkProps = ZKConfig.makeZKProps(c); String host = null; String clientPort = null; for (Entry&lt;Object, Object&gt; entry: zkProps.entrySet()) { String key = entry.getKey().toString().trim(); String value = entry.getValue().toString().trim(); if (key.startsWith("server.") &amp;&amp; host == null) { String[] parts = value.split(":"); host = parts[0]; } else if (key.endsWith("clientPort")) { clientPort = value; } if (host != null &amp;&amp; clientPort != null) break; } return host != null &amp;&amp; clientPort != null? host + ":" + clientPort: null; }the code will choose the fixed zookeeper server (here is the unavailable s3), which leads to the script fails</description>
      <version>0.94.2</version>
      <fixedVersion>0.98.0,0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperMainServerArg.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperMainServerArg.java</file>
    </fixedFiles>
  </bug>
  <bug id="7143" opendate="2012-11-10 00:00:00" fixdate="2012-11-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMetaMigrationRemovingHTD fails when used with Hadoop 0.23/2.x</summary>
      <description>TestMetaMigrationRemovingHTD fails when build is done with "-Dhadoop.profile=23" option. The reason is the changes of defaults in "-mkdir" CLI call. In 0.23/2.x, it doesn't create parent directories by default anymore.The patch will be submitted shortly.</description>
      <version>0.94.2</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.catalog.TestMetaMigrationConvertingToPB.java</file>
    </fixedFiles>
  </bug>
  <bug id="7202" opendate="2012-11-21 00:00:00" fixdate="2012-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Family Store Files are not archived on admin.deleteColumn()</summary>
      <description>using HBaseAdmin.deleteColumn() the files are not archived but deleted directory.This causes problems with snapshots, and other systems that relies on files to be archived.</description>
      <version>0.94.2,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.backup.TestHFileArchiving.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
    </fixedFiles>
  </bug>
  <bug id="7206" opendate="2012-11-22 00:00:00" fixdate="2012-12-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Foreign Exception framework v2 (simplifies and replaces HBASE-6571)</summary>
      <description>This provides a way of sending exceptions from 'external' threads/processes (not the main executing thread) to others that poll cooperatively for external exceptions. Some examples of how this can be used include: having a separate timeout thread that injects an exception when a time limit has elapsed (TimeoutExceptionInjector, was OperationAttemptTimer), or having an exception from an separate process delivered to a local thread. This simplified version is centered around the ExternalException class. Instead of using generics and ErrorListener interfaces, this more straight-forward implementation eliminates many of the builders/factories and generics.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.TestSingleExceptionDispatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.TestOperationAttemptTimer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.TestFaultInjectionPolicies.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.TestFaultInjecting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.TestExceptionOrchestrator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.SimpleErrorListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.PoliciedFaultInjector.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.FaultInjectionPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.ExceptionTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.server.errorhandling.impl.ExceptionForTesting.java</file>
    </fixedFiles>
  </bug>
  <bug id="722" opendate="2008-7-4 00:00:00" fixdate="2008-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shutdown and Compactions</summary>
      <description>Currently I thank the region server closes the region spits in a set order this same order is used in the compaction/spit thread after a startup and all regions need compaction.This causes the shutdown time to take a vary long time depending on what region the server is compacting.I have seen my cluster take more then 60 mins to shutdown waiting for several regions to finish compaction.The problem I am seeing is say the compaction thread is working on a on region 2 of 4 in the list.When the region server get the MSG_REGIONSERVER_QUIESCE from the master it closes in order from 1-4So it closes region 1 and waits for region 2 to finish compaction before closing it.The problem is it never closes region split 3 or 4 while waiting for region 2 to complete the compaction.So example say 3 and 4 regions are waiting to be compacted also. When region 2 is done region 3 start compaction within milliseconds of 2's finished compaction.This happens faster then the region server can close region 3 so the region server hangs around compacting regions until it run out of open regions needing compact.With a lot of regions this could hang some region server a long time to Shutdown the cluster.Solutions 1 or 2 below will work I thank1. Close all regions not currently getting compaction so when the compaction thread complete it has no other regions open to compact 2. Empty the compaction que so it has no other regions to compact when done with the current one.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7256" opendate="2012-12-2 00:00:00" fixdate="2012-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Quick Start Guide shows stable version as 0.95, in the stable folder it is 0.94</summary>
      <description>In the Quick Start Guide for HBase - http://hbase.apache.org/book/quickstart.htmlThe stable version is mentioned as - 0.95 in the line -"Choose a download site from this list of Apache Download Mirrors. Click on the suggested top link. This will take you to a mirror of HBase Releases. Click on the folder named stable and then download the file that ends in .tar.gz to your local filesystem; e.g. hbase-0.95-SNAPSHOT.tar.gz."But in the download folder at - http://apache.techartifact.com/mirror/hbase/stable/The version that can be found is - hbase-0.94.2-security.tar.gz i.e. 0.94So either the documentation or the download folder needs to be updated.</description>
      <version>0.94.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.getting.started.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7342" opendate="2012-12-12 00:00:00" fixdate="2012-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split operation without split key incorrectly finds the middle key in off-by-one error</summary>
      <description>I took a deeper look into issues I was having using region splitting when specifying a region (but not a key for splitting).The midkey calculation is off by one and when there are 2 rows, will pick the 0th one. This causes the firstkey to be the same as midkey and the split will fail. Removing the -1 causes it work correctly, as per the test I've added.Looking into the code here is what goes on:1. Split takes the largest storefile2. It puts all the keys into a 2-dimensional array called blockKeys[][]. Key i resides as blockKeys&amp;#91;i&amp;#93;3. Getting the middle root-level index should yield the key in the middle of the storefile4. In step 3, we see that there is a possible erroneous (-1) to adjust for the 0-offset indexing.5. In a result with where there are only 2 blockKeys, this yields the 0th block key. 6. Unfortunately, this is the same block key that 'firstKey' will be.7. This yields the result in HStore.java:1873 ("cannot split because midkey is the same as first or last row")8. Removing the -1 solves the problem (in this case).</description>
      <version>0.94.1,0.94.2,0.94.3,0.95.2</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="7633" opendate="2013-1-21 00:00:00" fixdate="2013-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric that tracks the current number of used RPC threads on the regionservers</summary>
      <description>One way to detect that you're hitting a "John Wayne" disk&amp;#91;1&amp;#93; would be if we could see when region servers exhausted their RPC handlers. This would also be useful when tuning the cluster for your workload to make sure that reads or writes were not starving the other operations out.&amp;#91;1&amp;#93; http://hbase.apache.org/book.html#bad.disk</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestZKProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.Procedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="7728" opendate="2013-1-31 00:00:00" fixdate="2013-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>deadlock occurs between hlog roller and hlog syncer</summary>
      <description>the hlog roller thread and hlog syncer thread may occur dead lock with the 'flushLock' and 'updateLock', and then cause all 'IPC Server handler' thread blocked on hlog append. the jstack info is as follow :"regionserver60020.logRoller": at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1305) waiting to lock &lt;0x000000067bf88d58&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1283) at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1456) at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:876) at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:657) locked &lt;0x000000067d54ace0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94) at java.lang.Thread.run(Thread.java:662)"regionserver60020.logSyncer": at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1314) waiting to lock &lt;0x000000067d54ace0&gt; (a java.lang.Object) locked &lt;0x000000067bf88d58&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1283) at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1456) at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1235) at java.lang.Thread.run(Thread.java:662)</description>
      <version>0.94.2</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="808" opendate="2008-8-8 00:00:00" fixdate="2008-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MAX_VERSIONS not respected.</summary>
      <description>Below is a report from the list. I confirmed playing in shell that indeed we have this problem. Lets fix for 0.2.1.Hello.I made some tests with HBase 0.2.0 (RC2), focused on insertion andtimestamps behaviour. I had some surprising results, and I was wondering ifpeople using hbase already tried such an usage, and what was theirconclusion.First of all I created a table with the default column attributes, usinghbase shell## TABLEhbase(main):008:0&gt; describe 'proxy-0.2'{NAME =&gt; 'proxy-0.2', IS_ROOT =&gt; 'false', IS_META =&gt; 'false', FAMILIES =&gt;[{NAME =&gt; 'status', BLOOMFILTER =&gt; 'false', IN_MEMORY =&gt; 'false', LENGTH =&gt; '2147483647', BLOCKCACHE =&gt; 'false',VERSIONS =&gt; '3', TTL =&gt; '-1', COMPRESSION =&gt; 'NONE'}, {NAME =&gt; 'header', BLOOMFILTER =&gt; 'false', IN_MEMORY =&gt;'false', LENGTH =&gt; '2147483647',BLOCKCACHE =&gt; 'false', VERSIONS =&gt; '3', TTL =&gt; '-1', COMPRESSION =&gt; 'NONE'},{NAME =&gt; 'bytes', BLOOMFILTER =&gt;'false', IN_MEMORY =&gt; 'false', LENGTH =&gt; '2147483647', BLOCKCACHE =&gt;'false', VERSIONS =&gt; '3', TTL =&gt; '-1', COMPRESSION =&gt; 'NONE'}, {NAME =&gt; 'info', BLOOMFILTER =&gt; 'false', IN_MEMORY =&gt;'false', LENGTH =&gt; '2147483647', BLOCKCACHE =&gt; 'false', VERSIONS =&gt; '3', TTL =&gt; '-1', COMPRESSION =&gt; 'NONE'}]}Test1I make a loop that inserts the same row with different values at differenttimestamps, arbitrary from 1000 incrementing from 10 to 10. I have a methodfor dumping the row history: it makes a query for the last version, andqueries for past version using the current version timestamp minus 1. Notethat my table object is created once for entire program life cycle.## GLOBAL CODE // somewhere in constructor t = new HTable(conf, TABLE_NAME); /** * Dump reversed history of a HBase row, querying for older version * using the max timestamp of all cells -1 until there is no cell returned * @param rowKey */ private void dumpRowVersions(String rowKey) { Logger.log.info("Versions or row : "+rowKey); try { // first query. The newest version of the row RowResult rr = t.getRow(rowKey); int version = 1; long maxTs; do { maxTs = -1; String line = ""; // go through all cells of the row for (Map.Entry en : rr.entrySet()) { long ts = en.getValue().getTimestamp(); maxTs = Math.max(maxTs, ts); line += new String(en.getKey()); line += " =&gt; " + new String(en.getValue().getValue()); line += " ["+ts+"], "; } // remove the last coma and space for smarter output if (line.length() &gt; 0) { line = line.substring(0, line.length()-2); } // prefix result with a version counter and the max timestamp // found in the cells line = "#"+version+" MXTS["+maxTs+"] "+line; if (maxTs != -1) { // there was resulting cell. Continue iteration Logger.log.info(line); // get previous version version++; rr = t.getRow(rowKey, maxTs-1); } } while (maxTs != -1); } catch (IOException ex) { throw new IllegalStateException("Cannot fetch history of row"+rowKey,ex); } }## LOOP CODE long ts = 1000; do { // insert the testrow with a new timestamp BatchUpdate bu = new BatchUpdate("testrow", ts); bu.put("bytes:", ("valbytes ts "+ts).getBytes()); bu.put("status:", ("valstat ts"+ts).getBytes()); t.commit(bu); Logger.log.info("-- Inserted ts "+ts); // dump row history Thread.sleep(70); dumpRowVersions("testrow"); // next iteration in two seconds ts += 10; Thread.sleep(2000); } while (true);## OUTPUT&gt; &gt; Connecting to hbase master... &gt; -- Inserted ts 1000 &gt; Versions or row : testrow &gt; #1 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1010 &gt; Versions or row : testrow &gt; #1 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #2 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1020 &gt; Versions or row : testrow &gt; #1 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #2 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #3 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1030 &gt; Versions or row : testrow &gt; #1 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #2 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #3 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #4 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1040 &gt; Versions or row : testrow &gt; #1 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #2 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #3 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #4 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #5 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1050 &gt; Versions or row : testrow &gt; #1 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #2 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #3 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #4 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #5 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #6 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1060 &gt; Versions or row : testrow &gt; #1 MXTS[1060] bytes: =&gt; valbytes ts 1060 [1060], status: =&gt; valstatts1060 [1060] &gt; #2 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #3 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #4 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #5 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #6 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #7 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1070 &gt; Versions or row : testrow &gt; #1 MXTS[1070] bytes: =&gt; valbytes ts 1070 [1070], status: =&gt; valstatts1070 [1070] &gt; #2 MXTS[1060] bytes: =&gt; valbytes ts 1060 [1060], status: =&gt; valstatts1060 [1060] &gt; #3 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #4 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #5 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #6 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #7 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #8 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1080 &gt; Versions or row : testrow &gt; #1 MXTS[1080] bytes: =&gt; valbytes ts 1080 [1080], status: =&gt; valstatts1080 [1080] &gt; #2 MXTS[1070] bytes: =&gt; valbytes ts 1070 [1070], status: =&gt; valstatts1070 [1070] &gt; #3 MXTS[1060] bytes: =&gt; valbytes ts 1060 [1060], status: =&gt; valstatts1060 [1060] &gt; #4 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #5 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #6 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #7 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #8 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #9 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1090 &gt; Versions or row : testrow &gt; #1 MXTS[1090] bytes: =&gt; valbytes ts 1090 [1090], status: =&gt; valstatts1090 [1090] &gt; #2 MXTS[1080] bytes: =&gt; valbytes ts 1080 [1080], status: =&gt; valstatts1080 [1080] &gt; #3 MXTS[1070] bytes: =&gt; valbytes ts 1070 [1070], status: =&gt; valstatts1070 [1070] &gt; #4 MXTS[1060] bytes: =&gt; valbytes ts 1060 [1060], status: =&gt; valstatts1060 [1060] &gt; #5 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #6 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #7 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #8 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #9 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #10 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000] &gt; -- Inserted ts 1100 &gt; Versions or row : testrow &gt; #1 MXTS[1100] bytes: =&gt; valbytes ts 1100 [1100], status: =&gt; valstatts1100 [1100] &gt; #2 MXTS[1090] bytes: =&gt; valbytes ts 1090 [1090], status: =&gt; valstatts1090 [1090] &gt; #3 MXTS[1080] bytes: =&gt; valbytes ts 1080 [1080], status: =&gt; valstatts1080 [1080] &gt; #4 MXTS[1070] bytes: =&gt; valbytes ts 1070 [1070], status: =&gt; valstatts1070 [1070] &gt; #5 MXTS[1060] bytes: =&gt; valbytes ts 1060 [1060], status: =&gt; valstatts1060 [1060] &gt; #6 MXTS[1050] bytes: =&gt; valbytes ts 1050 [1050], status: =&gt; valstatts1050 [1050] &gt; #7 MXTS[1040] bytes: =&gt; valbytes ts 1040 [1040], status: =&gt; valstatts1040 [1040] &gt; #8 MXTS[1030] bytes: =&gt; valbytes ts 1030 [1030], status: =&gt; valstatts1030 [1030] &gt; #9 MXTS[1020] bytes: =&gt; valbytes ts 1020 [1020], status: =&gt; valstatts1020 [1020] &gt; #10 MXTS[1010] bytes: =&gt; valbytes ts 1010 [1010], status: =&gt; valstatts1010 [1010] &gt; #11 MXTS[1000] bytes: =&gt; valbytes ts 1000 [1000], status: =&gt; valstatts1000 [1000]Despite the VERSIONS parameter of the columns (3) it seems that all versionsare stored. Question: is there some garbage collector process that removes the oldversions ? if yes, when does it take place ?</description>
      <version>None</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestGet2.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8080" opendate="2013-3-12 00:00:00" fixdate="2013-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>refactor default compactor to make its parts easier to reuse</summary>
      <description>Refactor default compactor to make its parts easier to reuse. To make eventual HBASE-7967 patch smaller.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="9393" opendate="2013-8-30 00:00:00" fixdate="2013-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region Server fails to properly close socket resulting in many CLOSE_WAIT to Data Nodes</summary>
      <description>HBase dose not close a dead connection with the datanode.This resulting in over 60K CLOSE_WAIT and at some point HBase can not connect to the datanode because too many mapped sockets from one host to another on the same port.The example below is with low CLOSE_WAIT count because we had to restart hbase to solve the porblem, later in time it will incease to 60-100K sockets on CLOSE_WAIT&amp;#91;root@hd2-region3 ~&amp;#93;# netstat -nap |grep CLOSE_WAIT |grep 21592 |wc -l13156&amp;#91;root@hd2-region3 ~&amp;#93;# ps -ef |grep 21592root 17255 17219 0 12:26 pts/0 00:00:00 grep 21592hbase 21592 1 17 Aug29 ? 03:29:06 /usr/java/jdk1.6.0_26/bin/java -XX:OnOutOfMemoryError=kill -9 %p -Xmx8000m -ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode -Dhbase.log.dir=/var/log/hbase -Dhbase.log.file=hbase-hbase-regionserver-hd2-region3.swnet.corp.log ...</description>
      <version>0.94.2,0.98.0,1.0.1.1,1.1.2</version>
      <fixedVersion>1.4.0,1.3.2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FSDataInputStreamWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug id="9708" opendate="2013-10-3 00:00:00" fixdate="2013-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Snapshot Name Error Message</summary>
      <description>The output for snapshots when you enter an invalid snapshot name talks about "User-space table names" instead of "Snapshot names". The error message should say "Snapshot names can only contain...".Here is an example of the output:hbase(main):001:0&gt; snapshot 'user', 'asdf asdf'ERROR: java.lang.IllegalArgumentException: Illegal character &lt;32&gt; at 4. User-space table names can only contain 'word characters': i.e. [a-zA-Z_0-9-.]: asdf asdfHere is some help for this command:Take a snapshot of specified table. Examples: hbase&gt; snapshot 'sourceTable', 'snapshotName'</description>
      <version>0.94.2</version>
      <fixedVersion>0.96.2,0.98.1,0.99.0,0.94.18</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.TableName.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.snapshot.ClientSnapshotDescriptionUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
