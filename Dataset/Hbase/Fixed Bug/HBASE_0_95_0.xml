<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="10101" opendate="2013-12-7 00:00:00" fixdate="2013-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>testOfflineRegionReAssginedAfterMasterRestart times out sometimes.</summary>
      <description>Sometimes, I got this test timed out. The log is attached. It could be because the new cluster takes a while to process the dead server, or assign meta.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.1,0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRestartCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="11168" opendate="2014-5-14 00:00:00" fixdate="2014-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[docs] Remove references to RowLocks in post 0.96 docs.</summary>
      <description>Row locks were removed in 0.95 by HBASE-7315 / HBASE-2332. There are a few vestiges of them in the docs. Remove.</description>
      <version>0.95.0</version>
      <fixedVersion>0.99.0,0.98.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4210" opendate="2011-8-17 00:00:00" fixdate="2011-2-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow coprocessor to interact with batches per region sent from a client</summary>
      <description>Currently the coprocessor write hooks - {pre|post}{Put|Delete} - are strictly one row|cell operations.It might be a good idea to allow a coprocessor to deal with batches of puts and deletes as they arrive from the client.</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.98.0,0.94.6,0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMiniBatchOperationInProgress.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MiniBatchOperationInProgress.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="4284" opendate="2011-8-29 00:00:00" fixdate="2011-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>document permissions that need to be set on importtsv output before completebulkload</summary>
      <description>I am using HBase 0.94 from CDH3u1.After running importtsv using the -Dimporttsv.bulk.output=&lt;output dir&gt; option, I find that completebulkload fails due to hbase not having write permissions on the contents of the output dir that importtsv wrote. I have to manually set write permissions on these contents before I can run completebulkload successfully.Ideally, I should not have to do that (set the permissions manually). Given that I do, this should at least be documented as a limitation of the importtsv utility.</description>
      <version>0.90.4,0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4285" opendate="2011-8-29 00:00:00" fixdate="2011-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partitions file created in user&amp;#39;s home directory by importtsv</summary>
      <description>I am using HBase 0.94 from CDH3u1.After running importtsv, I find that a temporary partitions_* file is written to my user home directory in HDFS. This file should really be deleted automatically when it is no longer needed.</description>
      <version>0.90.4,0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestImportTsv.java</file>
    </fixedFiles>
  </bug>
  <bug id="6134" opendate="2012-5-30 00:00:00" fixdate="2012-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement for split-worker to speed up distributed log splitting</summary>
      <description>First，we do the test between local-master-splitting and distributed-log-splittingEnvironment：34 hlog files, 5 regionservers,(after kill one, only 4 rs do ths splitting work), 400 regions in one hlog filelocal-master-split:60s+distributed-log-splitting:165s+In fact, in our production environment, distributed-log-splitting also took 60s with 30 regionservers for 34 hlog files (regionserver may be in high load)We found split-worker split one log file took about 20s(30ms~50ms per writer.close(); 10ms per create writers )I think we could do the improvement for this:Parallelizing the create and close writers in threadsIn the patch, change the logic for distributed-log-splitting same as the local-master-splitting and parallelizing the close in threads.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug id="6135" opendate="2012-5-30 00:00:00" fixdate="2012-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Style the Web UI to use Twitter&amp;#39;s Bootstrap.</summary>
      <description>Our web ui has lagged a little bit behind. While it's not a huge deal, it is one of the first things that new people see. As such styling it a little bit better would put a good foot forward.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.common.TaskMonitorTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="6583" opendate="2012-8-14 00:00:00" fixdate="2012-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance Hbase load test tool to automatically create column families if not present</summary>
      <description>The load test tool currently disables the table and applies any changes to the cf descriptor if any, but does not create the cf if not present.</description>
      <version>None</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.LoadTestTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="6643" opendate="2012-8-23 00:00:00" fixdate="2012-8-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accept encoded region name in compacting/spliting region from shell</summary>
      <description>Sometimes, the region name has binary characters. When compacting/splitting it from shell, the region name is not recognized. If we can support encoded region name, it will make things easier.</description>
      <version>None</version>
      <fixedVersion>0.94.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="6945" opendate="2012-10-4 00:00:00" fixdate="2012-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compilation errors when using non-Sun JDKs to build HBase-0.94</summary>
      <description>When using IBM Java 7 to build HBase-0.94.1, the following comilation error is seen. &amp;#91;INFO&amp;#93; -------------------------------------------------------------&amp;#91;ERROR&amp;#93; COMPILATION ERROR : &amp;#91;INFO&amp;#93; -------------------------------------------------------------&amp;#91;ERROR&amp;#93; /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java:&amp;#91;23,25&amp;#93; error: package com.sun.management does not exist&amp;#91;ERROR&amp;#93; /home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java:&amp;#91;46,25&amp;#93; error: cannot find symbol&amp;#91;ERROR&amp;#93; symbol: class UnixOperatingSystemMXBean location: class ResourceAnalyzer/home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java:&amp;#91;75,29&amp;#93; error: cannot find symbol&amp;#91;ERROR&amp;#93; symbol: class UnixOperatingSystemMXBean location: class ResourceAnalyzer/home/hadoop/hbase-0.94/src/test/java/org/apache/hadoop/hbase/ResourceChecker.java:&amp;#91;76,23&amp;#93; error: cannot find symbol&amp;#91;INFO&amp;#93; 4 errors &amp;#91;INFO&amp;#93; -------------------------------------------------------------&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------&amp;#91;INFO&amp;#93; BUILD FAILURE&amp;#91;INFO&amp;#93; ------------------------------------------------------------------------ I have a patch available which should work for all JDKs including Sun. I am in the process of testing this patch. Preliminary tests indicate the build is working fine with this patch. I will post this patch when I am done testing.</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.ResourceCheckerJUnitListener.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.OSMXBean.java</file>
    </fixedFiles>
  </bug>
  <bug id="7151" opendate="2012-11-12 00:00:00" fixdate="2012-11-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better log message for Per-CF compactions</summary>
      <description>A coworker pointed out that in HBASE-4913 it would be nice to include the column family in the log message for a per-CF compaction.</description>
      <version>None</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7507" opendate="2013-1-7 00:00:00" fixdate="2013-2-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make memstore flush be able to retry after exception</summary>
      <description>We will abort regionserver if memstore flush throws exception.I thinks we could do retry to make regionserver more stable because file system may be not ok in a transient time. e.g. Switching namenode in the NamenodeHA environmentHRegion#internalFlushcache(){...try {...}catch(Throwable t){DroppedSnapshotException dse = new DroppedSnapshotException("region: " + Bytes.toStringBinary(getRegionName()));dse.initCause(t);throw dse;}...}MemStoreFlusher#flushRegion(){...region.flushcache();... try {}catch(DroppedSnapshotException ex){server.abort("Replay of HLog required. Forcing server shutdown", ex);}...}</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="7669" opendate="2013-1-25 00:00:00" fixdate="2013-1-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ROOT region wouldn&amp;#39;t be handled by PRI-IPC-Handler</summary>
      <description>RPC reuqest about ROOT region should be handled by PRI-IPC-Handler, just the same as META region</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7671" opendate="2013-1-25 00:00:00" fixdate="2013-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flushing memstore again after last failure could cause data loss</summary>
      <description>See the following logs first:2013-01-23 18:58:38,801 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=9746535080, memsize=101.8m, into tmp file hdfs://dw77.kgb.sqa.cm4:9900/hbase-test3/writetest1/8dc14e35b4d7c0e481e0bb30849cff7d/.tmp/bebeeecc56364b6c8126cf1dc6782a252013-01-23 18:58:41,982 WARN org.apache.hadoop.hbase.regionserver.MemStore: Snapshot called again without clearing previous. Doing nothing. Another ongoing flush or did we fail last attempt?2013-01-23 18:58:43,274 INFO org.apache.hadoop.hbase.regionserver.Store: Flushed , sequenceid=9746599334, memsize=101.8m, into tmp file hdfs://dw77.kgb.sqa.cm4:9900/hbase-test3/writetest1/8dc14e35b4d7c0e481e0bb30849cff7d/.tmp/4eede32dc469480bb3d469aaff332313The first time memstore flush is failed when commitFile()(Logged the first edit above), then trigger server abort, but another flush is coming immediately(could caused by move/split,Logged the third edit above) and successful.For the same memstore's snapshot, we get different sequenceid, it causes data loss when replaying log editsSee details from the unit test case in the patch</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="7723" opendate="2013-1-30 00:00:00" fixdate="2013-3-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove NameNode URI from ZK splitlogs</summary>
      <description>When moving to HDFS HA or removing HA we end up changing the NN namespace. This can cause the HMaster not to start up fully due to trying to split phantom HLogs pointing to the wrong FS - java.lang.IllegalArgumentException: Wrong FS: error messages. The HLogs in question might not even be on HDFS anymore. You have to go in a manually clear out the ZK splitlogs directory to get HBase to properly boot up.</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.98.0,0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="7763" opendate="2013-2-4 00:00:00" fixdate="2013-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compactions not sorting based on size anymore.</summary>
      <description>Currently compaction selection is not sorting based on size. This causes selection to choose larger files to re-write than are needed when bulk loads are involved.</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="7768" opendate="2013-2-5 00:00:00" fixdate="2013-2-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>zkcluster in local mode not seeing configurations in hbase-{site|default}.xml</summary>
      <description>in case of local mode we are creating mini zk cluster with default constructor if (LocalHBaseCluster.isLocal(conf)) { final MiniZooKeeperCluster zooKeeperCluster = new MiniZooKeeperCluster(); public MiniZooKeeperCluster() { this(new Configuration()); }which is not reading any configurations in hbase-site|default.xmlI think we can pass configuration object to MiniZooKeeperCluster</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
    </fixedFiles>
  </bug>
  <bug id="7829" opendate="2013-2-12 00:00:00" fixdate="2013-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>zookeeper kerberos conf keytab and principal parameters interchanged</summary>
      <description></description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="7869" opendate="2013-2-18 00:00:00" fixdate="2013-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide way to not start LogSyncer thread</summary>
      <description>In our usecase we want all the Mutations on all the tables to be immediately written and synced to WAL. We dont want the LogSyncer thread to be running in such a case.In minbatch operation we write entries to WAL and we use postWALWrite and write some more WAL entries(Only write no sync). We want all these written data to be synced as one unit (at the sync step in doMiniBatchOperation) But if the LogSyncer thread is running in the system it can come and do the sync at any point in time which we dont want in our case.</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.98.0,0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="7876" opendate="2013-2-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Got exception when manually triggers a split on an empty region</summary>
      <description>We should allow a region to split successfully even if it does not yet have storefiles.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug id="7913" opendate="2013-2-23 00:00:00" fixdate="2013-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure Rest server should login before getting an instance of Rest servlet</summary>
      <description>Fails with exceptionavax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)] at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:194) at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:139) at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupSaslConnection(SecureClient.java:194) at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.access$500(SecureClient.java:92) at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:302) at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection$2.run(SecureClient.java:299) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1178) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.util.Methods.call(Methods.java:37) at org.apache.hadoop.hbase.security.User.call(User.java:590) at org.apache.hadoop.hbase.security.User.access$700(User.java:51) at org.apache.hadoop.hbase.security.User$SecureHadoopUser.runAs(User.java:444) at org.apache.hadoop.hbase.ipc.SecureClient$SecureConnection.setupIOstreams(SecureClient.java:298) at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1124) at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:974) at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Invoker.invoke(SecureRpcEngine.java:104) at $Proxy5.getProtocolVersion(Unknown Source) at org.apache.hadoop.hbase.ipc.SecureRpcEngine.getProxy(SecureRpcEngine.java:146) at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getMaster(HConnectionManager.java:711) at org.apache.hadoop.hbase.client.HBaseAdmin.&lt;init&gt;(HBaseAdmin.java:116) at org.apache.hadoop.hbase.rest.RESTServlet.&lt;init&gt;(RESTServlet.java:74) at org.apache.hadoop.hbase.rest.RESTServlet.getInstance(RESTServlet.java:57) at org.apache.hadoop.hbase.rest.Main.main(Main.java:81)Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt) at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:130) at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106) at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172) at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:209) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:195) at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:162) at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:175)</description>
      <version>0.94.6,0.95.0</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7954" opendate="2013-2-27 00:00:00" fixdate="2013-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the retrying logic of memstore flushes to avoid extra sleep</summary>
      <description>Matteo pointed out:"We can avoid the redundant sleep in the retrying logic."</description>
      <version>0.94.5,0.95.0</version>
      <fixedVersion>0.98.0,0.94.12</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="7968" opendate="2013-3-1 00:00:00" fixdate="2013-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Packaging of Trunk and 0.95 does not create the dependent jars in the lib folder</summary>
      <description>After recent changes to trunk and 0.95 branch when i try to build and package, i do not find the dependent jars in the lib folder.Prior to the changes, it was working fine.Am not a maven expert. Will try to see what is going wrong here.</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.hadoop-two-compat.xml</file>
      <file type="M">src.assembly.hadoop-one-compat.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop1-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="797" opendate="2008-8-5 00:00:00" fixdate="2008-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>IllegalAccessError running RowCounter</summary>
      <description>Below is from Billy Pearson up on the list:Billy Pearson wrote:&gt; I get this when I run RowCounter in the hbase jar&gt;&gt; java.lang.IllegalAccessError: tried to access method org.apache.hadoop.ipc.Client.incCount()V from class org.apache.hadoop.ipc.HBaseClient&gt; at org.apache.hadoop.ipc.HBaseClient.incCount(HBaseClient.java:39)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC$ClientCache.getClient(HbaseRPC.java:179)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC$ClientCache.access$200(HbaseRPC.java:156)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC$Invoker.&lt;init&gt;(HbaseRPC.java:224)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:336)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:327)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC.getProxy(HbaseRPC.java:364)&gt; at org.apache.hadoop.hbase.ipc.HbaseRPC.waitForProxy(HbaseRPC.java:302)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.getHRegionConnection(HConnectionManager.java:764)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRootRegion(HConnectionManager.java:815)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:457)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:431)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:510)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:467)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:431)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegionInMeta(HConnectionManager.java:510)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:471)&gt; at org.apache.hadoop.hbase.client.HConnectionManager$TableServers.locateRegion(HConnectionManager.java:431)&gt; at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:125)&gt; at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:110)&gt; at org.apache.hadoop.hbase.mapred.TableInputFormat.configure(TableInputFormat.java:60)&gt; at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:58)&gt; at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:82)&gt; at org.apache.hadoop.mapred.JobConf.getInputFormat(JobConf.java:400)&gt; at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:705)&gt; at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:973)&gt; at com.compspy.mapred.RowCounter.run(RowCounter.java:111)&gt; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)&gt; at com.compspy.mapred.RowCounter.main(RowCounter.java:126)&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&gt; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&gt; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&gt; at java.lang.reflect.Method.invoke(Method.java:597)&gt; at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)&gt; at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)&gt; at com.compspy.mapred.Driver.main(Driver.java:24)&gt; at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&gt; at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)&gt; at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)&gt; at java.lang.reflect.Method.invoke(Method.java:597)&gt; at org.apache.hadoop.util.RunJar.main(RunJar.java:155)&gt; at org.apache.hadoop.mapred.JobShell.run(JobShell.java:194)&gt; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)&gt; at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)&gt; at org.apache.hadoop.mapred.JobShell.main(JobShell.java:220)Sebastien Rainville just had a related issue. J-D investigating found a workaround. Adding hbase.jar to $HADOOP_HOME/conf/hadoop-env.sh#HADOOP_CLASSPATH</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
    </fixedFiles>
  </bug>
  <bug id="7977" opendate="2013-3-2 00:00:00" fixdate="2013-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Online merge should acquire table lock</summary>
      <description>Once online merge (HBASE-7403) is in, we should ensure that we acquire a table write lock during the merge.</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionMergeRequest.java</file>
    </fixedFiles>
  </bug>
  <bug id="7982" opendate="2013-3-2 00:00:00" fixdate="2013-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestReplicationQueueFailover* runs for a minute, spews 3/4million lines complaining &amp;#39;Filesystem closed&amp;#39;, has an NPE, and still passes?</summary>
      <description>I was trying to look at why the odd time Hudson OOMEs trying to make a report on 0.95 build #4 https://builds.apache.org/job/hbase-0.95/4/console:ERROR: Failed to archive test reportshudson.util.IOException2: remote file operation failed: /home/jenkins/jenkins-slave/workspace/hbase-0.95 at hudson.remoting.Channel@151a4e3e:ubuntu3 at hudson.FilePath.act(FilePath.java:861) at hudson.FilePath.act(FilePath.java:838) at hudson.tasks.junit.JUnitParser.parse(JUnitParser.java:87) at ...Caused by: java.lang.OutOfMemoryError: Java heap space at java.nio.HeapCharBuffer.&lt;init&gt;(HeapCharBuffer.java:57) at java.nio.CharBuffer.allocate(CharBuffer.java:329) at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:792) at java.nio.charset.Charset.decode(Charset.java:791) at hudson.tasks.junit.SuiteResult.&lt;init&gt;(SuiteResult.java:215)...We are trying to allocate a big buffer and failing.Looking at reports being generated, we have quite a few that are &gt; 10MB in size:durruti:0.95 stack$ find hbase-* -type f -size +10000k -exec ls -la {} \;-rw-r--r--@ 1 stack staff 11126492 Feb 27 06:14 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.backup.TestHFileArchiving-output.txt-rw-r--r--@ 1 stack staff 13296009 Feb 27 05:47 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.TestFromClientSide3-output.txt-rw-r--r--@ 1 stack staff 10541898 Feb 27 05:47 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.TestMultiParallel-output.txt-rw-r--r--@ 1 stack staff 25344601 Feb 27 05:51 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient-output.txt-rw-r--r--@ 1 stack staff 17966969 Feb 27 06:12 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction-output.txt-rw-r--r--@ 1 stack staff 17699068 Feb 27 06:09 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit-output.txt-rw-r--r--@ 1 stack staff 17701832 Feb 27 06:07 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.regionserver.wal.TestHLogSplitCompressed-output.txt-rw-r--r--@ 1 stack staff 717853709 Feb 27 06:17 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.replication.TestReplicationQueueFailover-output.txt-rw-r--r--@ 1 stack staff 563616793 Feb 27 06:17 hbase-server/target/surefire-reports/org.apache.hadoop.hbase.replication.TestReplicationQueueFailoverCompressed-output.txt... with TestReplicationQueueFailover* being order of magnitude bigger than the others.Looking in the test I see both spewing between 800 and 900 thousand lines in about a minute. Here is their fixation:8908998 2013-02-27 06:17:48,176 ERROR [RegionServer:1;hemera.apache.org,35712,1361945801803.logSyncer] wal.FSHLog$LogSyncer(1012): Error while syncing, requesting close of hlog.8908999 java.io.IOException: Filesystem closed8909000 ,...at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:319)8909001 ,...at org.apache.hadoop.hdfs.DFSClient.access$1200(DFSClient.java:78)8909002 ,...at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3843)8909003 ,...at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)8909004 ,...at org.apache.hadoop.io.SequenceFile$Writer.syncFs(SequenceFile.java:999)8909005 ,...at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:248)8909006 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1120)8909007 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1058)8909008 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1228)8909009 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog$LogSyncer.run(FSHLog.java:1010)8909010 ,...at java.lang.Thread.run(Thread.java:722)8909011 2013-02-27 06:17:48,176 FATAL [RegionServer:1;hemera.apache.org,35712,1361945801803.logSyncer] wal.FSHLog(1140): Could not sync. Requesting close of hlog8909012 java.io.IOException: Filesystem closed8909013 ,...at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:319)8909014 ,...at org.apache.hadoop.hdfs.DFSClient.access$1200(DFSClient.java:78)8909015 ,...at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.sync(DFSClient.java:3843)8909016 ,...at org.apache.hadoop.fs.FSDataOutputStream.sync(FSDataOutputStream.java:97)8909017 ,...at org.apache.hadoop.io.SequenceFile$Writer.syncFs(SequenceFile.java:999)8909018 ,...at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.sync(SequenceFileLogWriter.java:248)8909019 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1120)8909020 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.syncer(FSHLog.java:1058)8909021 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog.sync(FSHLog.java:1228)8909022 ,...at org.apache.hadoop.hbase.regionserver.wal.FSHLog$LogSyncer.run(FSHLog.java:1010)8909023 ,...at java.lang.Thread.run(Thread.java:722)...These tests are 'succeeding'?I also see in both: 3891 java.lang.NullPointerException 3892 ,...at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.seek(SequenceFileLogReader.java:261) 3893 ,...at org.apache.hadoop.hbase.replication.regionserver.ReplicationHLogReaderManager.seek(ReplicationHLogReaderManager.java:103) 3894 ,...at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.readAllEntriesToReplicateOrNextFile(ReplicationSource.java:415) 3895 ,...at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:333)</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7994" opendate="2013-3-4 00:00:00" fixdate="2013-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable unit tests under hbase-examples; they fail too often up on jenkins</summary>
      <description>The examples fail up on jenkins w/ some frequency. Disable them for the moment till we have a bunch of blue builds again then looking into getting them fixed again. Part of the problem is that when they fail there is no log. They run fine for me here locally.https://builds.apache.org/view/G-L/view/HBase/job/hbase-0.95/21/</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestZooKeeperScanPolicyObserver.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestRowCountEndpoint.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestBulkDeleteProtocol.java</file>
    </fixedFiles>
  </bug>
  <bug id="8" opendate="2008-1-23 00:00:00" fixdate="2008-2-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase] Delete table does not remove the table directory in the FS</summary>
      <description>I deleted a table but its directory stays behind in hdfs. See listing below. TestTable is the table removed.$ ./bin/hadoop fs -lsr //hbase123 &lt;dir&gt; 2008-01-23 00:47 rwxr-xr-x stack supergroup/hbase123/-ROOT- &lt;dir&gt; 2008-01-22 22:56 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052 &lt;dir&gt; 2008-01-22 05:10 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info &lt;dir&gt; 2008-01-22 05:10 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info/info &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info/info/2697897537613165523 &lt;r 3&gt; 9 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/-ROOT-/70236052/info/info/6044008799898415360 &lt;r 3&gt; 9 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/2697897537613165523 &lt;dir&gt; 2008-01-22 22:56 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/2697897537613165523/data &lt;r 3&gt; 336 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/2697897537613165523/index &lt;r 3&gt; 232 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/6044008799898415360 &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/6044008799898415360/data &lt;r 3&gt; 230 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/-ROOT-/70236052/info/mapfiles/6044008799898415360/index &lt;r 3&gt; 230 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/-ROOT-/compaction.dir &lt;dir&gt; 2008-01-22 22:56 rwxr-xr-x stack supergroup/hbase123/.META. &lt;dir&gt; 2008-01-22 19:12 rwxr-xr-x stack supergroup/hbase123/.META./1028785192 &lt;dir&gt; 2008-01-22 05:10 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info &lt;dir&gt; 2008-01-22 05:10 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/info &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/info/1626684907024277671 &lt;r 3&gt; 9 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/info/1714016229549960649 &lt;r 3&gt; 9 2008-01-22 19:12 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/info/8042950873311244716 &lt;r 3&gt; 9 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/mapfiles/1626684907024277671 &lt;dir&gt; 2008-01-23 00:46 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/mapfiles/1626684907024277671/data &lt;r 3&gt; 430 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles/1626684907024277671/index &lt;r 3&gt; 245 2008-01-23 00:46 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles/1714016229549960649 &lt;dir&gt; 2008-01-22 19:12 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/mapfiles/1714016229549960649/data &lt;r 3&gt; 1192 2008-01-22 19:12 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles/1714016229549960649/index &lt;r 3&gt; 247 2008-01-22 19:12 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles/8042950873311244716 &lt;dir&gt; 2008-01-22 22:56 rwxr-xr-x stack supergroup/hbase123/.META./1028785192/info/mapfiles/8042950873311244716/data &lt;r 3&gt; 430 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/.META./1028785192/info/mapfiles/8042950873311244716/index &lt;r 3&gt; 245 2008-01-22 22:56 rw-r--r-- stack supergroup/hbase123/.META./compaction.dir &lt;dir&gt; 2008-01-22 19:12 rwxr-xr-x stack supergroup/hbase123/TestTable &lt;dir&gt; 2008-01-23 00:48 rwxr-xr-x stack supergroup/hbase123/TestTable/compaction.dir &lt;dir&gt; 2008-01-22 18:52 rwxr-xr-x stack supergroup/hbase123/hbase.version &lt;r 3&gt; 5 2008-01-22 05:10 rw-r--r-- stack supergroup/hbase123/log_XX.XX.XX.140_1201049243642_60020 &lt;dir&gt; 2008-01-23 00:47 rwxr-xr-x stack supergroup/hbase123/log_XX.XX.XX.140_1201049243642_60020/hlog.dat.000 &lt;r 3&gt; 0 2008-01-23 00:47 rw-r--r-- stack supergroup</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableDelete.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8002" opendate="2013-3-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make TimeOut Management for Assignment optional in master and regionservers</summary>
      <description>See HBASE-7327</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8004" opendate="2013-3-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creating an existing table from Shell does not throw TableExistsException</summary>
      <description>When i try to create a same table from shell i don't get TableExistsException instead i getERROR: cannot load Java class org.apache.hadoop.hbase.TableNotFoundExceptionHere is some help for this command:Creates a table. Pass a table name, and a set of column familyspecifications (at least one), and, optionally, table configuration.Column specification can be a simple string (name), or a dictionary(dictionaries are described below in main help output), necessarilyincluding NAME attribute.Examples: hbase&gt; create 't1', {NAME =&gt; 'f1', VERSIONS =&gt; 5} hbase&gt; create 't1', {NAME =&gt; 'f1'}, {NAME =&gt; 'f2'}, {NAME =&gt; 'f3'} hbase&gt; # The above in shorthand would be the following:</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.ruby.shell.commands.rb</file>
      <file type="M">bin.region.status.rb</file>
      <file type="M">bin.region.mover.rb</file>
    </fixedFiles>
  </bug>
  <bug id="8008" opendate="2013-3-5 00:00:00" fixdate="2013-3-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix DirFilter usage to be consistent</summary>
      <description>Currently the DirFilter automatically filters out HConstants.HBASE_NON_USER_TABLE_DIRS, which is not needed in most cases. We should switch the usage so people actually using a directory filter and then have a special filter when looking for tables specifically.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8041" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebUI doesn&amp;#39;t display snapshots correctly</summary>
      <description>It seems that the jamon code got some problem during the merge.The connection is closed too early and the snapshots are not loaded.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8042" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Offline Meta Repair no longer points to the correct location</summary>
      <description>B.4.3. Special case: Root and META are corrupt.The most drastic corruption scenario is the case where the ROOT or META is corrupted and HBase will not start. In this case you can use the OfflineMetaRepair tool create new ROOT and META regions and tables. This tool assumes that HBase is offline. It then marches through the existing HBase home directory, loads as much information from region metadata files (.regioninfo files) as possible from the file system. If the region metadata has proper table integrity, it sidelines the original root and meta table directories, and builds new ones with pointers to the region directories and their data.$ ./bin/hbase org.apache.hadoop.hbase.util.OfflineMetaRepairThe path should be org.apache.hadoop.hbase.util.hbck.OfflineMetaRepair</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8043" opendate="2013-3-8 00:00:00" fixdate="2013-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a few javadoc warnings...</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.javadoc.org.apache.hadoop.hbase.thrift.package.html</file>
    </fixedFiles>
  </bug>
  <bug id="8058" opendate="2013-3-9 00:00:00" fixdate="2013-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade site plugin; fix assembly doc up on jenkins builds</summary>
      <description>Up on jenkins, we currently make assemblies but there no doc in them. The site goal runs last. You can't run it anywhere else else build fails. Upgrading the site plugin helps. Upgrading site plugin I notice that there are a bunch of extra reports generated that would be no harm showing on the web site; e.g. dependencies transitively included, what dependencies we have, etc. This issue is about upgrading site plugin to fix jenkins assemblies and to expose reports we are generating anyways (at least one report is new w/ the info-report upgrade from earlier today).</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8060" opendate="2013-3-10 00:00:00" fixdate="2013-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Num compacting KVs" diverges from "num compacted KVs" over time</summary>
      <description>I have been running what amounts to an ingestion test for a day or so. This is an all-in-one cluster launched with './bin/hbase master start' from sources. In the RS stats on the master UI, the "num compacting KVs" has diverged from "num compacted KVs" even though compaction has been completed from perspective of selection, no compaction tasks are running on the RS. I think this could be confusing &amp;#8211; is compaction happening or not?Or maybe I'm misunderstanding what this is supposed to show?</description>
      <version>0.94.6,0.95.0,0.95.2</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionProgress.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
    </fixedFiles>
  </bug>
  <bug id="8066" opendate="2013-3-11 00:00:00" fixdate="2013-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Admin.isTableAvailable() for a given table along with splitkeys</summary>
      <description>As part of HBASE-5583 if the master reboots during creation of table there is a chance that the table gets created with partial split keys. This api helps to check if the table was created with the required number of splitkeys.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8108" opendate="2013-3-14 00:00:00" fixdate="2013-3-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add m2eclispe lifecycle mapping to hbase-common</summary>
      <description>The maven-antrun-plugin execution doesn't have a default mapping in m2eclipse, so if you import the project into eclipse, you will get an error that the mapping is undefined. All that's needed is to define an execution via the org.eclipse.m2 lifecycle-mapping plugin - it doesn't actually affect the usual maven build at all.</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8123" opendate="2013-3-15 00:00:00" fixdate="2013-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace HashMap/HashSet with TreeMap/TreeSet where byte[] is used as key</summary>
      <description>We still have code using HashMap/HashSet with byte[] as key</description>
      <version>0.94.5,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.PutCombiner.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.mapreduce.IndexBuilder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTableMultiplexer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8130" opendate="2013-3-17 00:00:00" fixdate="2013-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>initialize TableLockManager before initializing AssignmentManager</summary>
      <description>Getting NullPointerException while recovering disabling/enabling tables in AM.AM.tableLockManager is not pointing to HM.tableLockManager initalized after AM initialization.So its always null.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="8133" opendate="2013-3-17 00:00:00" fixdate="2013-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid assign for disabling table regions(OPENING/PENDING_OPEN) in SSH</summary>
      <description>Disabling table regions which are in PENDING_OPEN or OPENING on dead server are getting assigned then unassiging. We can avoid this by just mark OFFLINE for the regions,any way znodes of the transitions got deleted as part of am.processServerShutdown(serverName).</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8141" opendate="2013-3-18 00:00:00" fixdate="2013-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove accidental uses of org.mortbay.log.Log</summary>
      <description>Remove accidental uses of org.mortbay.log.Log. Eclipse autocomplete is probably the culprit.</description>
      <version>0.94.6,0.95.0,0.95.2</version>
      <fixedVersion>0.94.6,0.95.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestDelayedRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileSeek.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.prefixtree.builder.TestTreeDepth.java</file>
    </fixedFiles>
  </bug>
  <bug id="8143" opendate="2013-3-19 00:00:00" fixdate="2013-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase on Hadoop 2 with local short circuit reads (ssr) causes OOM</summary>
      <description>We've run into an issue with HBase 0.94 on Hadoop2, with SSR turned on that the memory usage of the HBase process grows to 7g, on an -Xmx3g, after some time, this causes OOM for the RSs. Upon further investigation, I've found out that we end up with 200 regions, each having 3-4 store files open. Under hadoop2 SSR, BlockReaderLocal allocates DirectBuffers, which is unlike HDFS 1 where there is no direct buffer allocation. It seems that there is no guards against the memory used by local buffers in hdfs 2, and having a large number of open files causes multiple GB of memory to be consumed from the RS process. This issue is to further investigate what is going on. Whether we can limit the memory usage in HDFS, or HBase, and/or document the setup. Possible mitigation scenarios are: Turn off SSR for Hadoop 2 Ensure that there is enough unallocated memory for the RS based on expected # of store files Ensure that there is lower number of regions per region server (hence number of open files)Stack trace:org.apache.hadoop.hbase.DroppedSnapshotException: region: IntegrationTestLoadAndVerify,yC^P\xD7\x945\xD4,1363388517630.24655343d8d356ef708732f34cfe8946. at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1560) at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1439) at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1380) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:449) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushOneForGlobalPressure(MemStoreFlusher.java:215) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$500(MemStoreFlusher.java:63) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:237) at java.lang.Thread.run(Thread.java:662)Caused by: java.lang.OutOfMemoryError: Direct buffer memory at java.nio.Bits.reserveMemory(Bits.java:632) at java.nio.DirectByteBuffer.&lt;init&gt;(DirectByteBuffer.java:97) at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:288) at org.apache.hadoop.hdfs.util.DirectBufferPool.getBuffer(DirectBufferPool.java:70) at org.apache.hadoop.hdfs.BlockReaderLocal.&lt;init&gt;(BlockReaderLocal.java:315) at org.apache.hadoop.hdfs.BlockReaderLocal.newBlockReader(BlockReaderLocal.java:208) at org.apache.hadoop.hdfs.DFSClient.getLocalBlockReader(DFSClient.java:790) at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:888) at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:455) at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:645) at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:689) at java.io.DataInputStream.readFully(DataInputStream.java:178) at org.apache.hadoop.hbase.io.hfile.FixedFileTrailer.readFromStream(FixedFileTrailer.java:312) at org.apache.hadoop.hbase.io.hfile.HFile.pickReaderVersion(HFile.java:543) at org.apache.hadoop.hbase.io.hfile.HFile.createReaderWithEncoding(HFile.java:589) at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.&lt;init&gt;(StoreFile.java:1261) at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:512) at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:603) at org.apache.hadoop.hbase.regionserver.Store.validateStoreFile(Store.java:1568) at org.apache.hadoop.hbase.regionserver.Store.commitFile(Store.java:845) at org.apache.hadoop.hbase.regionserver.Store.access$500(Store.java:109) at org.apache.hadoop.hbase.regionserver.Store$StoreFlusherImpl.commit(Store.java:2209) at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:1541)</description>
      <version>0.98.0,0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.96.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.docbkx.performance.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8144" opendate="2013-3-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Limit number of attempts to assign a region</summary>
      <description>In sending a region open request to a region server, we make sure we try at most some configured times. However, once the request is accepted by the region server, the region could go through this transition forever: failed_open (in ZK) =&gt; closed =&gt; opening =&gt; failed_open (in ZK), assuming no RPC/network issue.It will be good to break the loop and limit the number of tries and move the region to failed_open state (will be introduced in HBASE-8137)</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8147" opendate="2013-3-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Integration Test for "The HCat Scenario"</summary>
      <description>HBASE-8140 needs an integration test.</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestImportTsv.java</file>
    </fixedFiles>
  </bug>
  <bug id="8148" opendate="2013-3-19 00:00:00" fixdate="2013-3-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow IPC to bind on a specific address</summary>
      <description>Sometimes it can be useful to have the master and region servers bind on a specific address, mainly 0.0.0.0. Adding this shouldn't change the default behavior.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.94.7,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="8203" opendate="2013-3-27 00:00:00" fixdate="2013-11-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>master ui should display region servers with host, port plus startcode</summary>
      <description>in master UI we are displaying only hostname of region servers.some shell commands need servername(host,port,startcode format) as argument A 'server_name' is the host, port plus startcode of a regionserver. Forexample: host187.example.com,60020,1289493121758 (find servername inmaster ui or when you do detailed status in shell)</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.96.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8208" opendate="2013-3-28 00:00:00" fixdate="2013-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>In some situations data is not replicated to slaves when deferredLogSync is enabled</summary>
      <description>This is a subtle issue. When deferredLogSync is enabled, there are chances we could flush data before syncing all HLog entries. Assuming we just flush the internal cache and the server dies with some unsynced hlog entries. Data is not lost at the source cluster while replication is based on WAL files and some changes we flushed at the source won't be replicated the slave clusters. Although enabling deferredLogSync with tolerances of data loss, it breaks the replication assumption that whatever persisted in the source should be replicated to its slave clusters. In short, the slave cluster could end up with double losses: the data loss in the source and some data stored in source cluster may not be replicated to slaves either.The fix of the issue isn't hard. Basically we can invoke sync during each flush when replication is enabled for a region server. Since sync returns immediately when nothing to sync so there should be no performance impact.Please let me know what you think!Thanks,-Jeffrey</description>
      <version>0.98.0,0.94.6,0.95.0</version>
      <fixedVersion>0.98.0,0.94.7,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="821" opendate="2008-8-12 00:00:00" fixdate="2008-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UnknownScanner happens too often.</summary>
      <description>Jean-Daniel up on the list in an exchange with Dru Jensen solved an issue by recommending longer lease for client scanners in a MR job. Lets make change to conf. This lessens the impact of Andrew Purtell added retry on USE in HBASE-816 in TableMap but will help in MR tasks that don't subclass TableMap.</description>
      <version>None</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8236" opendate="2013-4-1 00:00:00" fixdate="2013-4-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set finalName property in hbase-assembly else basename is hbase-assembly rather than hbase.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8242" opendate="2013-4-2 00:00:00" fixdate="2013-4-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to start HBase 0.95.0RC0 out of the box because of ZK trying to access /var/folders/</summary>
      <description>HBase 0.95.0RC0 is failing out of the box because of some ZooKeeper exceptions to write in /var/folders/jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ bin/start-hbase.sh jmspaggi@virtual:~/hbase-0.95.0-hadoop1$ tail -100f logs/hbase-jmspaggi-master-virtual.log mardi 2 avril 2013, 07:24:13 (UTC-0400) Starting master on virtualcore file size (blocks, -c) 0data seg size (kbytes, -d) unlimitedscheduling priority (-e) 0file size (blocks, -f) unlimitedpending signals (-i) 31634max locked memory (kbytes, -l) 64max memory size (kbytes, -m) unlimitedopen files (-n) 1024pipe size (512 bytes, -p) 8POSIX message queues (bytes, -q) 819200real-time priority (-r) 0stack size (kbytes, -s) 8192cpu time (seconds, -t) unlimitedmax user processes (-u) 31634virtual memory (kbytes, -v) unlimitedfile locks (-x) unlimited2013-04-02 07:24:16,093 INFO org.apache.hadoop.hbase.util.VersionInfo: HBase 0.95.0-hadoop12013-04-02 07:24:16,132 INFO org.apache.hadoop.hbase.util.VersionInfo: Subversion file:///Users/stack/checkouts/0.95/hbase-common -r Unknown2013-04-02 07:24:16,132 INFO org.apache.hadoop.hbase.util.VersionInfo: Compiled by stack on Mon Apr 1 15:38:48 PDT 20132013-04-02 07:24:17,475 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:zookeeper.version=3.4.5-1392090, built on 09/30/2012 17:52 GMT2013-04-02 07:24:17,475 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:host.name=virtual.distparser.com2013-04-02 07:24:17,586 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.version=1.7.0_132013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.vendor=Oracle Corporation2013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.home=/home/jmspaggi/jdk/jre2013-04-02 07:24:17,587 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.class.path=/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../conf:/home/jmspaggi/jdk//lib/tools.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/..:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/activation-1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/asm-3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-1.7.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-beanutils-core-1.8.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-cli-1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-codec-1.7.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-collections-3.2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-configuration-1.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-digester-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-el-1.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-httpclient-3.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-io-2.4.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-lang-2.6.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-logging-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-math-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/commons-net-1.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/core-3.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/findbugs-annotations-1.3.9-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/guava-12.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hadoop-core-1.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-client-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-common-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-examples-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop1-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-hadoop-compat-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-it-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-prefix-tree-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-protocol-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/hbase-server-0.95.0-hadoop1-tests.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/high-scale-lib-1.1.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/htrace-1.50.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpclient-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/httpcore-4.1.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-core-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-jaxrs-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-mapper-asl-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jackson-xc-1.8.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jamon-runtime-2.3.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-compiler-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jasper-runtime-5.5.23.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-api-2.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jaxb-impl-2.2.3-1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-core-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-json-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jersey-server-1.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jetty-util-6.1.26.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jruby-complete-1.6.8.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsp-api-2.1-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/jsr305-1.3.9.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/libthrift-0.9.0.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/log4j-1.2.17.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/metrics-core-2.1.2.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/netty-3.5.9.Final.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/protobuf-java-2.4.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/servlet-api-2.5-6.1.14.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-api-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/slf4j-log4j12-1.4.3.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/stax-api-1.0.1.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/xmlenc-0.52.jar:/home/jmspaggi/hbase-0.95.0-hadoop1/bin/../lib/zookeeper-3.4.5.jar:/etc/hadoop:/usr/lib/jvm/java-6-sun/lib/tools.jar:/usr/libexec/../share/hadoop:/usr/libexec/../share/hadoop/hadoop-core-1.1.1.jar:/usr/libexec/../share/hadoop/lib/asm-3.2.jar:/usr/libexec/../share/hadoop/lib/aspectjrt-1.6.11.jar:/usr/libexec/../share/hadoop/lib/aspectjtools-1.6.11.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-1.7.0.jar:/usr/libexec/../share/hadoop/lib/commons-beanutils-core-1.8.0.jar:/usr/libexec/../share/hadoop/lib/commons-cli-1.2.jar:/usr/libexec/../share/hadoop/lib/commons-codec-1.4.jar:/usr/libexec/../share/hadoop/lib/commons-collections-3.2.1.jar:/usr/libexec/../share/hadoop/lib/commons-configuration-1.6.jar:/usr/libexec/../share/hadoop/lib/commons-daemon-1.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-digester-1.8.jar:/usr/libexec/../share/hadoop/lib/commons-el-1.0.jar:/usr/libexec/../share/hadoop/lib/commons-httpclient-3.0.1.jar:/usr/libexec/../share/hadoop/lib/commons-io-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-lang-2.4.jar:/usr/libexec/../share/hadoop/lib/commons-logging-1.1.1.jar:/usr/libexec/../share/hadoop/lib/commons-logging-api-1.0.4.jar:/usr/libexec/../share/hadoop/lib/commons-math-2.1.jar:/usr/libexec/../share/hadoop/lib/commons-net-3.1.jar:/usr/libexec/../share/hadoop/lib/core-3.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-capacity-scheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-fairscheduler-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hadoop-thriftfs-1.1.1.jar:/usr/libexec/../share/hadoop/lib/hsqldb-1.8.0.10.jar:/usr/libexec/../share/hadoop/lib/jackson-core-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jackson-mapper-asl-1.8.8.jar:/usr/libexec/../share/hadoop/lib/jasper-compiler-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jasper-runtime-5.5.12.jar:/usr/libexec/../share/hadoop/lib/jdeb-0.8.jar:/usr/libexec/../share/hadoop/lib/jersey-core-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-json-1.8.jar:/usr/libexec/../share/hadoop/lib/jersey-server-1.8.jar:/usr/libexec/../share/hadoop/lib/jets3t-0.6.1.jar:/usr/libexec/../share/hadoop/lib/jetty-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jetty-util-6.1.26.jar:/usr/libexec/../share/hadoop/lib/jsch-0.1.42.jar:/usr/libexec/../share/hadoop/lib/junit-4.5.jar:/usr/libexec/../share/hadoop/lib/kfs-0.2.2.jar:/usr/libexec/../share/hadoop/lib/log4j-1.2.15.jar:/usr/libexec/../share/hadoop/lib/mockito-all-1.8.5.jar:/usr/libexec/../share/hadoop/lib/oro-2.0.8.jar:/usr/libexec/../share/hadoop/lib/servlet-api-2.5-20081211.jar:/usr/libexec/../share/hadoop/lib/slf4j-api-1.4.3.jar:/usr/libexec/../share/hadoop/lib/slf4j-log4j12-1.4.3.jar:/usr/libexec/../share/hadoop/lib/xmlenc-0.52.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-2.1.jar:/usr/libexec/../share/hadoop/lib/jsp-2.1/jsp-api-2.1.jar2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.io.tmpdir=/tmp2013-04-02 07:24:17,588 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:java.compiler=&lt;NA&gt;2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.name=Linux2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.arch=amd642013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:os.version=3.2.0-4-amd642013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.name=jmspaggi2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.home=/home/jmspaggi2013-04-02 07:24:17,589 INFO org.apache.zookeeper.server.ZooKeeperServer: Server environment:user.dir=/home/jmspaggi/hbase-0.95.0-hadoop12013-04-02 07:24:17,598 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Failed to start masterjava.io.IOException: Unable to create data directory /var/folders/bp/2z1cykc92rs6j24251cg__ph0000gp/T/hbase-stack/zookeeper/zookeeper_0/version-2 at org.apache.zookeeper.server.persistence.FileTxnSnapLog.&lt;init&gt;(FileTxnSnapLog.java:85) at org.apache.zookeeper.server.ZooKeeperServer.&lt;init&gt;(ZooKeeperServer.java:213) at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:161) at org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.startup(MiniZooKeeperCluster.java:131) at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:137) at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:107) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65) at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:78) at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:2482)</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8254" opendate="2013-4-3 00:00:00" fixdate="2013-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add lz4 to test compression util usage string</summary>
      <description>Add lz4 to the list of compression codecs on test compression util.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
    </fixedFiles>
  </bug>
  <bug id="8262" opendate="2013-4-3 00:00:00" fixdate="2013-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add testcase to verify HBASE-7876&amp;#39;s empty region split semantics change</summary>
      <description>HBASE-7678 change the semantics of splits and removed a test case but didn't not add one to verify behavior. We'll add one here.</description>
      <version>0.98.0,0.94.6,0.95.0,0.95.2</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="8300" opendate="2013-4-9 00:00:00" fixdate="2013-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSplitTransaction fails to delete files due to open handles left when region is split</summary>
      <description>This issue is related to HBASE-6823. logs below.TestSplitTransactionorg.apache.hadoop.hbase.regionserver.TestSplitTransactiontestWholesomeSplit(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/e5089331-c2bf-43d0-816d-25c6bed71f26/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/4851a041b5e9befef50c135b5659243b at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)testRollback(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/9140a440-3925-4eaf-8d5d-62744609d775/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/6f0ef0cbe59b3fb02c081ad1ffc78a9d at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)testFailAfterPONR(org.apache.hadoop.hbase.regionserver.TestSplitTransaction)java.io.IOException: Failed delete of C:/springSpace/org.apache.hbase.hbase-0.95.0-SNAPSHOT/hbase-server/target/test-data/9ad6728e-a425-4c2a-b7f2-00af842ec2a4/org.apache.hadoop.hbase.regionserver.TestSplitTransaction/table/65a714e4df0ea5878925c27f8815cd6f at org.apache.hadoop.hbase.regionserver.TestSplitTransaction.teardown(TestSplitTransaction.java:100) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:33) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50) at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390) at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug id="8303" opendate="2013-4-9 00:00:00" fixdate="2013-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increse the test timeout to 60s when they are less than 20s</summary>
      <description>Short test timeouts are dangerous because: if the test is executed in the same jvm as another, GC, thread priority can play a role we don't know the machine used to execute the tests, nor what's running on it;.For this reason, a test timeout of 60s allows us to be on the safe side.</description>
      <version>0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.94.7,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift.TestCallQueue.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestZKProcedureControllers.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestProcedureMember.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestProcedureCoordinator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.errorhandling.TestTimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.constraint.TestConstraint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8313" opendate="2013-4-9 00:00:00" fixdate="2013-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Bloom filter testing for HFileOutputFormat</summary>
      <description>HBASE-3776 added Bloom Filter Support to the HFileOutputFormat, but there's no test to verify that.</description>
      <version>None</version>
      <fixedVersion>0.94.7,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="8315" opendate="2013-4-10 00:00:00" fixdate="2013-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation should have more information of LRU Stats</summary>
      <description>Unfortunately, there's no documentation to explain the meaning of each LRU Stats in the regionserver logs. So this is for creating a new paragraph regarding this. My current idea is below, but it's a little bit difficult to explain the difference between 'cachingAccesses' and 'accesses' from an administrator or a user views.Could you guys help to improve the content?total: The current memory size of the cache in use.free: The total free memory currently available to store more cache entries.max: Maximum allowed memory size of the cache.blocks: Caches store blocks of data; this number is the current # of blocks stored, which use up the "total" memory space.accesses: The total number of times the cache was accessed, regardless of result.hits: The total number of times the cache was accessed and the result was a successful hit (presence of looked up element in cache is a hit).hitRatio: The current percentage for "hits / accesses".====Unclear:cachingAccesses: cachingHits + The number of getBlock requests that were cache misses, but only from requests that were set to use the block cache.cachingHits: The number of getBlock requests that were cache hits, but only from requests that were set to use the block cache. This is because all reads=====cachingHitsRatio: The current percentage for "cachintHits / cachingAccesses"evictions: The total number of times an eviction has occurred (based on the use of the LRU algorithm)evicted: The total number of blocks that have been evicted (based on the use of the LRU algorithm)evictedPerRun: The total number of blocks that have been evicted overall / The number of times an eviction has occurred overallAnd also, where should we add this paragraph in the documentation?</description>
      <version>0.95.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8316" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JoinedHeap for non essential column families should reseek instead of seek</summary>
      <description>This was raised by the Phoenix team. During a profiling session we noticed that catching the joinedHeap up to the current rows via seek causes a performance regression, which makes the joinedHeap only efficient when either a high or low percentage is matched by the filter.(High is fine, because the joinedHeap will not get behind as often and does not need to be caught up, low is fine, because the seek isn't happening frequently).In our tests we found that the solution is quite simple: Replace seek with reseek. Patch coming soon.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.94.7,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="8317" opendate="2013-4-10 00:00:00" fixdate="2013-4-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Seek returns wrong result with PREFIX_TREE Encoding</summary>
      <description>TestPrefixTreeEncoding#testSeekWithFixedData from the patch could reproduce the bug.An example of the bug case:Suppose the following rows:1.row3/c1:q1/2.row3/c1:q2/3.row3/c1:q3/4.row4/c1:q1/5.row4/c1:q2/After seeking the row 'row30', the expected peek KV is row4/c1:q1/, but actual is row3/c1:q1/.I just fix this bug case in the patch, Maybe we can do more for other potential problems if anyone is familiar with the code of PREFIX_TREE</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.prefixtree.row.TestPrefixTreeSearcher.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.decode.PrefixTreeArraySearcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="8324" opendate="2013-4-11 00:00:00" fixdate="2013-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHFileOutputFormat.testMRIncremental* fails against hadoop2 profile</summary>
      <description>Two tests cases are failing:testMRIncrementalLoad, testMRIncrementalloadWithSplit&lt;testcase time="33.942" classname="org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat" name="testMRIncrementalLoad"&gt; &lt;failure type="java.lang.AssertionError"&gt;java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.runIncrementalPELoad(TestHFileOutputFormat.java:468) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.doIncrementalLoadTest(TestHFileOutputFormat.java:378) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.testMRIncrementalLoad(TestHFileOutputFormat.java:348) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)... &lt;testcase time="34.324" classname="org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat" name="testMRIncrementalLoadWithSplit"&gt; &lt;failure type="java.lang.AssertionError"&gt;java.lang.AssertionError at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertTrue(Assert.java:52) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.runIncrementalPELoad(TestHFileOutputFormat.java:468) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.doIncrementalLoadTest(TestHFileOutputFormat.java:378) at org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.testMRIncrementalLoadWithSplit(TestHFileOutputFormat.java:354) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ...</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="8344" opendate="2013-4-15 00:00:00" fixdate="2013-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the assignment when node failures happen to choose the secondary RS as the new primary RS</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestFavoredNodeAssignmentHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodes.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="8350" opendate="2013-4-16 00:00:00" fixdate="2013-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable ChaosMonkey to run commands as different users</summary>
      <description>CM should be able to run under different user than the user under which hbase is running. Current options are insufficient to achieve that... it should ssh under configurable user, or sudo</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.94.8,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.developer.xml</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8352" opendate="2013-4-16 00:00:00" fixdate="2013-4-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename &amp;#39;.snapshot&amp;#39; directory</summary>
      <description>Testing HBase Snapshot on top of Hadoop's Snapshot branch (http://svn.apache.org/viewvc/hadoop/common/branches/HDFS-2802/), we found that both features used '.snapshot' directory to store metadata.HDFS (built from HDFS-2802 branch) doesn't allow paths with .snapshot as a componentFrom discussion on dev@hbase.apache.org, (see http://search-hadoop.com/m/kY6C3cXMs51), consensus was to rename '.snapshot' directory in HBase so that both features can co-exist smoothly.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.94.7,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8366" opendate="2013-4-17 00:00:00" fixdate="2013-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseServer logs the full query.</summary>
      <description>We log the query when we have an error. As a results, the logs are not readable when using stuff like multi.As a side note, this is as well a security issue (no need to encrypt the network and the storage if the logs contain everything). I'm not removing the full log line here; but just ask and I do it .</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.QosFunction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.IPCUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="837" opendate="2008-8-16 00:00:00" fixdate="2008-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit tests for ThriftServer.HBaseHandler</summary>
      <description>Tim Sell over in HBASE-697 suggests we add unit tests for our thrift guts (Shouldn't need to fire up thrift to do this IIRC).</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8383" opendate="2013-4-19 00:00:00" fixdate="2013-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support lib/*jar inside coprocessor jar</summary>
      <description>Currently, jar files inside a coprocessor jar should be under folder /lib/. It would be great to support jar files under lib/ too, i.e, no "/" at the front.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.94.8,0.95.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestClassLoading.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CoprocessorClassLoader.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ClassLoaderBase.java</file>
    </fixedFiles>
  </bug>
  <bug id="8390" opendate="2013-4-23 00:00:00" fixdate="2013-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk/0.95 cannot simply compile against Hadoop 1.0</summary>
      <description>Currently we can't simply compile against Hadoop 1.0 in 0.95 and newer, we are missing a dependency in common for Apache's commons-io. Easy fix, we could just add that dependency for all the profiles there. But doing it correctly requires adding a new profile.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.DefaultLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="8396" opendate="2013-4-23 00:00:00" fixdate="2013-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>balancerSwitch making two zookeeper calls everytime.</summary>
      <description>first time on balancerSwitch we will create balancerZnode.from second time onwards we will try to create node(1st time) which will fail with NodeExistsException then we will call setData(2nd time). try { ZKUtil.createAndWatch(watcher, watcher.balancerZNode, upData); } catch(KeeperException.NodeExistsException nee) { ZKUtil.setData(watcher, watcher.balancerZNode, upData); }We can change as below to avoid extra zk call from second time onwards. try { ZKUtil.setData(watcher, watcher.balancerZNode, upData); } catch(KeeperException.NoNodeException nne) { ZKUtil.createAndWatch(watcher, watcher.balancerZNode, upData); }</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.LoadBalancerTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="8402" opendate="2013-4-23 00:00:00" fixdate="2013-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ScanMetrics depends on number of rpc calls to the server.</summary>
      <description>Currently, scan metrics is not published in case there is one trip to server. I was testing it on a small row range (200 rows) with a large cache value (1000). It doesn't look right as metrics should not depend on number of rpc calls (number of rpc call is just one metrics fwiw).</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="8405" opendate="2013-4-23 00:00:00" fixdate="2013-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more custom options to how ClusterManager runs commands</summary>
      <description>You may want to run yet more custom commands (such as su as some local user) depending on test setup.</description>
      <version>None</version>
      <fixedVersion>0.94.8,0.95.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.developer.xml</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="8406" opendate="2013-4-23 00:00:00" fixdate="2013-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix rat check and rat warning in trunk</summary>
      <description>See recent trunk hadoop qa where it has test properties for audit set to 84 so when there is actually a rat check problem, it doesn't show up on a hadoop qa build:https://builds.apache.org/job/PreCommit-HBASE-Build/5403/consoleFullGrep for 'There appear to be 84 release audit warnings before the patch and 1 release audit warnings after applying the patch.'Also, we have a rat warning since we did site move back out of hbase-assembly (noticed by JD).</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8421" opendate="2013-4-24 00:00:00" fixdate="2013-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-0.95.0 tgz does not include lib/junit*.jar</summary>
      <description>The 0.95 release of hbase does not include junit-*.jar in the lib/ dir. This is required to run the hbase-it suite from the tarball.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8425" opendate="2013-4-24 00:00:00" fixdate="2013-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Per-region memstore size is missing in the new RS web UI</summary>
      <description>I like that metric, right now all we have is the whole memstore size in bytes.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8430" opendate="2013-4-25 00:00:00" fixdate="2013-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cell decoder/scanner/etc. should not hide exceptions</summary>
      <description>Cell scanner, base decoder, etc., hide IOException inside runtime exception. This can lead to unexpected behavior because a lot of code only expects IOException. There's no logical justification behind this hiding so it should be removed before it's too late (the sooner we do it the less throws declarations need to be added)</description>
      <version>0.95.0</version>
      <fixedVersion>0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestPutDeleteEtcCellIteration.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.codec.MessageCodec.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.prefixtree.row.data.TestRowDataSimple.java</file>
      <file type="M">hbase-prefix-tree.src.test.java.org.apache.hadoop.hbase.codec.prefixtree.row.data.TestRowDataSearcherRowMiss.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.KeyValueCodec.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.CodecException.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.CellCodec.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.BaseEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.BaseDecoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellScanner.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.ipc.TestPayloadCarryingRpcController.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.StoppedRpcClientException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.TableInfoMissingException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.RegionException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.PleaseHoldException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.HBaseSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.HBaseIOException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.exceptions.DoNotRetryIOException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.WrongRowIOException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8437" opendate="2013-4-25 00:00:00" fixdate="2013-5-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up tmp coprocessor jars</summary>
      <description>Users reported that some tmp coprocessor jars are not cleaned up properly, which ends up /tmp folder is flooded.When a HBase server starts up, we should do some clean up to make sure tmp jar files are removed properly.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestKeyLocker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestClassLoading.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.KeyLocker.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestDynamicClassLoader.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.ClassLoaderTestHelper.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CoprocessorClassLoader.java</file>
    </fixedFiles>
  </bug>
  <bug id="844" opendate="2008-8-25 00:00:00" fixdate="2008-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t pass script to hbase shell</summary>
      <description>Shell documentation says:~ stack$ ${HBASE_HOME}/bin/hbase shell PATH_TO_SCRIPTYour script can lean on the methods provided by the HBase Shell.This doesn't actually work.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="8441" opendate="2013-4-25 00:00:00" fixdate="2013-8-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[replication] Refactor KeeperExceptions thrown from replication state interfaces into replication specific exceptions</summary>
      <description>Currently, the replication state interfaces (state, peers and queues) throw KeeperExceptions from some of their methods. Refactor these into replication specific exceptions to prevent the implementation details of Zookeeper from leaking through.</description>
      <version>None</version>
      <fixedVersion>0.96.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.replication.VerifyReplication.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesClientZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeers.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8446" opendate="2013-4-26 00:00:00" fixdate="2013-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow parallel snapshot of different tables</summary>
      <description>currently only one snapshot at the time is allowed.Like for the restore, we should allow taking snapshot of different tables in parallel.</description>
      <version>0.95.0</version>
      <fixedVersion>0.94.8,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.RestoreSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.EnabledTableSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.DisabledTableSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.CloneSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SnapshotSentinel.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="8449" opendate="2013-4-26 00:00:00" fixdate="2013-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor recoverLease retries and pauses informed by findings over in hbase-8389</summary>
      <description>HBASE-8359 is an interesting issue that roams near and far. This issue is about making use of the findings handily summarized on the end of hbase-8359 which have it that trunk needs refactor around how it does its recoverLease handling (and that the patch committed against HBASE-8359 is not what we want going forward).This issue is about making a patch that adds a lag between recoverLease invocations where the lag is related to dfs timeouts &amp;#8211; the hdfs-side dfs timeout &amp;#8211; and optionally makes use of the isFileClosed API if it is available (a facility that is not yet committed to a branch near you and unlikely to be within your locality with a good while to come).</description>
      <version>0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSHDFSUtils.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="845" opendate="2008-8-26 00:00:00" fixdate="2008-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCM.isTableEnabled doesn&amp;#39;t really tell if it is, or not</summary>
      <description>In current trunk, if i load a table of 8M rows and then try and delete it, the disable returns saying the table was successfully deleted but when I then try to drop the table, it says table not disabled. I run the disable/drop cycle a few more times and still fails. Eventually, if I wait long enough, it succeeds. Maybe the table drop should just block if table is seen to have disabled regions in it. As is, its a little disorientating the way it works. Could lead admins to distrust status messages emitted.</description>
      <version>None</version>
      <fixedVersion>0.19.1,0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="8462" opendate="2013-4-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom timestamps should not be allowed to be negative</summary>
      <description>Client supplied timestamps should not be allowed to be negative, otherwise unpredictable results will follow. Especially, since we are encoding the ts using Bytes.Bytes(long), negative timestamps are sorted after positive ones. Plus, the new PB messages define ts' as uint64. Credit goes to Huned Lokhandwala for reporting this.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogFiltering.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.TimeRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.Cell.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
    </fixedFiles>
  </bug>
  <bug id="8465" opendate="2013-4-30 00:00:00" fixdate="2013-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto-drop rollback snapshot for snapshot restore</summary>
      <description>Below is an excerpt from snapshot restore javadoc: * Restore the specified snapshot on the original table. (The table must be disabled) * Before restoring the table, a new snapshot with the current table state is created. * In case of failure, the table will be rolled back to the its original state.We can improve the handling of rollbackSnapshot in two ways:1. give better name to the rollbackSnapshot (adding '-for-rollback-'). Currently the name is of the form: String rollbackSnapshot = snapshotName + "-" + EnvironmentEdgeManager.currentTimeMillis();2. drop rollbackSnapshot at the end of restoreSnapshot() if the restore is successful. We can introduce new config param, named 'hbase.snapshot.restore.drop.rollback', to keep compatibility with current behavior.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8466" opendate="2013-4-30 00:00:00" fixdate="2013-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty messages in the logs</summary>
      <description>We've got this:ATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a] OPENATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a] BOUND: 0.0.0.0/0.0.0.0:37250ATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a, 0.0.0.0/0.0.0.0:37250 =&gt; /226.1.1.3:60100] CONNECTED: /226.1.1.3:60100ATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a, 0.0.0.0/0.0.0.0:37250 =&gt; /226.1.1.3:60100] WRITTEN_AMOUNT: 129ATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a, 0.0.0.0/0.0.0.0:37250 :&gt; /226.1.1.3:60100] DISCONNECTEDATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a, 0.0.0.0/0.0.0.0:37250 :&gt; /226.1.1.3:60100] UNBOUNDATTENTION: The pipeline contains no upstream handlers; discarding: [id: 0x1f79354a, 0.0.0.0/0.0.0.0:37250 :&gt; /226.1.1.3:60100] CLOSEDWe can fix this by adding an upstream handler that discards the message without printing them.</description>
      <version>0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterStatusPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug id="8469" opendate="2013-4-30 00:00:00" fixdate="2013-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hadoop2] Several tests break because of HDFS-4305</summary>
      <description>Several unit tests will break due to HDFS-4305 (which enforces a min block size) because apprently, we set our block size smaller in these tests. Specified block size is less than configured minimum value (dfs.namenode.fs-limits.min-block-size): 1024 &lt; 1048576 at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:1816) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1795) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:418) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:205) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java:44134) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:453) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1002) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1696) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1692) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1690)org.apache.hadoop.hbase.regionserver.TestHRegion.testgetHDFSBlocksDistributionorg.apache.hadoop.hbase.regionserver.TestHRegionBusyWait.testgetHDFSBlocksDistributionorg.apache.hadoop.hbase.replication.TestMasterReplication.testCyclicReplicationorg.apache.hadoop.hbase.replication.TestMasterReplication.testSimplePutDeleteorg.apache.hadoop.hbase.replication.TestMultiSlaveReplication.testMultiSlaveReplicationorg.apache.hadoop.hbase.util.TestFSUtils.testcomputeHDFSBlocksDistribution</description>
      <version>0.98.0,0.94.6.1,0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="8471" opendate="2013-4-30 00:00:00" fixdate="2013-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Server-side, remove convertion from pb type to client type before we call method</summary>
      <description>In the regionserver, when the rpc receives a call, the call is described using protobufs. Before we make the server-side invocation, we do a transform on the pb param objects to make a native pojo &amp;#8211; e.g. from a pb Puts into an hbase o.a.h.h.client.Put &amp;#8211; and only then do we make the call against the server.On the way out, similar, before putting the result on the wire, we will do a convertion from o.a.h.h.client.Result into pb Result.This issue is about our first INVESTIGATING if it is possible to do away w/ this marshalling/unmarshalling serverside especially given the pb objects themselves are rich in accessor and getters, etc. If it is possible to do w/ pbs alone serverside, then we should go ahead and rip out all the serverside convertions.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.catalog.TestMetaReaderEditorNoCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8473" opendate="2013-5-1 00:00:00" fixdate="2013-7-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add note to ref guide about snapshots and ec2 reverse dns requirements.</summary>
      <description>From IRC from mighty Jeremy Carroll.17:10 &lt;jeremy_carroll&gt; jmhsieh: I think I found the root cuase. All my region servers reach the barrier, but it does not continue.17:11 &lt;jeremy_carroll&gt; jmhsieh: All RS have this in their logs: 2013-05-01 00:04:56,356 DEBUG org.apache.hadoop.hbase.procedure.Subprocedure: Subprocedure 'backup1' coordinator notified of 'acquire', waiting on 'reached' or 'abort' from coordinator.17:11 &lt;jeremy_carroll&gt; jmhsieh: Then the coordinator (Master) never sends anything. They just sit until the timeout.17:12 &lt;jeremy_carroll&gt; jmhsieh: So basically 'reached' is never obtained. Then abort it set, and it fails....17:24 &lt;jeremy_carroll&gt; jmhsieh: Found the bug. The hostnames dont match the master due to DNS resolution17:25 &lt;jeremy_carroll&gt; jmhsieh: The barrier aquired is putting in the local hostname from the regionservers. In EC2 (Where reverse DNS does not work well), the master hands the internal name to the client.17:25 &lt;jeremy_carroll&gt; jmhsieh: https://s3.amazonaws.com/uploads.hipchat.com/23947/185789/au94meik0h3y5ii/Screen%20Shot%202013-04-30%20at%2017.25.50.png 17:26 &lt;jeremy_carroll&gt; jmhsieh: So it's waiting for something like 'ip-10-155-208-202.ec2.internal,60020,1367366580066' zNode to show up, but instead 'hbasemetaclustera-d1b0a484,60020,1367366580066,' is being inserted. Barrier is not reached17:27 &lt;jeremy_carroll&gt; jmhsieh: Reason being in our environment the master does not have a reverse DNS entry. So we get stuff like this on RegionServer startup in our logs.17:27 &lt;jeremy_carroll&gt; jmhsieh: 2013-05-01 00:03:00,614 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Master passed us hostname to use. Was=hbasemetaclustera-d1b0a484, Now=ip-10-155-208-202.ec2.internal17:54 &lt;jeremy_carroll&gt; jmhsieh: That was it. Verified. Now that Reverse DNS is working, snapshots are working. Now how to figure out how to get Reverse DNS working on Route53. I wished there was something like 'slave.host.name' inside of Hadoop for this. Looking at source code.</description>
      <version>0.98.0,0.94.6.1,0.95.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  <bug id="852" opendate="2008-8-29 00:00:00" fixdate="2008-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot scan all families in a row with a LIMIT, STARTROW, etc.</summary>
      <description>Suggest moving specification of COLUMNS inside the hash of optional arguments rather than require it proceed the hash of optional LIMIT, STARTROW, etc.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug id="8528" opendate="2013-5-10 00:00:00" fixdate="2013-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hadoop2] TestMultiTableInputFormat always fails on hadoop with YARN-129 applied</summary>
      <description>TestMulitTableInputFormat always fails if YARN-129 is applied. Its error message isn't useful (assertion failed) &amp;#8211; but if you look at the stderr/stdout/syslog files of the jobs in hbase-server/target/org.apache.hadoop.mapred.MiniMRCluster* (minicluster data dirs) you'll find this class not found exception:Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/v2/app/MRAppMasterCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.v2.app.MRAppMaster at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247)Could not find the main class: org.apache.hadoop.mapreduce.v2.app.MRAppMaster. Program will exit.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="8529" opendate="2013-5-11 00:00:00" fixdate="2013-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>checkOpen is missing from multi, mutate, get and multiGet etc.</summary>
      <description>I saw we have checkOpen in all those functions in 0.94 while they're missing from trunk. Does anyone know why?For multi and mutate, if we don't call checkOpen we could flood our logs with bunch of "DFSOutputStream is closed" errors when we sync WAL.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="8532" opendate="2013-5-12 00:00:00" fixdate="2013-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Webui] Bootstrap based webui compatibility for IE and also fix some page format issues.</summary>
      <description>HBASE-7425 brings bootstrap based webui to hbase. While trying on trunk version, Firefox works well, but IE 8/9 layout and style look strange due to compatibility issue. Add "&lt;!DOCTYPE html ...&gt;" at the beginning of all jamon html/jsp templates to fix it.Seems HBase-2110 had a work to comment out the DOCTYPE for all .jsp to make the browser run the pages in Quirks mode (http://en.wikipedia.org/wiki/Quirks_mode) due to jetty issue at that time?To support the compatibility of webui across browsers (IE/Firefox/Chrome, etc.), there are some guidelines for choosing rendering the page under standard mode or quirk mode:http://en.wikipedia.org/wiki/Quirks_modehttp://hsivonen.iki.fi/doctype/According to document, "&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;" has the most extensive compatibility even for HTML 5. According to my test, add this could make webui works in IE (standard mode), while Firefox could not work well with styles. Looks like if in Firefox, we still need the quirk mode (no DOCTYPE declaration). So just adding conditional DOCTYPE declaration for IE,&lt;!--&amp;#91;if IE&amp;#93;&gt;&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"&gt;&lt;!&amp;#91;endif&amp;#93;--&gt;this could make webui works for both IE and Firefox, also for Chrome and other browsers.</description>
      <version>0.98.0,0.95.0,0.95.2</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.rest.rest.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="8539" opendate="2013-5-13 00:00:00" fixdate="2013-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Double(or tripple ...) ZooKeeper listeners of the same type when Master recovers from ZK SessionExpiredException</summary>
      <description>When Master tries to recover from zookeeper session expired exceptions, we don't clean old registered listener instances. Therefore, it may end up we have two(or more) listeners to double handling same events. Attached a screen shot from debugger to show the issue.I considered to limit one listener per class while I think that would limit the listener usage so I choose to clear exiting listeners during recovery for the fix.(This issue is unrelated to the issue HBASE-8365 because I verified there is no dup-listeners when HBASE-8365 happened)</description>
      <version>0.98.0,0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.94.8,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="854" opendate="2008-8-29 00:00:00" fixdate="2008-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-841 broke build on hudson?</summary>
      <description>Jim, you want to take a look at it?841 changed interfaces. Changed interfaces can make for odd issues like the hangs exhibited up on hudson (stuff is failing for me on my laptop since about the commit 841... timeouts. I don't have same issue on branch).</description>
      <version>None</version>
      <fixedVersion>0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="8548" opendate="2013-5-14 00:00:00" fixdate="2013-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>postOpen hook called twice</summary>
      <description>postOpen hook is called twice when a region is initializing:Once at the end of the body of the initializeRegionInternals() method of the HRegion class.Once at the end initializeRegionStores() method of the HRegion class; initializeRegionStores() is called inside initializeRegionInternals() and as such causes the postOpen hook to be called twice.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverInterface.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="8574" opendate="2013-5-17 00:00:00" fixdate="2013-5-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add how to rename a table in the docbook</summary>
      <description>Add a section "how to rename a table" in the doc book.The current easy solution without adding extra code in 94/95 is to use snapshotshbase shell&gt; disable 'tableName'hbase shell&gt; snapshot 'tableName', 'tableSnapshot'hbase shell&gt; clone 'tableSnapshot', 'newTableName'hbase shell&gt; delete_snapshot 'tableSnapshot'void rename(HBaseAdmin admin, String oldTableName, String newTableName) { String snapshotName = randomName(); admin.snapshot(snapshotName, oldTableName); admin.cloneSnapshot(snapshotName, newTableName); admin.deleteSnapshot(snapshotName); admin.deleteTable(oldTableName)}</description>
      <version>0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.94.8,0.95.1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8591" opendate="2013-5-21 00:00:00" fixdate="2013-5-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doc Improvement: Replication blog</summary>
      <description>Add a section for source level metrics and some truth about table batch ops at sink.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.replication.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8592" opendate="2013-5-22 00:00:00" fixdate="2013-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[documentation] some updates for the reference guide regarding recent questions on the ML</summary>
      <description>I looked at the recent questions that were asked on the mailing list and tried to filled some of our gaps.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
      <file type="M">src.main.docbkx.schema.design.xml</file>
      <file type="M">src.main.docbkx.performance.xml</file>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8596" opendate="2013-5-22 00:00:00" fixdate="2013-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[docs] Add docs about Region server "draining" mode</summary>
      <description>HBASE-4298 introduced "draining" mode for region servers to optimize rolling restarts to allow for multiple RS's going down simultaneously. There is a good blog post from the original author. I've added highlights from and and a link to it in the Node Decommissioning section of the ref guide.</description>
      <version>0.92.2,0.98.0,0.94.7,0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug id="8627" opendate="2013-5-27 00:00:00" fixdate="2013-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK can not fix meta not assigned issue</summary>
      <description>When meta table region is not assigned to any RS, HBCK run will get exception. I can see code added in checkMetaRegion() to solve this issue but it wont work. It still refers to ROOT region!</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug id="8642" opendate="2013-5-29 00:00:00" fixdate="2013-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Snapshot] List and delete snapshot by table</summary>
      <description>Support list and delete snapshots by table names.User scenario:A user wants to delete all the snapshots which were taken in January month for a table 't' where snapshot names starts with 'Jan'.</description>
      <version>0.98.0,0.95.0,0.95.1,0.95.2</version>
      <fixedVersion>0.98.14,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.shell.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug id="8643" opendate="2013-5-29 00:00:00" fixdate="2013-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not log full classnames in logs, just the last two levels</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="8689" opendate="2013-6-5 00:00:00" fixdate="2013-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cover all mutations rather than only Put while reporting for mutations not writing to WAL</summary>
      <description>Currently, only Puts are reported instead of all mutations (increment, append, delete) when it is not writing to WAL. It should do the book keeping for other mutations too.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop1-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="8723" opendate="2013-6-10 00:00:00" fixdate="2013-6-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Intgration tests are failing because of new defaults.</summary>
      <description>Currently any IT tests that have chaos monkey fail because we are not recovering regions before the number of RPC reties is exhausted.We should set that default higher.</description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="8875" opendate="2013-7-5 00:00:00" fixdate="2013-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect javadoc for EXCLUDE_FROM_MINOR_COMPACTION</summary>
      <description>/** Major compaction flag in FileInfo */+ /** Minor compaction flag in FileInfo */ public static final byte[] EXCLUDE_FROM_MINOR_COMPACTION_KEY = Bytes.toBytes("EXCLUDE_FROM_MINOR_COMPACTION");</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug id="8876" opendate="2013-7-5 00:00:00" fixdate="2013-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addendum to HBASE-8774 Add BatchSize and Filter to Thrift2 - Add BatchSize Test</summary>
      <description>HBASE-8774 adds support for batching through large rows. A unit test was missing though, which is added here. Further cleanup as well, to test scan, scan with filter, and scan with batch size separately.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="9093" opendate="2013-7-30 00:00:00" fixdate="2013-8-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hbase client API: Restore the writeToWal method</summary>
      <description>The writeToWal is used by downstream projects like Flume to disable writes to WAL to improve performance when durability is not strictly required. But renaming this method to setDurability forces us to use reflection to support hbase versions &lt; 95 - which in turn hits performance, as this method needs to be called on every single write. I recommend adding the old method back as deprecated and removing it once hbase-95/96 becomes the popular version used in prod.</description>
      <version>0.95.0</version>
      <fixedVersion>0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestPutWriteToWal.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Mutation.java</file>
    </fixedFiles>
  </bug>
  <bug id="9096" opendate="2013-7-31 00:00:00" fixdate="2013-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable split during log replay</summary>
      <description>When regions are allowed to take writes during recovery, we could end up in a situation where a split of a region might be triggered. That would close the old region leading to failure of the ongoing replay. In discussions with jeffreyz, it seemed to make sense to just disable split during recovery.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="9116" opendate="2013-8-2 00:00:00" fixdate="2013-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a view/edit tool for favored node mappings for regions</summary>
      <description>Add a tool that one can run offline to view the favored node mappings for regions, and also fix the mappings if needed. Such a tool exists in the 0.89-fb branch. Will port it over to trunk/0.95.</description>
      <version>0.95.0</version>
      <fixedVersion>0.96.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.RequestConverter.java</file>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9706" opendate="2013-10-3 00:00:00" fixdate="2013-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve detection of secure ZooKeeper</summary>
      <description>HBase client code assumes ZooKeeper is secured, as long as there is a java.security.auth.login.config property being set. When HBase client is embedded in other java program with other security configuration, it can produce wrong assumption that ZooKeeper is secured. Ideally, isSecureZooKeeper method should detect Jaas configuration specifically for ZooKeeper to ensure that client program doesn't have a false positive detection.</description>
      <version>0.94.4,0.95.0</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="9866" opendate="2013-10-31 00:00:00" fixdate="2013-12-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the mode where REST server authorizes proxy users</summary>
      <description>In one use case, someone was trying to authorize with the REST server as a proxy user. That mode is not supported today. The curl request would be something like (assuming SPNEGO auth) - curl -i --negotiate -u : http://&lt;HOST&gt;:&lt;PORT&gt;/version/cluster?doas=&lt;USER&gt;</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RESTServletContainer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="9871" opendate="2013-11-1 00:00:00" fixdate="2013-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PB issue : Increment attributes are not getting passed to server</summary>
      <description></description>
      <version>0.95.0</version>
      <fixedVersion>0.98.0,0.96.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
