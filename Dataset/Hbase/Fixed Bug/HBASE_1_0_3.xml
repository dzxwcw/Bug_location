<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="14250" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14251" opendate="2015-8-19 00:00:00" fixdate="2015-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14348" opendate="2015-8-31 00:00:00" fixdate="2015-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug id="14349" opendate="2015-9-1 00:00:00" fixdate="2015-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="14532" opendate="2015-10-1 00:00:00" fixdate="2015-11-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[book] dfs.client.read.shortcircuit is referenced as hbase-site.xml config and not described in section 7</summary>
      <description>After trying to figure out whether shortcircuit reads would work on my system, I studied the book and found conflicting information.It's suggested in section 92.2, that dfs.client.read.shortcircuit is an option in hbase-site.xml, but the supposedly complete default configuration in section 7 does not include this setting. This leads to confusion on whether it's sufficient to enable this setting in hdfs-site.xml, or whether it needs to be added to both configurations.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="14840" opendate="2015-11-19 00:00:00" fixdate="2015-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sink cluster reports data replication request as success though the data is not replicated</summary>
      <description>Scenario:Sink cluster is downCreate a table and enable table replicationPut some dataNow restart the sink clusterObservance:Data is not replicated in sink cluster but still source cluster updates the WAL log position in ZK, resulting in data loss in sink cluster.</description>
      <version>1.1.2,1.0.3,0.98.16</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug id="14928" opendate="2015-12-4 00:00:00" fixdate="2015-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  <bug id="15021" opendate="2015-12-21 00:00:00" fixdate="2015-12-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105 � NullPointer Run 2: TestRSStatusServlet.testBasic:105 � NullPointer Run 3: TestRSStatusServlet.testBasic:105 � NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 2: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 3: TestRSStatusServlet.testWithRegions:119 � NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="15032" opendate="2015-12-23 00:00:00" fixdate="2015-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase shell scan filter string assumes UTF-8 encoding</summary>
      <description>Current hbase shell scan filter string is assumed to be UTF-8 encoding, which makes the following scan not working.hbase(main):011:0&gt; scan 't1'ROW COLUMN+CELL r4 column=cf1:q1, timestamp=1450812398741, value=\x82 hbase(main):003:0&gt; scan 't1', {FILTER =&gt; "SingleColumnValueFilter ('cf1', 'q1', &gt;=, 'binary:\x80', true, true)"}ROW COLUMN+CELL 0 row(s) in 0.0130 seconds</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.table.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.table.rb</file>
    </fixedFiles>
  </bug>
  <bug id="15200" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znode ACL checks should only compare the shortname</summary>
      <description>After HBASE-13768 we check at startup in secure configurations if our znodes have the correct ACLs. However when checking the ACL we compare the Kerberos fullname, which includes the host component. We should only compare the shortname, the principal. Otherwise in a multimaster configuration we will unnecessarily reset ACLs whenever any master running on a host other than the one that initialized the ACLs makes the check. You can imagine this happening multiple times in a rolling restart scenario.</description>
      <version>1.2.0,1.0.3,1.1.3,0.98.17,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="15201" opendate="2016-2-1 00:00:00" fixdate="2016-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase-spark to hbase assembly</summary>
      <description>hbase-spark currently is missing from hbase assembly.We should add it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15203" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce garbage created by path.toString() during Checksum verification</summary>
      <description>When we try to read a block we do checksum verification for which we need the file name in which the block belongs to. So we do Path.toString() every time. This seems to create around 163MB of char[] that is garbage collected in a simple scan run. It is also visible in writes but the impact is lesser. In overall write/read profile the top 2 factors are byte[] and char[]. This toString() can easily be avoided and reduce its share from the total. To make it more precise in 1 min of profiling, among the 1.8G of garbage created by StringBuilder.toString - this path.toString() was contributing around 3.5%. After the patch this is totally not there.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.ChecksumUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="15206" opendate="2016-2-2 00:00:00" fixdate="2016-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey testSplitDaughtersNotInMeta test</summary>
      <description>Run into the following failure with hbase 1.0.0.Stacktracejava.lang.AssertionError: nullat org.junit.Assert.fail(Assert.java:86)at org.junit.Assert.assertTrue(Assert.java:41)at org.junit.Assert.assertNotNull(Assert.java:712)at org.junit.Assert.assertNotNull(Assert.java:722)at org.apache.hadoop.hbase.util.TestHBaseFsck.testSplitDaughtersNotInMeta(TestHBaseFsck.java:1723)From the log, the ntp issue caused clock skew and it woke up CatalogJanitor earlier. The CatalogJanitor cleaned up the parent region. It could happen with master branch as well. The fix is to disable CatalogJanitor to make sure this will not happen.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckOneRS.java</file>
    </fixedFiles>
  </bug>
  <bug id="15255" opendate="2016-2-11 00:00:00" fixdate="2016-2-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pointer to linkedin blog on putting jvm logs on fast disk</summary>
      <description>Add pointer to linked in blog: https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-trafficIIRC, tsdb says to do similar.Also add into perf section note on native crc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="15259" opendate="2016-2-12 00:00:00" fixdate="2016-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALEdits under replay will also be replicated</summary>
      <description>I need to verify this but seeing the codetry { // We are about to append this edit; update the region-scoped sequence number. Do it // here inside this single appending/writing thread. Events are ordered on the ringbuffer // so region sequenceids will also be in order. regionSequenceId = entry.stampRegionSequenceId(); // Edits are empty, there is nothing to append. Maybe empty when we are looking for a // region sequence id only, a region edit/sequence id that is not associated with an actual // edit. It has to go through all the rigmarole to be sure we have the right ordering. if (entry.getEdit().isEmpty()) { return; } // Coprocessor hook. if (!coprocessorHost.preWALWrite(entry.getHRegionInfo(), entry.getKey(), entry.getEdit())) { if (entry.getEdit().isReplay()) { // Set replication scope null so that this won't be replicated entry.getKey().setScopes(null); } } if (!listeners.isEmpty()) { for (WALActionsListener i: listeners) { // TODO: Why does listener take a table description and CPs take a regioninfo? Fix. i.visitLogEntryBeforeWrite(entry.getHTableDescriptor(), entry.getKey(), entry.getEdit()); } }When a WALEdit is in replay we set the Logkey to null. But in the visitLogEntryBeforeWrite() we again set the LogKey based on the replication scope associated with the cells. So the previous step of setting null does not work here?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSmallTests.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
    </fixedFiles>
  </bug>
  <bug id="1526" opendate="2009-6-15 00:00:00" fixdate="2009-6-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapreduce fixup</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.DisabledTestTableMapReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.LuceneDocumentWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IndexConfiguration.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.BuildTableIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="15261" opendate="2016-2-12 00:00:00" fixdate="2016-3-12 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Make Throwable t in DaughterOpener volatile</summary>
      <description>In the region split process, daughter regions are opened in different threads, Throwable t is set in these threads and it is checked in the calling thread. Need to make it volatile so the checking will not miss any exceptions from opening daughter regions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="15393" opendate="2016-3-4 00:00:00" fixdate="2016-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable table replication command will fail when parent znode is not default in peer cluster</summary>
      <description>Enable table replication command will fail when parent znode is not /hbase(default) in peer cluster and there is only one peer cluster added in the source cluster.</description>
      <version>1.0.3,0.98.17</version>
      <fixedVersion>1.3.0,1.2.1,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug id="15801" opendate="2016-5-9 00:00:00" fixdate="2016-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle for all branches</summary>
      <description>We should use the same checkstyle for all branches.</description>
      <version>1.3.0,1.2.1,1.0.3,0.98.19,1.4.0,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.0.4,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15925" opendate="2016-5-31 00:00:00" fixdate="2016-7-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>compat-module maven variable not evaluated</summary>
      <description>Looks like we've regressed on HBASE-8488. Have a look at the dependency artifacts list on http://mvnrepository.com/artifact/org.apache.hbase/hbase-testing-util/1.2.1. Notice the direct dependency's artifactId is ${compat.module}.</description>
      <version>1.0.0,1.1.0,1.2.0,1.2.1,1.0.3,1.1.5</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="15977" opendate="2016-6-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed variable substitution on home page</summary>
      <description>Check out the top-left of hbase.apache.org, there's an unevaluated variable $banner.name leaking through.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16347" opendate="2016-8-3 00:00:00" fixdate="2016-8-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unevaluated expressions in book</summary>
      <description>Have a look at the quickstart guide, step two$ tar xzvf hbase-&lt;?eval ${project.version}?&gt;-bin.tar.gz$ cd hbase-&lt;?eval ${project.version}?&gt;/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17514" opendate="2017-1-23 00:00:00" fixdate="2017-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Warn when Thrift Server 1 is configured for proxy users but not the HTTP transport</summary>
      <description>The config hbase.thrift.support.proxyuser is ignored if the Thrift Server 1 isn't configured to use an HTTP transport with hbase.regionserver.thrift.http.We should emit a warning if our configs request proxy user support but don't specify that HTTP should be used for the transport.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="17516" opendate="2017-1-23 00:00:00" fixdate="2017-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table quota not taking precedence over namespace quota</summary>
      <description>Romil Choksi found a bug in the current patch-set where a more restrictive table quota did not take priority over a less-restrictive namespace quota.Turns out some of the logic to handle this case was incorrect.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="17518" opendate="2017-1-24 00:00:00" fixdate="2017-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Reference Guide has a syntax error</summary>
      <description>The image of "HFile Version 2 Structure" in Appendix F of HBase Reference Guide (pdf) is missing because of a wrong asciidoc syntax:image:hfilev2.png&amp;#91;HFile Version 2&amp;#93;modified as:image::hfilev2.png&amp;#91;HFile Version 2&amp;#93;it should be a double colon instead of single one</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="17520" opendate="2017-1-24 00:00:00" fixdate="2017-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isTableEnabled/Disabled/Available methods</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="17522" opendate="2017-1-24 00:00:00" fixdate="2017-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RuntimeExceptions from MemoryMXBean should not take down server process</summary>
      <description>RegionServer died after MemoryMXBean threw an IllegalArgumentException while attempting to create a MemoryUsage object for the heap during construction of the server load.We shouldn't allow failure to get load information to take down the RS.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.9,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HeapMemoryManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.util.MemorySizeUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.CacheConfig.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
</bugrepository>
