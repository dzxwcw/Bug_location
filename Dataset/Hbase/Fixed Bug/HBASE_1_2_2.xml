<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="15707" opendate="2016-4-25 00:00:00" fixdate="2016-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ImportTSV bulk output does not support tags with hfile.format.version=3</summary>
      <description>Running the following command:hbase hbase org.apache.hadoop.hbase.mapreduce.ImportTsv \ -Dhfile.format.version=3 \ -Dmapreduce.map.combine.minspills=1 \ -Dimporttsv.separator=, \ -Dimporttsv.skip.bad.lines=false \ -Dimporttsv.columns="HBASE_ROW_KEY,cf1:a,HBASE_CELL_TTL" \ -Dimporttsv.bulk.output=/tmp/testttl/output/1 \ testttl \ /tmp/testttl/input The content of input is like:row1,data1,00000060 row2,data2,00000660 row3,data3,00000060 row4,data4,00000660When running hfile tool with the output hfile, there is no ttl tag.</description>
      <version>1.3.0,1.0.4,1.4.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,0.98.20,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  <bug id="16135" opendate="2016-6-28 00:00:00" fixdate="2016-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PeerClusterZnode under rs of removed peer may never be deleted</summary>
      <description>One of our cluster run out of space recently, and we found that the .oldlogs directory had almost the same size as the data directory.Finally we found the problem is that, we removed a peer abort 3 months ago, but there are still some replication queue znode under some rs nodes. This prevents the deletion of .oldlogs.</description>
      <version>1.3.0,1.4.0,1.1.5,1.2.2,0.98.20,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,0.98.21,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestTableBasedReplicationSourceManagerImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManagerZkImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16144" opendate="2016-6-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replication queue&amp;#39;s lock will live forever if RS acquiring the lock has died prematurely</summary>
      <description>In default, we will use multi operation when we claimQueues from ZK. But if we set hbase.zookeeper.useMulti=false, we will add a lock first, then copy nodes, finally clean old queue and the lock. However, if the RS acquiring the lock crash before claimQueues done, the lock will always be there and other RS can never claim the queue.</description>
      <version>1.3.0,1.4.0,1.2.2,0.98.20,1.1.6,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.ReplicationZKLockCleanerChore.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="16196" opendate="2016-7-7 00:00:00" fixdate="2016-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update jruby to a newer version.</summary>
      <description>Ruby 1.8.7 is no longer maintained.The TTY library in the old jruby is bad. The newer one is less bad.Since this is only a dependency on the hbase-shell module and not on hbase-client or hbase-server this should be a pretty simple thing that doesn't have any backwards compat issues.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.test.rsgroup.org.apache.hadoop.hbase.client.rsgroup.TestShellRSGroups.java</file>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.TestShellNoCluster.java</file>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.AbstractTestShell.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.formatter.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.rb</file>
      <file type="M">hbase-shell.src.main.ruby.irb.hirb.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.security.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.constants.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.NOTICE.vm</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
      <file type="M">bin.shutdown.regionserver.rb</file>
      <file type="M">bin.replication.copy.tables.desc.rb</file>
      <file type="M">bin.region.status.rb</file>
      <file type="M">bin.get-active-master.rb</file>
      <file type="M">bin.draining.servers.rb</file>
    </fixedFiles>
  </bug>
  <bug id="16239" opendate="2016-7-16 00:00:00" fixdate="2016-12-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better logging for RPC related exceptions</summary>
      <description>On many occasions, we have to debug RPC related issues, but it is hard in AP + RetryingRpcCaller since we mask the stack traces until all retries have been exhausted (which takes 10 minutes by default).</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.BlockingRpcConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="1624" opendate="2009-7-7 00:00:00" fixdate="2009-7-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t sort Puts if only one in list in HCM#processBatchOfRows</summary>
      <description>Suggestion by Bryan Duxbury.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="16242" opendate="2016-7-18 00:00:00" fixdate="2016-6-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Avro to 1.7.7</summary>
      <description>I'd like to see Avro upgraded to 1.8.1 or at the very least 1.7.7</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16266" opendate="2016-7-21 00:00:00" fixdate="2016-7-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not throw ScannerTimeoutException when catch UnknownScannerException</summary>
      <description>Now in scanner we have heartbeat to prevent timeout. The time blocked on ResultScanner.next() may much longer than scanner timeout. So it is no need any more to throw ScannerTimeoutException when server throws UnknownScannerException, we can just reset the scanner like NotServingRegionException</description>
      <version>1.3.0,1.4.0,1.2.2,1.1.6,2.0.0</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="16283" opendate="2016-7-26 00:00:00" fixdate="2016-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batch Append/Increment will always fail if set ReturnResults to false</summary>
      <description>If set Append/Increment's ReturnResult attribute to false, and batch the appends/increments to server. The batch operation will always return false.The reason is that, since return result is set to false, append/increment will return null instead of Result object. But in ResponseConverter#getResults, there is some check code if (requestRegionActionCount != responseRegionActionResultCount) { throw new IllegalStateException("Request mutation count=" + requestRegionActionCount + " does not match response mutation result count=" + responseRegionActionResultCount); }That means if the result count is not meet with request mutation count, it will fail the request.The solution is simple, instead of returning a null result, returning a empty result if ReturnResult set to false.</description>
      <version>1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestDurability.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestIncrementsFromClientSide.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="16284" opendate="2016-7-26 00:00:00" fixdate="2016-8-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unauthorized client can shutdown the cluster</summary>
      <description>An unauthorized client can shutdown the cluster as AccessDeniedException is ignored during Admin.stopMaster and Admin.shutdown.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.21,1.2.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16306" opendate="2016-7-29 00:00:00" fixdate="2016-7-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add specific imports to avoid namespace clash in defaultSource.scala</summary>
      <description>I am working on adding Hybrid Logical clocks to HBase. As part of it, I wish to add TimestampType file in hbase common. Spark has some types defined in - org.apache.spark.sql.types, all of which are being imported. It also has a TimestampType defined. As currently in this file, all of the hbase common is imported, creating a namespace clash. I have changed to only import specific hbase common classes that are required in this file.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug id="16308" opendate="2016-8-1 00:00:00" fixdate="2016-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Contain protobuf references</summary>
      <description>Clean up our protobuf references so contained to just a few classes rather than being spread about the codebase. Doing this work will make it easier landing the parent issue and will make it more clear where the division between shaded protobuf and unshaded protobuf lies (we need to continue with unshaded protobuf for HDFS references by AsyncWAL and probably EndPoint Coprocessors)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetryingCallable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerHeartbeatMessages.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mob.mapreduce.TestMobSweepMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestRegionLocationFinder.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicasClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.Canary.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableStateManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestAsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.MetaTableAccessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.TimeLimitedRpcController.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcControllerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AbstractRegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSimpleScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.FlushRegionCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MasterCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MasterKeepAliveConnection.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MultiServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.PayloadCarryingServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionAdminServiceCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetryingTimeTracker.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ReversedScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.SecureBulkLoadClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.MasterCoprocessorRpcChannel.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.PayloadCarryingRpcController.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientScanner.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HBaseIOException.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ExceptionUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ExpiredMobFileCleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.ExpiredMobFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.Sweeper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEditsReplaySink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientTimeouts.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicaWithCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoad.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoadWithOldClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionServerBulkLoadWithOldSecureEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNamespace.java</file>
      <file type="M">hbase-spark.src.main.java.org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionCallable.java</file>
    </fixedFiles>
  </bug>
  <bug id="16419" opendate="2016-8-16 00:00:00" fixdate="2016-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>check REPLICATION_SCOPE&amp;#39;s value more stringently</summary>
      <description>When create table or modify table, the master will check if the value of REPLICATION_SCOPE is less than 0, however the value of REPLICATION_SCOPE must be 0 or 1. Otherwise will lead to regionserver shutdown, so I think should be check the values of REPLICATION_SCOPE more stringent.Beginning I don't fully understand the usage of REPLICATION_SCOPE, then set REPLICATION_SCOPE to 2 by mistake.when I insert data to table,the regionservers abort one by one,finanlythe cluster abortï¼Œthe exceptions as follow:2016-08-16 12:34:45,245 WARN &amp;#91;regionserver/host:60023.append-pool1-t1&amp;#93; wal.FSHLog: Append sequenceId=94, requesting roll of WALjava.lang.NullPointerException at org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939) at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:618) at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1886) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1750) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1672) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:744)2016-08-16 12:34:45,293 INFO &amp;#91;MemStoreFlusher.0&amp;#93; regionserver.HStore: Added hdfs://hbase-test-27/hbase1.2.2/data/default/usertable/2aa98c17845c9c6d5c8760b87b3ba09a/i/35825c94e72945c0bf7df3f0adefa1b6, entries=1161600, sequenceid=59, filesize=167.6 M2016-08-16 12:34:45,296 FATAL &amp;#91;MemStoreFlusher.0&amp;#93; regionserver.HRegionServer: ABORTING region server hbase-10-166-141-99,60023,1471262434177: Replay of WAL required. Forcing server shutdownorg.apache.hadoop.hbase.DroppedSnapshotException: region: usertable,,1471262560009.2aa98c17845c9c6d5c8760b87b3ba09a. at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushCacheAndCommit(HRegion.java:2427) at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2105) at org.apache.hadoop.hbase.regionserver.HRegion.internalFlushcache(HRegion.java:2067) at org.apache.hadoop.hbase.regionserver.HRegion.flushcache(HRegion.java:1958) at org.apache.hadoop.hbase.regionserver.HRegion.flush(HRegion.java:1884) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:510) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.flushRegion(MemStoreFlusher.java:471) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher.access$900(MemStoreFlusher.java:75) at org.apache.hadoop.hbase.regionserver.MemStoreFlusher$FlushHandler.run(MemStoreFlusher.java:259) at java.lang.Thread.run(Thread.java:744)Caused by: org.apache.hadoop.hbase.regionserver.wal.DamagedWALException: Append sequenceId=94, requesting roll of WAL at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1898) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1750) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.onEvent(FSHLog.java:1672) at com.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:128) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ... 1 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hbase.protobuf.generated.WALProtos$FamilyScope$Builder.setScopeType(WALProtos.java:3939) at org.apache.hadoop.hbase.wal.WALKey.getBuilder(WALKey.java:618) at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.append(ProtobufLogWriter.java:118) at org.apache.hadoop.hbase.regionserver.wal.FSHLog$RingBufferEventHandler.append(FSHLog.java:1886) ... 6 more</description>
      <version>1.2.2,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="16422" opendate="2016-8-16 00:00:00" fixdate="2016-8-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tighten our guarantees on compatibility across patch versions</summary>
      <description>Out of the dev discussion &amp;#91;1&amp;#93;, comes this change that tightens up our guarantees adding new API; that we'll do it in a non-breaking way across patch versions.1. http://mail-archives.apache.org/mod_mbox/hbase-dev/201608.mbox/browser</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="16450" opendate="2016-8-18 00:00:00" fixdate="2016-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell tool to dump replication queues</summary>
      <description>Currently there is no way to dump list of the configured queues and the replication queues when replication is enabled. Unfortunately the HBase master only offers an option to dump the whole content of the znodes but not details on the queues being processed on each RS.</description>
      <version>1.3.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,0.98.24,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16451" opendate="2016-8-19 00:00:00" fixdate="2016-8-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Test WAL protobuf entry size limit</summary>
      <description>Add a test to make sure that we are able to read/write procedures with a big "data" size.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.util.ByteSlot.java</file>
    </fixedFiles>
  </bug>
  <bug id="16522" opendate="2016-8-29 00:00:00" fixdate="2016-8-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Cache system user and avoid IOException</summary>
      <description>We can cache the system user and avoid the IOException that we have to carry around when we create procedures</description>
      <version>1.3.0,1.1.5,1.2.2,2.0.0</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.RestoreSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DispatchMergingRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CloneSnapshotProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.Superusers.java</file>
    </fixedFiles>
  </bug>
  <bug id="16554" opendate="2016-9-2 00:00:00" fixdate="2016-9-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Recover &amp;#39;updated&amp;#39; part of WAL tracker if trailer is corrupted.</summary>
      <description>If the last wal was closed cleanly, the global tracker will be the last wal tracker (no rebuild needed)if the last wal does not have a tracker (corrupted/master-killed). on load() we will rebuild the global tracker.To compute quickly which files should be deleted, we also want the tracker of each file.if the wal was closed properly and has a tracker we are good, if not we need to rebuild the tracker for that file.each file tracker contains a bitmap about what is in the wal (the updated bitmap), which is easy to compute just by reading each entry of the wal.The 'deleted' bitmap keeps track of the "running procedures" up to that wal, however, it's not required by WAL cleaner, so we don't bother recovering it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormatReader.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormat.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFile.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
    </fixedFiles>
  </bug>
  <bug id="16678" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce jobs do not update counters from ScanMetrics</summary>
      <description>Was inspecting a perf issue, where we needed the scanner metrics as counters for a MR job. Turns out that the HBase scan counters are no longer working in 1.0+. I think it got broken via HBASE-13030. These are the counters: HBase Counters BYTES_IN_REMOTE_RESULTS=0 BYTES_IN_RESULTS=280 MILLIS_BETWEEN_NEXTS=11 NOT_SERVING_REGION_EXCEPTION=0 NUM_SCANNER_RESTARTS=0 NUM_SCAN_RESULTS_STALE=0 REGIONS_SCANNED=1 REMOTE_RPC_CALLS=0 REMOTE_RPC_RETRIES=0 RPC_CALLS=3 RPC_RETRIES=0</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="16682" opendate="2016-9-22 00:00:00" fixdate="2016-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Shell tests failure. NoClassDefFoundError for MiniKdc</summary>
      <description>Stacktracejava.lang.NoClassDefFoundError: org/apache/hadoop/minikdc/MiniKdc at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.minikdc.MiniKdc at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="16856" opendate="2016-10-17 00:00:00" fixdate="2016-10-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exception message in SyncRunner.run() should print currentSequence</summary>
      <description>A very small bug, a typo in exception message:if (syncFutureSequence &gt; currentSequence) { throw new IllegalStateException("currentSequence=" + syncFutureSequence + ", syncFutureSequence=" + syncFutureSequence); }It should print currentSequence and syncFutureSequence, but print two syncFutureSequence</description>
      <version>1.2.2,1.1.7,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
    </fixedFiles>
  </bug>
  <bug id="17713" opendate="2017-3-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>the interface &amp;#39;/version/cluster&amp;#39; with header &amp;#39;Accept: application/json&amp;#39; return is not JSON but plain text</summary>
      <description>Hbase REST API, this interface `get 'version/cluster'`, when I use the header `Accept: application/json`, the response is not JSON but plain text. curl -X GET \ -H "Accept: application/json" \ "http://localhost:8888/version/cluster" "1.2.2"But when I use `Accept: text/xml`, the response is correct XML. curl -X GET \ -H "Accept: text/xml" \ "http://localhost:8888/version/cluster" &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;&lt;ClusterVersion&gt;1.2.2&lt;/ClusterVersion&gt;</description>
      <version>1.2.2</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestVersionResource.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestStorageClusterVersionModel.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.client.TestXmlParsing.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel.java</file>
    </fixedFiles>
  </bug>
  <bug id="17717" opendate="2017-3-1 00:00:00" fixdate="2017-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect ZK ACL set for HBase superuser</summary>
      <description>Shreya was doing some testing of a deploy of HBase, verifying that the ZK ACLs were actually set as we expect (yay, security).She noticed that, in some cases, we were seeing multiple ACLs for the same user.'world,'anyone: r'sasl,'hbase: cdrwa'sasl,'hbase: cdrwaAfter digging into this (and some insight from the mighty enis), we realized that this was happening because of an overridden value for hbase.superuser. However, the ACL value doesn't match what we'd expect to see (as hbase.superuser was set to cstm-hbase).After digging into this code, it seems like the auth ACL scheme in ZooKeeper does not work as we expect. if (superUser != null) { acls.add(new ACL(Perms.ALL, new Id("auth", superUser))); }In the above, the "auth" scheme ignores any provided "subject" in the Id object. It only considers the authentication of the current connection. As such, our usage of this never actually sets the ACL for the superuser correctly.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,1.1.10,1.2.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="19223" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to Date Tiered compaction from branch-1.2 and branch-1.1 ref guide</summary>
      <description>I have download hbase 1.2.6 src package. But not found class DateTieredStoreEngine, which is present in document: https://hbase.apache.org/1.2/book.html.</description>
      <version>1.1.5,1.2.2</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
</bugrepository>
