<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="22012" opendate="2019-3-8 00:00:00" fixdate="2019-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpaceQuota DisableTableViolationPolicy will cause cycles of enable/disable table</summary>
      <description>Space Quota: Policy state is getting changed from disable to Observance after sometime automatically.Steps:1: Create a table with space quota policy as Disable2: Put some data so that table state is in space quota violation3: So observe that table state is in violation4: Now wait for some time5: Observe that after some time table state is changing to to Observance however table is still disablededit (elserj): The table is automatically moved back from the violation state because of the code added that tried to ride over RITs. When a Region is not online (whether normally or abnormally), the RegionSizeReports are not sent from RS to Master. Eventually, enough Regions are not reported which dips below the acceptable threshold and we automatically move the table back to the "acceptable" space quota state (not in violation). We could skip this failsafe when we're checking for a quota that has the DisableTable violation policy.</description>
      <version>3.0.0-alpha-1,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotaBasicFunctioning.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22013" opendate="2019-3-8 00:00:00" fixdate="2019-9-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SpaceQuotas - getNumRegions() returning wrong number of regions due to region replicas</summary>
      <description>Space Quota: Space Quota Issue: If a table is created with region replica then quota calculation is not happeningSteps:1: Create a table with 100 regions with region replica 32: Observe that 'hbase:quota' table doesn't have entry of usage for this table So In UI only policy Limit and Policy is shown but not Usage and State.Reason: It looks like File system utilization core is sending data of 100 reasons but not the size of region replicas. But in quota observer chore, it is considering total region(actual regions+ replica reasons) So the ratio of reported regions is less then configured percentRegionsReportedThreshold.SO quota calculation is not happening</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.SpaceQuotaHelperForTests.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22016" opendate="2019-3-8 00:00:00" fixdate="2019-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite the block reading methods by using hbase.nio.ByteBuff</summary>
      <description>We've some useful discussion in HBASE-22005, so open an new JIRA for the ByteBuffer block reading.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockPositionalRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestChecksum.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="22020" opendate="2019-3-8 00:00:00" fixdate="2019-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to yetus 0.9.0</summary>
      <description>branch-1/jdk7 checkstyle dtd xml parse complaint; "script engine for language js can not be found"See parent for some context. Checkstyle references dtds that were hosted on puppycrawl, then on sourceforge up until ten days ago. Nightlies are failing for among other reasons, complaint that there is bad xml in the build... notably, the unresolvable DTDs.I'd just update the DTDs but there is a need for a js engine some where and openjdk7 doesn't ship with one (openjdk8 does). That needs addressing and then we can backport the parent issue...See https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Nightly/job/branch-1/710/artifact/output-general/xml.txt ... which in case its rolled away, is filled with this message:"script engine for language js can not be found"</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="22025" opendate="2019-3-11 00:00:00" fixdate="2019-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>RAT check fails in nightlies; fails on (old) test data files.</summary>
      <description>The nightly runs where we check RM steps fails in branch-2.1 because the rat test complains about old test data files not having licenses. See HBASE-22022 for how we turned up this issue. This JIRA adds exclusions for these files that cause failure.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22379" opendate="2019-5-8 00:00:00" fixdate="2019-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Markdown for "Voting on Release Candidates" in book</summary>
      <description>The Markdown in the section "Voting on Release Candidates" of the HBase book seems to be broken. It looks like that there should be a quote, which isn't displayed correctly. Same is true for the formatting of the Maven RAT command.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22618" opendate="2019-6-22 00:00:00" fixdate="2019-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>added the possibility to load custom cost functions</summary>
      <description>Hi,We wouls like to open the discussion about bringing the possibility to have regions deployed on Heterogeneous deployment, i.e Hbase cluster running different kind of hardware.Why? Cloud deployments means that we may not be able to have the same hardware throughout the years Some tables may need special requirements such as SSD whereas others should be using hard-drives  in our usecase(single table, dedicated HBase and Hadoop tuned for our usecase, good key distribution), the number of regions per RS was the real limit for us.Our usecaseWe found out that in our usecase(single table, dedicated HBase and Hadoop tuned for our usecase, good key distribution), the number of regions per RS was the real limit for us.Over the years, due to historical reasons and also the need to benchmark new machines, we ended-up with differents groups of hardware: some servers can handle only 180 regions, whereas the biggest can handle more than 900. Because of such a difference, we had to disable the LoadBalancing to avoid the roundRobinAssigmnent. We developed some internal tooling which are responsible for load balancing regions across RegionServers. That was 1.5 year ago.Our Proof-of-conceptWe did work on a Proof-of-concept here, and some early tests here, here, and here. We wrote the balancer for our use-case, which means that: there is one table there is no region-replica good key dispersion there is no regions on masterA rule file is loaded before balancing. It contains lines of rules. A rule is composed of a regexp for hostname, and a limit. For example, we could have: rs&amp;#91;0-9&amp;#93; 200rs1&amp;#91;0-9&amp;#93; 50 RegionServers with hostname matching the first rules will have a limit of 200, and the others 50. If there's no match, a default is set.Thanks to the rule, we have two informations: the max number of regions for this cluster, and the rules for each servers. HeterogeneousBalancer will try to balance regions according to their capacity.Let's take an example. Let's say that we have 20 RS: 10 RS, named through rs0 to rs9 loaded with 60 regions each, and each can handle 200 regions. 10 RS, named through rs10 to rs19 loaded with 60 regions each, and each can support 50 regions.Based on the following rules: rs&amp;#91;0-9&amp;#93; 200rs1&amp;#91;0-9&amp;#93; 50 The second group is overloaded, whereas the first group has plenty of space.We know that we can handle at maximum 2500 regions (200*10 + 50*10) and we have currently 1200 regions (60*20). HeterogeneousBalancer will understand that the cluster is full at 48.0% (1200/2500). Based on this information, we will then try to put all the RegionServers to ~48% of load according to the rules. In this case, it will move regions from the second group to the first.The balancer will: compute how many regions needs to be moved. In our example, by moving 36 regions on rs10, we could go from 120.0% to 46.0% select regions with lowest data-locality try to find an appropriate RS for the region. We will take the lowest available RS.Other implementations and ideasClay Baenziger proposed this idea on the dev ML:Could it work to have the stochastic load balancer use pluggable cost functions instead of this static list of cost functions? Then, could this type of a load balancer be implemented simply as a new cost function which folks could choose to load and mix with the others?I think this could be an interesting way to include user-functions in the mix. As you know your hardawre and the pattern access, you can easily know which metrics is important for balancing, for us, it will only be the number of regions, but we could mix-it with the incoming writes! bhupendra.jain proposed also the ideas of "labels" Internally, we are also having discussion to develop similar solution. In our approach, We were also thinking of adding "RS Label" Feature similar to Hadoop Node Label feature. Each RS can have a label to denote its capabilities / resources . When user create table, there can be extra attributes with its descriptor. The balancer can decide to host region of table based on RS label and these attributes further.   With RS label feature, Balancer can be more intelligent.  Example tables with high read load needs more cache backed by SSDs , So such table regions should be hosted on RS having SSDs ... I love the idea, but I think Clay's idea is better for a better and faster first set of commits on the subject! What do you think?</description>
      <version>3.0.0-alpha-1,2.2.0,2.2.1,2.1.6,1.4.11,2.1.7</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestStochasticLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.StochasticLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2262" opendate="2010-2-24 00:00:00" fixdate="2010-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZKW.ensureExists should check for existence</summary>
      <description>The fact that ZKW.ensureExists relies on KeeperException.NodeExistsException creates a lot of chatter in HBase and Zookeeper logs and also confuses users. We should use ZooKeeper.exists instead.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22689" opendate="2019-7-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Line break for fix version in documentation</summary>
      <description>The section describing the policy for the fix version in JIRA is missing line breaks.</description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22743" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClientUtils for hbase-examples</summary>
      <description>hbase-examples have many Client Demo classes that use many utility methods that can be put in a Utility class e.g. initializing LoginContext for every demo class to utilize.</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift.HttpDoAsClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift.DemoClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift2.DemoClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.rest.RESTDemoClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22800" opendate="2019-8-6 00:00:00" fixdate="2019-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add mapreduce dependencies to hbase-shaded-testing-util</summary>
      <description>MiniMRCluster is missing from the generated hbase-shaded-testing-util artifact.</description>
      <version>3.0.0-alpha-1,2.2.1,2.1.6</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-testing-util.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22801" opendate="2019-8-6 00:00:00" fixdate="2019-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven build issue on Github PRs</summary>
      <description>PreCommit job for GitHub pull requests started to fail frequently with maven issues.[ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.2:install (default-install) on project hbase-http: Failed to install metadata org.apache.hbase:hbase-http/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hbase/hbase-http/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got a (position: END_TAG seen ...&lt;/metadata&gt;\na... @12:2) -&gt; [Help 1]The local .m2 repository might be used by concurrent builds.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  <bug id="22824" opendate="2019-8-9 00:00:00" fixdate="2019-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show filesystem path for the orphans regions on filesystem</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestHbckChore.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.hbck.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22859" opendate="2019-8-15 00:00:00" fixdate="2019-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[HBCK2] Fix the orphan regions on filesystem</summary>
      <description>Plan to add this feature to HBCK2 tool firstly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,hbase-operator-tools-1.0.0,2.2.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.BulkLoadHFilesTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="2286" opendate="2010-3-4 00:00:00" fixdate="2010-4-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Transactional Contrib] Correctly handle or avoid cases where writes occur in same millisecond</summary>
      <description>This patch fixes a few issues where puts/deletes occur with the same timestamp.In the indexing layer, we avoid a Delete followed by a Put to the same row for the index update. When the row is the same, we can just do the put.In the transactional layer, we correctly handled put, put, scan. This way the scan will see the last put, even if they have the same timestamp.Remove the sleep to fix the putPutScan transactional test, and run it many times to be sure we hit the case where they are in the same millisecond.Also some small cleanup, null handling, and fail-fast changes.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.transactional.src.test.java.org.apache.hadoop.hbase.client.transactional.TestTransactions.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
      <file type="M">contrib.transactional.src.main.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23035" opendate="2019-9-17 00:00:00" fixdate="2019-9-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain region to the last RegionServer make the failover slower</summary>
      <description>Now if one RS crashed, the regions will try to use the old location for the region deploy. But one RS only have 3 threads to open region by default. If a RS have hundreds of regions, the failover is very slower. Assign to same RS may have good locality if the Datanode is deploied on same host. But slower failover make the availability worse. And the locality is not big deal when deploy HBase on cloud.This was introduced by HBASE-18946.</description>
      <version>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithRestartScenarios.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRetainAssignmentOnRestartSplitWithoutZk.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRetainAssignmentOnRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSCPBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="23092" opendate="2019-9-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the RM tooling in dev-tools/create-release generic</summary>
      <description>The dev-tools/create-release scripts were originally about creating hbase core RCs (Original idea and script versions were copied over from apache spark). Subsequently, they were checked into hbase-operator-tools repo and genericized so they worked in that context. Today, after a few mods, the create-release scripts from hbase-operator-tools w/ some edits generated an RC of hbase-thirdparty.This issue is edits on the dev-tools/create-tools on master branch so the scripts can create RCs across these three repos at least (with more to follow).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.vote.tmpl</file>
      <file type="M">dev-support.create-release.release-util.sh</file>
      <file type="M">dev-support.create-release.release-tag.sh</file>
      <file type="M">dev-support.create-release.release-build.sh</file>
      <file type="M">dev-support.create-release.README.txt</file>
      <file type="M">dev-support.create-release.do-release.sh</file>
      <file type="M">dev-support.create-release.do-release-docker.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23172" opendate="2019-10-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Canary region success count metrics reflect column family successes, not region successes</summary>
      <description>HBase Canary reads once per column family per region. The current "region success count" should actually be "column family success count," which means we need another metric that actually reflects region success count. Additionally, the region read and write latencies only store the latencies of the last column family of the region read. Instead of a map of regions to a single latency value and success value, we should map each region to a list of such values.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0,2.1.5,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestCanaryTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.CanaryTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="23182" opendate="2019-10-17 00:00:00" fixdate="2019-11-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The create-release scripts are broken</summary>
      <description>Only several small bugs but it does make the releasing fail...Mostly introduced by HBASE-23092.Will upload the patch after I successully published 2.2.2RC0...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.release-util.sh</file>
      <file type="M">dev-support.create-release.release-build.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23187" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update parent region state to SPLIT in meta</summary>
      <description>When split, the parent region set to SPLIT in memory, but the meta do not, so in some circumstances will bring back the parent region.</description>
      <version>2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
    </fixedFiles>
  </bug>
  <bug id="2319" opendate="2010-3-13 00:00:00" fixdate="2010-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] multiuser mode: request shaping</summary>
      <description>Simple token bucket strategy. Return HTTP 509 (commonly used for "Bandwidth Limit Exceeded" but not in the HTTP spec) when request rate exhausts tokens. Note this is about request rate limiting, not about network traffic shaping.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.MiniClusterTestBase.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestJDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.test.java.org.apache.hadoop.hbase.stargate.auth.TestHBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.VersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.TableResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterVersionResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.StorageClusterStatusResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.SchemaResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerInstanceResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowSpec.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RootResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RESTServlet.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RegionsResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.Constants.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.ZooKeeperAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.User.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.JDBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.HTableAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.HBCAuthenticator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.auth.Authenticator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23202" opendate="2019-10-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExportSnapshot (import) will fail if copying files to root directory takes longer than cleaner TTL</summary>
      <description>HBASE-17330 removed the checking of the snapshot .tmp directory when determining which files are candidates for deletes. It appears that in the latest branches, this isn't an issue for taking a snapshot as it checks whether a snapshot is in progress via the SnapshotManager.However, when using the ExportSnapshot tool to import a snapshot into a cluster, it will first copy the snapshot manifest into /.snapshot/.tmp/&lt;snapshot&gt; &amp;#91;1&amp;#93;, copies the files, and then renames the snapshot manifest to the final snapshot directory. If the copyFiles job takes longer than the cleaner TTL, the ExportSnapshot job will fail because HFiles will get deleted before the snapshot is committed to the final directory. The ExportSnapshot tool already has a functionality to skipTmp and write the manifest directly to the final location. However, this has unintended consequences such as the snapshot appearing to the user before it is usable. So it looks like we will have to bring back the tmp directory check to avoid this situation.&amp;#91;1&amp;#93; https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java#L1029</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.1,1.4.11,2.1.7</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCacheWithDifferentWorkingDir.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="23318" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadTestTool doesn&amp;#39;t start</summary>
      <description>./bin/hbase ltt after unpacking a binary tarball distribution doesn't start with a CNFE. We are missing the tests jar from hbase-zookeeper. The client tarball includes this but if one wants to launch it on a server or a general purpose deploy (i.e. not the client tarball) the test jar has to be in the server classpath as well.</description>
      <version>2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.8,2.2.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.components.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23320" opendate="2019-11-19 00:00:00" fixdate="2019-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade surefire plugin to 3.0.0-M4</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3,2.1.9</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23321" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck2] fixHoles of fixMeta doesn&amp;#39;t update in-memory state</summary>
      <description>If hbase:meta has holes, you can run fixMeta from hbck2. This will close the holes but you have to restart the Master for it to notice the new region additions. Also, we were plugging holes by adding regions but no state for the region which makes it awkward to subsequently assign. Fix.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMetaFixer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetaFixer.java</file>
    </fixedFiles>
  </bug>
  <bug id="23322" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck2] Simplification on HBCKSCP scheduling</summary>
      <description>I can make the scheduling of HBCKSCP simpler. I can also fix a bug in parent issue that I notice after exercising it a bunch on a cluster.The bug is that 'Unknown Servers' seem to be retained in the Map of reporting servers. They are usually cleared just before an SCP is scheduled but scheduling HBCKSCP doesn't go the usual route.The patch here forces HBCKSCP via the usual SCP route only at the scheduling time, context dictates whether SCP or the scouring HBCKSCP.Let me put up a patch and will test in meantime.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestClusterRestartFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.HBCKServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.ServerStateNode.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2418" opendate="2010-4-7 00:00:00" fixdate="2010-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for ZooKeeper authentication</summary>
      <description>Some users may run a ZooKeeper cluster in "multi tenant mode" meaning that more than one client service wouldlike to share a single ZooKeeper service instance (cluster). In this case the client services typically want to protecttheir data (ZK znodes) from access by other services (tenants) on the cluster. Say you are running HBase and Solr and Neo4j, or multiple HBase instances, etc... having authentication/authorization on the znodes is important for both security and helping to ensure that services don't interact negatively (touch each other's data).Today HBase does not have support for authentication or authorization. This should be added to the HBase clientsthat are accessing the ZK cluster. In general it means calling addAuthInfo once after a session is established:http://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/ZooKeeper.html#addAuthInfo(java.lang.String, byte[])with a user specific credential, often times this is a shared secret or certificate. You may be able to statically configure thisin some cases (config string or file to read from), however in my case in particular you may need to access it programmatically,which adds complexity as the end user may need to load code into HBase for accessing the credential.Secondly you need to specify a non "world" ACL when interacting with znodes (create primarily):http://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/data/ACL.htmlhttp://hadoop.apache.org/zookeeper/docs/current/api/org/apache/zookeeper/ZooDefs.htmlFeel free to ping the ZooKeeper team if you have questions. It might also be good to discuss with some potential end users - in particular regarding how the end user can specify the credential.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="24180" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Edit test doc around forkcount and speeding up test runs</summary>
      <description>Give the 'Running unit tests' section an edit. Fold in findings of late around how to speed up tests, hardware limits and sizings, etc.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
</bugrepository>
