<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="10274" opendate="2014-1-3 00:00:00" fixdate="2014-1-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniZookeeperCluster should close ZKDatabase when shutdown ZooKeeperServers</summary>
      <description>HBASE-6820 points out the problem but not fix completely.killCurrentActiveZooKeeperServer() and killOneBackupZooKeeperServer() will shutdown the ZooKeeperServer and need to close ZKDatabase as well.</description>
      <version>0.94.3</version>
      <fixedVersion>0.98.0,0.96.2,0.99.0,0.94.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="1028" opendate="2008-11-26 00:00:00" fixdate="2008-11-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>If key does not exist, return null in getRow rather than an empty RowResult</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="10346" opendate="2014-1-15 00:00:00" fixdate="2014-1-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Documentation for stateless scanner</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.98.0,0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.package.html</file>
    </fixedFiles>
  </bug>
  <bug id="10370" opendate="2014-1-17 00:00:00" fixdate="2014-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction in out-of-date Store causes region split failure</summary>
      <description>In out product cluster, we encounter a problem that two daughter regions can not been opened for FileNotFoundException.2014-01-14,20:12:46,927 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Running rollback/cleanup of failed split of user_profile,xxxxxxxxx,1389671863815.99e016485b0bc142d67ae07a884f6966.; Failed lg-hadoop-st34.bj,21600,1389060755669-daughterOpener=ec8bbda0f132c481b451fa40e7152b98java.io.IOException: Failed lg-hadoop-st34.bj,21600,1389060755669-daughterOpener=ec8bbda0f132c481b451fa40e7152b98 at org.apache.hadoop.hbase.regionserver.SplitTransaction.openDaughters(SplitTransaction.java:375) at org.apache.hadoop.hbase.regionserver.SplitTransaction.execute(SplitTransaction.java:467) at org.apache.hadoop.hbase.regionserver.SplitRequest.run(SplitRequest.java:69) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)Caused by: java.io.IOException: java.io.IOException: java.io.FileNotFoundException: File does not exist: /hbase/lgprc-xiaomi/user_profile/99e016485b0bc142d67ae07a884f6966/A/5e05d706e4a84f34acc2cf00f089a4cf....The reason is that a compaction in an out-of-date Store deletes the hfiles, which are referenced by the daughter regions after split. This will cause the daughter regions can not be opened forever. The timeline is that Assumption: there are two hfiles: a, b in Store A in Region Rt0: A compaction request of Store A(a+b) in Region R is sent.t1: First Split for Region R. But this split is timeout and rollbacked. In the rollback, region reinitializes all store objects , see SplitTransaction #824. Now the store is Region R is A'(a+b).t2: Run the compaction sent in t0 . (hfile: a + b -&gt; c): A(a+b) -&gt; A(c). Hfile a and b are archived.t3: Another Split for Region R. R splits into two region R.0, R.1, which create hfile references for hfile a, b from Store A'(a + b)t4: For hfile a, b have been deleted, the opening for region R.0 and R.1 will failed for FileNotFoundException.I have add a test to identity this problem.After search the jira, maybe HBASE-8502 is the same problem. goldin</description>
      <version>0.94.3,0.98.0,0.99.0</version>
      <fixedVersion>0.98.0,0.96.2,0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="10371" opendate="2014-1-17 00:00:00" fixdate="2014-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction creates empty hfile, then selects this file for compaction and creates empty hfile and over again</summary>
      <description>(1) Select HFile for compaction2014-01-16 01:01:25,111 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactSelection: Deleting the expired store file by compaction: hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/f3e38d10d579420494079e17a2557f0b whose maxTimeStamp is -1 while the max expired timestamp is 1389632485111(2) Compact2014-01-16 01:01:26,042 DEBUG org.apache.hadoop.hbase.regionserver.Compactor: Compacting hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/f3e38d10d579420494079e17a2557f0b, keycount=0, bloomtype=NONE, size=534, encoding=NONE2014-01-16 01:01:26,045 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/.tmp/40de5d79f80e4fb197e409fb99ab0fd8 with permission=rwxrwxrwx2014-01-16 01:01:26,076 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming compacted file at hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/.tmp/40de5d79f80e4fb197e409fb99ab0fd8 to hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd82014-01-16 01:01:26,142 INFO org.apache.hadoop.hbase.regionserver.Store: Completed compaction of 1 file(s) in a of storagetable,01:,1369377609136.7d8941661904fb99a41f79a1fce47767. into 40de5d79f80e4fb197e409fb99ab0fd8, size=534; total size for store is 399.0 M2014-01-16 01:01:26,142 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest: completed compaction: regionName=storagetable,01:,1369377609136.7d8941661904fb99a41f79a1fce47767., storeName=a, fileCount=1, fileSize=534, priority=16, time=18280340606333745; duration=0sec(3) Select HFile for compaction2014-01-16 03:48:05,120 INFO org.apache.hadoop.hbase.regionserver.compactions.CompactSelection: Deleting the expired store file by compaction: hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd8 whose maxTimeStamp is -1 while the max expired timestamp is 1389642485120(4) Compact2014-01-16 03:50:17,731 DEBUG org.apache.hadoop.hbase.regionserver.Compactor: Compacting hdfs://dump002002.cm6:9000/hbase-0.90/storagetable/7d8941661904fb99a41f79a1fce47767/a/40de5d79f80e4fb197e409fb99ab0fd8, keycount=0, bloomtype=NONE, size=534, encoding=NONE2014-01-16 03:50:17,732 DEBUG org.apache.hadoop.hbase.util.FSUtils: Creating file=hdfs://dump002002.cm6:9000/hbase-0.90... this loop for ever.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.2,0.99.0,0.94.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultCompactSelection.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MockStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="10506" opendate="2014-2-12 00:00:00" fixdate="2014-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail-fast if client connection is lost before the real call be executed in RPC layer</summary>
      <description>In current HBase rpc impletement, there is no any connection double-checking just before the "call" be invoked, considing there's a gc or other OS scheduling or the call queue is full enough(e.g. the server side is slow/hang due to some issues), and if the client side has a small rpc timeout value, it could be possible when this request be taken from call queue, the client connection is lost in that moment. we'd better has some fail-fast code before the reall "call" be invoked, it just waste the server side resource.Here is a strace trace from our production env:2014-02-11,18:16:19,525 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server Responder, call get([B@3eae6c77, {"timeRange":&amp;#91;0,9223372036854775807&amp;#93;,"totalColumns":1,"cacheBlocks":true,"families":{"X":["T"]},"maxVersions":1,"row":"074103000000001-m8997060"}), rpc version=1, client version=29, methodsFingerPrint=-241105381 from 10.101.10.181:43252: output error2014-02-11,18:16:19,526 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server handler 151 on 12600 caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: null2014-02-11,18:16:19,797 ERROR org.apache.hadoop.hbase.regionserver.HRegionServer:org.apache.hadoop.hbase.ipc.CallerDisconnectedException: Aborting call get([B@3f10ffd2, {"timeRange":&amp;#91;0,9223372036854775807&amp;#93;,"totalColumns":1,"cacheBlocks":true,"families":{"X":["T"]},"maxVersions":1,"row":"4245978-m7281526"}), rpc version=1, client version=29, methodsFingerPrint=-241105381 from 10.101.10.181:43259 after 0 ms, since caller disconnectedat org.apache.hadoop.hbase.ipc.HBaseServer$Call.throwExceptionIfCallerDisconnected(HBaseServer.java:450)at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.nextInternal(HRegion.java:3633)at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3590)at org.apache.hadoop.hbase.regionserver.HRegion$RegionScannerImpl.next(HRegion.java:3615)at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4414)at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:4387)at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:2075)at sun.reflect.GeneratedMethodAccessor29.invoke(Unknown Source)at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597)at org.apache.hadoop.hbase.ipc.SecureRpcEngine$Server.call(SecureRpcEngine.java:460)at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1457)2014-02-11,18:16:19,802 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server Responder, call get([B@3f10ffd2, {"timeRange":&amp;#91;0,9223372036854775807&amp;#93;,"totalColumns":1,"cacheBlocks":true,"families":{"X":["T"]},"maxVersions":1,"row":"4245978-m7281526"}), rpc version=1, client version=29, methodsFingerPrint=-241105381 from 10.101.10.181:43259: output error2014-02-11,18:16:19,802 WARN org.apache.hadoop.ipc.HBaseServer: IPC Server handler 46 on 12600 caught a ClosedChannelException, this means that the server was processing a request but the client went away. The error message was: nullWith this fix, we can reduce this hit probability at least the upstream hadoop has this checking already, see: https://github.com/apache/hadoop-common/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java#L2034-L2036</description>
      <version>0.94.3</version>
      <fixedVersion>0.98.0,0.96.2,0.99.0,0.94.17</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug id="12949" opendate="2015-1-30 00:00:00" fixdate="2015-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanner can be stuck in infinite loop if the HFile is corrupted</summary>
      <description>We've encountered problem where compaction hangs and never completes.After looking into it further, we found that the compaction scanner was stuck in a infinite loop. See stack below.org.apache.hadoop.hbase.regionserver.KeyValueHeap.generalizedSeek(KeyValueHeap.java:296)org.apache.hadoop.hbase.regionserver.KeyValueHeap.reseek(KeyValueHeap.java:257)org.apache.hadoop.hbase.regionserver.StoreScanner.reseek(StoreScanner.java:697)org.apache.hadoop.hbase.regionserver.StoreScanner.seekToNextRow(StoreScanner.java:672)org.apache.hadoop.hbase.regionserver.StoreScanner.next(StoreScanner.java:529)org.apache.hadoop.hbase.regionserver.compactions.Compactor.performCompaction(Compactor.java:223)We identified the hfile that seems to be corrupted. Using HFile tool shows the following:[biadmin@hdtest009 bin]$ hbase org.apache.hadoop.hbase.io.hfile.HFile -v -k -m -f /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b715/01/23 11:53:17 INFO Configuration.deprecation: hadoop.native.lib is deprecated. Instead, use io.native.lib.available15/01/23 11:53:18 INFO util.ChecksumType: Checksum using org.apache.hadoop.util.PureJavaCrc3215/01/23 11:53:18 INFO util.ChecksumType: Checksum can use org.apache.hadoop.util.PureJavaCrc32C15/01/23 11:53:18 INFO Configuration.deprecation: fs.default.name is deprecated. Instead, use fs.defaultFSScanning -&gt; /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b7WARNING, previous row is greater then current row filename -&gt; /user/biadmin/CUMMINS_INSITE_V1/7106432d294dd844be15996ccbf2ba84/attributes/f1a7e3113c2c4047ac1fc8fbcb41d8b7 previous -&gt; \x00/20110203-094231205-79442793-1410161293068203000\x0Aattributes16794406\x00\x00\x01\x00\x00\x00\x00\x00\x00 current -&gt;Exception in thread "main" java.nio.BufferUnderflowException at java.nio.Buffer.nextGetIndex(Buffer.java:489) at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:347) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.readKeyValueLen(HFileReaderV2.java:856) at org.apache.hadoop.hbase.io.hfile.HFileReaderV2$ScannerV2.next(HFileReaderV2.java:768) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.scanKeysValues(HFilePrettyPrinter.java:362) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.processFile(HFilePrettyPrinter.java:262) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.run(HFilePrettyPrinter.java:220) at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) at org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.main(HFilePrettyPrinter.java:539) at org.apache.hadoop.hbase.io.hfile.HFile.main(HFile.java:802)Turning on Java Assert shows the following:Exception in thread "main" java.lang.AssertionError: Key 20110203-094231205-79442793-1410161293068203000/attributes:16794406/1099511627776/Minimum/vlen=15/mvcc=0 followed by a smaller key //0/Minimum/vlen=0/mvcc=0 in cf attributes at org.apache.hadoop.hbase.regionserver.StoreScanner.checkScanOrder(StoreScanner.java:672)It shows that the hfile seems to be corrupted &amp;#8211; the keys don't seem to be right.But Scanner is not able to give a meaningful error, but stuck in an infinite loop in here:KeyValueHeap.generalizedSeek()while ((scanner = heap.poll()) != null) {}</description>
      <version>0.94.3,0.98.10</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="3925" opendate="2011-5-26 00:00:00" fixdate="2011-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Shell&amp;#39;s -d and debug cmd behave the same</summary>
      <description>The -d option switches log4j to DEBUG and leaves the backtrace level at the default. When using the supplied debug command we only switch the backtrace, but I would think this also should set the log4j levels:# Debugging methoddef debug if @shell.debug @shell.debug = false conf.back_trace_limit = 0 else @shell.debug = true conf.back_trace_limit = 100 end debug?endcould be something like # Debugging methoddef debug if @shell.debug @shell.debug = false conf.back_trace_limit = 0 log_level = org.apache.log4j.Level::ERROR else @shell.debug = true conf.back_trace_limit = 100 log_level = org.apache.log4j.Level::DEBUG end org.apache.log4j.Logger.getLogger("org.apache.zookeeper").setLevel(log_level) org.apache.log4j.Logger.getLogger("org.apache.hadoop.hbase").setLevel(log_level) debug?end</description>
      <version>0.90.3,0.90.7,0.92.2,0.94.3,0.98.0,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="6816" opendate="2012-9-18 00:00:00" fixdate="2012-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WINDOWS] line endings on checkout for .sh files</summary>
      <description>On code checkout from svn or git, we need to ensure that the line endings for .sh files are LF, so that they work with cygwin. This is important for getting src/saveVersion.sh to work.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.images.hbase.logo.svg</file>
    </fixedFiles>
  </bug>
  <bug id="682" opendate="2008-6-12 00:00:00" fixdate="2008-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regularize toString</summary>
      <description>Make all of our toStrings work the same. While at it, make them ruby Hash style so they play well in the (jruby) shell</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestToString.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="6820" opendate="2012-9-18 00:00:00" fixdate="2012-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WINDOWS] MiniZookeeperCluster should ensure that ZKDatabase is closed upon shutdown()</summary>
      <description>MiniZookeeperCluster.shutdown() shuts down the ZookeeperServer and NIOServerCnxnFactory. However, MiniZookeeperCluster uses a deprecated ZookeeperServer constructor, which in turn constructs its own FileTxnSnapLog, and ZKDatabase. Since ZookeeperServer.shutdown() does not close() the ZKDatabase, we have to explicitly close it in MiniZookeeperCluster.shutdown().Tests effected by this areTestSplitLogManagerTestSplitLogWorkerTestOfflineMetaRebuildBaseTestOfflineMetaRebuildHoleTestOfflineMetaRebuildOverlap</description>
      <version>0.94.3,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="6822" opendate="2012-9-18 00:00:00" fixdate="2012-11-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WINDOWS] MiniZookeeperCluster multiple daemons bind to the same port</summary>
      <description>TestHBaseTestingUtility.testMiniZooKeeper() tests whether the mini zk cluster is working by launching 5 threads corresponding to zk servers. NIOServerCnxnFactory.configure() configures the socket as: this.ss = ServerSocketChannel.open(); ss.socket().setReuseAddress(true);setReuseAddress() is set, because it allows the server to come back up and bind to the same port before the socket is timed-out by the kernel.Under windows, the behavior on ServerSocket.setReuseAddress() is different than on linux, in which it allows any process to bind to an already-bound port. This causes ZK nodes starting on the same node, to be able to bind to the same port. The following part of the patch at https://issues.apache.org/jira/browse/HADOOP-8223 deals with this case for Hadoop:if(Shell.WINDOWS) {+ // result of setting the SO_REUSEADDR flag is different on Windows+ // http://msdn.microsoft.com/en-us/library/ms740621(v=vs.85).aspx+ // without this 2 NN's can start on the same machine and listen on + // the same port with indeterminate routing of incoming requests to them+ ret.setReuseAddress(false);+ }We should do the same in Zookeeper (I'll open a ZOOK issue). But in the meantime, we can fix hbase tests to not rely on BindException to resolve for bind errors. Especially, in MiniZKCluster.startup() when starting more than 1 servers, we already know that we have to increment the port number.</description>
      <version>0.94.3,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
    </fixedFiles>
  </bug>
  <bug id="6825" opendate="2012-9-19 00:00:00" fixdate="2012-1-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WINDOWS] Java NIO socket channels does not work with Windows ipv6</summary>
      <description>While running the test TestAdmin.testCheckHBaseAvailableClosesConnection(), I noticed that it takes very long, since it sleeps for 2sec * 500, because of zookeeper retries. The root cause of the problem is that ZK uses Java NIO to create ServerSorcket's from ServerSocketChannels. Under windows, the ipv4 and ipv6 is implemented independently, and Java seems that it cannot reuse the same socket channel for both ipv4 and ipv6 sockets. We are getting "java.net.SocketException: Address family not supported by protocolfamily" exceptions. When, ZK client resolves "localhost", it gets both v4 127.0.0.1 and v6 ::1 address, but the socket channel cannot bind to both v4 and v6. The problem is reported as:http://bugs.sun.com/view_bug.do?bug_id=6230761http://stackoverflow.com/questions/1357091/binding-an-ipv6-server-socket-on-windowsAlthough the JDK bug is reported as resolved, I have tested with jdk1.6.0_33 without any success. Although JDK7 seems to have fixed this problem. In ZK, we can replace the ClientCnxnSocket implementation from ClientCnxnSocketNIO to a non-NIO one, but I am not sure that would be the way to go.Disabling ipv6 resolution of "localhost" is one other approach. I'll test it to see whether it will be any good.</description>
      <version>0.94.3,0.95.2</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="6836" opendate="2012-9-19 00:00:00" fixdate="2012-9-19 01:00:00" resolution="Later">
    <buginformation>
      <summary>[89-fb] Parallel deletes in HBase Thrift server</summary>
      <description>We need to expose server-side parallel batch deletes through the Thrift server.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotsFromAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSTableDescriptors.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.UnknownSnapshotException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotCreationException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.HBaseSnapshotException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.server.errorhandling.impl.ExceptionOrchestrator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.MasterAdminProtocol.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.executor.EventHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.MasterAdmin.proto</file>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.MasterAdminProtos.java</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="7250" opendate="2012-11-30 00:00:00" fixdate="2012-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create integration test for balancing regions and killing region servers - 2</summary>
      <description>The original test is too general; need another one that would be more targeted and would test master logic in particular (e.g. not kill master). I re-discovered HBASE-6060 using it on the first run</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.util.ChaosMonkey.java</file>
    </fixedFiles>
  </bug>
  <bug id="7296" opendate="2012-12-7 00:00:00" fixdate="2012-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase.master.loadbalancer.class in the documentation</summary>
      <description>hbase.master.loadbalancer.class information is missing from the documentation. Might be useful to add it.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.96.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug id="730" opendate="2008-7-8 00:00:00" fixdate="2008-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>On startup, rinse STARTCODE and SERVER from .META.</summary>
      <description>Look into what it would take purging startcode and server from .META. on startup. It might make startup run faster. In particular, Clint Morgan was asking for faster startup. Below is from a reply. The +1 is from JK.&gt; &gt; Looking at code, we have the concept of an 'initial' scan. I&gt; &gt; wonder if things would run faster for you if on the initial&gt; &gt; scan we just cleared all SERVER and STARTCODE entries in&gt; &gt; .META. rather than wait on regionserver reports?+1</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7301" opendate="2012-12-7 00:00:00" fixdate="2012-12-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Force ipv4 for unit tests</summary>
      <description>These two tests are failing when I run them locally:Failed tests: testMultiSlaveReplication(org.apache.hadoop.hbase.replication.TestMultiSlaveReplication): Waited too much time for put replication testCyclicReplication(org.apache.hadoop.hbase.replication.TestMasterReplication): Waited too much time for put replication testSimplePutDelete(org.apache.hadoop.hbase.replication.TestMasterReplication): Waited too much time for put replicationThe TestMasterReplication is NPE'ingMighty JD said he'd take a looksee.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7342" opendate="2012-12-12 00:00:00" fixdate="2012-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split operation without split key incorrectly finds the middle key in off-by-one error</summary>
      <description>I took a deeper look into issues I was having using region splitting when specifying a region (but not a key for splitting).The midkey calculation is off by one and when there are 2 rows, will pick the 0th one. This causes the firstkey to be the same as midkey and the split will fail. Removing the -1 causes it work correctly, as per the test I've added.Looking into the code here is what goes on:1. Split takes the largest storefile2. It puts all the keys into a 2-dimensional array called blockKeys[][]. Key i resides as blockKeys&amp;#91;i&amp;#93;3. Getting the middle root-level index should yield the key in the middle of the storefile4. In step 3, we see that there is a possible erroneous (-1) to adjust for the 0-offset indexing.5. In a result with where there are only 2 blockKeys, this yields the 0th block key. 6. Unfortunately, this is the same block key that 'firstKey' will be.7. This yields the result in HStore.java:1873 ("cannot split because midkey is the same as first or last row")8. Removing the -1 solves the problem (in this case).</description>
      <version>0.94.1,0.94.2,0.94.3,0.95.2</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
    </fixedFiles>
  </bug>
  <bug id="7361" opendate="2012-12-15 00:00:00" fixdate="2012-12-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix all javadoc warnings in hbase-server/{,mapreduce}</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.MultithreadedTableMapper.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.MasterAdminProtocol.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.Action.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRowProcessorEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.BitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ByteArrayComparable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnCountGetFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnPaginationFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnPrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FamilyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FilterWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyValueMatchingQualifiersFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.FuzzyRowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.KeyOnlyFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.MultipleColumnPrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.NullComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.QualifierFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RandomRowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.RowFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueExcludeFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.TimestampsFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.ValueFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.filter.WhileMatchFilter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.fs.HFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.FileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.HFileLink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="7380" opendate="2012-12-18 00:00:00" fixdate="2012-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[replication] When transferring queues, check if the peer still exists before copying the znodes</summary>
      <description>Right now it's a pain if you remove a peer and still have rogue queues because they get moved on and on and on. NodeFailoverWorker needs to run the check:if (!zkHelper.getPeerClusters().containsKey(src.getPeerClusterId())) {before this:SortedMap&lt;String, SortedSet&lt;String&gt;&gt; newQueues = zkHelper.copyQueuesFromRS(rsZnode);And test.</description>
      <version>0.94.3</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationStateZKBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="7383" opendate="2012-12-18 00:00:00" fixdate="2012-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>create integration test for HBASE-5416 (improving scan performance for certain filters)</summary>
      <description>HBASE-5416 is risky and needs an integration test.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.RestartMetaTest.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedWriter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedReader.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedAction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.LoadTestTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestEncodedSeekers.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestLoadTestKVGenerator.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.test.LoadTestKVGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug id="7384" opendate="2012-12-18 00:00:00" fixdate="2012-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introducing waitForCondition function into test cases</summary>
      <description>Recently I'm working on flaky test cases and found we have many places using while loop and sleep to wait for a condition to be true. There are several issues in existing ways:1) Many similar code doing the same thing2) When time out happens, different errors are reported without explicitly indicating a time out situation3) When we want to increase the max timeout value to verify if a test case fails due to a not-enough time out value, we have to recompile &amp; redeploy codeI propose to create a waitForCondition function as a test utility function like the following: public interface WaitCheck { public boolean Check() ; } public boolean waitForCondition(int timeOutInMilliSeconds, int checkIntervalInMilliSeconds, WaitCheck s) throws InterruptedException { int multiplier = 1; String multiplierProp = System.getProperty("extremeWaitMultiplier"); if(multiplierProp != null) { multiplier = Integer.parseInt(multiplierProp); if(multiplier &lt; 1) { LOG.warn(String.format("Invalid extremeWaitMultiplier property value:%s. is ignored.", multiplierProp)); multiplier = 1; } } int timeElapsed = 0; while(timeElapsed &lt; timeOutInMilliSeconds * multiplier) { if(s.Check()) { return true; } Thread.sleep(checkIntervalInMilliSeconds); timeElapsed += checkIntervalInMilliSeconds; } assertTrue("WaitForCondition failed due to time out(" + timeOutInMilliSeconds + " milliseconds expired)", false); return false; }By doing the above way, there are several advantages:1) Clearly report time out error when such situation happens2) Use System property extremeWaitMultiplier to increase max time out dynamically for a quick verification3) Standardize current wait situationsPleas let me know what your thoughts on this.Thanks,-Jeffrey</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitLogWorker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug id="7505" opendate="2013-1-7 00:00:00" fixdate="2013-1-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Server will hang when stopping cluster, caused by waiting for split threads</summary>
      <description>We will retry 100 times (about 3200 minitues) for HRegionServer#postOpenDeployTasks now, see HConnectionManager#setServerSideHConnectionRetries.However, when we stopping the cluster, we will wait for split threads in HRegionServer#join,if META/ROOT server has already been stopped, the split thread won't exit because it is in the retrying for HRegionServer#postOpenDeployTasks</description>
      <version>0.94.3</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
    </fixedFiles>
  </bug>
  <bug id="7516" opendate="2013-1-8 00:00:00" fixdate="2013-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make compaction policy pluggable</summary>
      <description>Currently, the compaction selection is pluggable. It will be great to make the compaction algorithm pluggable too so that we can implement and play with other compaction algorithms.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultCompactSelection.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.HFileReadWriteTest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Compactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactionTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug id="7533" opendate="2013-1-10 00:00:00" fixdate="2013-3-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write an RPC Specification for 0.96</summary>
      <description>RPC format is changing for 0.96 to accomodate our protobufing all around. Here is a first cut. Please shred: https://docs.google.com/document/d/1-1RJMLXzYldmHgKP7M7ynK6euRpucD03fZ603DlZfGI/edit</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.rpc.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug id="7536" opendate="2013-1-10 00:00:00" fixdate="2013-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test that confirms that multiple concurrent snapshot requests are rejected.</summary>
      <description>Currently the rule is that we can only have online snapshot running at a time. This test tries to prove this.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestFlushSnapshotFromClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="7540" opendate="2013-1-11 00:00:00" fixdate="2013-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make znode dump to print a dump of replication znodes</summary>
      <description>It will be nice to have a dump of replication related znodes on the master UI (along with other znode dump). It helps while using replication.</description>
      <version>0.94.3</version>
      <fixedVersion>0.94.5,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="7633" opendate="2013-1-21 00:00:00" fixdate="2013-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a metric that tracks the current number of used RPC threads on the regionservers</summary>
      <description>One way to detect that you're hitting a "John Wayne" disk&amp;#91;1&amp;#93; would be if we could see when region servers exhausted their RPC handlers. This would also be useful when tuning the cluster for your workload to make sure that reads or writes were not starving the other operations out.&amp;#91;1&amp;#93; http://hbase.apache.org/book.html#bad.disk</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestZKProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.Procedure.java</file>
    </fixedFiles>
  </bug>
  <bug id="7706" opendate="2013-1-29 00:00:00" fixdate="2013-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove reservation blocks from region server</summary>
      <description>Here is link to discussion on dev@hbase:http://search-hadoop.com/m/HSjjh1ICjjI1/resevoir+blocks+for+region+server&amp;subj=Re+resevoir+blocks+for+region+serverFrom J-D:Since we default to OnOutOfMemoryError kill -9 then we should just getrid of it IMO.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="8522" opendate="2013-5-10 00:00:00" fixdate="2013-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Archived hfiles and old hlogs may be deleted immediately by HFileCleaner, LogCleaner in HMaster</summary>
      <description>TimeToLiveHFileCleaner is configed to 'hbase.master.hfilecleaner.plugins' in hbase-default.xml. And timeToLiveHFileCleaner uses the modify time of the hfile to determine if it should be deleted. But, the modify time of the hdfs file is time when its writer is closed. The rename op will not change the modify time of the hfile. So the hfile may be deleted immediatly by HFileCleaner after it is moved to archives. See log:2013-05-08 08:15:46,053 DEBUG org.apache.hadoop.hbase.master.cleaner.CleanerChore: Checking directory: hdfs://hbase/.archive/table/4e48ffc1ec089082c66e6d1b5f018fb5/M/729e8bc1430540cb9b2c147c90039cdc2013-05-08 08:15:46,055 DEBUG org.apache.hadoop.ipc.ProtobufRpcEngine: Call: getFileInfo took 1ms2013-05-08 08:15:46,055 DEBUG org.apache.hadoop.hbase.master.cleaner.TimeToLiveHFileCleaner: Life:40033567, ttl:300000, current:1367972146054, from: 13679321124872013-05-08 08:15:46,055 DEBUG org.apache.hadoop.hbase.master.cleaner.CleanerChore: Removing:hdfs://hbase/.archive/table/4e48ffc1ec089082c66e6d1b5f018fb5/M/729e8bc1430540cb9b2c147c90039cdc from archiveThe same to old hlogs.And my solution is very simple: When hfiles and hlogs are archived, we set the modify time of files after rename.</description>
      <version>0.94.3</version>
      <fixedVersion>0.98.0,0.95.1,0.94.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
    </fixedFiles>
  </bug>
  <bug id="8631" opendate="2013-5-28 00:00:00" fixdate="2013-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Meta Region First Recovery</summary>
      <description>We have a separate wal for meta region. While log splitting logic haven't taken the advantage of this and splitlogworker still picks a wal file randomly. Imaging if we have multiple region servers including meta RS fails about the same time while meta wal is recovered last, all failed regions have to wait meta recovered and then can be online again. The open JIRA is to let splitlogworker to pick a meta wal file firstly and then others.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestDataIngestWithChaosMonkey.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IngestIntegrationTestBase.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
