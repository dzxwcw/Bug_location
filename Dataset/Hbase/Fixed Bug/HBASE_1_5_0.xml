<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HBASE">
  <bug id="14498" opendate="2015-9-28 00:00:00" fixdate="2015-1-28 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Master stuck in infinite loop when all Zookeeper servers are unreachable (and RS may run after losing its znode)</summary>
      <description>We met a weird scenario in our production environment.In a HA cluster,&gt; Active Master (HM1) is not able to connect to any Zookeeper server (due to N/w breakdown on master machine network with Zookeeper servers).2015-09-26 15:24:47,508 INFO [HM1-Host:16000.activeMasterManager-SendThread(ZK-Host:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 33463ms for sessionid 0x104576b8dda0002, closing socket connection and attempting reconnect2015-09-26 15:24:47,877 INFO [HM1-Host:16000.activeMasterManager-SendThread(ZK-Host1:2181)] client.FourLetterWordMain: connecting to ZK-Host1 21812015-09-26 15:24:48,236 INFO [main-SendThread(ZK-Host1:2181)] client.FourLetterWordMain: connecting to ZK-Host1 21812015-09-26 15:24:49,879 WARN [HM1-Host:16000.activeMasterManager-SendThread(ZK-Host1:2181)] zookeeper.ClientCnxn: Can not get the principle name from server ZK-Host12015-09-26 15:24:49,879 INFO [HM1-Host:16000.activeMasterManager-SendThread(ZK-Host1:2181)] zookeeper.ClientCnxn: Opening socket connection to server ZK-Host1/ZK-IP1:2181. Will not attempt to authenticate using SASL (unknown error)2015-09-26 15:24:50,238 WARN [main-SendThread(ZK-Host1:2181)] zookeeper.ClientCnxn: Can not get the principle name from server ZK-Host12015-09-26 15:24:50,238 INFO [main-SendThread(ZK-Host1:2181)] zookeeper.ClientCnxn: Opening socket connection to server ZK-Host1/ZK-Host1:2181. Will not attempt to authenticate using SASL (unknown error)2015-09-26 15:25:17,470 INFO [main-SendThread(ZK-Host1:2181)] zookeeper.ClientCnxn: Client session timed out, have not heard from server in 30023ms for sessionid 0x2045762cc710006, closing socket connection and attempting reconnect2015-09-26 15:25:17,571 WARN [master/HM1-Host/HM1-IP:16000] zookeeper.RecoverableZooKeeper: Possibly transient ZooKeeper, quorum=ZK-Host:2181,ZK-Host1:2181,ZK-Host2:2181, exception=org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master2015-09-26 15:25:17,872 INFO [main-SendThread(ZK-Host:2181)] client.FourLetterWordMain: connecting to ZK-Host 21812015-09-26 15:25:19,874 WARN [main-SendThread(ZK-Host:2181)] zookeeper.ClientCnxn: Can not get the principle name from server ZK-Host2015-09-26 15:25:19,874 INFO [main-SendThread(ZK-Host:2181)] zookeeper.ClientCnxn: Opening socket connection to server ZK-Host/ZK-IP:2181. Will not attempt to authenticate using SASL (unknown error)&gt; Since HM1 was not able to connect to any ZK, so session timeout didnt happen at Zookeeper server side and HM1 didnt abort.&gt; On Zookeeper session timeout standby master (HM2) registered himself as an active master. &gt; HM2 is keep on waiting for region server to report him as part of active master intialization. 2015-09-26 15:24:44,928 | INFO | HM2-Host:21300.activeMasterManager | Waiting for region servers count to settle; currently checked in 0, slept for 0 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms. | org.apache.hadoop.hbase.master.ServerManager.waitForRegionServers(ServerManager.java:1011)------2015-09-26 15:32:50,841 | INFO | HM2-Host:21300.activeMasterManager | Waiting for region servers count to settle; currently checked in 0, slept for 483913 ms, expecting minimum of 1, maximum of 2147483647, timeout of 4500 ms, interval of 1500 ms. | org.apache.hadoop.hbase.master.ServerManager.waitForRegionServers(ServerManager.java:1011)&gt; At other end, region servers are reporting to HM1 on 3 sec interval. Here region server retrieve master location from zookeeper only when they couldn't connect to Master (ServiceException).Region Server will not report HM2 as per current design until unless HM1 abort,so HM2 will exit(InitializationMonitor) and again wait for region servers in loop.</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0,2.2.0</version>
      <fixedVersion>3.0.0-beta-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperWatcher.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug id="16385" opendate="2016-8-9 00:00:00" fixdate="2016-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have hbase-rest pull hbase.rest.port from Constants.java</summary>
      <description>A while back, we hardcoded the hbase.rest.port instead of getting it from Constants.java. We should fix that.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.3,0.98.22,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.RESTServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="16388" opendate="2016-8-10 00:00:00" fixdate="2016-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prevent client threads being blocked by only one slow region server</summary>
      <description>It is a general use case for HBase's users that they have several threads/handlers in their service, and each handler has its own Table/HTable instance. Generally users think each handler is independent and won't interact each other.However, in an extreme case, if a region server is very slow, every requests to this RS will timeout, handlers of users' service may be occupied by the long-waiting requests even requests belong to other RS will also be timeout.For example: If we have 100 handlers in a client service(timeout is 1000ms) and HBase has 10 region servers whose average response time is 50ms. If no region server is slow, we can handle 2000 requests per second.Now this service's QPS is 1000. If there is one region server very slow and all requests to it will be timeout. Users hope that only 10% requests failed, and 90% requests' response time is still 50ms, because only 10% requests are located to the slow RS. However, each second we have 100 long-waiting requests which exactly occupies all 100 handles. So all handlers is blocked, the availability of this service is almost zero.To prevent this case, we can limit the max concurrent requests to one RS in process-level. Requests exceeding the limit will throws ServerBusyException(extends DoNotRetryIOE) immediately to users. In the above case, if we set this limit to 20, only 20 handlers will be occupied and other 80 handlers can still handle requests to other RS. The availability of this service is 90% as expected.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AbstractRpcClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="16390" opendate="2016-8-10 00:00:00" fixdate="2016-9-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix documentation around setAutoFlush</summary>
      <description>Our documentation is a little confused around setAutoFlush. Talks of Table but setAutoFlush is not in the Table interface. It was on HTable but was deprecated and since removed. Clean up the doc:100.4. HBase Client: AutoFlushWhen performing a lot of Puts, make sure that setAutoFlush is set to falseon your Table&lt;http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Table.html&gt;instance.Otherwise, the Puts will be sent one at a time to the RegionServer. Putsadded via table.add(Put) and table.add( &lt;List&gt; Put) wind up in the samewrite buffer. If autoFlush = false, these messages are not sent until thewrite-buffer is filled. To explicitly flush the messages, call flushCommits.Calling close on the Table instance will invoke flushCommitsSpotted by Jeff Shmain.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18269" opendate="2017-6-26 00:00:00" fixdate="2017-6-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jython docs out of date</summary>
      <description>The documentation describing how to launch Jython + HBase is out of date. - https://hbase.apache.org/book.html#jythonFirst, we would set the classpath differently:HBASE_CLASSPATH=/home/hbase/jython.jar bin/hbase org.python.util.jythonThen, the actual code example is out of date too:&gt;&gt;&gt; desc = HTableDescriptor(tablename)&gt;&gt;&gt; desc.addFamily(HColumnDescriptor("content:"))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; at org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(HColumnDescriptor.java:566) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:470) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:425) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:390) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:338) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:327) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.python.core.PyReflectedConstructor.constructProxy(PyReflectedConstructor.java:211)We should make sure that the examples we claim are runnable actually are.</description>
      <version>1.3.1,1.2.6,1.5.0,1.4.2,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18626" opendate="2017-8-18 00:00:00" fixdate="2017-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle the incompatible change about the replication TableCFs&amp;#39; config</summary>
      <description>About compatibility, there is one incompatible change about the replication TableCFs' config. The old config is a string and it concatenate the list of tables and column families in format "table1:cf1,cf2;table2:cfA,cfB" in zookeeper for table-cf to replication peer mapping. When parse the config, it use ":" to split the string. If table name includes namespace, it will be wrong (See HBASE-11386). It is a problem since we support namespace (0.98). So HBASE-11393 (and HBASE-16653) changed it to a PB object. When rolling update cluster, you need rolling master first. And the master will try to translate the string config to a PB object. But there are two problems.1. Permission problem. The replication client can write the zookeeper directly. So the znode may have different owner. And master may don't have the write permission for the znode. It maybe failed to translate old table-cfs string to new PB Object. See HBASE-169382. We usually keep compatibility between old client and new server. But the old replication client may write a string config to znode directly. Then the new server can't parse them.</description>
      <version>3.0.0-alpha-1,1.4.0,1.5.0,2.0.0-alpha-3</version>
      <fixedVersion>1.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="18630" opendate="2017-8-18 00:00:00" fixdate="2017-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prune dependencies; as is branch-2 has duplicates</summary>
      <description>Purge duplicate includes and try to prune back our dependencies (Suggestion by elserj up on the 2.0.0-alpha2 vote). Just looking at my current issue, we have vestiges we include even though the root justification has passed.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18631" opendate="2017-8-18 00:00:00" fixdate="2017-8-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow configuration of ChaosMonkey properties via hbase-site</summary>
      <description>I noticed in some internal test automation that the code was attempting to configure some of the chaos-monkey actions via hbase-site.xml, but these weren't taking effect.After reading the code, I found that these can only be specified via a special properties file otherwise specified. It would be nice to also allow configuration via hbase-site.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBase.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.factories.MonkeyConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="18667" opendate="2017-8-23 00:00:00" fixdate="2017-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable error-prone for hbase-protocol-shaded</summary>
      <description>This is all generated code that we shouldn't be running extra analysis on because it adds a lot of noise to the build, and also takes a very long time (15 minutes on my machine). Let's make it fast and simple.Even when we run with error-prone enabled for the rest of the build, it should not apply here.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18740" opendate="2017-9-1 00:00:00" fixdate="2017-9-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Zookeeper version to 3.4.10</summary>
      <description>Branch 1.4 and branch 1 are still on Zookeeper 3.4.6.Branch 2 and master branch have upgraded to 3.4.9.There are some important fixes we'd like to have. See the linked JIRAs.Another critical fix is ZOOKEEPER-2146, which can be explored maliciously.</description>
      <version>1.4.0,1.5.0</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="18789" opendate="2017-9-11 00:00:00" fixdate="2017-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Displays the reporting interval of each RS on the Master page</summary>
      <description>RegionServer will periodically send the RS Load to Master.If RS is abnormal(eg.Full gc), RS cannot send the RS Load to the Master.If every RS's reporting time is displayed on the Master page, we can easily know which regionserver is abnormal.</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0-alpha-3</version>
      <fixedVersion>1.4.0,2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
    </fixedFiles>
  </bug>
  <bug id="18875" opendate="2017-9-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server supports read-only mode</summary>
      <description>Provide option for thrift server to support read-only mode.To start the thrift server, use the -ro option or set hbase.thrift.readonly to true.false: Both read and write request are permitted.(all methods)true : Only the read request is permitted. (only get/scan method)</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="18876" opendate="2017-9-25 00:00:00" fixdate="2017-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backup create command fails to take queue parameter as option</summary>
      <description>Backup help shows we can set queue using -q parameter.-q &lt;arg&gt; Yarn queue name to run backup restore command onBut when we give ./hbase backup create full hdfs://localhost:8020/test/ -t test1 -q hbaseIt throws following error "Error when parsing command-line arguments: Unrecognized option: -q"</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="18878" opendate="2017-9-26 00:00:00" fixdate="2017-10-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Optional&lt;T&gt; return types when T can be null</summary>
      <description>I've already done lots of Nullable to Optional change when purging the interfaces for CP. This is a big one so open a separated issue for it.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-4,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestWithDisabledAuthorization.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestProtobufRpcServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.token.TokenProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RegionServerRpcQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.ServerCall.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcCallContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.ObserverContext.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.CoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.VersionInfoUtil.java</file>
      <file type="M">hbase-endpoint.src.test.java.org.apache.hadoop.hbase.coprocessor.ProtobufCoprocessorService.java</file>
      <file type="M">hbase-endpoint.src.main.java.org.apache.hadoop.hbase.coprocessor.Export.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Classes.java</file>
    </fixedFiles>
  </bug>
  <bug id="18934" opendate="2017-10-4 00:00:00" fixdate="2017-10-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>precommit on branch-1 isn&amp;#39;t supposed to run against hadoop 3</summary>
      <description>Hadoop 3 doesn't work with HBase 1.y and we haven't done any work to backport the efforts to make HBase 2.y work with it. Precommit shouldn't tell contributors otherwise.see HBASE-18923 for an example of a branch-1 patch that had hadoop 3 compilation checked.</description>
      <version>1.5.0</version>
      <fixedVersion>1.4.0,1.3.2,1.1.13,2.0.0-alpha-4,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19227" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly jobs should archive JVM dumpstream files</summary>
      <description>came up on dev@ discussion about some of our current nightly test failures. when surefire fails to launch a test JVM instance, the details go into a file that we currently don't archive:&amp;#91;ERROR&amp;#93; Please refer to dump files (if any exist) &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dump, &amp;#91;date&amp;#93;.dumpstream and &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dumpstream.Add them to the default archive pattern.</description>
      <version>None</version>
      <fixedVersion>1.0.4,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="19228" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly job should gather machine stats.</summary>
      <description>leverage the script added in HBASE-19189 to get machine stats when running nightly</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.gather.machine.environment.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19229" opendate="2017-11-9 00:00:00" fixdate="2017-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly script to check source artifact should not do a destructive git operation without opt-in</summary>
      <description>right now we have a "git please destroy all this stuff" command in the check of the source artifact. we shouldn't do this unless the person invoking the script has indicated that's okay (e..g through a cli flag).</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.source-artifact.sh</file>
    </fixedFiles>
  </bug>
  <bug id="19290" opendate="2017-11-17 00:00:00" fixdate="2017-12-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce zk request when doing split log</summary>
      <description>We observe once the cluster has 1000+ nodes and when hundreds of nodes abort and doing split log, the split is very very slow, and we find the regionserver and master wait on the zookeeper response, so we need to reduce zookeeper request and pressure for big cluster.(1) Reduce request to rsZNode, every time calculateAvailableSplitters will get rsZNode's children from zookeeper, when cluster is huge, this is heavy. This patch reduce the request. (2) When the regionserver has max split tasks running, it may still trying to grab task and issue zookeeper request, we should sleep and wait until we can grab tasks again.</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0</version>
      <fixedVersion>1.5.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.java</file>
    </fixedFiles>
  </bug>
  <bug id="19399" opendate="2017-12-1 00:00:00" fixdate="2017-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Purge curator dependency from hbase-client</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKWatcher.java</file>
      <file type="M">hbase-zookeeper.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-zookeeper.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKMetrics.java</file>
      <file type="M">hbase-zookeeper.src.test.java.org.apache.hadoop.hbase.zookeeper.TestInstancePending.java</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKMainServer.java</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestZKAsyncRegistry.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ZKAsyncRegistry.java</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="19417" opendate="2017-12-4 00:00:00" fixdate="2017-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove boolean return value from postBulkLoadHFile hook</summary>
      <description>See the discussion at the tail of HBASE-17123 where Appy pointed out that the override of loaded should be placed inside else block: } else { // secure bulk load map = regionServer.secureBulkLoadManager.secureBulkLoadHFiles(region, request); } BulkLoadHFileResponse.Builder builder = BulkLoadHFileResponse.newBuilder(); if (map != null) { loaded = true; }This issue is to address the review comment.After several review iterations, here are the changes: Return value of boolean for postBulkLoadHFile() hook are changed to void. Coprocessor hooks (pre and post) are added for the scenario where bulk load manager is used.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.SimpleRegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="19755" opendate="2018-1-10 00:00:00" fixdate="2018-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error message for non-existent namespace is inaccurate</summary>
      <description>On a secure cluster, when I issued this command where ns1 didn't exist:hbase(main):002:0&gt; create 'ns1:t1', 'f1', SPLITS =&gt; ['10', '20', '30', '40']ERROR: Unknown namespace ns1:t1!Creates a table. Pass a table name, and a set of column familyspecifications (at least one), and, optionally, table configurationHere is related code: raise "Unknown namespace #{args.first}!"Simply quoting the argument is not accurate - namespace should be extracted from the argument</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.rb</file>
    </fixedFiles>
  </bug>
  <bug id="20066" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region sequence id may go backward after split or merge</summary>
      <description>The problem is that, now we have markers which will be written to WAL but not in store file. For a normal region close, we will write a sequence id file under the region directory, and when opening we will use this as the open sequence id. But for split and merge, we do not copy the sequence id file to the newly generated regions so the sequence id may go backwards since when closing the region we will write flush marker and close marker into WAL...</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.AbstractTestDLS.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AbstractStateMachineTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
    </fixedFiles>
  </bug>
  <bug id="20068" opendate="2018-2-24 00:00:00" fixdate="2018-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopcheck project health check uses default maven repo instead of yetus managed ones</summary>
      <description>Recently had a precommit run fail hadoop check for all 3 versions with [ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.2:install (default-install) on project hbase-thrift: Failed to install metadata org.apache.hbase:hbase-thrift:3.0.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hbase/hbase-thrift/3.0.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got / (position: END_TAG seen ...&lt;/metadata&gt;\n/... @25:2) -&gt; [Help 1]Looks like maven repo corruption.Also the path /home/jenkins/.m2/repository means that those invocations are using the jenkins user repo, which isn't safe since there are multiple executors. either the plugin isn't using the yetus provided maven repo path or our yetus invocation isn't telling yetus to provide its own maven repo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.4,2.0.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20069" opendate="2018-2-24 00:00:00" fixdate="2018-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix existing findbugs errors in hbase-server</summary>
      <description>now that findbugs is running on precommit we have some cleanup to do.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ClusterStatusPublisher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.compaction.MajorCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.nio.TestMultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.nio.MultiByteBuff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.EncodedDataBlock.java</file>
    </fixedFiles>
  </bug>
  <bug id="20070" opendate="2018-2-24 00:00:00" fixdate="2018-3-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>website generation is failing</summary>
      <description>website generation has been failing since Feb 20thChecking out files: 100% (68971/68971), done.Usage: grep [OPTION]... PATTERN [FILE]...Try 'grep --help' for more information.PUSHED is 2 is not yet mentioned in the hbase-site commit log. Assuming we don't have it yet. 2Building HBaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Failure: mvn clean siteBuild step 'Execute shell' marked build as failureThe status email saysBuild status: Still FailingThe HBase website has not been updated to incorporate HBase commit ${CURRENT_HBASE_COMMIT}.Looking at the code where that grep happens, it looks like the env variable CURRENT_HBASE_COMMIT isn't getting set. That comes from some git command. I'm guessing the version of git changed on the build hosts and upended our assumptions.we should fix this to 1) rely on git's porcelain interface, and 2) fail as soon as that git command fails</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-spark-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-resource-bundle.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-build-configuration.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-annotations.pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20163" opendate="2018-3-9 00:00:00" fixdate="2018-4-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forbid major compaction when standby cluster replay the remote wals</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.MockRegionServerServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="20164" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>failed hadoopcheck should add footer link</summary>
      <description>thought for sure this already had an issue, busbey, but I can't find it.</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.3,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20165" opendate="2018-3-9 00:00:00" fixdate="2018-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell command to make a normal peer to be a serial replication peer</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.replication.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.list.peers.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.replication.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug id="2017" opendate="2009-11-30 00:00:00" fixdate="2009-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set configurable max value size check to 10MB</summary>
      <description>Make the user think about whether storing larger values than 10MB is a good idea.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="20264" opendate="2018-3-23 00:00:00" fixdate="2018-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Java prerequisite section with LTS rec and status of current GA JDKs</summary>
      <description>Per the thread [DISCUSS] strategy on Java versions Add Java 9 and Java 10 to the support matrix as NT Add a NOTE to Java prereqs about "use a LTS version"For now, leave out talk about planning for timelines on LTS additions or dropping older JDK support. Once we get over the initial hurdle of prepping for Java 11 we'll hopefully have enough info to know how realistic the things talked about in the thread are and we can include a writeup.</description>
      <version>1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20401" opendate="2018-4-12 00:00:00" fixdate="2018-3-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make `MAX_WAIT` and `waitIfNotFinished` in CleanerContext configurable</summary>
      <description>When backporting HBASE-18309 in HBASE-20352, the deleteFiles calls CleanerContext.java#getResult with a waitIfNotFinished timeout to wait for notification (notify) from the fs.delete file thread. there might be two situation need to tune the MAX_WAIT in CleanerContext or waitIfNotFinished when LogClearner call getResult. fs.delete never complete (strange but possible), then we need to wait for a max of 60 seconds. here, 60 seconds might be too long getResult is waiting in the period of 500 milliseconds, but the fs.delete has completed and setFromClear is set but yet notify(). one might want to tune this 500 milliseconds to 200 or less .</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0-beta-1,1.4.4,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,1.4.6,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.LogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.HFileCleaner.java</file>
    </fixedFiles>
  </bug>
  <bug id="20581" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book documentation wrong for REST operations on schema endpoints</summary>
      <description>On https://hbase.apache.org/book.html#_using_rest_endpointsThe documentation states that to update a table schema (the configuration for a column family), the PUT HTTP verb will update the current configuration with the "fragment" of configuration provided, while the POST HTTP verb will replace the current configuration with whatever is provided.In reality, the opposite is true: POST updates the configuration, PUT replaces. The old javadoc for the o.a.h.h.rest package got it right, but the entry on the HBase book transposed this.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="20582" opendate="2018-5-14 00:00:00" fixdate="2018-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up JRuby version because of some reported vulnerabilities</summary>
      <description>There are some vulnerabilities reported with two of the libraries used in HBase.Jruby(version:9.1.10.0):CVE-2009-5147CVE-2013-4363CVE-2014-4975CVE-2014-8080CVE-2014-8090CVE-2015-3900CVE-2015-7551CVE-2015-9096CVE-2017-0899CVE-2017-0900CVE-2017-0901CVE-2017-0902CVE-2017-0903CVE-2017-10784CVE-2017-14064CVE-2017-9224CVE-2017-9225CVE-2017-9226CVE-2017-9227CVE-2017-9228Tool somehow able to relate the vulnerability of Ruby with JRuby(Java implementation). (Jackson will be handled in a different issue.)Not all of them directly affects HBase but elserj suggested that it is better to be on the updated version to avoid issues during an audit in security sensitive organization. </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20704" opendate="2018-6-7 00:00:00" fixdate="2018-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sometimes some compacted storefiles are not archived on region close</summary>
      <description>During region close compacted files which have not yet been archived by the discharger are archived as part of the region closing process. It is important that these files are wholly archived to insure data consistency. ie a storefile containing delete tombstones can be archived while older storefiles containing cells that were supposed to be deleted are left unarchived thereby undeleting those cells. On region close a compacted storefile is skipped from archiving if it has read references (ie open scanners). This behavior is correct for when the discharger chore runs but on region close consistency is of course more important so we should add a special case to ignore any references on the storefile and go ahead and archive it. Attached patch contains a unit test that reproduces the problem and the proposed fix.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.4.8,2.1.1,2.0.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="20705" opendate="2018-6-7 00:00:00" fixdate="2018-8-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Having RPC Quota on a table prevents Space quota to be recreated/removed</summary>
      <description>Property hbase.quota.remove.on.table.delete is set to true by default Create a table and set RPC and Space quotahbase(main):022:0&gt; create 't2','cf1'Created table t2Took 0.7420 seconds=&gt; Hbase::Table - t2hbase(main):023:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't2', LIMIT =&gt; '1G', POLICY =&gt; NO_WRITESTook 0.0105 secondshbase(main):024:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; '10M/sec'Took 0.0186 secondshbase(main):025:0&gt; list_quotasTABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINETABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, LIMIT =&gt; 1073741824, VIOLATION_POLICY =&gt; NO_WRITES Drop the table and the Space quota is set to REMOVE =&gt; truehbase(main):026:0&gt; disable 't2'Took 0.4363 secondshbase(main):027:0&gt; drop 't2'Took 0.2344 secondshbase(main):028:0&gt; list_quotasTABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; trueUSER =&gt; u1 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINE Recreate the table and set Space quota back. The Space quota on the table is still set to REMOVE =&gt; truehbase(main):029:0&gt; create 't2','cf1'Created table t2Took 0.7348 seconds=&gt; Hbase::Table - t2hbase(main):031:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't2', LIMIT =&gt; '1G', POLICY =&gt; NO_WRITESTook 0.0088 secondshbase(main):032:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINETABLE =&gt; t2 TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; true Remove RPC quota and drop the table, the Space Quota is not removedhbase(main):033:0&gt; set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; NONETook 0.0193 secondshbase(main):036:0&gt; disable 't2'Took 0.4305 secondshbase(main):037:0&gt; drop 't2'Took 0.2353 secondshbase(main):038:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t2                               TYPE =&gt; SPACE, TABLE =&gt; t2, REMOVE =&gt; true Deleting the quota entry from hbase:quota seems to be the option to reset it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterQuotasObserver.java</file>
    </fixedFiles>
  </bug>
  <bug id="20724" opendate="2018-6-13 00:00:00" fixdate="2018-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sometimes some compacted storefiles are still opened after region failover</summary>
      <description>It is important that compacted storefiles of a given compaction execution are wholly opened or archived to insure data consistency. ie a storefile containing delete tombstones can be archived while older storefiles containing cells that were supposed to be deleted are left unarchived thereby undeleting those cells.When a server fails compaction markers (in the wal edit) are used to determine which storefiles are compacted and should be excluded during region open (during failover). But the WALs containing compaction markers can be prematurely archived even though there are still compacted storefiles for that particular compaction event that hasn't been archived yet. Thus losing compaction information that needs to be replayed in the event of an RS crash. This is because hlog archiving logic only keeps track of flushed storefiles and not compacted ones.https://issues.apache.org/jira/browse/HBASE-20704?focusedCommentId=16507680&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16507680</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSwitchToStreamRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCleanupCompactedFileOnRegionClose.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotasWithSnapshots.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSnapshotQuotaObserverChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileWriter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.StripeCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AbstractMultiFileWriter.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.HFile.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="20801" opendate="2018-6-28 00:00:00" fixdate="2018-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix broken TestReplicationShell</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.replication.admin.test.rb</file>
    </fixedFiles>
  </bug>
  <bug id="20806" opendate="2018-6-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split style journal for flushes and compactions</summary>
      <description>In 1.x we have split transaction journal that gives a clear picture of when various stages of splits took place. We should have a similar thing for flushes and compactions so as to have insights into time spent in various stages, which we can use to identify regressions that might creep up.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.6,1.2.7,2.0.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.monitoring.TestTaskMonitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="20808" opendate="2018-6-28 00:00:00" fixdate="2018-7-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong shutdown order between Chores and ChoreService</summary>
      <description>When stopping master, ChoreService, which serves all the chores, is stopped before canceling all running chores.It should cancel all running chores, then shutdown ChoreService.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="2081" opendate="2009-12-30 00:00:00" fixdate="2009-12-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the retries higher in shell since client pause is lower</summary>
      <description>Client pause went from 2 to1 second and in the shell we only retry 5 times. I propose we set that to 6 or 7 now to keep the same behavior as before.</description>
      <version>None</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug id="20813" opendate="2018-6-28 00:00:00" fixdate="2018-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove RPC quotas when the associated table/Namespace is dropped off</summary>
      <description>In short, the below scenario shouldn't be the case.hbase(main):023:0&gt; create 't2','cf1' Created table t2 Took 0.7405 seconds =&gt; Hbase::Table - t2 hbase(main):024:0&gt; hbase(main):025:0* hbase(main):026:0* set_quota TYPE =&gt; THROTTLE, TABLE =&gt; 't2', LIMIT =&gt; '10M/sec' Took 0.0082 seconds hbase(main):027:0&gt; list_quotas OWNER QUOTAS TABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINE 1 row(s) Took 0.0291 seconds hbase(main):028:0&gt; scan 'hbase:quota' ROW COLUMN+CELL t.t2 column=q:s, timestamp=1530165010888, value=PBUF\x12\x0B\x12\x09\x08\x04\x10\x80\x80\x80\x05 \x02 1 row(s) Took 0.0037 seconds hbase(main):029:0&gt; disable 't2' Took 0.4328 seconds hbase(main):030:0&gt; drop 't2' Took 0.2285 seconds hbase(main):031:0&gt; list_quotas OWNER QUOTAS TABLE =&gt; t2 TYPE =&gt; THROTTLE, THROTTLE_TYPE =&gt; REQUEST_SIZE, LIMIT =&gt; 10M/sec, SCOPE =&gt; MACHINE 1 row(s) Took 0.0230 seconds hbase(main):032:0&gt; scan 'hbase:quota' ROW COLUMN+CELL t.t2 column=q:s, timestamp=1530165010888, value=PBUF\x12\x0B\x12\x09\x08\x04\x10\x80\x80\x80\x05 \x02 1 row(s) Took 0.0038 seconds</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterSpaceQuotaObserverWithMocks.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterSpaceQuotaObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterSpaceQuotaObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="20890" opendate="2018-7-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE filterScan seems to be stuck forever</summary>
      <description>Command Used~/current/bigdata-hbase/hbase/hbase/bin/hbase pe --nomapred randomWrite 1 &gt; write 2&gt;&amp;1~/current/bigdata-hbase/hbase/hbase/bin/hbase pe --nomapred filterScan 1 &gt; filterScan 2&gt;&amp;1 OutputThis kept running for several hours just printing the below messages in logs -bash-4.1$ grep "Advancing internal scanner to startKey" filterScan.1 | head2018-07-13 10:44:45,188 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359'2018-07-13 10:44:45,976 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359'2018-07-13 10:44:46,695 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359'.....-bash-4.1$ grep "Advancing internal scanner to startKey" filterScan.1 | tail2018-07-15 06:20:22,353 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359'2018-07-15 06:20:23,044 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359'2018-07-15 06:20:23,768 DEBUG [TestClient-0] client.ClientScanner - Advancing internal scanner to startKey at '0000000000000000000052359' </description>
      <version>3.0.0-alpha-1,1.5.0,1.3.3,2.2.0,1.4.7</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.4.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-mapreduce.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21021" opendate="2018-8-7 00:00:00" fixdate="2018-9-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Result returned by Append operation should be ordered</summary>
      <description>Problem:The result returned by the append operation should be ordered. Currently, it returns an unordered list, which may cause problems like if the user tries to perform Result.getValue(byte[] family, byte[] qualifier), even if the returned result has a value corresponding to (family, qualifier), the method may return null as it performs a binary search over the  unsorted result (which should have been sorted actually). The result is enumerated by iterating over each entry of tempMemstore hashmap (which will never be ordered) and adding the values (see HRegion.java#L7882). Actual: The returned result is unorderedExpected: Similar to increment op, the returned result should be ordered.</description>
      <version>1.3.0,1.5.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21058" opendate="2018-8-15 00:00:00" fixdate="2018-8-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly tests for branches 1 fail to build ref guide</summary>
      <description>Nightly on all branches 1 reports failure to get a PDF version of the ref guide-1 refguide 2m 14s patch failed to produce the pdf version of the reference guide.Actual build log looks clean[INFO] --- asciidoctor-maven-plugin:1.5.2.1:process-asciidoc (output-pdf) @ hbase ---asciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for passasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_indextermasciidoctor: WARNING: conversion missing in backend pdf for inline_imageasciidoctor: WARNING: conversion missing in backend pdf for inline_image[INFO] Rendered /testptch/hbase/src/main/asciidoc/book.adoc</description>
      <version>1.5.0,1.3.3,1.2.7,1.4.7</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.2.7,2.1.1,2.0.2,1.4.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21074" opendate="2018-8-20 00:00:00" fixdate="2018-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDK7 branches need to pass "-Dhttps.protocols=TLSv1.2" to maven when building</summary>
      <description>Maven central now requires TLSv1.2 and by default JDK7 doesn't use it. So anyone building from a clean repo will fail like our nightly check of building the convenience binary from the source tarball e.g. 1.4[INFO] Scanning for projects...[INFO] Downloading from apache release: https://repository.apache.org/content/repositories/releases/org/apache/apache/18/apache-18.pom[INFO] Downloaded from apache release: https://repository.apache.org/content/repositories/releases/org/apache/apache/18/apache-18.pom (16 kB at 14 kB/s)[INFO] Downloading from Nexus: http://repository.apache.org/snapshots/org/apache/felix/maven-bundle-plugin/2.5.3/maven-bundle-plugin-2.5.3.pom[INFO] Downloading from central: https://repo.maven.apache.org/maven2/org/apache/felix/maven-bundle-plugin/2.5.3/maven-bundle-plugin-2.5.3.pom[ERROR] [ERROR] Some problems were encountered while processing the POMs:[ERROR] Unresolveable build extension: Plugin org.apache.felix:maven-bundle-plugin:2.5.3 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.felix:maven-bundle-plugin:jar:2.5.3 @ @ [ERROR] The build could not read 1 project -&gt; [Help 1][ERROR] [ERROR] The project org.apache.hbase:hbase:1.4.7-SNAPSHOT (/home/jenkins/jenkins-slave/workspace/HBase_Nightly_branch-1.4-EDDBHIHAYHZVAGB2FQL37O5LZNSEJJEXGP55DEGOA4FQKBLNWBAQ/unpacked_src_tarball/pom.xml) has 1 error[ERROR] Unresolveable build extension: Plugin org.apache.felix:maven-bundle-plugin:2.5.3 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.felix:maven-bundle-plugin:jar:2.5.3: Could not transfer artifact org.apache.felix:maven-bundle-plugin:pom:2.5.3 from/to central (https://repo.maven.apache.org/maven2): Received fatal alert: protocol_version -&gt; [Help 2][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/PluginManagerExceptionif we pass "-Dhttps.protocols=TLSv1.2" to maven then it should work for any JDK7 version.</description>
      <version>1.5.0,1.3.3,1.2.7,1.4.7</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.2.7,2.1.1,2.0.2,1.4.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21077" opendate="2018-8-20 00:00:00" fixdate="2018-8-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR job launched by hbase incremental backup command failed with FileNotFoundException</summary>
      <description>Discovered during internal testing by Romil Choksi.MR job launched by hbase incremental backup command failed with FileNotFoundExceptionfrom test console log2018-06-12 04:27:31,160|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:31,160 INFO [main] mapreduce.JobSubmitter: Submitting tokens for job: job_1528766389356_00442018-06-12 04:27:31,186|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:31,184 INFO [main] mapreduce.JobSubmitter: Executing with tokens: [Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:ns1, Ident: (token for hbase: HDFS_DELEGATION_TOKEN owner=hbase@EXAMPLE.COM, renewer=yarn, realUser=, issueDate=1528777648605, maxDate=1529382448605, sequenceNumber=175, masterKeyId=2), Kind: kms-dt, Service: 172.27.68.203:9393, Ident: (kms-dt owner=hbase, renewer=yarn, realUser=, issueDate=1528777649149, maxDate=1529382449149, sequenceNumber=49, masterKeyId=2), Kind: HBASE_AUTH_TOKEN, Service: bc71e347-78ff-4f95-af44-006f9b549a84, Ident: (org.apache.hadoop.hbase.security.token.AuthenticationTokenIdentifier@5), Kind: kms-dt, Service: 172.27.52.14:9393, Ident: (kms-dt owner=hbase, renewer=yarn, realUser=, issueDate=1528777648918, maxDate=1529382448918, sequenceNumber=50, masterKeyId=2)]2018-06-12 04:27:31,477|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:31,477 INFO [main] conf.Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.0.0-1476/0/resource-types.xml2018-06-12 04:27:31,527|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:31,527 INFO [main] impl.TimelineClientImpl: Timeline service address: ctr-e138-1518143905142-359429-01-000004.hwx.site:81902018-06-12 04:27:32,563|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:32,562 INFO [main] impl.YarnClientImpl: Submitted application application_1528766389356_00442018-06-12 04:27:32,634|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:32,634 INFO [main] mapreduce.Job: The url to track the job: https://ctr-e138-1518143905142-359429-01-000003.hwx.site:8090/proxy/application_1528766389356_0044/2018-06-12 04:27:32,635|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:32,635 INFO [main] mapreduce.Job: Running job: job_1528766389356_00442018-06-12 04:27:44,807|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:44,806 INFO [main] mapreduce.Job: Job job_1528766389356_0044 running in uber mode : false2018-06-12 04:27:44,809|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:44,809 INFO [main] mapreduce.Job: map 0% reduce 0%2018-06-12 04:27:54,926|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:54,925 INFO [main] mapreduce.Job: map 5% reduce 0%2018-06-12 04:27:56,950|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:56,950 INFO [main] mapreduce.Job: Task Id : attempt_1528766389356_0044_m_000002_0, Status : FAILED2018-06-12 04:27:56,979|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|Error: java.io.FileNotFoundException: File does not exist: hdfs://ns1/apps/hbase/data/oldWALs/ctr-e138-1518143905142-359429-01-000004.hwx.site%2C16020%2C1528776085205.15287761609152018-06-12 04:27:56,979|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1583)2018-06-12 04:27:56,980|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576)2018-06-12 04:27:56,980|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)2018-06-12 04:27:56,980|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591)2018-06-12 04:27:56,980|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.regionserver.wal.ReaderBase.init(ReaderBase.java:64)2018-06-12 04:27:56,981|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.init(ProtobufLogReader.java:165)2018-06-12 04:27:56,981|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.wal.WALFactory.createReader(WALFactory.java:289)2018-06-12 04:27:56,981|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.wal.WALFactory.createReader(WALFactory.java:271)2018-06-12 04:27:56,981|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.wal.WALFactory.createReader(WALFactory.java:259)2018-06-12 04:27:56,981|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.wal.WALFactory.createReader(WALFactory.java:395)2018-06-12 04:27:56,982|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.wal.AbstractFSWALProvider.openReader(AbstractFSWALProvider.java:449)2018-06-12 04:27:56,982|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.openReader(WALInputFormat.java:166)2018-06-12 04:27:56,982|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.hbase.mapreduce.WALInputFormat$WALRecordReader.initialize(WALInputFormat.java:158)2018-06-12 04:27:56,983|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:560)2018-06-12 04:27:56,983|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:798)2018-06-12 04:27:56,983|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)2018-06-12 04:27:56,983|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)2018-06-12 04:27:56,983|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at java.security.AccessController.doPrivileged(Native Method)2018-06-12 04:27:56,984|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at javax.security.auth.Subject.doAs(Subject.java:422)2018-06-12 04:27:56,984|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)2018-06-12 04:27:56,984|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)2018-06-12 04:27:56,984|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|But looks like it did find the file on hdfs, test script runs incremental backup command as HBase user.2018-06-12 04:27:30,756|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:30,755 DEBUG [main] mapreduce.WALInputFormat: Scanning hdfs://ns1/apps/hbase/data/oldWALs/ctr-e138-1518143905142-359429-01-000004.hwx.site%2C16020%2C1528776085205.1528776160915 for WAL files2018-06-12 04:27:30,758|INFO|MainThread|machine.py:167 - run()||GUID=cb1d85c9-023c-4bc5-bf87-9c231c917eab|2018-06-12 04:27:30,758 INFO [main] mapreduce.WALInputFormat: Found: HdfsLocatedFileStatus{path=hdfs://ns1/apps/hbase/data/oldWALs/ctr-e138-1518143905142-359429-01-000004.hwx.site%2C16020%2C1528776085205.1528776160915; isDirectory=false; length=18031; replication=3; blocksize=268435456; modification_time=1528776689363; access_time=1528776160921; owner=hbase; group=hdfs; permission=rwx--x--x; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.test.java.org.apache.hadoop.hbase.backup.TestIncrementalBackup.java</file>
      <file type="M">hbase-backup.src.test.java.org.apache.hadoop.hbase.backup.TestBackupBase.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2123" opendate="2010-1-14 00:00:00" fixdate="2010-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove &amp;#39;master&amp;#39; command-line option from PE.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21465" opendate="2018-11-11 00:00:00" fixdate="2018-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry on reportRegionStateTransition can lead to unexpected errors</summary>
      <description>It is possible that the reportRegionStateTransition method is succeeded at master side, but before returning the the region server, the rpc connection is broken, or the master restarts. So next when the region server try again,we will find that the state for the region and the TRSP is not correct, and can lead to a RS abort or something even worse.We should be able to determine whether a reportRegionStateTransition call is just a retry and has already been succeeded, and just ignore it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestAssignmentManagerBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestReportRegionStateTransitionRetry.java</file>
    </fixedFiles>
  </bug>
  <bug id="21547" opendate="2018-12-4 00:00:00" fixdate="2018-1-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Precommit uses master flaky list for other branches</summary>
      <description>Precommit job downloads the flaky exclude list for master branch when the uploaded patch file is made for different branches.As an example check https://builds.apache.org/job/PreCommit-HBASE-Build/15192 which was against branch-1 but the unit test downloaded master's flaky list.15:26:05 [Tue Dec 4 14:26:04 UTC 2018 INFO]: Personality: patch unit15:26:05 [Tue Dec 4 14:26:04 UTC 2018 INFO]: EXCLUDE_TESTS_URL=https://builds.apache.org/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/excludes/15:26:05 [Tue Dec 4 14:26:04 UTC 2018 INFO]: INCLUDE_TESTS_URL=15:26:05 --2018-12-04 14:26:04-- https://builds.apache.org/job/HBase-Find-Flaky-Tests/job/master/lastSuccessfulBuild/artifact/excludes/15:26:05 Resolving builds.apache.org (builds.apache.org)... 195.201.213.130, 2a01:4f8:c0:2cc9::215:26:05 Connecting to builds.apache.org (builds.apache.org)|195.201.213.130|:443... connected.15:26:06 HTTP request sent, awaiting response... 200 15:26:06 Length: 866 [application/octet-stream]15:26:06 Saving to: 'excludes'15:26:06 15:26:06 0K 100% 43.0M=0s15:26:06 15:26:06 2018-12-04 14:26:06 (43.0 MB/s) - 'excludes' saved [866/866]15:26:06 15:26:09 cd /testptch/hbase/hbase-thrift15:26:09 mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-slave/workspace/PreCommit-HBASE-Build/yetus-m2/hbase-branch-1-patch-1 -DHBasePatchProcess -Dhttps.protocols=TLSv1.2 -PrunAllTests -Dtest.exclude.pattern=**/master.cleaner.TestSnapshotFromMaster.java,**/client.TestRestoreSnapshotFromClientAfterSplittingRegions.java,**/regionserver.TestRegionMergeTransactionOnCluster.java,**/client.TestCloneSnapshotFromClientAfterSplittingRegion.java,**/master.assignment.TestAssignmentManager.java,**/master.assignment.TestAMAssignWithRandExec.java,**/client.TestMobCloneSnapshotFromClientAfterSplittingRegion.java,**/regionserver.TestCompactingToCellFlatMapMemStore.java,**/replication.TestReplicationSmallTestsSync.java,**/TestMultiVersions.java,**/client.TestMobRestoreSnapshotFromClientAfterSplittingRegions.java,**/client.TestRestoreSnapshotFromClientWithRegionReplicas.java,**/regionserver.TestRegionServerAbortTimeout.java,**/replication.TestMasterReplication.java,**/backup.TestIncrementalBackupWithBulkLoad.java,**/master.replication.TestRegisterPeerWorkerWhenRestarting.java clean test -fae &gt; /testptch/patchprocess/patch-unit-hbase-thrift.txt 2&gt;&amp;1</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.2.10,1.4.10,2.1.3,2.0.5,1.3.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2155" opendate="2010-1-22 00:00:00" fixdate="2010-8-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the option to bind to a specific IP address to the Nonblocking Thrift servers</summary>
      <description>This is not possible in Thrift 0.2.0 so we'll have to wait until the next version is released (which includes THRIFT-684). After that is released this is an easy and quick fix. For a few more details see HBASE-1373 and HBASE-65.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21595" opendate="2018-12-13 00:00:00" fixdate="2018-1-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Print thread&amp;#39;s information and stack traces when RS is aborting forcibly</summary>
      <description>After HBASE-21325 RS terminate forcibly  on abort timeout.We should print the thread info before terminating, will be useful to analyze the RS abort timeout problem. </description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21606" opendate="2018-12-14 00:00:00" fixdate="2018-7-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document use of the meta table load metrics added in HBASE-19722</summary>
      <description>HBASE-19722 added a great new tool for figuring out where cluster load is coming from. Needs a section in the ref guide When should I use this? Why shouldn't I use it all the time? What does using it look like? How do I use it?I think all the needed info for making something to answer these questions is in the discussion on HBASE-19722</description>
      <version>3.0.0-alpha-1,1.5.0,1.4.6,2.2.0,2.0.2,2.1.3</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21620" opendate="2018-12-19 00:00:00" fixdate="2018-12-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem in scan query when using more than one column prefix filter in some cases.</summary>
      <description>In some cases, unable to get the scan results when using more than one column prefix filter.Attached a java file to import the data which we used and a text file containing the values..While executing the following query (hbase shell as well as java program) it is waiting indefinitely and after RPC timeout we got the following error.. Also we noticed high cpu, high load average and very frequent young gc  in the region server containing this row...scan 'namespace:tablename',{STARTROW =&gt; 'test',ENDROW =&gt; 'test', FILTER =&gt; "ColumnPrefixFilter('1544770422942010001_') OR ColumnPrefixFilter('1544769883529010001_')"}ROW                                                  COLUMN+CELL                                                                   ERROR: Call id=18, waitTime=60005, rpcTimetout=60000 Note: Table scan operation and scan with a single column prefix filter works fine in this case.When we check the same query in hbase-1.2.5 it is working fine.Can you please help me on this..</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,1.4.8,2.1.2,2.0.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.2,2.0.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterListOnMini.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterList.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterListWithOR.java</file>
    </fixedFiles>
  </bug>
  <bug id="21645" opendate="2018-12-26 00:00:00" fixdate="2018-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Perform sanity check and disallow table creation/modification with region replication &lt; 1</summary>
      <description>We should perform sanity check and disallow table creation with region replication &lt; 1 or modification of an existing table with new region replication value &lt; 1.</description>
      <version>3.0.0-alpha-1,1.5.0,2.1.1,2.1.2</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  <bug id="21688" opendate="2019-1-8 00:00:00" fixdate="2019-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address WAL filesystem issues</summary>
      <description>Scan and fix code base to use new way of instantiating WAL File System. https://issues.apache.org/jira/browse/HBASE-21457?focusedCommentId=16734688&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16734688</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.6,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestWALEntryStream.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.AbstractTestDLS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.fs.TestBlockReorderMultiBlocks.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.WALEntryStream.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceWALReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.WALLink.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestLoadAndVerify.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
      <file type="M">hbase-backup.src.test.java.org.apache.hadoop.hbase.backup.master.TestBackupLogCleaner.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.IncrementalBackupManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AbstractFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterWalManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="2178" opendate="2010-2-2 00:00:00" fixdate="2010-2-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hooks for replication</summary>
      <description>This issue is about getting all the hooks for mdc replication in core HBase.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="21780" opendate="2019-1-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid a wide line on the RegionServer webUI for many ZooKeeper servers</summary>
      <description>HBASE-8812 made this change for MasterUI but not for RegionServer UI. </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug id="21791" opendate="2019-1-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade thrift dependency to 0.12.0</summary>
      <description>As somebody have already known, that there is a CVE for thrift from 0.5.0 to 0.11.0.https://nvd.nist.gov/vuln/detail/CVE-2018-1320As the CVE is already public, let's upgrade our thrift dependency and release new versions ASAP.</description>
      <version>3.0.0-alpha-1,1.5.0,1.3.3,2.2.0,1.4.9,2.1.2,1.2.10,2.0.4</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.TAppend.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TReadType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDeleteType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TConsistency.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompareOp.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  <bug id="21794" opendate="2019-1-27 00:00:00" fixdate="2019-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the Coprocessor observer example given in section 111.1 of the ref guide.</summary>
      <description>The given example should be changed after the CP changes (HBASE-17732)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="21795" opendate="2019-1-28 00:00:00" fixdate="2019-2-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client application may get stuck (time bound) if a table modify op is called immediately after split op</summary>
      <description>Steps: Create a table Split the table Modify the table immediately after splittingExpected: The modify table procedure completes and control returns back to clientActual: The modify table procedure completes and control does not return back to client, until catalog janitor runs and deletes parent or future timeout occurs</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21806" opendate="2019-1-29 00:00:00" fixdate="2019-1-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add an option to roll WAL on very slow syncs</summary>
      <description>In large heterogeneous clusters sometimes a slow datanode can cause WAL syncs to be very slow. In this case, before the bad datanode recovers, or is discovered and repaired, it would be helpful to roll WAL on a very slow sync to get a new pipeline.Otherwise the slow WAL will impact write latency for a long time (slow writes result in less writes result in the WAL not being rolled for longer)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AsyncFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.java</file>
    </fixedFiles>
  </bug>
  <bug id="21884" opendate="2019-2-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix box/unbox findbugs warning in secure bulk load</summary>
      <description>Reason TestsFindBugs module:hbase-serverBoxed value is unboxed and then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:then immediately reboxed in org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint.incrementUgiReference(UserGroupInformation) At SecureBulkLoadEndpoint.java:[line 268]Looking at branch-2 and master I suspect we're doing the same wasteful operation but findbugs can't see it through the lambda definition.</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,2.1.1,2.0.3,2.1.2,2.0.4,1.4.10,1.3.4,1.2.11,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.0.5,1.3.4,1.2.11,2.3.0,2.1.4</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21889" opendate="2019-2-13 00:00:00" fixdate="2019-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use thrift 0.12.0 when build thrift by compile-thrift profile</summary>
      <description>Build command.mvn compile -Pcompile-thrift</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21907" opendate="2019-2-15 00:00:00" fixdate="2019-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should set priority for rpc request</summary>
      <description>Now in async client we just ignored the priority for RpcController.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncSingleRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncServerRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncScanSingleRegionRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncMasterRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdminRequestRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  <bug id="21909" opendate="2019-2-15 00:00:00" fixdate="2019-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate the put instance before executing in AsyncTable.put method</summary>
      <description>Align with the sync client implementation.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncBufferMutator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionConfiguration.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBufferedMutatorImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilderImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBufferedMutatorBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="21944" opendate="2019-2-22 00:00:00" fixdate="2019-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Validate put for batch operation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableBatch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21945" opendate="2019-2-22 00:00:00" fixdate="2019-2-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maintain the original order when sending batch request</summary>
      <description>Find this when implementing HBASE-21717. In some UT we put several good requests and bad requests together, and expect only the bad ones to fail. This usually depends on the grouping at rs side, if we group the good one and the bad one together as a batch, it will fail them all. So usually in test we will insert an increment or append in the middle to break them into two groups when executing at RS side.So if we do not maintain the order, at the rs side, the increment or append may comes first or last, then the good ones and bad ones will be grouped and cause all of them to fail.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableBatch.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  <bug id="2196" opendate="2010-2-9 00:00:00" fixdate="2010-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support more than one slave cluster</summary>
      <description>Currently replication supports only 1 slave cluster, need to ability to add more.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">src.main.ruby.shell.rb</file>
      <file type="M">src.main.ruby.hbase.replication.admin.rb</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.ReplicationZookeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.replication.ReplicationAdmin.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.replication.TestReplicationAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="21962" opendate="2019-2-27 00:00:00" fixdate="2019-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filters do not work in ThriftTable</summary>
      <description>Filters in ThriftTable is not working, this issue is to fix it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftConnection.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="22225" opendate="2019-4-12 00:00:00" fixdate="2019-4-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Profiler tab on Master/RS UI not working w/o comprehensive message</summary>
      <description>As titled, when checking 1.5.0 RC3 binary package, clicking the "Profiler" tab on HMaster/RegionServer web UI, it complains page not found error like below:Problem accessing /prof. Reason: NOT_FOUND</description>
      <version>1.5.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-http.src.main.java.org.apache.hadoop.hbase.http.ProfileServlet.java</file>
      <file type="M">hbase-http.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22264" opendate="2019-4-17 00:00:00" fixdate="2019-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out jars related to JDK 11 into a folder in /lib</summary>
      <description>UPDATE:Separate out the the jars related to JDK 11 and add control their addition to the classpath using an environment variable or auto-detection of the jdk version installed.OLD:This is in continuation with HBASE-22249. When compiled with jdk 8 and run on jdk 11, the master branch throws the following exception during an attempt to start the hbase rest server:Exception in thread "main" java.lang.NoClassDefFoundError: javax/annotation/Priority at org.glassfish.jersey.model.internal.ComponentBag.modelFor(ComponentBag.java:483) at org.glassfish.jersey.model.internal.ComponentBag.access$100(ComponentBag.java:89) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:408) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:398) at org.glassfish.jersey.internal.Errors.process(Errors.java:315) at org.glassfish.jersey.internal.Errors.process(Errors.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:228) at org.glassfish.jersey.model.internal.ComponentBag.registerModel(ComponentBag.java:398) at org.glassfish.jersey.model.internal.ComponentBag.register(ComponentBag.java:235) at org.glassfish.jersey.model.internal.CommonConfig.register(CommonConfig.java:420) at org.glassfish.jersey.server.ResourceConfig.register(ResourceConfig.java:425) at org.apache.hadoop.hbase.rest.RESTServer.run(RESTServer.java:245) at org.apache.hadoop.hbase.rest.RESTServer.main(RESTServer.java:421)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug id="22379" opendate="2019-5-8 00:00:00" fixdate="2019-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Markdown for "Voting on Release Candidates" in book</summary>
      <description>The Markdown in the section "Voting on Release Candidates" of the HBase book seems to be broken. It looks like that there should be a quote, which isn't displayed correctly. Same is true for the formatting of the Maven RAT command.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22380" opendate="2019-5-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>break circle replication when doing bulkload</summary>
      <description>when enabled master-master bulkload replication, HFiles will be replicated circularly between two clusters</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0,1.4.10,2.0.5,2.3.0,2.1.5,1.3.5</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.BulkLoadHFilesTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SecureBulkLoadManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.AsyncClusterConnectionImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.AsyncClusterConnection.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.WAL.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestBulkLoadHFilesSplitRecovery.java</file>
    </fixedFiles>
  </bug>
  <bug id="22384" opendate="2019-5-8 00:00:00" fixdate="2019-5-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formatting issues in administration section of book</summary>
      <description>The administration section in the book (64.3.2. Administration) has some formatting issues. Due to that issues the list count is not accurate, as well as the indentation of some code snippets.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22399" opendate="2019-5-12 00:00:00" fixdate="2019-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default hadoop-two.version to 2.8.x and remove the 2.7.x hadoop checks</summary>
      <description>Our nightly is failing so let's do this first, and for the ref guide changes can be done in another sub task.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestReversedScannerCallable.java</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="22400" opendate="2019-5-12 00:00:00" fixdate="2019-5-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the adapter code in async fs implementation for hadoop-2.7.x</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputSaslHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="22405" opendate="2019-5-13 00:00:00" fixdate="2019-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Ref Guide for EOL of Hadoop 2.7</summary>
      <description></description>
      <version>3.0.0-alpha-1,1.5.0,2.2.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22408" opendate="2019-5-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a metric for regions OPEN on non-live servers</summary>
      <description>This serves 2 purposes for monitoring:1) Catching when regions are on dead servers due to long WAL splitting or other delays in SCP. At that time, the regions are not listed as RITs; we'd like to be able to have alerts in such cases.2) Catching various bugs in assignment and procWAL corruption, etc. that leave region "OPEN" on a server that no longer exists, again to alert the administrator via a metric.Later, it might be possible to add more logic to distinguish 1 and 2, and to mitigate 2 automatically and also set some metric to alert the administrator to investigate later.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManagerSource.java</file>
    </fixedFiles>
  </bug>
  <bug id="2241" opendate="2010-2-19 00:00:00" fixdate="2010-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change balancer sloppyness from 0.1 to 0.3</summary>
      <description>This is a quick workaround until we do a better balancer.Taking a region off line when cluster is under load is bad news. Latency goes up as we wait on regions to come up in new locations.The load balancer should only cut in if the cluster is way out of alignment.I'd argue that 10% deviance from the avg. is not good enough reason moving regions around when cluster is under load.Balancer already has a knack for cutting in at most inopportune moments: during cluster startup, when new node is added to a small cluster, or moving a region just after its been opened on a node. We'll need to do a better balancer but meantime lets just allow that region loading can be sloppier, say 20% or 30% off the average before balancer cuts in.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="22422" opendate="2019-5-15 00:00:00" fixdate="2019-5-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain an ByteBuff with refCnt=0 when getBlock from LRUCache</summary>
      <description>After runing YCSB scan/get benchmark in our XiaoMi cluster, we found the get QPS dropped from 25000/s to hunderds per second in a cluster with five nodes. After enable the debug log at YCSB client side, I found the following stacktrace , see https://issues.apache.org/jira/secure/attachment/12968745/image-2019-05-15-12-00-03-641.png. After looking into the stractrace, I can ensure that the zero refCnt block is an intermedia index block, see &amp;#91;2&amp;#93; http://hbase.apache.org/images/hfilev2.pngNeed a patch to fix this.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCombinedBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.bucket.BucketCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22460" opendate="2019-5-22 00:00:00" fixdate="2019-10-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reopen a region if store reader references may have leaked</summary>
      <description>We can leak store reader references if a coprocessor or core function somehow opens a scanner, or wraps one, and then does not take care to call close on the scanner or the wrapped instance. A reasonable mitigation for a reader reference leak would be a fast reopen of the region on the same server (initiated by the RS) This will release all resources, like the refcount, leases, etc. The clients should gracefully ride over this like any other region transition. This reopen would be like what is done during schema change application and ideally would reuse the relevant code. If the refcount is over some ridiculous threshold this mitigation could be triggered along with a fat WARN in the logs.</description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ReopenTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.ClusterStatus.proto</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerMetricsBuilder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionMetricsBuilder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug id="22628" opendate="2019-6-25 00:00:00" fixdate="2019-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the custom WAL directory (hbase.wal.dir) usage</summary>
      <description>Custom WAL directory usage must be documented, otherwise it may lead to inconsistent data during migrating to new WAL dir path. You can consider below scenario while migrating to custom WAL directory. Setup HBase cluster with the default setting (all WAL files are under the root directory ie. /hbase/WALs). Create table 't1' and insert few records Flush meta table (so that table region entries persist in FS) Forcibly kill HBase processes (HM &amp; RS). Configure the hbase.wal.dir to outside the root dir (say /hbaseWAL) Start the HBase servers Scan 't1'Ideally HMaster should submit split task of old RS(s) WAL files (created under /hbase/WALs) and old data should be replayed. But currently, during HM startup we populate the previous dead servers from the current WAL dir ( hbase.wal.dir -&gt; /hbaseWAL). Since WAL dir path is new, so you need to copy RegionServer WAL directories manualy from old WAL dir to new path. </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="2263" opendate="2010-2-24 00:00:00" fixdate="2010-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] multiuser mode: authenticator for zookeeper</summary>
      <description>Add an authenticator module for zookeeper.Use a tree like:/stargate/ users/ &lt;token&gt; -- JSON formatted user record with keys 'token', 'name', 'admin', and 'disabled'</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22689" opendate="2019-7-13 00:00:00" fixdate="2019-7-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Line break for fix version in documentation</summary>
      <description>The section describing the policy for the fix version in JIRA is missing line breaks.</description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug id="22738" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fallback to default group to choose RS when there are no RS in current group</summary>
      <description>We configure one regionserver for hbase system table. But when rolling upgrade, you need move the region to other regionservers. But because there are no other regionservers in this group, you cannot move the region...  </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupBasedLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2274" opendate="2010-2-27 00:00:00" fixdate="2010-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] support specification of filters for scanners: JSON descriptors</summary>
      <description>Before opening up server side to arbitrary (filter) object construction, just accept descriptions of filter structure as JSON.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.HBase.rb</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.WritableByteArrayComparable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.WhileMatchFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.CompareFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.ColumnCountGetFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">contrib.stargate.src.main.resources.org.apache.hadoop.hbase.stargate.protobuf.ScannerMessage.proto</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ScannerResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.RowResource.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.ResultGenerator.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.VersionMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableSchemaMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableListMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ScannerMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellSetMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.protobuf.generated.CellMessage.java</file>
      <file type="M">contrib.stargate.src.main.java.org.apache.hadoop.hbase.stargate.model.ScannerModel.java</file>
      <file type="M">contrib.stargate.pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22743" opendate="2019-7-26 00:00:00" fixdate="2019-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClientUtils for hbase-examples</summary>
      <description>hbase-examples have many Client Demo classes that use many utility methods that can be put in a Utility class e.g. initializing LoginContext for every demo class to utilize.</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift.HttpDoAsClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift.DemoClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift2.DemoClient.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.rest.RESTDemoClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22824" opendate="2019-8-9 00:00:00" fixdate="2019-8-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show filesystem path for the orphans regions on filesystem</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestHbckChore.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.hbck.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22838" opendate="2019-8-12 00:00:00" fixdate="2019-8-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>assembly:single failure: user id or group id &amp;#39;xxxxx&amp;#39; is too big</summary>
      <description> tarball build with assembly:single command fails with user id(mac) or group id(ubuntu) too big error:$ mvn clean install package assembly:single -DskipTests............[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (default-cli) on project hbase-assembly: Execution default-cli of goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single failed: user id 'xxxxxxxx' is too big ( &gt; 2097151 ). -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException[ERROR][ERROR] After correcting the problems, you can resume the build with the command[ERROR]   mvn &lt;goals&gt; -rf :hbase-assemblyTo avoid this error and to get better features for tarball build, we should upgrade tarLongFileMode from gnu to posix: MPOM-132This works for assembly plugin &gt;= 2.5.0: MASSEMBLY-728 </description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22911" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>fewer concurrent github PR builds</summary>
      <description>we've been regularly getting 4-5 concurrent builds of PRs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11,2.0.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  <bug id="22913" opendate="2019-8-24 00:00:00" fixdate="2019-8-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hadoop label for nightly builds</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="22935" opendate="2019-8-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TaskMonitor warns MonitoredRPCHandler task may be stuck when it recently started</summary>
      <description>After setting hbase.taskmonitor.rpc.warn.time to 180000, the logs show WARN messages such as these2019-08-08 21:50:02,601 WARN [read for TaskMonitor] monitoring.TaskMonitor - Task may be stuck: RpcServer.FifoWFPBQ.default.handler=4,queue=4,port=60020: status=Servicing call from &lt;ip&gt;:55164: Scan, state=RUNNING, startTime=1563305858103, completionTime=-1, queuetimems=1565301002599, starttimems=1565301002599, clientaddress=&lt;ip&gt;, remoteport=55164, packetlength=370, rpcMethod=ScanNotice that the first starttimems is far in the past. The second starttimems and the queuetimems are much closer to the log timestamp than 180 seconds. I think this is because the warnTime is initialized to the time that MonitoredTaskImpl is created, but never updated until we write a warn message to the log.</description>
      <version>3.0.0-alpha-1,1.4.0,1.5.0,1.3.3,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,1.3.6,1.4.11,2.1.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.monitoring.TestTaskMonitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredRPCHandlerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug id="23139" opendate="2019-10-9 00:00:00" fixdate="2019-10-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce jobs lauched from convenience distribution are nonfunctional</summary>
      <description>CNFE thirdparty GSON, need to add thirdparty jar to job deps.Error: java.lang.ClassNotFoundException: org.apache.hbase.thirdparty.com.google.gson.GsonBuilder at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.hadoop.hbase.util.GsonUtil.createGson(GsonUtil.java:44) at org.apache.hadoop.hbase.util.JsonMapper.&lt;clinit&gt;(JsonMapper.java:37) at org.apache.hadoop.hbase.client.Operation.toJSON(Operation.java:70) at org.apache.hadoop.hbase.client.Operation.toString(Operation.java:96) at org.apache.hadoop.hbase.client.Operation.toString(Operation.java:110) at org.apache.hadoop.hbase.mapreduce.TableSplit.toString(TableSplit.java:368) at java.lang.String.valueOf(String.java:2994) at java.lang.StringBuilder.append(StringBuilder.java:131) at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:762) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:177) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:171)</description>
      <version>1.5.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,1.3.6,1.4.11,2.2.2,2.1.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="23172" opendate="2019-10-14 00:00:00" fixdate="2019-10-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Canary region success count metrics reflect column family successes, not region successes</summary>
      <description>HBase Canary reads once per column family per region. The current "region success count" should actually be "column family success count," which means we need another metric that actually reflects region success count. Additionally, the region read and write latencies only store the latencies of the last column family of the region read. Instead of a map of regions to a single latency value and success value, we should map each region to a list of such values.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0,2.1.5,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestCanaryTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.CanaryTool.java</file>
    </fixedFiles>
  </bug>
  <bug id="23202" opendate="2019-10-23 00:00:00" fixdate="2019-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ExportSnapshot (import) will fail if copying files to root directory takes longer than cleaner TTL</summary>
      <description>HBASE-17330 removed the checking of the snapshot .tmp directory when determining which files are candidates for deletes. It appears that in the latest branches, this isn't an issue for taking a snapshot as it checks whether a snapshot is in progress via the SnapshotManager.However, when using the ExportSnapshot tool to import a snapshot into a cluster, it will first copy the snapshot manifest into /.snapshot/.tmp/&lt;snapshot&gt; &amp;#91;1&amp;#93;, copies the files, and then renames the snapshot manifest to the final snapshot directory. If the copyFiles job takes longer than the cleaner TTL, the ExportSnapshot job will fail because HFiles will get deleted before the snapshot is committed to the final directory. The ExportSnapshot tool already has a functionality to skipTmp and write the manifest directly to the final location. However, this has unintended consequences such as the snapshot appearing to the user before it is usable. So it looks like we will have to bring back the tmp directory check to avoid this situation.&amp;#91;1&amp;#93; https://github.com/apache/hbase/blob/master/hbase-mapreduce/src/main/java/org/apache/hadoop/hbase/snapshot/ExportSnapshot.java#L1029</description>
      <version>3.0.0-alpha-1,1.5.0,2.2.1,1.4.11,2.1.7</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestSnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCacheWithDifferentWorkingDir.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotDescriptionUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.TakeSnapshotHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotHFileCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotFileCache.java</file>
    </fixedFiles>
  </bug>
  <bug id="23207" opendate="2019-10-23 00:00:00" fixdate="2019-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log a region open journal</summary>
      <description>Like HBASE-22828, but for region opening.Also, tweak the calls to enableStatusJournal to pass through 'true' as parameter to include the current status in the journal, for slightly more context.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,1.3.7,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug id="23227" opendate="2019-10-29 00:00:00" fixdate="2019-10-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson-databind to 2.9.10.1 to avoid recent CVEs</summary>
      <description>Several net new CVEs were raised against jackson-databind 2.9.10.CVE-2019-16942CVE-2019-169432.9.10.1 is released, which I believe addresses these two CVEs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,2.1.8,2.2.3</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23228" opendate="2019-10-29 00:00:00" fixdate="2019-11-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for jdk8 specific modules on branch-1 in precommit/nightly testing</summary>
      <description>At least 1 feature backport is waiting on proper handling of jdk8 activated modules for our yetus personality (HBASE-22114 tinylfu).Implement the general handling here so that we don't have to worry about pushes to the PR branch overwriting it.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.4.12,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="23289" opendate="2019-11-13 00:00:00" fixdate="2019-11-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update links to Hadoop wiki in code and book</summary>
      <description>Seems Hadoop has moved their wiki, so now links throughout our book are broken. We've found and fixed a couple one-offs, but we should do a sweep and clean up the rest.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.metrics.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">hbase-mapreduce.src.main.java.org.apache.hadoop.hbase.mapreduce.package-info.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ColumnFamilyDescriptorBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="23829" opendate="2020-2-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get `-PrunSmallTests` passing on JDK11</summary>
      <description>Start with the small tests, shaking out issues identified by the harness. So far it seems like -Dhadoop.profile=3.0 and -Dhadoop-three.version=3.3.0-SNAPSHOT maybe be required.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-http.src.test.java.org.apache.hadoop.hbase.http.log.TestLogLevel.java</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestFutureUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2523" opendate="2010-5-7 00:00:00" fixdate="2010-6-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add check for licenses before rolling an RC, add to how-to-release doc. and check for inlining a tool that does this for us</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>
