<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  <bug fixdate="2015-4-20 01:00:00" id="13516" opendate="2015-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase PermSize to 128MB</summary>
      <description>HBase uses ~40MB, and with Phoenix we use ~56MB of Perm space out of 64MB by default. Every Filter and Coprocessor increases that.Running out of perm space triggers a stop the world full GC of the entire heap. We have seen this in misconfigured cluster. Should we default to -XX:PermSize=128m -XX:MaxPermSize=128m out of the box as a convenience for users?</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.hbase-env.cmd</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-24 01:00:00" id="13554" opendate="2015-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book clarifying API stability guarantees</summary>
      <description>From the "Clarifying interface evolution freedom in patch releases" thread on dev@h.a.oSeems we have consensus that "HBase uses Semantic Versioning" isn't quite correct (or desired) at the moment. Update the documentation to make sure we're not misrepresenting any guarantees to users.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-28 01:00:00" id="13586" opendate="2015-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book on Hadoop and Java supported versions for 1.1.x</summary>
      <description>Should update http://hbase.apache.org/book.html#basic.prerequisites with the latest info.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-15 01:00:00" id="13694" opendate="2015-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallQueueSize is incorrectly decremented until the response is sent</summary>
      <description>We should decrement the CallQueueSize as soon as we no longer need the call around, e.g. after RpcServer.CurCall.set(null) otherwise we will be only pushing back other client requests while we send the response back to the client that originated the call.</description>
      <version>1.1.0,0.98.12,1.0.2,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-23 01:00:00" id="13755" opendate="2015-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide single super user check implementation</summary>
      <description>Followup for HBASE-13375.</description>
      <version>None</version>
      <fixedVersion>1.1.0,0.98.14,1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.ExpAsStringVisibilityLabelServiceImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.SecureTestUtil.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQosFunction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPriorityRpc.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityLabelsCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.DefaultVisibilityLabelServiceImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlLists.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.AuthUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-24 01:00:00" id="13760" opendate="2015-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup Findbugs keySet iterator warnings</summary>
      <description>Cleanup Findbugs keySet iterator warnings</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-25 01:00:00" id="13767" opendate="2015-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ZKAclReset to set and not just clear ZK ACLs</summary>
      <description>The ZKAclReset tool allows to clear ZK ACLs, which is useful if you are migrating from a secure to unsecure cluster setup.If you want to make sure that your znode ACLs are correct, a -set-acls option, which allows to enforce the proper ACLs on the znodes in a secure setup, can be useful too.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZkAclReset.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-5-6 01:00:00" id="1377" opendate="2009-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS address is null in master web UI</summary>
      <description>My patch in HBASE-1279 was targeted for branch 0.19 and was missing a line to make it work for trunk in the copy constructor of HSI.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-26 01:00:00" id="13776" opendate="2015-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting illegal versions for HColumnDescriptor does not throw IllegalArgumentException</summary>
      <description>HColumnDescriptor hcd = new HColumnDescriptor( new HColumnDescriptor(HConstants.CATALOG_FAMILY) .setInMemory(true) .setScope(HConstants.REPLICATION_SCOPE_LOCAL) .setBloomFilterType(BloomType.NONE) .setCacheDataInL1(true)); final int minVersions = 123; final int maxVersions = 234; hcd.setMaxVersions(minVersions); hcd.setMinVersions(maxVersions);//no exception throw</description>
      <version>0.98.14,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-28 01:00:00" id="13801" opendate="2015-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop src checksum is shown instead of HBase src checksum in master / RS UI</summary>
      <description>Simple bug. We are showing the Hadoop's source MD5 checksum in the master UI instead of the HBase's one.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-30 01:00:00" id="13816" opendate="2015-5-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build shaded modules only in release profile</summary>
      <description>Shaded modules are great, but not needed in the regular build + test cycle. I noticed that around 30-40% of the build time goes in the actual shading. I think we can just build the shaded jars in the release profile. hadoopqe and mvn publishing should not be affected.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-2 01:00:00" id="13826" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to create table when group acls are appropriately set.</summary>
      <description>Steps for reproducing the issue. Create user 'test' and group 'hbase-admin'. Grant global create permissions to 'hbase-admin'. Add user 'test' to 'hbase-admin' group. Create table operation for 'test' user will throw ADE.</description>
      <version>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-6-14 01:00:00" id="13899" opendate="2015-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jacoco instrumentation fails under jdk8</summary>
      <description>Moving the post-commit build for master to also cover jdk8 shows failures when attempting to instrument test for jacoco coverage.example: Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:386) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:401)Caused by: java.lang.RuntimeException: Class java/util/UUID could not be instrumented. at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:138) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:99) at org.jacoco.agent.rt.internal_5d10cad.PreMain.createRuntime(PreMain.java:51) at org.jacoco.agent.rt.internal_5d10cad.PreMain.premain(PreMain.java:43) ... 6 moreCaused by: java.lang.NoSuchFieldException: $jacocoAccess at java.lang.Class.getField(Class.java:1695) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:136) ... 9 moreFATAL ERROR in native method: processing of -javaagent failedAborted</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="13910" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add branch-1.2 to precommit branches</summary>
      <description>update the precommit test properties so that patches targeting branch-1.2 work</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-16 01:00:00" id="13911" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.2 to prereq tables in ref guide</summary>
      <description>update the ref guide to have a column for 1.2.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-18 01:00:00" id="13930" opendate="2015-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Findbugs packages from shaded jars</summary>
      <description>Looking at 1.1.1RC0 shaded artifacts, looks like classes from find bugs are under the edu prefix and are not shaded. We should exclude find bugs from the shaded builds, and/or shade shade the edu prefix as well.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-22 01:00:00" id="13948" opendate="2015-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand hadoop2 versions built on the pre-commit</summary>
      <description>For the HBase 1.1 line I've been validating builds against the following hadoop versions: 2.2.0 2.3.0 2.4.0 2.4.1 2.5.0 2.5.1 2.5.2 2.6.0 2.7.0. Let's do the same in pre-commit.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-8 01:00:00" id="1395" opendate="2009-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>InfoServers no longer put up a UI</summary>
      <description>InfoServers do not work in an vanilla Hadoop 0.20 and HBase 0.20-dev installation. Directory listings instead of UI.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-23 01:00:00" id="13956" opendate="2015-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add myself as 1.1 release manager</summary>
      <description>Just saw we have an RM section. Add myself.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-25 01:00:00" id="13967" opendate="2015-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add jdk profiles for jdk.tools dependency</summary>
      <description>Right now hbase-annotations uses jdk7 jdk.tools and exposes that to downstream via hbase-client. We need it for building and using our custom doclet, but we should be using a jdk.tools version based on our java version (use jdk activated profiles to set it)</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-annotations.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-7-26 01:00:00" id="13975" opendate="2015-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add 1.2 RM to docs</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-26 01:00:00" id="13976" opendate="2015-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>release manager list in ref guide is missing 0.94 line</summary>
      <description>0.94 has slowed substantially, but we haven't EOLed it yet so we should make sure folks looking to get a fix included know where to look.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-29 01:00:00" id="13992" opendate="2015-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate SparkOnHBase into HBase</summary>
      <description>This Jira is to ask if SparkOnHBase can find a home in side HBase core.Here is the github: https://github.com/cloudera-labs/SparkOnHBaseI am the core author of this project and the license is Apache 2.0A blog explaining this project is herehttp://blog.cloudera.com/blog/2014/12/new-in-cloudera-labs-sparkonhbase/A spark Streaming example is herehttp://blog.cloudera.com/blog/2014/11/how-to-do-near-real-time-sessionization-with-spark-streaming-and-apache-hadoop/A real customer using this in produce is blogged herehttp://blog.cloudera.com/blog/2015/03/how-edmunds-com-used-spark-streaming-to-build-a-near-real-time-dashboard/Please debate and let me know what I can do to make this happen.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-4 01:00:00" id="14026" opendate="2015-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify "Web API" in version and compatibility docs</summary>
      <description>per discussion on HBASE-13861, update our version and compatibility section to clarify under operational compatibility that by "Web page API" we mean the /jmx endpoint.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-7-15 01:00:00" id="14086" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-15 01:00:00" id="14087" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-8-19 01:00:00" id="14249" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14250" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14251" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14260" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t build javadocs for hbase-protocol module</summary>
      <description>I'm not sure I have all the affected versions, but it seems that something is amiss in making our javadocs: mvn -Papache-release -Prelease -DskipTests clean package... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache HBase ....................................... SUCCESS [ 11.149 s][INFO] Apache HBase - Checkstyle .......................... SUCCESS [ 1.249 s][INFO] Apache HBase - Resource Bundle ..................... SUCCESS [ 0.539 s][INFO] Apache HBase - Annotations ......................... SUCCESS [ 4.438 s][INFO] Apache HBase - Protocol ............................ SUCCESS [10:15 min][INFO] Apache HBase - Common .............................. SUCCESS [ 48.465 s][INFO] Apache HBase - Procedure ........................... SUCCESS [ 14.375 s][INFO] Apache HBase - Client .............................. SUCCESS [ 45.187 s][INFO] Apache HBase - Hadoop Compatibility ................ SUCCESS [ 6.998 s][INFO] Apache HBase - Hadoop Two Compatibility ............ SUCCESS [ 14.891 s][INFO] Apache HBase - Prefix Tree ......................... SUCCESS [ 14.214 s][INFO] Apache HBase - Server .............................. SUCCESS [02:01 min][INFO] Apache HBase - Testing Util ........................ SUCCESS [ 12.779 s][INFO] Apache HBase - Thrift .............................. SUCCESS [01:15 min][INFO] Apache HBase - Shell ............................... SUCCESS [ 6.649 s][INFO] Apache HBase - Integration Tests ................... SUCCESS [ 6.429 s][INFO] Apache HBase - Examples ............................ SUCCESS [ 13.200 s][INFO] Apache HBase - Rest ................................ SUCCESS [ 27.831 s][INFO] Apache HBase - Assembly ............................ SUCCESS [ 19.400 s][INFO] Apache HBase - Shaded .............................. SUCCESS [ 0.419 s][INFO] Apache HBase - Shaded - Client ..................... SUCCESS [ 23.707 s][INFO] Apache HBase - Shaded - Server ..................... SUCCESS [ 43.654 s][INFO] Apache HBase - Spark ............................... SUCCESS [02:22 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 21:13 min[INFO] Finished at: 2015-08-19T15:48:00-05:00[INFO] Final Memory: 181M/1513M[INFO] ------------------------------------------------------------------------</description>
      <version>0.98.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-20 01:00:00" id="14271" opendate="2015-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Nexus staging instructions</summary>
      <description>Refine the Nexus staging instructions a bit. (A promise I made a long time ago.)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-8-27 01:00:00" id="14325" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add snapshotinfo command to hbase script</summary>
      <description>Since we already have commands like hbck, hfile, wal etc. that are used for getting various types of information about HBase components it make sense to me to add SnapshotInfo tool to collection. If nobody objects i would add patch for this.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-27 01:00:00" id="14326" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book: fix definition of max min size to compact</summary>
      <description>I think we need to change wording/definition of these config parameters in HBase book, they are misleading:hbase.hstore.compaction.min.sizeDescriptionA StoreFile smaller than this size will always be eligible for minor compaction. HFiles this size or larger are evaluated by hbase.hstore.compaction.ratio to determine if they are eligible. Because this limit represents the "automatic include"limit for all StoreFiles smaller than this value, this value may need to be reduced in write-heavy environments where many StoreFiles in the 1-2 MB range are being flushed, because every StoreFile will be targeted for compaction and the resulting StoreFiles may still be under the minimum size and require further compaction. If this parameter is lowered, the ratio check is triggered more quickly. This addressed some issues seen in earlier versions of HBase but changing this parameter is no longer necessary in most situations. Default: 128 MB expressed in bytes.Default134217728hbase.hstore.compaction.max.sizeDescriptionA StoreFile larger than this size will be excluded from compaction. The effect of raising hbase.hstore.compaction.max.size is fewer, larger StoreFiles that do not get compacted often. If you feel that compaction is happening too often without much benefit, you can try raising this value. Default: the value of LONG.MAX_VALUE, expressed in bytes.hbase.hstore.compaction.ratioDescriptionFor minor compaction, this ratio is used to determine whether a given StoreFile which is larger than hbase.hstore.compaction.min.size is eligible for compaction. Its effect is to limit compaction of large StoreFiles. The value of hbase.hstore.compaction.ratio is expressed as a floating-point decimal. A large ratio, such as 10, will produce a single giant StoreFile. Conversely, a low value, such as .25, will produce behavior similar to the BigTable compaction algorithm, producing four StoreFiles. A moderate value of between 1.0 and 1.4 is recommended. When tuning this value, you are balancing write costs with read costs. Raising the value (to something like 1.4) will have more write costs, because you will compact larger StoreFiles. However, during reads, HBase will need to seek through fewer StoreFiles to accomplish the read. Consider this approach if you cannot take advantage of Bloom filters. Otherwise, you can lower this value to something like 1.0 to reduce the background cost of writes, and use Bloom filters to control the number of StoreFiles touched during reads. For most cases, the default value is appropriate.Default1.2FFor details, see HBASE-14263.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-30 01:00:00" id="14338" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-30 01:00:00" id="14340" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-31 01:00:00" id="14346" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in FamilyFilter</summary>
      <description>I think there's a typo. "qualifier name" should read "column family name"Family Filter This filter takes a compare operator and a comparator. It compares each qualifier name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that column.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.thrift.filter.language.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="14348" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-1 01:00:00" id="14349" opendate="2015-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-9 01:00:00" id="14385" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close the sockets that is missing in connection closure.</summary>
      <description>As per heading. Due credit to one of our awesome customers for digging into this and helping me craft the unit test.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-9 01:00:00" id="14387" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction improvements: Maximum off-peak compaction size</summary>
      <description>Make max compaction size for peak and off peak separate config options.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerOnlineConfigChange.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-21 01:00:00" id="14459" opendate="2015-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add request and response sizes metrics</summary>
      <description>Adding metrics that should be useful:Request sizeResponse size</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.15,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestRpcMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-23 01:00:00" id="14473" opendate="2015-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute region locality in parallel</summary>
      <description>Right now on large clusters it's necessary to turn off the locality balance cost as it takes too long to compute the region locality. This is because it's computed when need in serial.We should compute this in parallel before it's needed.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.RegionLocationFinder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-30 01:00:00" id="14515" opendate="2015-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow spark module unit tests to be skipped with a profile</summary>
      <description>we can skip the tests for individual modules with profile, e.g. skipServerTests, we should have the same for spark.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-6-27 01:00:00" id="1453" opendate="2009-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HADOOP-4681 to our bundled hadoop, add to &amp;#39;gettting started&amp;#39; recommendation that hbase users backport</summary>
      <description>HADOOP-4681, while not perfect, should help with our too frequent 'no such block' message incidence.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.hadoop-0.20.0-core.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-2 01:00:00" id="14544" opendate="2015-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow HConnectionImpl to not refresh the dns on errors</summary>
      <description>Some clusters will have static ip addresses and forced dns lookup can cause extra instability. Allow users to tun that feature off, if wanted.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-5-29 01:00:00" id="1458" opendate="2009-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>64 commit logs as upper bound is too many -- make it half</summary>
      <description>Running an upload, 64 commit logs as upper bound before we force flushes to clear the oldest edit is too much. I can see an upload running in my little cluster and even after running for an hour we still have not hit the 64 logs max. The more logs we have, the longer recovery on crash. We should halve the number I'd say.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-16 01:00:00" id="14633" opendate="2015-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try fluid width UI</summary>
      <description>Our UI is often too long. Lets give it more room if available.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-rest.src.main.resources.hbase-webapps.rest.rest.jsp</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-10-24 01:00:00" id="14690" opendate="2015-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix css so there&amp;#39;s no left/right scroll bar</summary>
      <description>2 em of extra padding needs to be reomved.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-11-29 01:00:00" id="14723" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix IT tests split too many times</summary>
      <description>Splitting the whole table is happening too often. Lets make this happen less frequently as there are more regions.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.SplitAllRegionOfTableAction.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-2 01:00:00" id="14745" opendate="2015-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade the last few dependencies in hbase-shaded-client</summary>
      <description>junit hadoop common</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.13,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-5 01:00:00" id="14772" opendate="2015-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve zombie detector; be more discerning</summary>
      <description>Currently, any surefire process with the hbase flag is a potential zombie. Our zombie check currently takes a reading and if it finds candidate zombies, it waits 30 seconds and then does another reading. If a concurrent build going on, in both cases the zombie detector will come up positive though the adjacent test run may be making progress; i.e. the cast of surefire processes may have changed between readings but our detector just sees presence of hbase surefire processes.Here is example:Suspicious java process found - waiting 30s to see if there are just slow to stopThere appear to be 5 zombie tests, they should have been killed by surefire but survived12823 surefirebooter852180186418035480.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true7653 surefirebooter8579074445899448699.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true12614 surefirebooter136529596936417090.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true7836 surefirebooter3217047564606450448.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true13566 surefirebooter2084039411151963494.jar -enableassertions -Dhbase.test -Xmx2800m -XX:MaxPermSize=256m -Djava.security.egd=file:/dev/./urandom -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true************ BEGIN zombies jstack extract************ END zombies jstack extract5 is the number of forked processes we allow when doing medium and large tests.... so an adjacent build will always show as '5 zombies'.Need to add discerning if list of processes changes between readings.Can I also add a tag per build run that all forked processes pick up so I can look at the current builds progeny only?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.zombie-detector.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-5 01:00:00" id="14773" opendate="2015-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HBase shell tests are skipped when skipping server tests.</summary>
      <description>Looks like a copy pasta error.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-6 01:00:00" id="14778" opendate="2015-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make block cache hit percentages not integer in the metrics system</summary>
      <description>Once you're close to the 90%+ it's hard to see a difference because getting a full percent change is rare.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-6 01:00:00" id="14781" opendate="2015-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn per cf flushing on for ITBLL by default</summary>
      <description/>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-10 01:00:00" id="14795" opendate="2015-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the spark-hbase scan operations</summary>
      <description>This is a sub-jira of HBASE-14789. This jira is to focus on the replacement of TableInputFormat for a more custom scan implementation that will make the following use case more effective.Use case:In the case you have multiple scan ranges on a single table with in a single query. TableInputFormat will scan the the outer range of the scan start and end range where this implementation can be more pointed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-11 01:00:00" id="14796" opendate="2015-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the Gets in the connector</summary>
      <description>Current the Spark-Module Spark SQL implementation gets records from HBase from the driver if there is something like the following found in the SQL.rowkey = 123The reason for this original was normal sql will not have many equal operations in a single where clause.Zhan, had brought up too points that have value.1. The SQL may be generated and may have many many equal statements in it so moving the work to an executor protects the driver from load2. In the correct implementation the drive is connecting to HBase and exceptions may cause trouble with the Spark application and not just with the a single task execution</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseResources.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-13 01:00:00" id="14804" opendate="2015-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell&amp;#39;s create table command ignores &amp;#39;NORMALIZATION_ENABLED&amp;#39; attribute</summary>
      <description>I am trying to create a new table and set the NORMALIZATION_ENABLED as true, but seems like the argument NORMALIZATION_ENABLED is being ignored. And the attribute NORMALIZATION_ENABLED is not displayed on doing a desc command on that tablehbase(main):020:0&gt; create 'test-table-4', 'cf', {NORMALIZATION_ENABLED =&gt; 'true'}An argument ignored (unknown or overridden): NORMALIZATION_ENABLED0 row(s) in 4.2670 seconds=&gt; Hbase::Table - test-table-4hbase(main):021:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4 COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0430 secondsHowever, on doing an alter command on that table we can set the NORMALIZATION_ENABLED attribute for that tablehbase(main):022:0&gt; alter 'test-table-4', {NORMALIZATION_ENABLED =&gt; 'true'}Unknown argument ignored: NORMALIZATION_ENABLEDUpdating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.3640 secondshbase(main):023:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4, {TABLE_ATTRIBUTES =&gt; {NORMALIZATION_ENABLED =&gt; 'true'} COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0190 secondsI think it would be better to have a single step process to enable normalization while creating the table itself, rather than a two step process to alter the table later on to enable normalization</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-13 01:00:00" id="14805" opendate="2015-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>status should show the master in shell</summary>
      <description>status 'simple' or 'detailed' only shows the regionservers and regions, but not the active master. Actually, there is no way to know about the active masters from the shell it seems.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-12-19 01:00:00" id="14849" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-6-6 01:00:00" id="1491" opendate="2009-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper errors: "Client has seen zxid 0xe our last zxid is 0xd"</summary>
      <description>We have been seeing a lot of these messages in tests:&amp;#91;junit&amp;#93; 2009-06-02 11:57:23,658 ERROR &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(514): Client has seen zxid 0xe our last zxid is 0xdThey usually repeat in a seemingly endless loop, such as: &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,083 INFO &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(737): Priming connection to java.nio.channels.SocketChannel&amp;#91;connected local=/0:0:0:0:0:0:0:1%0:56511 remote=localhost/0:0:0:0:0:0:0:1:21810&amp;#93; &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,084 INFO &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(889): Server connection successful &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,093 INFO &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(532): Connected to /0:0:0:0:0:0:0:1%0:56511 lastZxid 16 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 ERROR &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(543): Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 WARN &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(444): Exception causing close of session 0x0 due to java.io.IOException: Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 DEBUG &amp;#91;NIOServerCxn.Facto777ry:21810&amp;#93; server.NIOServerCnxn(447): IOException stack trace &amp;#91;junit&amp;#93; java.io.IOException: Client has seen zxid 0x10 our last zxid is 0x4 &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn.readConnectRequest(NIOServerCnxn.java:544) &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn.doIO(NIOServerCnxn.java:331) &amp;#91;junit&amp;#93; at org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:176) &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,094 INFO &amp;#91;NIOServerCxn.Factory:21810&amp;#93; server.NIOServerCnxn(777): closing session:0x0 NIOServerCnxn: java.nio.channels.SocketChannel&amp;#91;connected local=/0:0:0:0:0:0:0:1%0:21810 remote=/0:0:0:0:0:0:0:1%0:56511&amp;#93; &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,097 WARN &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(919): Exception closing session 0x121a2a7c43a0002 to sun.nio.ch.SelectionKeyImpl@2c662b4e &amp;#91;junit&amp;#93; java.io.IOException: Read error rc = -1 java.nio.DirectByteBuffer&amp;#91;pos=0 lim=4 cap=4&amp;#93; &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.doIO(ClientCnxn.java:653) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:897) &amp;#91;junit&amp;#93; 2009-06-02 13:27:54,097 WARN &amp;#91;main-SendThread&amp;#93; zookeeper.ClientCnxn$SendThread(953): Ignoring exception during shutdown input &amp;#91;junit&amp;#93; java.net.SocketException: Socket is not connected &amp;#91;junit&amp;#93; at sun.nio.ch.SocketChannelImpl.shutdown(Native Method) &amp;#91;junit&amp;#93; at sun.nio.ch.SocketChannelImpl.shutdownInput(SocketChannelImpl.java:640) &amp;#91;junit&amp;#93; at sun.nio.ch.SocketAdaptor.shutdownInput(SocketAdaptor.java:360) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.cleanup(ClientCnxn.java:951) &amp;#91;junit&amp;#93; at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:922)</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-4 01:00:00" id="14928" opendate="2015-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-12-8 01:00:00" id="14952" opendate="2015-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-assembly source artifact has some incorrect modules</summary>
      <description>After generating a tarball we noticed: that hbase-external-blockcache was missing. that hbase-spark is missing that there are duplicate hbase-shaded-{client,server} modules</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-14 01:00:00" id="14974" opendate="2015-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Total number of Regions in Transition number on UI incorrect</summary>
      <description>Total number of Regions in Transition shows 100 when there are 100 or more regions in transition.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-14 01:00:00" id="14975" opendate="2015-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t color the total RIT line yellow if it&amp;#39;s zero</summary>
      <description>Right now if there are regions in transition, sometimes the RIT over 60 seconds line is colored yellow. It shouldn't be colored yellow if there are no regions that have been in transition too long.</description>
      <version>None</version>
      <fixedVersion>1.3.0,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-15 01:00:00" id="14979" opendate="2015-12-15 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Update to the newest Zookeeper release</summary>
      <description>ZOOKEEPER-706 is nice to have for anyone running replication that sometimes gets stalled. We should update to the latest patch version.</description>
      <version>1.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-15 01:00:00" id="14984" opendate="2015-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow memcached block cache to set optimze to false</summary>
      <description>In order to keep latency consistent it might not be good to allow the spy memcached client to optimize.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-18 01:00:00" id="15005" opendate="2015-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use value array in computing block length for 1.2 and 1.3</summary>
      <description>Follow on to HBASE-14978</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiRespectsLimits.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-18 01:00:00" id="15007" opendate="2015-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update Hadoop support matrix to list 2.6.1+ as supported</summary>
      <description>The Hadoop community responded very well to our request for more maintenance releases and have now put out 2.6.1 - 2.6.3. The first of those included the fix for our catastrophic failure under HDFS encryption.We should update the book to point out those versions are fine.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-18 01:00:00" id="15015" opendate="2015-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checktyle plugin shouldn&amp;#39;t check Jamon-generated Java classes</summary>
      <description/>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-21 01:00:00" id="15021" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105 � NullPointer Run 2: TestRSStatusServlet.testBasic:105 � NullPointer Run 3: TestRSStatusServlet.testBasic:105 � NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 2: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 3: TestRSStatusServlet.testWithRegions:119 � NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-23 01:00:00" id="15036" opendate="2015-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase Spark documentation to include bulk load with thin records</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-14 01:00:00" id="15104" opendate="2016-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Occasional failures due to NotServingRegionException in IT tests</summary>
      <description>IntegrationTestAcidGuarantees fails when trying to cleanup with NotServerRegionExceptions giving up (after 36 attempts) .5/11/09 09:19:24 INFO client.AsyncProcess: #33, waiting for some tasks to finish. Expected max=0, tasksInProgress=915/11/09 09:19:33 INFO client.AsyncProcess: #45, table=TestAcidGuarantees, attempt=10/35 failed=1ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region TestAcidGuarantees,test_row_1,1447089367019.032439ef4f3353cb894d20337ba043bc. is not online on node-4.internal,22101,1447089152259 at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName(HRegionServer.java:2786) at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion(RSRpcServices.java:922) at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi(RSRpcServices.java:1893) at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod(ClientProtos.java:32213) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2035) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:107) at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:130) at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:107) at java.lang.Thread.run(Thread.java:745)...Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:Mon Nov 09 09:19:53 PST 2015, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68104: row 'test_row_1'Looked at the RS log, the following exception is found:2015-11-10 10:07:49,091 ERROR org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed open of region=TestAcidGuarantees,,1447177733243.f1be6b850fe3958c5c9b5e330b5dfb00., starting to roll back the global memstore size.org.apache.hadoop.hbase.DoNotRetryIOException: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.hadoop.compression.lzo.LzoCodec at org.apache.hadoop.hbase.util.CompressionTest.testCompression(CompressionTest.java:102) at org.apache.hadoop.hbase.regionserver.HRegion.checkCompressionCodecs(HRegion.java:6011) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5995) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5967) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5938) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5894) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:5845) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:356) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:126) at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:128) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745)</description>
      <version>1.2.0</version>
      <fixedVersion>0.94.28,1.2.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.ChangeCompressionAction.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-1-21 01:00:00" id="15147" opendate="2016-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should use Admin.listTableNames() instead of Admin.listTables()</summary>
      <description>It seems that getTableDescriptors() in master checks for A and C permissions while getTableNames() checks for any privilege on the table. The reasoning is explained here: https://issues.apache.org/jira/browse/HBASE-12564?focusedCommentId=14234504&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14234504 We should change the shell command for list to use the getTableNames() version because of this. Otherwise a user having only R or W cannot list the table name. This has been reported from a user here: https://community.hortonworks.com/questions/10742/why-does-a-user-need-create-permission-for-list-co.html#comment-11000. While we are at it, should we revisit the fact that you cannot get a table descriptor if you have only R or W? It seems strange that you cannot even know the CF names of a table that you can read from. I could not find info about the "describe" privileges on SQL databases. However, if there are use cases where Table descriptor might contain sensitive info, the current semantics seems fine. cc apurtell and mbertozzi.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-21 01:00:00" id="15151" opendate="2016-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rely on nightly tests for findbugs compliance on existing branch</summary>
      <description>the "-1" for extant findbugs warnings has confused interpretation of our precommit checks enough that we should switch to non-strict mode.It will still record the number of findbugs warnings present before the patch, but it'll vote "0" rather than calling attention to things via a -1.</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.3,2.0.0,1.2.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="15200" opendate="2016-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znode ACL checks should only compare the shortname</summary>
      <description>After HBASE-13768 we check at startup in secure configurations if our znodes have the correct ACLs. However when checking the ACL we compare the Kerberos fullname, which includes the host component. We should only compare the shortname, the principal. Otherwise in a multimaster configuration we will unnecessarily reset ACLs whenever any master running on a host other than the one that initialized the ACLs makes the check. You can imagine this happening multiple times in a rolling restart scenario.</description>
      <version>1.2.0,1.0.3,1.1.3,0.98.17,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="15201" opendate="2016-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase-spark to hbase assembly</summary>
      <description>hbase-spark currently is missing from hbase assembly.We should add it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-2-11 01:00:00" id="15255" opendate="2016-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pointer to linkedin blog on putting jvm logs on fast disk</summary>
      <description>Add pointer to linked in blog: https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-trafficIIRC, tsdb says to do similar.Also add into perf section note on native crc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-2-20 01:00:00" id="15298" opendate="2016-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing or wrong asciidoc anchors in the reference guide</summary>
      <description>There are some missing or wrong asciidoc anchors in the reference guide.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ycsb.adoc</file>
      <file type="M">src.main.asciidoc..chapters.unit.testing.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.preface.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.compression.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.contributing.to.documentation.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="15302" opendate="2016-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reenable the other tests disabled by HBASE-14678</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-6-17 01:00:00" id="1532" opendate="2009-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UI Visibility into ZooKeeper</summary>
      <description>Add ZooKeeper information/administration to UI.Discussion showed particular interest in a tree-viewer application, something like ZOOKEEPER-418.There was talk between Lars/JimK about how often the viewer should update its data.See HBASE-1329 for more information.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-26 01:00:00" id="15354" opendate="2016-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use same criteria for clearing meta cache for all operations</summary>
      <description>Currently we do not clear/update meta cache for some special exceptions if the operation went through AsyncProcess#submit like HTable#put calls. But, we clear meta cache without checking for these special exceptions in case of other operations like gets, deletes etc because they directly go through the RpcRetryingCaller#callWithRetries instead of the AsyncProcess.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,1.2.1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMetaCache.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionAdminServiceCallable.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-4-7 01:00:00" id="15407" opendate="2016-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SASL support for fan out OutputStream</summary>
      <description>Otherwise we can not use it in secure environment.Should be a netty handler, but seehttps://github.com/netty/netty/issues/1966I do not think it will be available in the near future, so we need to do it by ourselves.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFanOutOneBlockAsyncDFSOutput.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FanOutOneBlockAsyncDFSOutputHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FanOutOneBlockAsyncDFSOutput.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-8-14 01:00:00" id="15461" opendate="2016-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ref guide has bad links to blogs originally posted on cloudera website</summary>
      <description>The ref guide section on "Secure Client Access to Apache HBase" starts with a link to a blog post from Matteo, but the link is broken.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.other.info.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-4-15 01:00:00" id="15466" opendate="2016-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>precommit should not run all java goals when given a docs-only patch</summary>
      <description>Right now docs-only patches (those that only impact the top level src/main/site, src/main/asciidoc, or src/main/xslt) run through all of the java related precommit checks, including test4tests and the full unit test suite.Since we know these paths don't require those checks, we should update our personality to skip them. (or fix our project structure to match "the maven way".)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,1.4.4,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-16 01:00:00" id="15470" opendate="2016-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a setting for Priority queue length</summary>
      <description>Meta can have very different request rates than any other region. We shouldn't use the same call queue length.For example:If a normal request takes 100ms ( long scan or large get )but a call to meta is all in memory so it takes &lt; 1msSo a call queue length of 100 represents multiple seconds of work for normal requests, but less than a second for meta.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-16 01:00:00" id="15471" opendate="2016-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add num calls in priority and general queue to RS UI</summary>
      <description>1.2 added the queue size. We should add the number of calls in the queue.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-17 01:00:00" id="15473" opendate="2016-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation for the usage of hbase dataframe user api (JSON, Avro, etc)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-29 01:00:00" id="15557" opendate="2016-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add guidance on HashTable/SyncTable to the RefGuide</summary>
      <description>The docs for SyncTable are insufficient. Brief description from davelatham HBASE-13639 comment:Sorry for the lack of better documentation, Abhishek Soni. Thanks for bringing it up. I'll try to provide a better explanation. You may have already seen it, but if not, the design doc linked in the description above may also give you some better clues as to how it should be used.Briefly, the feature is intended to start with a pair of tables in remote clusters that are already substantially similar and make them identical by comparing hashes of the data and copying only the diffs instead of having to copy the entire table. So it is targeted at a very specific use case (with some work it could generalize to cover things like CopyTable and VerifyRepliaction but it's not there yet). To use it, you choose one table to be the "source", and the other table is the "target". After the process is complete the target table should end up being identical to the source table.In the source table's cluster, run org.apache.hadoop.hbase.mapreduce.HashTable and pass it the name of the source table and an output directory in HDFS. HashTable will scan the source table, break the data up into row key ranges (default of 8kB per range) and produce a hash of the data for each range.Make the hashes available to the target cluster - I'd recommend using DistCp to copy it across.In the target table's cluster, run org.apache.hadoop.hbase.mapreduce.SyncTable and pass it the directory where you put the hashes, and the names of the source and destination tables. You will likely also need to specify the source table's ZK quorum via the --sourcezkcluster option. SyncTable will then read the hash information, and compute the hashes of the same row ranges for the target table. For any row range where the hash fails to match, it will open a remote scanner to the source table, read the data for that range, and do Puts and Deletes to the target table to update it to match the source.I hope that clarifies it a bit. Let me know if you need a hand. If anyone wants to work on getting some documentation into the book, I can try to write some more but would love a hand on turning it into an actual book patch.</description>
      <version>1.2.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-9 01:00:00" id="15623" opendate="2016-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update refguide to change hadoop &lt;= 2.3.x from NT to X for hbase-1.2.x</summary>
      <description>This issue is about updating our hadoop supported versions grid in the prerequisites section of refguide. Here is thread proposing this change up on dev list: http://osdir.com/ml/general/2016-04/msg09194.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="15666" opendate="2016-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded dependencies for hbase-testing-util</summary>
      <description>Folks that make use of our shaded client but then want to test things using the hbase-testing-util end up getting all of our dependencies again in the test scope.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">NOTICE.txt</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-with-hadoop-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-it.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-18 01:00:00" id="15667" opendate="2016-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more clarity to Reference Guide related to importing Eclipse Formatter</summary>
      <description>In Hbase Reference Guide: 141.1.1. Code Formattingin procedure bullet point 2: It is not clear what the menu item is. It should be changed to the following:"In Preferences, click Java-&gt;Code Style-&gt;Formatter"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-23 01:00:00" id="15698" opendate="2016-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increment TimeRange not serialized to server</summary>
      <description>Before HBase-1.2, the Increment TimeRange set on the client was serialized over to the server. As of HBase 1.2, this appears to no longer be true, as my preIncrement coprocessor always gets HConstants.LATEST_TIMESTAMP as the value of increment.getTimeRange().getMax() regardless of what the client has specified.</description>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-7-31 01:00:00" id="15925" opendate="2016-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>compat-module maven variable not evaluated</summary>
      <description>Looks like we've regressed on HBASE-8488. Have a look at the dependency artifacts list on http://mvnrepository.com/artifact/org.apache.hbase/hbase-testing-util/1.2.1. Notice the direct dependency's artifactId is ${compat.module}.</description>
      <version>1.0.0,1.1.0,1.2.0,1.2.1,1.0.3,1.1.5</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-7 01:00:00" id="15985" opendate="2016-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clarify promises about edits from replication in ref guide</summary>
      <description>we should make clear in a call out that replication only provides at-least-once delivery and doesn't guarantee ordering so that e.g. folks using increments aren't surprised.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-7-6 01:00:00" id="16183" opendate="2016-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct errors in example programs of coprocessor in Ref Guide</summary>
      <description>There are some errors in the example programs for coprocessor in Ref Guide. Such as using deprecated APIs, generic...</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-9-22 01:00:00" id="16678" opendate="2016-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapReduce jobs do not update counters from ScanMetrics</summary>
      <description>Was inspecting a perf issue, where we needed the scanner metrics as counters for a MR job. Turns out that the HBase scan counters are no longer working in 1.0+. I think it got broken via HBASE-13030. These are the counters: HBase Counters BYTES_IN_REMOTE_RESULTS=0 BYTES_IN_RESULTS=280 MILLIS_BETWEEN_NEXTS=11 NOT_SERVING_REGION_EXCEPTION=0 NUM_SCANNER_RESTARTS=0 NUM_SCAN_RESULTS_STALE=0 REGIONS_SCANNED=1 REMOTE_RPC_CALLS=0 REMOTE_RPC_RETRIES=0 RPC_CALLS=3 RPC_RETRIES=0</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-22 01:00:00" id="16682" opendate="2016-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Shell tests failure. NoClassDefFoundError for MiniKdc</summary>
      <description>Stacktracejava.lang.NoClassDefFoundError: org/apache/hadoop/minikdc/MiniKdc at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.minikdc.MiniKdc at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.getDeclaredMethods0(Native Method) at java.lang.Class.privateGetDeclaredMethods(Class.java:2701) at java.lang.Class.getDeclaredMethods(Class.java:1975) at org.jruby.javasupport.JavaClass.getMethods(JavaClass.java:2110) at org.jruby.javasupport.JavaClass.setupClassMethods(JavaClass.java:955) at org.jruby.javasupport.JavaClass.access$700(JavaClass.java:99) at org.jruby.javasupport.JavaClass$ClassInitializer.initialize(JavaClass.java:650) at org.jruby.javasupport.JavaClass.setupProxy(JavaClass.java:689) at org.jruby.javasupport.Java.createProxyClass(Java.java:526) at org.jruby.javasupport.Java.getProxyClass(Java.java:455) at org.jruby.javasupport.Java.getInstance(Java.java:364) at org.jruby.javasupport.JavaUtil.convertJavaToUsableRubyObject(JavaUtil.java:166) at org.jruby.javasupport.JavaEmbedUtils.javaToRuby(JavaEmbedUtils.java:291) at org.jruby.embed.variable.AbstractVariable.updateByJavaObject(AbstractVariable.java:81) at org.jruby.embed.variable.GlobalVariable.&lt;init&gt;(GlobalVariable.java:69) at org.jruby.embed.variable.GlobalVariable.getInstance(GlobalVariable.java:60) at org.jruby.embed.variable.VariableInterceptor.getVariableInstance(VariableInterceptor.java:97) at org.jruby.embed.internal.BiVariableMap.put(BiVariableMap.java:321) at org.jruby.embed.ScriptingContainer.put(ScriptingContainer.java:1123) at org.apache.hadoop.hbase.client.AbstractTestShell.setUpBeforeClass(AbstractTestShell.java:61) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runners.Suite.runChild(Suite.java:128) at org.junit.runners.Suite.runChild(Suite.java:27) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at org.junit.runner.JUnitCore.run(JUnitCore.java:115) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.createRequestAndRun(JUnitCoreWrapper.java:108) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.executeEager(JUnitCoreWrapper.java:78) at org.apache.maven.surefire.junitcore.JUnitCoreWrapper.execute(JUnitCoreWrapper.java:54) at org.apache.maven.surefire.junitcore.JUnitCoreProvider.invoke(JUnitCoreProvider.java:144) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:203) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:155) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-21 01:00:00" id="17502" opendate="2017-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document hadoop pre-2.6.1 and Java 1.8 Kerberos problem in our hadoop support matrix</summary>
      <description>Hadoop pre-2.6.1 on JDK 1.8 has problem with Kerberos keytabe relogin.HADOOP-10786 fixed the problem in Hadoop 2.6.1.Let's document it in the Hadoop support matrix.This was brought up in HBase 1.3.0 RC0 voting.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-23 01:00:00" id="17516" opendate="2017-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table quota not taking precedence over namespace quota</summary>
      <description>Romil Choksi found a bug in the current patch-set where a more restrictive table quota did not take priority over a less-restrictive namespace quota.Turns out some of the logic to handle this case was incorrect.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="17518" opendate="2017-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Reference Guide has a syntax error</summary>
      <description>The image of "HFile Version 2 Structure" in Appendix F of HBase Reference Guide (pdf) is missing because of a wrong asciidoc syntax:image:hfilev2.png&amp;#91;HFile Version 2&amp;#93;modified as:image::hfilev2.png&amp;#91;HFile Version 2&amp;#93;it should be a double colon instead of single one</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-24 01:00:00" id="17520" opendate="2017-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement isTableEnabled/Disabled/Available methods</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-8-17 01:00:00" id="1771" opendate="2009-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE sequentialWrite is 7x slower because of MemStoreFlusher#checkStoreFileCount</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-17 01:00:00" id="1772" opendate="2009-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up the default ZK session timeout from 30seconds to 60seconds</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-15 01:00:00" id="18049" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>It is not necessary to re-open the region when MOB files cannot be found</summary>
      <description>In HBASE-17712, we try to re-open the region when store files cannot be found. This is useful for store files in a region, but is not necessary when the MOB files cannot be found, because the store files in a region only contain the references to the MOB files and a re-open of a region doesn't help the lost MOB files.In this JIRA, we will directly throw DNRIOE only when the MOB files are not found in MobStoreScanner and ReversedMobStoreScanner. Other logics keep the same.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-23 01:00:00" id="18263" opendate="2017-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve NPE in backup Master UI when access to procedures.jsp</summary>
      <description>When accessing procedures.jsp ,the NPE comes in backup master UI:HTTP ERROR 500Problem accessing /procedures.jsp. Reason: INTERNAL_SERVER_ERRORCaused by:java.lang.NullPointerException at org.apache.hadoop.hbase.generated.master.procedures_jsp._jspService(procedures_jsp.java:67) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98) at javax.servlet.http.HttpServlet.service(HttpServlet.java:820) at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221) at org.apache.hadoop.hbase.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:113) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.ClickjackingPreventionFilter.doFilter(ClickjackingPreventionFilter.java:48) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1354) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.apache.hadoop.hbase.http.NoCacheFilter.doFilter(NoCacheFilter.java:49) at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212) at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)In server ,only the active master initialize procedureStore in HMaster. so ,i think it will be better to remove procedures.jsp link in backup Master UI</description>
      <version>1.2.0,2.0.0-alpha-1</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-6 01:00:00" id="18327" opendate="2017-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>redo test-patch personality &amp;#39;hadoopcheck&amp;#39; to better account for feature branches</summary>
      <description>right now our 'which hadoop checks do we need' check looks like this: if [[ "${PATCH_BRANCH}" = "master" ]]; then hbase_hadoop2_versions=${HBASE_MASTER_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_MASTER_HADOOP3_VERSIONS} elif [[ ${PATCH_BRANCH} = branch-2* ]]; then hbase_hadoop2_versions=${HBASE_BRANCH2_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_BRANCH2_HADOOP3_VERSIONS} else hbase_hadoop2_versions=${HBASE_HADOOP2_VERSIONS} hbase_hadoop3_versions=${HBASE_HADOOP3_VERSIONS} fithe check is basically "if master do this, if like branch-2 do that, otherwise behave like branch-1".we often have feature branches that thus end up being treated like branch-1, even though those branches should all be based off of master. (since we follow a master-first development approach.)we should redo this check so it's "if branch-1 do this, if branch-2 do that, otherwise behave like master"</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-11 01:00:00" id="18577" opendate="2017-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded client includes several non-relocated third party dependencies</summary>
      <description>we have some unexpected unrelocated third party dependencies in our shaded artifacts.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-5 01:00:00" id="18759" opendate="2017-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-shaded-check-invariants failure</summary>
      <description>Here's the error.[ERROR] Found artifact with unexpected contents: '/Users/appy/apache/hbase/hbase-shaded/hbase-shaded-server/target/hbase-shaded-server-3.0.0-SNAPSHOT.jar' Please check the following and either correct the build or update the allowed list with reasoning. about.html org/eclipse/ org/eclipse/jetty/ org/eclipse/jetty/http/ org/eclipse/jetty/http/encoding.properties org/eclipse/jetty/http/GZIPContentDecoder$State.class org/eclipse/jetty/http/HttpFields$Itr.class org/eclipse/jetty/http/HttpParser$HttpHandler.class org/eclipse/jetty/http/HttpScheme.class org/eclipse/jetty/http/HttpURI.class org/eclipse/jetty/http/pathmap/ org/eclipse/jetty/http/pathmap/PathMappings$1.class org/eclipse/jetty/http/pathmap/PathSpecGroup.class org/eclipse/jetty/http/pathmap/PathSpecSet$1.class org/eclipse/jetty/http/pathmap/RegexPathSpec.class org/eclipse/jetty/http/PathMap$MappedEntry.class org/eclipse/jetty/http/QuotedCSV$State.class org/eclipse/jetty/http/QuotedQualityCSV$1.class org/eclipse/jetty/http/DateParser.class org/eclipse/jetty/http/Http1FieldPreEncoder.class org/eclipse/jetty/http/HttpContent$ContentFactory.class org/eclipse/jetty/http/HttpFields$1.class org/eclipse/jetty/http/HttpFields.class org/eclipse/jetty/http/HttpStatus$Code.class org/eclipse/jetty/http/pathmap/UriTemplatePathSpec.class org/eclipse/jetty/http/ResourceHttpContent.class org/eclipse/jetty/http/BadMessageException.class org/eclipse/jetty/http/GZIPContentDecoder$1.class org/eclipse/jetty/http/HttpGenerator$Result.class org/eclipse/jetty/http/HttpParser$State.class org/eclipse/jetty/http/HttpURI$State.class org/eclipse/jetty/http/QuotedCSV.class org/eclipse/jetty/http/CompressedContentFormat.class org/eclipse/jetty/http/HttpField$LongValueHttpField.class org/eclipse/jetty/http/HttpGenerator$2.class org/eclipse/jetty/http/HttpMethod.class org/eclipse/jetty/http/HttpParser$ComplianceHandler.class org/eclipse/jetty/http/HttpParser$RequestHandler.class org/eclipse/jetty/http/HttpStatus.class org/eclipse/jetty/http/MetaData$Response.class org/eclipse/jetty/http/MimeTypes$Type.class org/eclipse/jetty/http/pathmap/PathMappings.class org/eclipse/jetty/http/PathMap$PathSet.class org/eclipse/jetty/http/QuotedQualityCSV.class org/eclipse/jetty/http/CookieCompliance.class org/eclipse/jetty/http/GZIPContentDecoder.class org/eclipse/jetty/http/HttpFields$2.class org/eclipse/jetty/http/HttpGenerator$State.class org/eclipse/jetty/http/HttpHeader.class org/eclipse/jetty/http/HttpParser$IllegalCharacterException.class org/eclipse/jetty/http/HttpTokens$EndOfContent.class org/eclipse/jetty/http/HttpURI$1.class org/eclipse/jetty/http/mime.properties org/eclipse/jetty/http/PreEncodedHttpField.class org/eclipse/jetty/http/DateGenerator$1.class org/eclipse/jetty/http/HostPortHttpField.class org/eclipse/jetty/http/HttpCompliance.class org/eclipse/jetty/http/HttpFieldPreEncoder.class org/eclipse/jetty/http/HttpGenerator$PreparedResponse.class org/eclipse/jetty/http/HttpParser$1.class org/eclipse/jetty/http/HttpParser$ResponseHandler.class org/eclipse/jetty/http/HttpTokens.class org/eclipse/jetty/http/HttpVersion.class org/eclipse/jetty/http/MetaData.class org/eclipse/jetty/http/pathmap/MappedResource.class org/eclipse/jetty/http/pathmap/PathSpec.class org/eclipse/jetty/http/pathmap/PathSpecSet.class org/eclipse/jetty/http/PreEncodedHttpField$1.class org/eclipse/jetty/http/QuotedCSV$1.class org/eclipse/jetty/http/Syntax.class org/eclipse/jetty/http/DateGenerator.class org/eclipse/jetty/http/HttpContent.class org/eclipse/jetty/http/HttpField$IntValueHttpField.class org/eclipse/jetty/http/HttpField.class org/eclipse/jetty/http/HttpGenerator.class org/eclipse/jetty/http/MetaData$Request.class org/eclipse/jetty/http/MimeTypes.class org/eclipse/jetty/http/pathmap/ServletPathSpec$1.class org/eclipse/jetty/http/PathMap.class org/eclipse/jetty/http/DateParser$1.class org/eclipse/jetty/http/HttpCookie.class org/eclipse/jetty/http/HttpGenerator$1.class org/eclipse/jetty/http/HttpHeaderValue.class org/eclipse/jetty/http/HttpParser$CharState.class org/eclipse/jetty/http/HttpParser$FieldState.class org/eclipse/jetty/http/HttpParser.class org/eclipse/jetty/http/pathmap/ServletPathSpec.class org/eclipse/jetty/http/PrecompressedHttpContent.class org/eclipse/jetty/http/useragents org/eclipse/jetty/io/ org/eclipse/jetty/io/AbstractEndPoint.class org/eclipse/jetty/io/ByteBufferPool$Lease.class org/eclipse/jetty/io/ChannelEndPoint$2.class org/eclipse/jetty/io/ChannelEndPoint$4.class org/eclipse/jetty/io/ChannelEndPoint$RunnableTask.class org/eclipse/jetty/io/Connection$UpgradeTo.class org/eclipse/jetty/io/ManagedSelector$EndPointCloser.class org/eclipse/jetty/io/ManagedSelector$Selectable.class org/eclipse/jetty/io/ssl/ org/eclipse/jetty/io/ssl/SslConnection$1.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint.class org/eclipse/jetty/io/WriterOutputStream.class org/eclipse/jetty/io/AbstractConnection$1.class org/eclipse/jetty/io/ArrayByteBufferPool.class org/eclipse/jetty/io/ByteArrayEndPoint.class org/eclipse/jetty/io/Connection$UpgradeFrom.class org/eclipse/jetty/io/EofException.class org/eclipse/jetty/io/ManagedSelector$CreateEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Server.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$WriteCallBack$1.class org/eclipse/jetty/io/AbstractConnection$ReadCallback.class org/eclipse/jetty/io/ByteArrayEndPoint$1.class org/eclipse/jetty/io/Connection$Listener$Adapter.class org/eclipse/jetty/io/FillInterest.class org/eclipse/jetty/io/ManagedSelector$Connect.class org/eclipse/jetty/io/ManagedSelector$DumpKeys.class org/eclipse/jetty/io/ssl/ALPNProcessor.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$WriteCallBack.class org/eclipse/jetty/io/ssl/SslConnection.class org/eclipse/jetty/io/WriteFlusher$FailedState.class org/eclipse/jetty/io/WriteFlusher$PendingState.class org/eclipse/jetty/io/WriteFlusher$WritingState.class org/eclipse/jetty/io/AbstractConnection.class org/eclipse/jetty/io/Connection.class org/eclipse/jetty/io/LeakTrackingByteBufferPool$1.class org/eclipse/jetty/io/ManagedSelector$1.class org/eclipse/jetty/io/ManagedSelector$Acceptor.class org/eclipse/jetty/io/ManagedSelector$SelectorProducer.class org/eclipse/jetty/io/NegotiatingClientConnection.class org/eclipse/jetty/io/NetworkTrafficListener$Adapter.class org/eclipse/jetty/io/QuietException.class org/eclipse/jetty/io/SelectChannelEndPoint.class org/eclipse/jetty/io/SocketChannelEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Server$1.class org/eclipse/jetty/io/AbstractEndPoint$1.class org/eclipse/jetty/io/ByteBufferPool$Bucket.class org/eclipse/jetty/io/ByteBufferPool.class org/eclipse/jetty/io/ChannelEndPoint$1.class org/eclipse/jetty/io/ChannelEndPoint$3.class org/eclipse/jetty/io/ChannelEndPoint$RunnableCloseable.class org/eclipse/jetty/io/ChannelEndPoint.class org/eclipse/jetty/io/LeakTrackingByteBufferPool.class org/eclipse/jetty/io/ManagedSelector$CloseEndPoints.class org/eclipse/jetty/io/ManagedSelector$ConnectTimeout.class org/eclipse/jetty/io/ManagedSelector.class org/eclipse/jetty/io/NetworkTrafficSelectChannelEndPoint.class org/eclipse/jetty/io/ssl/ALPNProcessor$Client$1.class org/eclipse/jetty/io/ssl/SslConnection$2.class org/eclipse/jetty/io/ssl/SslHandshakeListener$Event.class org/eclipse/jetty/io/ssl/SslHandshakeListener.class org/eclipse/jetty/io/WriteFlusher$1.class org/eclipse/jetty/io/WriteFlusher$CompletingState.class org/eclipse/jetty/io/WriteFlusher$IdleState.class org/eclipse/jetty/io/WriteFlusher$State.class org/eclipse/jetty/io/WriteFlusher$StateType.class org/eclipse/jetty/io/WriteFlusher.class org/eclipse/jetty/io/AbstractEndPoint$2.class org/eclipse/jetty/io/ClientConnectionFactory.class org/eclipse/jetty/io/Connection$Listener.class org/eclipse/jetty/io/ConnectionStatistics.class org/eclipse/jetty/io/IdleTimeout.class org/eclipse/jetty/io/ManagedSelector$Accept.class org/eclipse/jetty/io/ManagedSelector$NonBlockingAction.class org/eclipse/jetty/io/ssl/SslClientConnectionFactory.class org/eclipse/jetty/io/ssl/SslConnection$4.class org/eclipse/jetty/io/AbstractEndPoint$3.class org/eclipse/jetty/io/EndPoint.class org/eclipse/jetty/io/ManagedSelector$CloseSelector.class org/eclipse/jetty/io/ssl/ALPNProcessor$Client.class org/eclipse/jetty/io/ssl/SslConnection$3.class org/eclipse/jetty/io/ssl/SslConnection$RunnableTask.class org/eclipse/jetty/io/AbstractEndPoint$State.class org/eclipse/jetty/io/IdleTimeout$1.class org/eclipse/jetty/io/MappedByteBufferPool$Tagged.class org/eclipse/jetty/io/MappedByteBufferPool.class org/eclipse/jetty/io/NegotiatingClientConnectionFactory.class org/eclipse/jetty/io/NetworkTrafficListener.class org/eclipse/jetty/io/RuntimeIOException.class org/eclipse/jetty/io/SelectorManager.class org/eclipse/jetty/io/ssl/SslConnection$DecryptedEndPoint$FailWrite.class org/eclipse/jetty/security/ org/eclipse/jetty/security/authentication/ org/eclipse/jetty/security/authentication/DeferredAuthentication$2.class org/eclipse/jetty/security/authentication/DigestAuthenticator$Nonce.class org/eclipse/jetty/security/authentication/LoginAuthenticator.class org/eclipse/jetty/security/Authenticator.class org/eclipse/jetty/security/RunAsToken.class org/eclipse/jetty/security/SecurityHandler.class org/eclipse/jetty/security/AbstractLoginService$RolePrincipal.class org/eclipse/jetty/security/authentication/DeferredAuthentication.class org/eclipse/jetty/security/authentication/DigestAuthenticator.class org/eclipse/jetty/security/authentication/SpnegoAuthenticator.class org/eclipse/jetty/security/DefaultUserIdentity.class org/eclipse/jetty/security/JDBCLoginService$JDBCUserPrincipal.class org/eclipse/jetty/security/RoleRunAsToken.class org/eclipse/jetty/security/SecurityHandler$NotChecked.class org/eclipse/jetty/security/ServerAuthException.class org/eclipse/jetty/security/AbstractLoginService.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormResponse.class org/eclipse/jetty/security/authentication/SessionAuthentication.class org/eclipse/jetty/security/DefaultIdentityService.class org/eclipse/jetty/security/IdentityService.class org/eclipse/jetty/security/RoleInfo.class org/eclipse/jetty/security/UserStore.class org/eclipse/jetty/security/AbstractLoginService$UserPrincipal.class org/eclipse/jetty/security/authentication/DigestAuthenticator$Digest.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormAuthentication.class org/eclipse/jetty/security/authentication/LoginCallback.class org/eclipse/jetty/security/Authenticator$Factory.class org/eclipse/jetty/security/DefaultAuthenticatorFactory.class org/eclipse/jetty/security/LoginService.class org/eclipse/jetty/security/SecurityHandler$1.class org/eclipse/jetty/security/SpnegoUserIdentity.class org/eclipse/jetty/security/AbstractUserAuthentication.class org/eclipse/jetty/security/authentication/FormAuthenticator.class org/eclipse/jetty/security/HashLoginService.class org/eclipse/jetty/security/PropertyUserStore$UserListener.class org/eclipse/jetty/security/PropertyUserStore.class org/eclipse/jetty/security/UserAuthentication.class org/eclipse/jetty/security/authentication/BasicAuthenticator.class org/eclipse/jetty/security/authentication/FormAuthenticator$FormRequest.class org/eclipse/jetty/security/authentication/LoginCallbackImpl.class org/eclipse/jetty/security/ConstraintMapping.class org/eclipse/jetty/security/JDBCLoginService.class org/eclipse/jetty/security/SpnegoLoginService.class org/eclipse/jetty/security/UserDataConstraint.class org/eclipse/jetty/security/authentication/ClientCertAuthenticator.class org/eclipse/jetty/security/Authenticator$AuthConfiguration.class org/eclipse/jetty/security/ConstraintSecurityHandler.class org/eclipse/jetty/security/SecurityHandler$3.class org/eclipse/jetty/security/authentication/DeferredAuthentication$1.class org/eclipse/jetty/security/ConstraintAware.class org/eclipse/jetty/security/SecurityHandler$2.class org/eclipse/jetty/security/SpnegoUserPrincipal.class org/glassfish/ org/glassfish/jsp/ org/glassfish/jsp/api/ org/glassfish/jsp/api/ByteWriter.class org/glassfish/jsp/api/JspProbeEmitter.class org/glassfish/jsp/api/ResourceInjector.class com/ com/sun/ com/sun/el/ com/sun/el/ExpressionFactoryImpl.class com/sun/el/lang/ com/sun/el/lang/ELArithmetic$BigDecimalDelegate.class com/sun/el/lang/ELArithmetic$BigIntegerDelegate.class com/sun/el/lang/ELArithmetic$DoubleDelegate.class com/sun/el/lang/ELArithmetic$LongDelegate.class com/sun/el/lang/ELArithmetic.class com/sun/el/lang/ELSupport.class com/sun/el/lang/EvaluationContext.class com/sun/el/lang/ExpressionBuilder$1.class com/sun/el/lang/ExpressionBuilder$NodeSoftReference.class com/sun/el/lang/ExpressionBuilder$SoftConcurrentHashMap.class com/sun/el/lang/ExpressionBuilder.class com/sun/el/lang/FunctionMapperFactory.class com/sun/el/lang/FunctionMapperImpl$Function.class com/sun/el/lang/FunctionMapperImpl.class com/sun/el/lang/VariableMapperFactory.class com/sun/el/lang/VariableMapperImpl.class com/sun/el/Messages.properties com/sun/el/MethodExpressionImpl.class com/sun/el/MethodExpressionLiteral.class com/sun/el/parser/ com/sun/el/parser/ArithmeticNode.class com/sun/el/parser/AstAnd.class com/sun/el/parser/AstAssign.class com/sun/el/parser/AstBracketSuffix.class com/sun/el/parser/AstChoice.class com/sun/el/parser/AstCompositeExpression.class com/sun/el/parser/AstConcat.class com/sun/el/parser/AstDeferredExpression.class com/sun/el/parser/AstDiv.class com/sun/el/parser/AstDotSuffix.class com/sun/el/parser/AstDynamicExpression.class com/sun/el/parser/AstEmpty.class com/sun/el/parser/AstEqual.class com/sun/el/parser/AstFalse.class com/sun/el/parser/AstFloatingPoint.class com/sun/el/parser/AstFunction.class com/sun/el/parser/AstGreaterThan.class com/sun/el/parser/AstGreaterThanEqual.class com/sun/el/parser/AstIdentifier.class com/sun/el/parser/AstInteger.class com/sun/el/parser/AstLambdaExpression.class com/sun/el/parser/AstLambdaParameters.class com/sun/el/parser/AstLessThan.class com/sun/el/parser/AstLessThanEqual.class com/sun/el/parser/AstListData.class com/sun/el/parser/AstLiteralExpression.class com/sun/el/parser/AstMapData.class com/sun/el/parser/AstMapEntry.class com/sun/el/parser/AstMethodArguments.class com/sun/el/parser/AstMinus.class com/sun/el/parser/AstMod.class com/sun/el/parser/AstMult.class com/sun/el/parser/AstNegative.class com/sun/el/parser/AstNot.class com/sun/el/parser/AstNotEqual.class com/sun/el/parser/AstNull.class com/sun/el/parser/AstOr.class com/sun/el/parser/AstPlus.class com/sun/el/parser/AstSemiColon.class com/sun/el/parser/AstString.class com/sun/el/parser/AstTrue.class com/sun/el/parser/AstValue$Target.class com/sun/el/parser/AstValue.class com/sun/el/parser/BooleanNode.class com/sun/el/parser/ELParser$1.class com/sun/el/parser/ELParser$JJCalls.class com/sun/el/parser/ELParser$LookaheadSuccess.class com/sun/el/parser/ELParser.class com/sun/el/parser/ELParserConstants.class com/sun/el/parser/ELParserTokenManager.class com/sun/el/parser/ELParserTreeConstants.class com/sun/el/parser/JJTELParserState.class com/sun/el/parser/Node.class com/sun/el/parser/NodeVisitor.class com/sun/el/parser/ParseException.class com/sun/el/parser/SimpleCharStream.class com/sun/el/parser/SimpleNode.class com/sun/el/parser/Token.class com/sun/el/parser/TokenMgrError.class com/sun/el/stream/ com/sun/el/stream/Operator.class com/sun/el/stream/Optional.class com/sun/el/stream/Stream$1$1.class com/sun/el/stream/Stream$1.class com/sun/el/stream/Stream$2$1.class com/sun/el/stream/Stream$2.class com/sun/el/stream/Stream$3$1.class com/sun/el/stream/Stream$3.class com/sun/el/stream/Stream$4$1.class com/sun/el/stream/Stream$4.class com/sun/el/stream/Stream$5.class com/sun/el/stream/Stream$6$1.class com/sun/el/stream/Stream$6.class com/sun/el/stream/Stream$7$1.class com/sun/el/stream/Stream$7$2.class com/sun/el/stream/Stream$7.class com/sun/el/stream/Stream$8$1.class com/sun/el/stream/Stream$8$2.class com/sun/el/stream/Stream$8.class com/sun/el/stream/Stream$9$1.class com/sun/el/stream/Stream$9.class com/sun/el/stream/Stream$Iterator0.class com/sun/el/stream/Stream$Iterator1.class com/sun/el/stream/Stream$Iterator2.class com/sun/el/stream/Stream.class com/sun/el/stream/StreamELResolver$1.class com/sun/el/stream/StreamELResolver.class com/sun/el/util/ com/sun/el/util/MessageFactory.class com/sun/el/util/ReflectionUtil$1.class com/sun/el/util/ReflectionUtil$ConstructorWrapper.class com/sun/el/util/ReflectionUtil$MethodWrapper.class com/sun/el/util/ReflectionUtil$Wrapper.class com/sun/el/util/ReflectionUtil.class com/sun/el/ValueExpressionImpl.class com/sun/el/ValueExpressionLiteral.class javax/ javax/el/ javax/el/ArrayELResolver.class javax/el/BeanELResolver$1.class javax/el/BeanELResolver$BeanProperties.class javax/el/BeanELResolver$BeanProperty.class javax/el/BeanELResolver$BPSoftReference.class javax/el/BeanELResolver$SoftConcurrentHashMap.class javax/el/BeanELResolver.class javax/el/BeanNameELResolver.class javax/el/BeanNameResolver.class javax/el/CompositeELResolver$CompositeIterator.class javax/el/CompositeELResolver.class javax/el/ELClass.class javax/el/ELContext.class javax/el/ELContextEvent.class javax/el/ELContextListener.class javax/el/ELException.class javax/el/ELManager.class javax/el/ELProcessor.class javax/el/ELResolver.class javax/el/ELUtil$1.class javax/el/ELUtil$ConstructorWrapper.class javax/el/ELUtil$MethodWrapper.class javax/el/ELUtil$Wrapper.class javax/el/ELUtil.class javax/el/EvaluationListener.class javax/el/Expression.class javax/el/ExpressionFactory.class javax/el/FactoryFinder.class javax/el/FunctionMapper.class javax/el/ImportHandler.class javax/el/LambdaExpression.class javax/el/ListELResolver.class javax/el/MapELResolver.class javax/el/MethodExpression.class javax/el/MethodInfo.class javax/el/MethodNotFoundException.class javax/el/PrivateMessages.properties javax/el/PropertyNotFoundException.class javax/el/PropertyNotWritableException.class javax/el/ResourceBundleELResolver.class javax/el/StandardELContext$1.class javax/el/StandardELContext$DefaultFunctionMapper.class javax/el/StandardELContext$DefaultVariableMapper.class javax/el/StandardELContext$LocalBeanNameResolver.class javax/el/StandardELContext.class javax/el/StaticFieldELResolver.class javax/el/TypeConverter.class javax/el/ValueExpression.class javax/el/ValueReference.class javax/el/VariableMapper.class javax/servlet/ javax/servlet/jsp/ javax/servlet/jsp/el/ javax/servlet/jsp/el/ELException.class javax/servlet/jsp/el/ELParseException.class javax/servlet/jsp/el/Expression.class javax/servlet/jsp/el/ExpressionEvaluator.class javax/servlet/jsp/el/FunctionMapper.class javax/servlet/jsp/el/ImplicitObjectELResolver$1.class javax/servlet/jsp/el/ImplicitObjectELResolver$EnumeratedMap.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$1.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$2.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$3.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$4.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$5.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$6.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$7.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$8.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects$9.class javax/servlet/jsp/el/ImplicitObjectELResolver$ImplicitObjects.class javax/servlet/jsp/el/ImplicitObjectELResolver.class javax/servlet/jsp/el/ScopedAttributeELResolver.class javax/servlet/jsp/el/VariableResolver.class javax/servlet/jsp/ErrorData.class javax/servlet/jsp/HttpJspPage.class javax/servlet/jsp/JspApplicationContext.class javax/servlet/jsp/JspContext.class javax/servlet/jsp/JspEngineInfo.class javax/servlet/jsp/JspException.class javax/servlet/jsp/JspFactory.class javax/servlet/jsp/JspPage.class javax/servlet/jsp/JspTagException.class javax/servlet/jsp/JspWriter.class javax/servlet/jsp/PageContext.class javax/servlet/jsp/SkipPageException.class javax/servlet/jsp/tagext/ javax/servlet/jsp/tagext/BodyContent.class javax/servlet/jsp/tagext/BodyTag.class javax/servlet/jsp/tagext/BodyTagSupport.class javax/servlet/jsp/tagext/DynamicAttributes.class javax/servlet/jsp/tagext/FunctionInfo.class javax/servlet/jsp/tagext/IterationTag.class javax/servlet/jsp/tagext/JspFragment.class javax/servlet/jsp/tagext/JspIdConsumer.class javax/servlet/jsp/tagext/JspTag.class javax/servlet/jsp/tagext/PageData.class javax/servlet/jsp/tagext/SimpleTag.class javax/servlet/jsp/tagext/SimpleTagSupport.class javax/servlet/jsp/tagext/Tag.class javax/servlet/jsp/tagext/TagAdapter.class javax/servlet/jsp/tagext/TagAttributeInfo.class javax/servlet/jsp/tagext/TagData.class javax/servlet/jsp/tagext/TagExtraInfo.class javax/servlet/jsp/tagext/TagFileInfo.class javax/servlet/jsp/tagext/TagInfo.class javax/servlet/jsp/tagext/TagLibraryInfo.class javax/servlet/jsp/tagext/TagLibraryValidator.class javax/servlet/jsp/tagext/TagSupport.class javax/servlet/jsp/tagext/TagVariableInfo.class javax/servlet/jsp/tagext/TryCatchFinally.class javax/servlet/jsp/tagext/ValidationMessage.class javax/servlet/jsp/tagext/VariableInfo.class javax/servlet/annotation/ javax/servlet/annotation/HandlesTypes.class javax/servlet/annotation/HttpConstraint.class javax/servlet/annotation/HttpMethodConstraint.class javax/servlet/annotation/MultipartConfig.class javax/servlet/annotation/package.html javax/servlet/annotation/ServletSecurity$EmptyRoleSemantic.class javax/servlet/annotation/ServletSecurity$TransportGuarantee.class javax/servlet/annotation/ServletSecurity.class javax/servlet/annotation/WebFilter.class javax/servlet/annotation/WebInitParam.class javax/servlet/annotation/WebListener.class javax/servlet/annotation/WebServlet.class javax/servlet/AsyncContext.class javax/servlet/AsyncEvent.class javax/servlet/AsyncListener.class javax/servlet/descriptor/ javax/servlet/descriptor/JspConfigDescriptor.class javax/servlet/descriptor/JspPropertyGroupDescriptor.class javax/servlet/descriptor/package.html javax/servlet/descriptor/TaglibDescriptor.class javax/servlet/DispatcherType.class javax/servlet/Filter.class javax/servlet/FilterChain.class javax/servlet/FilterConfig.class javax/servlet/FilterRegistration$Dynamic.class javax/servlet/FilterRegistration.class javax/servlet/GenericServlet.class javax/servlet/http/ javax/servlet/http/Cookie.class javax/servlet/http/HttpServlet.class javax/servlet/http/HttpServletRequest.class javax/servlet/http/HttpServletRequestWrapper.class javax/servlet/http/HttpServletResponse.class javax/servlet/http/HttpServletResponseWrapper.class javax/servlet/http/HttpSession.class javax/servlet/http/HttpSessionActivationListener.class javax/servlet/http/HttpSessionAttributeListener.class javax/servlet/http/HttpSessionBindingEvent.class javax/servlet/http/HttpSessionBindingListener.class javax/servlet/http/HttpSessionContext.class javax/servlet/http/HttpSessionEvent.class javax/servlet/http/HttpSessionIdListener.class javax/servlet/http/HttpSessionListener.class javax/servlet/http/HttpUpgradeHandler.class javax/servlet/http/HttpUtils.class javax/servlet/http/LocalStrings.properties javax/servlet/http/LocalStrings_es.properties javax/servlet/http/LocalStrings_fr.properties javax/servlet/http/LocalStrings_ja.properties javax/servlet/http/NoBodyOutputStream.class javax/servlet/http/NoBodyResponse.class javax/servlet/http/package.html javax/servlet/http/Part.class javax/servlet/http/WebConnection.class javax/servlet/HttpConstraintElement.class javax/servlet/HttpMethodConstraintElement.class javax/servlet/LocalStrings.properties javax/servlet/LocalStrings_fr.properties javax/servlet/LocalStrings_ja.properties javax/servlet/MultipartConfigElement.class javax/servlet/package.html javax/servlet/ReadListener.class javax/servlet/Registration$Dynamic.class javax/servlet/Registration.class javax/servlet/RequestDispatcher.class javax/servlet/Servlet.class javax/servlet/ServletConfig.class javax/servlet/ServletContainerInitializer.class javax/servlet/ServletContext.class javax/servlet/ServletContextAttributeEvent.class javax/servlet/ServletContextAttributeListener.class javax/servlet/ServletContextEvent.class javax/servlet/ServletContextListener.class javax/servlet/ServletException.class javax/servlet/ServletInputStream.class javax/servlet/ServletOutputStream.class javax/servlet/ServletRegistration$Dynamic.class javax/servlet/ServletRegistration.class javax/servlet/ServletRequest.class javax/servlet/ServletRequestAttributeEvent.class javax/servlet/ServletRequestAttributeListener.class javax/servlet/ServletRequestEvent.class javax/servlet/ServletRequestListener.class javax/servlet/ServletRequestWrapper.class javax/servlet/ServletResponse.class javax/servlet/ServletResponseWrapper.class javax/servlet/ServletSecurityElement.class javax/servlet/SessionCookieConfig.class javax/servlet/SessionTrackingMode.class javax/servlet/SingleThreadModel.class javax/servlet/UnavailableException.class javax/servlet/WriteListener.class org/apache/commons/ org/apache/commons/lang/ org/apache/commons/lang/ArrayUtils.class org/apache/commons/lang/BitField.class org/apache/commons/lang/BooleanUtils.class org/apache/commons/lang/builder/ org/apache/commons/lang/builder/CompareToBuilder.class org/apache/commons/lang/builder/EqualsBuilder.class org/apache/commons/lang/builder/HashCodeBuilder.class org/apache/commons/lang/builder/IDKey.class org/apache/commons/lang/builder/ReflectionToStringBuilder.class org/apache/commons/lang/builder/StandardToStringStyle.class org/apache/commons/lang/builder/ToStringBuilder.class org/apache/commons/lang/builder/ToStringStyle$DefaultToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$MultiLineToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$NoFieldNameToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$ShortPrefixToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$SimpleToStringStyle.class org/apache/commons/lang/builder/ToStringStyle.class org/apache/commons/lang/CharEncoding.class org/apache/commons/lang/CharRange$1.class org/apache/commons/lang/CharRange$CharacterIterator.class org/apache/commons/lang/CharRange.class org/apache/commons/lang/CharSet.class org/apache/commons/lang/CharSetUtils.class org/apache/commons/lang/CharUtils.class org/apache/commons/lang/ClassUtils.class org/apache/commons/lang/Entities$ArrayEntityMap.class org/apache/commons/lang/Entities$BinaryEntityMap.class org/apache/commons/lang/Entities$EntityMap.class org/apache/commons/lang/Entities$HashEntityMap.class org/apache/commons/lang/Entities$LookupEntityMap.class org/apache/commons/lang/Entities$MapIntMap.class org/apache/commons/lang/Entities$PrimitiveEntityMap.class org/apache/commons/lang/Entities$TreeEntityMap.class org/apache/commons/lang/Entities.class org/apache/commons/lang/enum/ org/apache/commons/lang/enum/Enum$Entry.class org/apache/commons/lang/enum/Enum.class org/apache/commons/lang/enum/EnumUtils.class org/apache/commons/lang/enum/ValuedEnum.class org/apache/commons/lang/enums/ org/apache/commons/lang/enums/Enum$Entry.class org/apache/commons/lang/enums/Enum.class org/apache/commons/lang/enums/EnumUtils.class org/apache/commons/lang/enums/ValuedEnum.class org/apache/commons/lang/exception/ org/apache/commons/lang/exception/CloneFailedException.class org/apache/commons/lang/exception/ExceptionUtils.class org/apache/commons/lang/exception/Nestable.class org/apache/commons/lang/exception/NestableDelegate.class org/apache/commons/lang/exception/NestableError.class org/apache/commons/lang/exception/NestableException.class org/apache/commons/lang/exception/NestableRuntimeException.class org/apache/commons/lang/IllegalClassException.class org/apache/commons/lang/IncompleteArgumentException.class org/apache/commons/lang/IntHashMap$Entry.class org/apache/commons/lang/IntHashMap.class org/apache/commons/lang/LocaleUtils.class org/apache/commons/lang/math/ org/apache/commons/lang/math/DoubleRange.class org/apache/commons/lang/math/FloatRange.class org/apache/commons/lang/math/Fraction.class org/apache/commons/lang/math/IEEE754rUtils.class org/apache/commons/lang/math/IntRange.class org/apache/commons/lang/math/JVMRandom.class org/apache/commons/lang/math/LongRange.class org/apache/commons/lang/math/NumberRange.class org/apache/commons/lang/math/NumberUtils.class org/apache/commons/lang/math/RandomUtils.class org/apache/commons/lang/math/Range.class org/apache/commons/lang/mutable/ org/apache/commons/lang/mutable/Mutable.class org/apache/commons/lang/mutable/MutableBoolean.class org/apache/commons/lang/mutable/MutableByte.class org/apache/commons/lang/mutable/MutableDouble.class org/apache/commons/lang/mutable/MutableFloat.class org/apache/commons/lang/mutable/MutableInt.class org/apache/commons/lang/mutable/MutableLong.class org/apache/commons/lang/mutable/MutableObject.class org/apache/commons/lang/mutable/MutableShort.class org/apache/commons/lang/NotImplementedException.class org/apache/commons/lang/NullArgumentException.class org/apache/commons/lang/NumberRange.class org/apache/commons/lang/NumberUtils.class org/apache/commons/lang/ObjectUtils$Null.class org/apache/commons/lang/ObjectUtils.class org/apache/commons/lang/RandomStringUtils.class org/apache/commons/lang/reflect/ org/apache/commons/lang/reflect/ConstructorUtils.class org/apache/commons/lang/reflect/FieldUtils.class org/apache/commons/lang/reflect/MemberUtils.class org/apache/commons/lang/reflect/MethodUtils.class org/apache/commons/lang/SerializationException.class org/apache/commons/lang/SerializationUtils.class org/apache/commons/lang/StringEscapeUtils.class org/apache/commons/lang/StringUtils.class org/apache/commons/lang/SystemUtils.class org/apache/commons/lang/text/ org/apache/commons/lang/text/CompositeFormat.class org/apache/commons/lang/text/ExtendedMessageFormat.class org/apache/commons/lang/text/FormatFactory.class org/apache/commons/lang/text/StrBuilder$StrBuilderReader.class org/apache/commons/lang/text/StrBuilder$StrBuilderTokenizer.class org/apache/commons/lang/text/StrBuilder$StrBuilderWriter.class org/apache/commons/lang/text/StrBuilder.class org/apache/commons/lang/text/StrLookup$MapStrLookup.class org/apache/commons/lang/text/StrLookup.class org/apache/commons/lang/text/StrMatcher$CharMatcher.class org/apache/commons/lang/text/StrMatcher$CharSetMatcher.class org/apache/commons/lang/text/StrMatcher$NoMatcher.class org/apache/commons/lang/text/StrMatcher$StringMatcher.class org/apache/commons/lang/text/StrMatcher$TrimMatcher.class org/apache/commons/lang/text/StrMatcher.class org/apache/commons/lang/text/StrSubstitutor.class org/apache/commons/lang/text/StrTokenizer.class org/apache/commons/lang/time/ org/apache/commons/lang/time/DateFormatUtils.class org/apache/commons/lang/time/DateUtils$DateIterator.class org/apache/commons/lang/time/DateUtils.class org/apache/commons/lang/time/DurationFormatUtils$Token.class org/apache/commons/lang/time/DurationFormatUtils.class org/apache/commons/lang/time/FastDateFormat$CharacterLiteral.class org/apache/commons/lang/time/FastDateFormat$NumberRule.class org/apache/commons/lang/time/FastDateFormat$PaddedNumberField.class org/apache/commons/lang/time/FastDateFormat$Pair.class org/apache/commons/lang/time/FastDateFormat$Rule.class org/apache/commons/lang/time/FastDateFormat$StringLiteral.class org/apache/commons/lang/time/FastDateFormat$TextField.class org/apache/commons/lang/time/FastDateFormat$TimeZoneDisplayKey.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNameRule.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNumberRule.class org/apache/commons/lang/time/FastDateFormat$TwelveHourField.class org/apache/commons/lang/time/FastDateFormat$TwentyFourHourField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitMonthField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitNumberField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitYearField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedMonthField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedNumberField.class org/apache/commons/lang/time/FastDateFormat.class org/apache/commons/lang/time/StopWatch.class org/apache/commons/lang/UnhandledException.class org/apache/commons/lang/Validate.class org/apache/commons/lang/WordUtils.class[ERROR] Found artifact with unexpected contents: '/Users/appy/apache/hbase/hbase-shaded/hbase-shaded-client/target/hbase-shaded-client-3.0.0-SNAPSHOT.jar' Please check the following and either correct the build or update the allowed list with reasoning. org/apache/commons/ org/apache/commons/lang/ org/apache/commons/lang/ArrayUtils.class org/apache/commons/lang/BitField.class org/apache/commons/lang/BooleanUtils.class org/apache/commons/lang/builder/ org/apache/commons/lang/builder/CompareToBuilder.class org/apache/commons/lang/builder/EqualsBuilder.class org/apache/commons/lang/builder/HashCodeBuilder.class org/apache/commons/lang/builder/IDKey.class org/apache/commons/lang/builder/ReflectionToStringBuilder.class org/apache/commons/lang/builder/StandardToStringStyle.class org/apache/commons/lang/builder/ToStringBuilder.class org/apache/commons/lang/builder/ToStringStyle$DefaultToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$MultiLineToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$NoFieldNameToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$ShortPrefixToStringStyle.class org/apache/commons/lang/builder/ToStringStyle$SimpleToStringStyle.class org/apache/commons/lang/builder/ToStringStyle.class org/apache/commons/lang/CharEncoding.class org/apache/commons/lang/CharRange$1.class org/apache/commons/lang/CharRange$CharacterIterator.class org/apache/commons/lang/CharRange.class org/apache/commons/lang/CharSet.class org/apache/commons/lang/CharSetUtils.class org/apache/commons/lang/CharUtils.class org/apache/commons/lang/ClassUtils.class org/apache/commons/lang/Entities$ArrayEntityMap.class org/apache/commons/lang/Entities$BinaryEntityMap.class org/apache/commons/lang/Entities$EntityMap.class org/apache/commons/lang/Entities$HashEntityMap.class org/apache/commons/lang/Entities$LookupEntityMap.class org/apache/commons/lang/Entities$MapIntMap.class org/apache/commons/lang/Entities$PrimitiveEntityMap.class org/apache/commons/lang/Entities$TreeEntityMap.class org/apache/commons/lang/Entities.class org/apache/commons/lang/enum/ org/apache/commons/lang/enum/Enum$Entry.class org/apache/commons/lang/enum/Enum.class org/apache/commons/lang/enum/EnumUtils.class org/apache/commons/lang/enum/ValuedEnum.class org/apache/commons/lang/enums/ org/apache/commons/lang/enums/Enum$Entry.class org/apache/commons/lang/enums/Enum.class org/apache/commons/lang/enums/EnumUtils.class org/apache/commons/lang/enums/ValuedEnum.class org/apache/commons/lang/exception/ org/apache/commons/lang/exception/CloneFailedException.class org/apache/commons/lang/exception/ExceptionUtils.class org/apache/commons/lang/exception/Nestable.class org/apache/commons/lang/exception/NestableDelegate.class org/apache/commons/lang/exception/NestableError.class org/apache/commons/lang/exception/NestableException.class org/apache/commons/lang/exception/NestableRuntimeException.class org/apache/commons/lang/IllegalClassException.class org/apache/commons/lang/IncompleteArgumentException.class org/apache/commons/lang/IntHashMap$Entry.class org/apache/commons/lang/IntHashMap.class org/apache/commons/lang/LocaleUtils.class org/apache/commons/lang/math/ org/apache/commons/lang/math/DoubleRange.class org/apache/commons/lang/math/FloatRange.class org/apache/commons/lang/math/Fraction.class org/apache/commons/lang/math/IEEE754rUtils.class org/apache/commons/lang/math/IntRange.class org/apache/commons/lang/math/JVMRandom.class org/apache/commons/lang/math/LongRange.class org/apache/commons/lang/math/NumberRange.class org/apache/commons/lang/math/NumberUtils.class org/apache/commons/lang/math/RandomUtils.class org/apache/commons/lang/math/Range.class org/apache/commons/lang/mutable/ org/apache/commons/lang/mutable/Mutable.class org/apache/commons/lang/mutable/MutableBoolean.class org/apache/commons/lang/mutable/MutableByte.class org/apache/commons/lang/mutable/MutableDouble.class org/apache/commons/lang/mutable/MutableFloat.class org/apache/commons/lang/mutable/MutableInt.class org/apache/commons/lang/mutable/MutableLong.class org/apache/commons/lang/mutable/MutableObject.class org/apache/commons/lang/mutable/MutableShort.class org/apache/commons/lang/NotImplementedException.class org/apache/commons/lang/NullArgumentException.class org/apache/commons/lang/NumberRange.class org/apache/commons/lang/NumberUtils.class org/apache/commons/lang/ObjectUtils$Null.class org/apache/commons/lang/ObjectUtils.class org/apache/commons/lang/RandomStringUtils.class org/apache/commons/lang/reflect/ org/apache/commons/lang/reflect/ConstructorUtils.class org/apache/commons/lang/reflect/FieldUtils.class org/apache/commons/lang/reflect/MemberUtils.class org/apache/commons/lang/reflect/MethodUtils.class org/apache/commons/lang/SerializationException.class org/apache/commons/lang/SerializationUtils.class org/apache/commons/lang/StringEscapeUtils.class org/apache/commons/lang/StringUtils.class org/apache/commons/lang/SystemUtils.class org/apache/commons/lang/text/ org/apache/commons/lang/text/CompositeFormat.class org/apache/commons/lang/text/ExtendedMessageFormat.class org/apache/commons/lang/text/FormatFactory.class org/apache/commons/lang/text/StrBuilder$StrBuilderReader.class org/apache/commons/lang/text/StrBuilder$StrBuilderTokenizer.class org/apache/commons/lang/text/StrBuilder$StrBuilderWriter.class org/apache/commons/lang/text/StrBuilder.class org/apache/commons/lang/text/StrLookup$MapStrLookup.class org/apache/commons/lang/text/StrLookup.class org/apache/commons/lang/text/StrMatcher$CharMatcher.class org/apache/commons/lang/text/StrMatcher$CharSetMatcher.class org/apache/commons/lang/text/StrMatcher$NoMatcher.class org/apache/commons/lang/text/StrMatcher$StringMatcher.class org/apache/commons/lang/text/StrMatcher$TrimMatcher.class org/apache/commons/lang/text/StrMatcher.class org/apache/commons/lang/text/StrSubstitutor.class org/apache/commons/lang/text/StrTokenizer.class org/apache/commons/lang/time/ org/apache/commons/lang/time/DateFormatUtils.class org/apache/commons/lang/time/DateUtils$DateIterator.class org/apache/commons/lang/time/DateUtils.class org/apache/commons/lang/time/DurationFormatUtils$Token.class org/apache/commons/lang/time/DurationFormatUtils.class org/apache/commons/lang/time/FastDateFormat$CharacterLiteral.class org/apache/commons/lang/time/FastDateFormat$NumberRule.class org/apache/commons/lang/time/FastDateFormat$PaddedNumberField.class org/apache/commons/lang/time/FastDateFormat$Pair.class org/apache/commons/lang/time/FastDateFormat$Rule.class org/apache/commons/lang/time/FastDateFormat$StringLiteral.class org/apache/commons/lang/time/FastDateFormat$TextField.class org/apache/commons/lang/time/FastDateFormat$TimeZoneDisplayKey.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNameRule.class org/apache/commons/lang/time/FastDateFormat$TimeZoneNumberRule.class org/apache/commons/lang/time/FastDateFormat$TwelveHourField.class org/apache/commons/lang/time/FastDateFormat$TwentyFourHourField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitMonthField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitNumberField.class org/apache/commons/lang/time/FastDateFormat$TwoDigitYearField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedMonthField.class org/apache/commons/lang/time/FastDateFormat$UnpaddedNumberField.class org/apache/commons/lang/time/FastDateFormat.class org/apache/commons/lang/time/StopWatch.class org/apache/commons/lang/UnhandledException.class org/apache/commons/lang/Validate.class org/apache/commons/lang/WordUtils.class</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-27 01:00:00" id="19635" opendate="2017-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce a thread at RS side to call reportProcedureDone</summary>
      <description>So that we can do some batching and also prevent blocking too many threads when HMaster is temporary done.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionServerStatus.proto</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-24 01:00:00" id="20068" opendate="2018-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopcheck project health check uses default maven repo instead of yetus managed ones</summary>
      <description>Recently had a precommit run fail hadoop check for all 3 versions with [ERROR] Failed to execute goal org.apache.maven.plugins:maven-install-plugin:2.5.2:install (default-install) on project hbase-thrift: Failed to install metadata org.apache.hbase:hbase-thrift:3.0.0-SNAPSHOT/maven-metadata.xml: Could not parse metadata /home/jenkins/.m2/repository/org/apache/hbase/hbase-thrift/3.0.0-SNAPSHOT/maven-metadata-local.xml: in epilog non whitespace content is not allowed but got / (position: END_TAG seen ...&lt;/metadata&gt;\n/... @25:2) -&gt; [Help 1]Looks like maven repo corruption.Also the path /home/jenkins/.m2/repository means that those invocations are using the jenkins user repo, which isn't safe since there are multiple executors. either the plugin isn't using the yetus provided maven repo path or our yetus invocation isn't telling yetus to provide its own maven repo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.4,2.0.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-24 01:00:00" id="20070" opendate="2018-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>website generation is failing</summary>
      <description>website generation has been failing since Feb 20thChecking out files: 100% (68971/68971), done.Usage: grep [OPTION]... PATTERN [FILE]...Try 'grep --help' for more information.PUSHED is 2 is not yet mentioned in the hbase-site commit log. Assuming we don't have it yet. 2Building HBaseJava HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0Failure: mvn clean siteBuild step 'Execute shell' marked build as failureThe status email saysBuild status: Still FailingThe HBase website has not been updated to incorporate HBase commit ${CURRENT_HBASE_COMMIT}.Looking at the code where that grep happens, it looks like the env variable CURRENT_HBASE_COMMIT isn't getting set. That comes from some git command. I'm guessing the version of git changed on the build hosts and upended our assumptions.we should fix this to 1) rely on git's porcelain interface, and 2) fail as soon as that git command fails</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-spark-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-resource-bundle.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-build-configuration.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">hbase-annotations.pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-19 01:00:00" id="20909" opendate="2018-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.0 to the download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-8 01:00:00" id="21697" opendate="2019-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.2 to the download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-1 01:00:00" id="2177" opendate="2010-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timestamping to gc logging options</summary>
      <description>http://forums.sun.com/thread.jspa?threadID=5165451</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-17 01:00:00" id="22264" opendate="2019-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out jars related to JDK 11 into a folder in /lib</summary>
      <description>UPDATE:Separate out the the jars related to JDK 11 and add control their addition to the classpath using an environment variable or auto-detection of the jdk version installed.OLD:This is in continuation with HBASE-22249. When compiled with jdk 8 and run on jdk 11, the master branch throws the following exception during an attempt to start the hbase rest server:Exception in thread "main" java.lang.NoClassDefFoundError: javax/annotation/Priority at org.glassfish.jersey.model.internal.ComponentBag.modelFor(ComponentBag.java:483) at org.glassfish.jersey.model.internal.ComponentBag.access$100(ComponentBag.java:89) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:408) at org.glassfish.jersey.model.internal.ComponentBag$5.call(ComponentBag.java:398) at org.glassfish.jersey.internal.Errors.process(Errors.java:315) at org.glassfish.jersey.internal.Errors.process(Errors.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:228) at org.glassfish.jersey.model.internal.ComponentBag.registerModel(ComponentBag.java:398) at org.glassfish.jersey.model.internal.ComponentBag.register(ComponentBag.java:235) at org.glassfish.jersey.model.internal.CommonConfig.register(CommonConfig.java:420) at org.glassfish.jersey.server.ResourceConfig.register(ResourceConfig.java:425) at org.apache.hadoop.hbase.rest.RESTServer.run(RESTServer.java:245) at org.apache.hadoop.hbase.rest.RESTServer.main(RESTServer.java:421)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-29 01:00:00" id="22495" opendate="2019-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SyncTable section, explaining from which specific minor versions "doDeletes/doPuts" is available</summary>
      <description>On SyncTable section from ref guide for 1.2, we are mentioning "doDeletes/doPuts", but these were only added by HBASE-20305. Original patch proposed there was for master branch only, and related branch-1 and branch-2 patches were only committed recently, so minimal supported version is 1.4.10, for branch-1 releases, and 2.1.5 for branch-2. Am proposing a patch that mentions which are the minimum versions that support such commands.Also, we should add this whole section to 2.0 and 2.1 Documentations (these are currently missing).Thanks npopa for pointing this out.</description>
      <version>1.2.0</version>
      <fixedVersion>1.5.0,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
</bugrepository>