<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  
  <bug fixdate="2014-5-16 01:00:00" id="11196" opendate="2014-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update description of -ROOT- in ref guide</summary>
      <description>Since the resolution of HBASE-3171, &amp;#45;ROOT- is no longer used to store the location(s) of .META. . Unfortunately, not all of our documentation has been updated to reflect this change in architecture.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-13 01:00:00" id="11340" opendate="2014-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to xcievers in documentation</summary>
      <description>While most references to xcievers were removed in HBase as part of HBASE-5697, the ref guide still contains a few stray references.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
      <file type="M">src.main.docbkx.configuration.xml</file>
      <file type="M">src.main.docbkx.case.studies.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-1-21 01:00:00" id="1147" opendate="2009-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the scripts to use Zookeeper</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-7 01:00:00" id="11474" opendate="2014-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Thrift2] support authentication/impersonation</summary>
      <description>This is to do the same as HBASE-11349 for Thrift2.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandlerWithLabels.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ConnectionCache.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-8 01:00:00" id="11476" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand &amp;#39;Conceptual View&amp;#39; section of Data Model chapter</summary>
      <description>Could use some updating and expansion to emphasize the differences between HBase and an RDBMS. I found http://jimbojw.com/wiki/index.php?title=Understanding_Hbase_and_BigTable which is just excellent and we should link to it.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-11 01:00:00" id="11502" opendate="2014-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track down broken images in Ref Guide</summary>
      <description>I realized that image support was broken in the Ref Guide. I fixed it by making some changes to the pom.xml (in HBASE-11400). I need to track down what images are broken besides the new ones I added, find out where they are, and make them work again.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.images.timeline.consitency.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-14 01:00:00" id="11513" opendate="2014-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Combine SingleMultiple Queue RpcExecutor into a single class</summary>
      <description>Its a little odd that we use multiple classes, leading to mutliple if-else conditions for rpc execution when we could just combine them into one. Makes the logic and also puts the code into one place</description>
      <version>0.99.0,0.98.4,2.0.0</version>
      <fixedVersion>0.99.0,0.98.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SingleQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MultipleQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-21 01:00:00" id="11555" opendate="2014-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableSnapshotRegionSplit should be public</summary>
      <description>This class extends Writable and so should be public so it can be used outside of the existing code line we ship. This will be consistent with TableSplit, which is also public.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapred.TableSnapshotInputFormat.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableSnapshotInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-24 01:00:00" id="11585" opendate="2014-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE: Allows warm-up</summary>
      <description>When we measure the latency, warm-up helps to get repeatable and useful measures.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-8-6 01:00:00" id="11681" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update and move doc about disabling the WAL</summary>
      <description>Move the docs about disabling the WAL from the Performance section to the WAL section, point to the new location, and add info about getDurability and setDurability methods. Leave the big fat warnings about data loss in both the Performance and WAL sections.</description>
      <version>0.99.0,2.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.performance.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-6 01:00:00" id="11682" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain hotspotting</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.schema.design.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-6 01:00:00" id="11687" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to abort on postOpenDeployTasks exception if region opening is cancelled</summary>
      <description>With ZK-less region assignment, if region opening is in postOpenDeployTasks step and the opening is cancelled, the region server will abort because it can't report the transition to the master. We should cancel postOpenDeployTasks instead. At least, there is no need to abort.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-6 01:00:00" id="11689" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track meta in transition</summary>
      <description>With ZK-less region assignment, user regions in transition are tracked in meta. We need a way to track meta in transition too.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestMetaTableLocator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterNoCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.MetaServerShutdownHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.ZooKeeper.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ZooKeeperProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.MetaTableLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.master.RegionState.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-11 01:00:00" id="11719" opendate="2014-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some unused paths in AsyncClient</summary>
      <description>sershe you're ok with these changes?</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-13 01:00:00" id="11732" opendate="2014-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should not preemptively offline a region</summary>
      <description>Clean up force region unassignment.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMergeTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManagerOnCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.UnAssignCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.BulkReOpen.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-14 01:00:00" id="11734" opendate="2014-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document changed behavior of hbase.hstore.time.to.purge.deletes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-22 01:00:00" id="11804" opendate="2014-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Raise default heap size if unspecified</summary>
      <description>On master running pe randomWrite with ten threads and the default heap size crashes the system. This works on 0.98 and before.It's been a long time that 1000mb has been our default. Maybe we should start looking at raising that limit on master.</description>
      <version>2.0.0</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-28 01:00:00" id="11847" opendate="2014-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HFile tool should be able to print block headers</summary>
      <description>Printing the block index is helpful, but sometimes you want to see more info about the blocks themselves.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.98.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFilePrettyPrinter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-28 01:00:00" id="11849" opendate="2014-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up orphaned private audience classes</summary>
      <description>We have some classes in master that are private audience and no longer used internally. We should remove them.I'll build a list for server-side modules along with when they got orphaned so we can decide on removal from older branches.</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.OrphanHLogAfterSplitException.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.encode.ThreadLocalEncoderPool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-28 01:00:00" id="11855" opendate="2014-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase handbook chapter 18.9 out of date</summary>
      <description>Recently posix4e committed a change HBASE-4955 renaming Dsurefire.*PartThreadCount to Dsurefire.*PartForkCount for the 2.0 version.I'm excited to help with documentation and the webpage going forward. Can someone help mentor me with the process? Thanks.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-28 01:00:00" id="11859" opendate="2014-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;hadoop jar&amp;#39; references in documentation should mention hbase-server.jar, not hbase.jar</summary>
      <description>There are various org.apache.hadoop.util.Tool implementations mentioned in the documentation as being run with "hadoop jar hbase-VERSION.jar &lt;toolname&gt;". These classes now live in in the hbase-server module, so that jar name should be hbase-server-VERSION.jar.The same applies to the documentation on running MapReduce jobs against HBase.</description>
      <version>0.99.0,0.98.6,2.0.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-4-30 01:00:00" id="11864" opendate="2014-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance HLogPrettyPrinter to print information from WAL Header</summary>
      <description>HBASE-11620 and HBASE-11762 added Writer classname and Cell Codec classname to WALHeader.This issue is to enhance HLogPrettyPrinter such that it prints the classnames, if such information is available in WALHeader.</description>
      <version>2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALPrettyPrinter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SecureProtobufLogReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ProtobufLogReader.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-9-8 01:00:00" id="11909" opendate="2014-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region count listed by HMaster UI and hbck are different</summary>
      <description>The count displayed in the HMaster UI can be lower than the count of regions as done by hbck or by counting subdirectories of /hbase/&lt;table&gt;. This is explained in the comments &amp;#91;1&amp;#93; but I think it should be documented as well&amp;#91;1&amp;#93; https://git-wip-us.apache.org/repos/asf?p=hbase.git;a=blob;f=hbase-server/src/main/java/org/apache/hadoop/hbase/master/RegionStates.java#l578</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-16 01:00:00" id="11985" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document sizing rules of thumb</summary>
      <description>I'm looking for tuning/sizing rules of thumb to put in the Ref Guide.Info I have gleaned so far:A reasonable region size is between 10 GB and 50 GB.A reasonable maximum cell size is 1 MB to 10 MB. If your cells are larger than 10 MB, consider storing the cell contents in HDFS and storing a reference to the location in HBase. Pending MOB work for 10 MB - 64 MB window.When you size your regions and cells, keep in mind that a region cannot split across a row. If your row size is too large, or your region size is too small, you can end up with a single row per region, which is not a good pattern. It is also possible that one big column causes splits while other columns are tiny, and this may not be great.A large # of columns probably means you are doing it wrong.Column names need to be short because they get stored for every value (barring encoding). Don't need to be self-documenting like in RDBMS.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="11986" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document MOB in Ref Guide</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hbase-11339,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.hbase.mob.xml</file>
      <file type="M">src.main.docbkx.schema.design.xml</file>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-16 01:00:00" id="11990" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make setting the start and stop row for a specific prefix easier</summary>
      <description>If you want to set a scan from your application to scan for a specific row prefix this is actually quite hard.As described in several places you can set the startRow to the prefix; yet the stopRow should be set to the prefix '+1'If the prefix 'ASCII' put into a byte[] then this is easy because you can simply increment the last byte of the array. But if your application uses real binary rowids you may run into the scenario that your prefix is something like { 0x12, 0x23, 0xFF, 0xFF } Then the increment should be { 0x12, 0x24 }I have prepared a proposed patch that makes setting these values correctly a lot easier.</description>
      <version>None</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWithScanLimits.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="11991" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region states may be out of sync</summary>
      <description>Region states could be out of sync under a rare scenario. The regions hosted by a server could be wrong.</description>
      <version>None</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-10-17 01:00:00" id="12011" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add namespace column during display of user tables</summary>
      <description>Currently, the namespaces are not being explicitly stated while displaying the user tables. This will help in decoupling the table names and their corresponding namespaces.</description>
      <version>None</version>
      <fixedVersion>0.98.7,0.99.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2014-9-24 01:00:00" id="12080" opendate="2014-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shorten the run time of integration test by default when using mvn failsafe:integration-test</summary>
      <description>By default, using "mvn failsafe:integration-test" to execute the integration test with MOB will run more than 10 minutes. In this JIRA, we'll shorten this run time by default.</description>
      <version>2.0.0</version>
      <fixedVersion>hbase-11339,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestIngestWithMOB.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-2-23 01:00:00" id="1210" opendate="2009-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow truncation of output for scan and get commands in shell</summary>
      <description>Allow to chop the output of the values to be able to scan a table for specific rows and columns but without having the shell dump all the content of the cell value as it can potentially be very large. I suggest a hash called MAXLENGTH that can be added to the commands. For example:get 'docs', 'ABCDE', { COLUMNS =&gt; 'contents:', MAXLENGTH =&gt; 150 }and scan 'docs', { COLUMNS =&gt; 'contents:', MAXLENGTH =&gt; 150 }</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-26 01:00:00" id="12109" opendate="2014-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>user_permission command for namespace does not return correct result</summary>
      <description>The existing user_permission command does not return permissions related to namespace. The permissions exist in the acl table, but the user_permission.rb does not handle namespaces.</description>
      <version>2.0.0</version>
      <fixedVersion>0.98.8,0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.security.rb</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.access.AccessControlClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-2 01:00:00" id="12160" opendate="2014-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Surefire&amp;#39;s argLine configurable in the command line</summary>
      <description>On tests machines with larger sets of ram it can really help to reduce test time to run with larger heaps. Lets make that a property so that mvn command line can set it.</description>
      <version>0.99.1,0.98.6.1,2.0.0</version>
      <fixedVersion>0.98.7,0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-7 01:00:00" id="12198" opendate="2014-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the bug of not updating location cache</summary>
      <description>Fix the bug of not updating location cache.Add a testcase for it.</description>
      <version>1.0.0,0.98.7,2.0.0</version>
      <fixedVersion>0.98.8,0.99.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTableMultiplexer.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-8 01:00:00" id="12207" opendate="2014-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A script to help keep your Git repo fresh</summary>
      <description>I have a script that does a git pull --rebase on each tracking branch, and then attempts an automatic rebase of each local branch against its tracking branch. It also prompts you to delete local branches for HBASE- JIRAs that have been closed. I think this script may help to enforce good Git practices. It may be a good candidate to be included in dev-support/.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-9 01:00:00" id="12210" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid KeyValue in Prefix Tree</summary>
      <description>Avoid KeyValue recreate where all possible in the PrefixTree module.</description>
      <version>None</version>
      <fixedVersion>0.99.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestPrefixTree.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.decode.PrefixTreeCell.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellComparator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-14 01:00:00" id="12251" opendate="2014-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[book] Hadoop compat matrix 0.94 section needs cleaned up</summary>
      <description>The compatibility matrix has instructions for compiling 0.94 vs Hadoop 2.2 inline in the table. These should sit outside the compatibility matrix table as a referenced section.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-15 01:00:00" id="12261" opendate="2014-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add checkstyle to HBase build process</summary>
      <description>We should add checkstyle to hadoop qa for our builds. That would free committers up from checking patches for stylistic issues and leave them free to check the real meat of the patches.Additionally we should have the check for empty try catch blocks running so that we can't regress on catching exceptions.</description>
      <version>2.0.0</version>
      <fixedVersion>0.98.8,0.99.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-17 01:00:00" id="12283" opendate="2014-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up some checkstyle errors</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.DaemonThreadFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.CoordinatedStateManagerFactory.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HBaseInterfaceAudience.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CompoundConfiguration.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellKey.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HRegionLocation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ClusterStatus.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-21 01:00:00" id="12308" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in hbase-rest profile name</summary>
      <description>Change skipRestTets to skipRestTests.</description>
      <version>0.99.2,2.0.0</version>
      <fixedVersion>0.98.8,0.99.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-22 01:00:00" id="12322" opendate="2014-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add clean up command to ITBLL</summary>
      <description>Right now ITBLL can leave a table and some files on HDFS. It's then up to the user to clean them up. This can be a little messy. Lets give a single command to do that.</description>
      <version>2.0.0</version>
      <fixedVersion>0.99.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-29 01:00:00" id="12378" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a test to verify that the read-replica is able to read after a compaction</summary>
      <description>Add a unit test that verify that the secondary read-replica is still able to read all the data even when the files on the primary are archived and the store file refresh is not executed.basically is to have a test that verifies that the file-link logic is not removed.(there are a couple of test that probably wants to do that.. but since they operate on small data they will never trigger the file-link reopen)</description>
      <version>0.99.1,2.0.0</version>
      <fixedVersion>0.99.2</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-29 01:00:00" id="12379" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try surefire 2.18-SNAPSHOT</summary>
      <description>Hopefully has a fix for:&amp;#91;ERROR&amp;#93; Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test (secondPartTestsExecution) on project hbase-server: ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.io.IOException: Stream Closed -&gt; &amp;#91;Help 1&amp;#93;eclark says its been working for him and crew.</description>
      <version>None</version>
      <fixedVersion>0.99.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-30 01:00:00" id="12381" opendate="2014-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add maven enforcer rules for build assumptions</summary>
      <description>our ref guide says that you need maven 3 to build. add an enforcer rule so that people find out early that they have the wrong maven version, rather then however things fall over if someone tries to build with maven 2.</description>
      <version>None</version>
      <fixedVersion>0.98.8,0.94.25,0.99.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-30 01:00:00" id="12382" opendate="2014-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore incremental compilation</summary>
      <description>The build changes in HBASE-11912 required an upgrade of the Maven compiler plugin from 2.5.1 to something &gt;= 3.0. We're now using 3.2. We also switch from whatever Maven does by default with an embedding of tools.jar to invocation of javac. We are no longer getting incremental builds due to Maven bugs hit by these changes. http://jira.codehaus.org/browse/MCOMPILER-209 suggests paradoxically setting useIncrementalCompilation to 'false' will restore incremental compilation behavior.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-31 01:00:00" id="12397" opendate="2014-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CopyTable fails to compile with the Hadoop 1 profile</summary>
      <description>&amp;#91;ERROR&amp;#93; /usr/src/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/mapreduce/CopyTable.java:&amp;#91;88,17&amp;#93; error: cannot find symbolThis was introduced in f31edd80commit f31edd8004226c795ee46fbe9e93d10671ab895aAuthor: Ted Yu &lt;tedyu@apache.org&gt;Date: Thu Oct 9 15:52:18 2014 +0000 HBASE-11997 CopyTable with bulkload (Yi Deng)tedyu, daviddengcn, please have a look at this or I will revert it on 0.98.</description>
      <version>None</version>
      <fixedVersion>0.98.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-3 01:00:00" id="12409" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add actual tunable parameters for finding optimal # of regions per RS</summary>
      <description>http://hbase.apache.org/book/ops.capacity.html#ops.capacity.regions.count should mention:HBase 0.94:(RS Xmx)(hbase.regionserver.global.memstore.upperLimit)/((hbase.hregion.memstore.flush.size)(# column families))HBase 0.98+: replace upperLimit with hbase.regionserver.global.memstore.size</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-5 01:00:00" id="1241" opendate="2009-3-5 00:00:00" resolution="Invalid">
    <buginformation>
      <summary>HBase additions to ZooKeeper</summary>
      <description>This issue is to batch all of the edits and additions we make to ZooKeeper for its use in HBase. Rather than wasting lots of our (and ZK's) time with little edit patches, we will send them batch patches from here when things stabilize.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniZooKeeperCluster.java</file>
      <file type="M">lib.zookeeper-3.0.1.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-3 01:00:00" id="12412" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update the ref guide(currently Example 10.2) to show "update an existing CF" with the new API modifyFamily in master</summary>
      <description>In the new implementation of HTableDescriptor, updating an existing CF doesn't use the addFamily any more(An IllegalArgumentException is thrown in such a case.), the new API modifyFamily is used instead.We need to update the ref guide (currently Example 10.2) to show "update an existing CF" with the new API modifyFamily in master.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase.apis.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-4 01:00:00" id="12425" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the phases of the split transaction</summary>
      <description>See PDF document attached to parent issue</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-12-19 01:00:00" id="12537" opendate="2014-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase should log the remote host on replication error</summary>
      <description>Replication source was getting 'queue full' but didn't say which server was suffering the full queue. Makes it hard to debug...</description>
      <version>None</version>
      <fixedVersion>0.98.9,0.99.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-25 01:00:00" id="12568" opendate="2014-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adopt Semantic Versioning and document it in the book</summary>
      <description>See http://search-hadoop.com/m/DHED4LFNzP/semantic+versioning&amp;subj=Re+HBase+Semantic+VersioningWe should put that in the book.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.upgrading.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-2 01:00:00" id="12615" opendate="2014-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document GC conserving guidelines for contributors</summary>
      <description>LinkedIn put up a blog post with a nice concise list of GC conserving techniques we should document for contributors. Additionally, when we're at a point our build supports custom error-prone plugins, we can develop warnings for some of them. Source: http://engineering.linkedin.com/performance/linkedin-feed-faster-less-jvm-garbage Be careful with Iterators Estimate the size of a collection when initializing Defer expression evaluation Compile the regex patterns in advance Cache it if you can String Interns are useful but dangerousAll good advice and practice that I know we aim for.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-3 01:00:00" id="12622" opendate="2014-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>user_permission should require global admin to display global and ns permissions</summary>
      <description>user_permission check the user permission only on the table level (requiring at least a table-level admin)global and namespace permission listing is done without checking anything.but only a global admins should be able to perform this operations.</description>
      <version>0.98.8,0.99.2,2.0.0</version>
      <fixedVersion>1.0.0,0.98.9</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestNamespaceCommands.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-3 01:00:00" id="12623" opendate="2014-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove pre-0.96 to 0.96 upgrade code</summary>
      <description>Since we require a cluster to be 1.0+ prior to upgrading to 2.0, we should remove code that is only used for handling upgrades prior to that version.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ZKDataMigrator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.migration.NamespaceUpgrade.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-12-11 01:00:00" id="12675" opendate="2014-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use interface methods in shell scripts</summary>
      <description>There are places in the shell script code that use methods from HTable or HConnection or etc. This patch will fix at least some of them.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.table.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.security.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-12 01:00:00" id="12677" opendate="2014-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update replication docs to clarify terminology</summary>
      <description>Remove use of master-master and cyclical replication and just talk about topologies</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
      <file type="M">hbase-server.src.main.javadoc.org.apache.hadoop.hbase.replication.package.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-13 01:00:00" id="12681" opendate="2014-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>truncate_preserve command fails with undefined method `getTable&amp;#39; error</summary>
      <description>hbase(main):002:0&gt; truncate_preserve 'a'Truncating 'a' table (it may take a while):ERROR: undefined method `getTable' for nil:NilClassHere is some help for this command: Disables, drops and recreates the specified table while still maintaing the previous region boundaries.</description>
      <version>0.99.2,2.0.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-15 01:00:00" id="12687" opendate="2014-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Book is missing styling</summary>
      <description>The online book is intended to be styled, there's a file freebsd_docbook.css. It's not being applied; fix that.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-15 01:00:00" id="12693" opendate="2014-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[docs] nit fix in HBase and MapReduce section</summary>
      <description>trivial nit fix</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-16 01:00:00" id="12695" opendate="2014-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDK 1.8 compilation broken</summary>
      <description>Looks like trunk only.https://code.google.com/p/error-prone/issues/detail?id=240https://code.google.com/p/error-prone/issues/detail?id=246</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-17 01:00:00" id="12701" opendate="2014-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to set the split policy on a given table</summary>
      <description>Need to document in the ref guide how to set/change the region split policy for a single table user the API and the HBase shell as noted below as an example.Using Java:HTableDescriptor tableDesc = new HTableDescriptor("test");tableDesc.setValue(HTableDescriptor.SPLIT_POLICY, ConstantSizeRegionSplitPolicy.class.getName());tableDesc.addFamily(new HColumnDescriptor(Bytes.toBytes("cf1")));admin.createTable(tableDesc);Using HBase Shell:create 'test', {METHOD =&gt; 'table_att', CONFIG =&gt; {'SPLIT_POLICY' =&gt; 'org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy'}},{NAME =&gt; 'cf1'}</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-18 01:00:00" id="12708" opendate="2014-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document newly introduced params for using Thrift-over-HTTPS.</summary>
      <description>Per the description.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-18 01:00:00" id="12709" opendate="2014-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[mvn] Add unit test excludes command line flag to the build.</summary>
      <description>I've added a simple way to specify unit test classes to skip when executing unit test runs. I've added a -D variable called test.exclude.pattern that you can using like this:mvn test -Dtest.exclude.pattern=**/TestFoo.java,**/TestBar.javato exclude the unit tests form TestFoo and TestBar in this run. By default there is nothing excluded.</description>
      <version>1.0.0,0.98.10,2.0.0</version>
      <fixedVersion>1.0.0,hbase-11339,0.98.10,1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-4-20 01:00:00" id="1271" opendate="2009-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow multiple tests to run on one machine</summary>
      <description>Currently, if we try to run two tests on one machine (e.g. in two checkouts) the second one will fail because its servers won't be able to bind to ports. We should use random ports in our servers in the tests to fix this.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniZooKeeperCluster.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-5-20 01:00:00" id="1272" opendate="2009-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unreadable log messages -- "... to the only server localhost_1237525439599_56094" &lt;- You&amp;#39;d have to be perverse to recognize that as a hostname, startcode, and port number.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-20 01:00:00" id="12735" opendate="2014-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor TAG so it can live as unit test and as an integration test</summary>
      <description>Parent task moved TAG to IT wholesale. This is about keeping a bit of ACID coverage going in UT. Jon offered to refactor TAG so can live in IT and UT.</description>
      <version>None</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestAcidGuarantees.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-22 01:00:00" id="12738" opendate="2014-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Chunk Ref Guide into file-per-chapter</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.upgrading.xml</file>
      <file type="M">src.main.docbkx.performance.xml</file>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">src.main.docbkx.hbase-default.xml</file>
      <file type="M">src.main.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-1-29 01:00:00" id="12768" opendate="2014-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support enable cache_data_on_write in Shell while creating table</summary>
      <description>A simple approach to support cache_data_on_write while creating table in shell.</description>
      <version>1.0.0,0.94.27,2.0.0</version>
      <fixedVersion>1.0.0,0.98.10,1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-1-30 01:00:00" id="12785" opendate="2014-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use FutureTask to timeout the attempt to get the lock for hbck</summary>
      <description>In reviewing HBASE-12607, Sean pointed out:It would be nice if we used a FutureTask to timeout the attempt to get the lock rather than wait the whole period and then fail.This issue is to address Sean's review comment.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-9 01:00:00" id="12831" opendate="2015-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changing the set of vis labels a user has access to doesn&amp;#39;t generate an audit log event</summary>
      <description>Right now, the AccessController makes sure that (when users care about audit events) we generate an audit log event for any access change, like granting or removing a permission from a user.When the set of labels a user has access to is altered, it gets handled by the VisibilityLabelService and we don't log anything to the audit log.</description>
      <version>1.0.0,0.98.6,2.0.0</version>
      <fixedVersion>1.0.0,0.98.10,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.VisibilityController.java</file>
      <file type="M">conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-3-23 01:00:00" id="1284" opendate="2009-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop table drops all disabled tables</summary>
      <description>To reproduce in the shell:create 'A'create 'B'disable 'A'disable 'B'drop 'B'enable 'A' -&gt; exception table 'A' not found</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableDelete.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-13 01:00:00" id="12848" opendate="2015-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Utilize Flash storage for WAL</summary>
      <description>One way to improve data ingestion rate is to make use of Flash storage.HDFS is doing the heavy lifting - see HDFS-7228.We assume an environment where:1. Some servers have a mix of flash, e.g. 2 flash drives and 4 traditional drives.2. Some servers have all traditional storage.3. RegionServers are deployed on both profiles within one HBase cluster.This JIRA allows WAL to be managed on flash in a mixed-profile environment.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-3-24 01:00:00" id="1285" opendate="2009-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forcing compactions should be available via thrift.</summary>
      <description>It would be useful to be able to trigger compactions via thrift just as we can with the ruby shell.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-21 01:00:00" id="12892" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a class to allow taking a snapshot from the command line</summary>
      <description>It's easier to automate taking a snapshot from the command line than from the hbase shell. We should provide a command that can be called to snapshot a table.</description>
      <version>2.0.0</version>
      <fixedVersion>1.0.0,0.98.10,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-21 01:00:00" id="12897" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimum memstore size is a percentage</summary>
      <description>We have a cluster which is optimized for random reads. Thus we have a large block cache and a small memstore. Currently our heap is 20GB and we wanted to configure the memstore to take 4% or 800MB. Right now the minimum memstore size is 5%. What do you guys think about reducing the minimum size to 1%? Suppose we log a warning if the memstore is below 5% but allow it?What do you folks think?</description>
      <version>0.98.10,1.1.0,2.0.0</version>
      <fixedVersion>1.0.0,1.1.0,0.98.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.util.HeapMemorySizeUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-3 01:00:00" id="12961" opendate="2015-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Negative values in read and write region server metrics</summary>
      <description>HMaster web page ui, shows the read/write request per region server. They are currently displayed by using 32 bit integers. Hence, if the servers are up for a long time the values can be shown as negative.</description>
      <version>2.0.0</version>
      <fixedVersion>1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestServerLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-7 01:00:00" id="12985" opendate="2015-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Javadoc warning and findbugs fixes to get us green again</summary>
      <description>A few findbugs fixes to get us under our findbugs warning number again and a javadoc warning fix.</description>
      <version>2.0.0</version>
      <fixedVersion>1.0.0,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.SplitLogCounters.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.token.ZKSecretWatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALCellCodec.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.procedure.flush.FlushTableSubprocedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.ClientSideRegionScanner.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.thrift.HttpDoAsClient.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-2-10 01:00:00" id="12999" opendate="2015-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make foreground_start return the correct exit code</summary>
      <description/>
      <version>1.1.0,0.98.11,2.0.0</version>
      <fixedVersion>1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-10 01:00:00" id="13006" opendate="2015-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document visibility label support for groups</summary>
      <description>This is to document the changes added from HBASE-12745.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.set.auths.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.get.auths.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.clear.auths.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-11 01:00:00" id="13014" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java Tool For Region Moving</summary>
      <description>As per discussion on HBASE-12989 we should move the functionality of region_mover.rb into a Java tool and use region_mover.rb only only as a wrapper around it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestRegionMover.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionMover.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-11 01:00:00" id="13016" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up remnants of table states stored in table descriptors</summary>
      <description>We didn't released 2.0 version with states in table descriptors, so we don't need to keep things compatible with lower versions. Hence it will be much better to remove state from descriptors entirely and support only migration zk &lt;-&gt; meta storage introduced in HBASE-12035.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestTableDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.TableDescriptor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableStateManager.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.TableState.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-18 01:00:00" id="13062" opendate="2015-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation coverage for configuring dns server with thrift and rest gateways</summary>
      <description>Currently, the documentation doesn't cover about configuring DNS with thrift or rest gateways, though code base does provide provision for doing so. The following parameters are being used for accomplishing the same.For REST: hbase.rest.dns.interface hbase.rest.dns.nameserverFor Thrift: hbase.thrift.dns.interface hbase.thrift.dns.nameserver</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-18 01:00:00" id="13065" opendate="2015-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increasing -Xmx when running TestDistributedLogSplitting</summary>
      <description>Found this in PreCommit Build reportshttps://builds.apache.org/job/PreCommit-HBASE-Build/12885/artifact/hbase-server/target/surefire-reports/org.apache.hadoop.hbase.master.TestDistributedLogSplitting-output.txt2015-02-18 03:45:42,141 WARN [RS:4;asf901:41265] util.Sleeper(97): We slept 59018ms instead of 1000ms, this is likely due to a long garbage collecting pause and it's usually bad, see http://hbase.apache.org/book.html#trouble.rs.runtime.zkexpired2015-02-18 03:45:26,750 WARN [JvmPauseMonitor] util.JvmPauseMonitor$Monitor(167): Detected pause in JVM or host machine (eg GC): pause of approximately 39767msGC pool 'PS MarkSweep' had collection(s): count=65 time=47720msMaybe we should increase the max heap size since this test starts 6 regionservers.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-18 01:00:00" id="13067" opendate="2015-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix caching of stubs to allow IP address changes of restarted remote servers</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-20 01:00:00" id="13081" opendate="2015-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Branch precommit builds are not updating to branch head before patch application</summary>
      <description>See for example https://builds.apache.org/job/PreCommit-HBASE-Build/12922//consolegit checkout 0.98Previous HEAD position was 03d8918... HBASE-13069 Thrift Http Server returns an error code of 500 instead of 401 when authentication fails (Srikanth Srungarapu)Switched to branch '0.98'Your branch is behind 'origin/0.98' by 48 commits, and can be fast-forwarded. (use "git pull" to update your local branch)git statusOn branch 0.98Your branch is behind 'origin/0.98' by 48 commits, and can be fast-forwarded. (use "git pull" to update your local branch)Untracked files: (use "git add &lt;file&gt;..." to include in what will be committed) patchprocess/nothing added to commit but untracked files present (use "git add" to track)Because the local tree is 48 commits behind the head of the 0.98 branch, the contributor's patch based on the head of 0.98 branch cannot cleanly apply.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-23 01:00:00" id="13086" opendate="2015-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show ZK root node on Master WebUI</summary>
      <description>Currently we show a well-formed ZK quorum on the master webUI but not the root node. Root node can be changed based on deployment, so we should list it here explicitly. This information is helpful for folks playing around with phoenix.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-26 01:00:00" id="13111" opendate="2015-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>truncate_preserve command is failing with undefined method error</summary>
      <description>hbase(main):001:0&gt; truncate_preserve 't1'Truncating 't1' table (it may take a while):ERROR: undefined method `getTable' for nil:NilClassHere is some help for this command: Disables, drops and recreates the specified table while still maintaing the previous region boundaries.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-27 01:00:00" id="13123" opendate="2015-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minor bug in ROW bloom filter</summary>
      <description>While checking the code for Bloom filter found that while checking if a key passes the ROW bloom check we try to create a bloom key. The bloom key should be constructed only with the row part of the key. But try to form the bloom key including the meta data part of the key.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-27 01:00:00" id="13127" opendate="2015-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add timeouts on all tests so less zombie sightings</summary>
      <description>Apache9 and octo47 have been working hard at trying to get our builds passing again. They are almost there. TRUNK just failed with a zombie TestMasterObserver. Help the lads out by adding timeouts on all tests so less zombie incidence... will help identify the frequent failing issues.</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-2 01:00:00" id="13135" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move replication ops mgmt stuff from Javadoc to Ref Guide</summary>
      <description>As per discussion with jmhsieh and saint.ack@gmail.com</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">hbase-server.src.main.javadoc.org.apache.hadoop.hbase.replication.package.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-3 01:00:00" id="13149" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase MR is broken on Hadoop 2.5+ Yarn</summary>
      <description>Running the server MR tools is not working on Yarn version 2.5+.Running org.apache.hadoop.hbase.mapreduce.Export:Exception in thread "main" java.lang.NoSuchMethodError: org.codehaus.jackson.map.ObjectMapper.setSerializationInclusion(Lorg/codehaus/jackson/map/annotate/JsonSerialize$Inclusion;)Lorg/codehaus/jackson/map/ObjectMapper; at org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider.configObjectMapper(YarnJacksonJaxbJsonProvider.java:59) at org.apache.hadoop.yarn.util.timeline.TimelineUtils.&lt;clinit&gt;(TimelineUtils.java:47) at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:166) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) at org.apache.hadoop.mapred.ResourceMgrDelegate.serviceInit(ResourceMgrDelegate.java:102) at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) at org.apache.hadoop.mapred.ResourceMgrDelegate.&lt;init&gt;(ResourceMgrDelegate.java:96) at org.apache.hadoop.mapred.YARNRunner.&lt;init&gt;(YARNRunner.java:112) at org.apache.hadoop.mapred.YarnClientProtocolProvider.create(YarnClientProtocolProvider.java:34) at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:95) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:82) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:75) at org.apache.hadoop.mapreduce.Job$9.run(Job.java:1266) at org.apache.hadoop.mapreduce.Job$9.run(Job.java:1262) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:415) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628) at org.apache.hadoop.mapreduce.Job.connect(Job.java:1261) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1290) at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314) at org.apache.hadoop.hbase.mapreduce.Export.main(Export.java:189)The problem seems to be the jackson jar version. HADOOP-10104 updated jackson version to 1.9.13. YARN-2092 reported a problem as well.HBase is using jackson 1.8.8. This version of the jar in the classpath seem to cause the problem.Should we upgrade to jackson 1.9.13?</description>
      <version>1.0.0,0.98.10.1,2.0.0</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-5 01:00:00" id="13156" opendate="2015-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix minor rat violation recently introduced (asciidoctor.css).</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-8 01:00:00" id="13177" opendate="2015-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document HBASE-13012</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hbase-11339,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase.mob.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-11 01:00:00" id="13196" opendate="2015-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add info about default number of versions when using the export tool</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-3-11 01:00:00" id="13206" opendate="2015-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TableLock tableName log format</summary>
      <description>The TableName log from Table Lock result in something like:[tableName=^Gdefault^R^Cfoo, lockOwner=localhost,60000because it uses tableNameProto.toByteArray()the fix will result in the proper view[tableName=default:testMissingLastRegion, lockOwner=localh</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.hbck.TableLockChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-11 01:00:00" id="13208" opendate="2015-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Patch build should match the patch filename and not the whole relative URL in findBranchNameFromPatchName</summary>
      <description>In HBASE-1319 we saw that the patch got applied to the wrong branch, the problem is findBranchNameFromPatchName matching a regex that contains wildcard symbols against the whole URL, in this case the regex is 0.94 and the relativePatchURL is /jira/secure/attachment/12703942/HBASE-13193-v4.patch where 0394 is a match.Thanks to jonathan.lawlor for reporting this.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-3-11 01:00:00" id="13213" opendate="2015-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split out locality metrics among primary and secondary region</summary>
      <description>This task provides the ability to track locality of primary and secondary region replicas.We already have percentFilesLocal metric.There should be two sets of metrics - one for primaries and another for secondary / tertiary regions.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-12 01:00:00" id="13218" opendate="2015-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct the syntax shown for using ExportSnapshot tool in the book</summary>
      <description>It is $ bin/hbase class org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16It should be$ bin/hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-13 01:00:00" id="13226" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document enable_table_replication and disable_table_replication shell commands</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="13227" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadIncrementalHFile should skip non-files inside a possible family-dir</summary>
      <description>if we have random files/dirs inside the bulkload family dir, we should try to skip them.</description>
      <version>1.0.0,1.1.0,0.98.11,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="13228" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create procedure v2 branch and add it to QA branch list</summary>
      <description>to develop Procedure V2 quickly, we are going to commit stuff to an hbase-12439 branch.In theory we can have QA running if the patch name is HBASE-xyz-hbase-12439.patch</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="13232" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConnectionManger : Batch pool threads and metaLookup pool threads should use different name pattern</summary>
      <description>This is a small issue happened with HBASE-13036. Passing different names to getThreadPool as nameHint but it is not been used. By checking HBASE-13219 found this small issue.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="13233" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hbase-11339 branch to the patch testing script</summary>
      <description>adding hbase-11339 to the BRANCH_NAMES so we can use the apache bot to test patches on that branch.</description>
      <version>hbase-11339,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-13 01:00:00" id="13234" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the obviousness of the download link on hbase.apache.org</summary>
      <description>Update the hbase.apache.org homepage to include a very obvious section describing how a user can "Download HBase Software Here" with a link.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.index.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-13 01:00:00" id="13237" opendate="2015-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve trademark marks on the hbase.apache.org homepage</summary>
      <description>Ensure trademark marks are next to first and prominent uses of "HBase" on the hbase.apache.org homepage</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.index.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-15 01:00:00" id="13244" opendate="2015-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test delegation token generation with kerberos enabled</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.HBaseKerberosUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-16 01:00:00" id="13255" opendate="2015-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bad grammar in RegionServer status page</summary>
      <description>Noticed on the rs-status page, the blurb under the Regions section could use some grammatical improvements.</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-17 01:00:00" id="13265" opendate="2015-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make thrift2 usable from c++</summary>
      <description>Currently the c++ code generated from our thrift2 idl doesn't compile. Mostly this is a naming issue for parameters.</description>
      <version>1.0.0,1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-19 01:00:00" id="13281" opendate="2015-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;hbase.bucketcache.size&amp;#39; description in hbase book is not correct</summary>
      <description>The description about 'hbase.bucketcache.size' is not correct. We either specify it as a float or in MB's and the default value that is mentioned is never used.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-19 01:00:00" id="13289" opendate="2015-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>typo in splitSuccessCount metric</summary>
      <description>Our split metrics have a misspelled Count and it shows up in our jmx metrics "splitSuccessCounnt" : 0,</description>
      <version>1.0.0,0.98.10,1.1.0,0.98.11,0.98.12,0.98.10.1,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-21 01:00:00" id="13309" opendate="2015-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some tests do not reset EnvironmentEdgeManager</summary>
      <description>while playing with client timeouts, I hit lots of flakys in HBaseFsck. the cause was just an EnvironmentEdge.inject() not followed by a reset().looks like TestFsUtils and TestHRegion are the only other two that have a missing reset(). all the other tests using inject() seems to be written properly.</description>
      <version>None</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-4-20 01:00:00" id="1331" opendate="2009-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lower the default scanner caching value</summary>
      <description>The current default value for the scanner caching is 30; this causing headaches to many new users who may use a row from a scanner for more than 60 seconds. Let's set it at 1, then folks will be able to optimize.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-3-24 01:00:00" id="13325" opendate="2015-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Protocol Buffers 2.5 no longer available for download on code.google.com</summary>
      <description>Same as HADOOP-11738Google recently switched off Google Code. They transferred the Protocol Buffers project to GitHub, and binaries are available from Google's developer page. However, only the most recent version is available. We use version 2.5 to be compatible with Hadoop. That version isn't available for download.Let the fun begin</description>
      <version>2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.rpc.adoc</file>
      <file type="M">hbase-protocol.README.txt</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.package.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-24 01:00:00" id="13328" opendate="2015-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadIncrementalHFile.doBulkLoad(Path,HTable) should handle managed connections</summary>
      <description>This seems to be a regression from HBASE-12783 discovered in testing Phoenix with 1.1.0-SNAPSHOT. Phoenix uses an HTable (with managed connection) to pass to doBulkLoad() which throws NeedUnmanagedConnectionException (see IndexToolIT.java and IndexTool.java in Phoenix).</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-26 01:00:00" id="13337" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table regions are not assigning back, after restarting all regionservers at once.</summary>
      <description>Regions of the table are continouly in state=FAILED_CLOSE.Region State RIT time (ms)8f62e819b356736053e06240f7f7c6fd t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd. state=FAILED_CLOSE, ts=Thu Mar 26 15:05:36 IST 2015 (113s ago), server=VM1,16040,1427362531818 113929caf59209ae65ea80fca6bdc6996a7d68 t1,dddddddd,1427362431330.caf59209ae65ea80fca6bdc6996a7d68. state=FAILED_CLOSE, ts=Thu Mar 26 15:05:36 IST 2015 (113s ago), server=VM2,16040,1427362533691 113929db52a74988f71e5cf257bbabf31f26f3 t1,44444444,1427362431330.db52a74988f71e5cf257bbabf31f26f3. state=FAILED_CLOSE, ts=Thu Mar 26 15:05:36 IST 2015 (113s ago), server=VM3,16040,1427362533691 11392043f3a65b9f9ff283f598c5450feab1f8 t1,88888888,1427362431330.43f3a65b9f9ff283f598c5450feab1f8. state=FAILED_CLOSE, ts=Thu Mar 26 15:05:36 IST 2015 (113s ago), server=VM1,16040,1427362531818 113920Steps to reproduce:1. Start HBase cluster with more than one regionserver.2. Create a table with precreated regions. (lets say 15 regions)3. Make sure the regions are well balanced.4. Restart all the Regionservers process at once across the cluster, except HMaster process5. After restarting the Regionservers, successfully will connect to the HMaster.Bug:But no regions are assigning back to the Regionservers.Master log shows as follows:2015-03-26 15:05:36,201 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStates: Transition {8f62e819b356736053e06240f7f7c6fd state=OFFLINE, ts=1427362536106, server=VM2,16040,1427362242602} to {8f62e819b356736053e06240f7f7c6fd state=PENDING_OPEN, ts=1427362536201, server=VM1,16040,1427362531818}2015-03-26 15:05:36,202 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStateStore: Updating row t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd. with state=PENDING_OPEN&amp;sn=VM1,16040,14273625318182015-03-26 15:05:36,244 DEBUG [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Force region state offline {8f62e819b356736053e06240f7f7c6fd state=PENDING_OPEN, ts=1427362536201, server=VM1,16040,1427362531818}2015-03-26 15:05:36,244 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStates: Transition {8f62e819b356736053e06240f7f7c6fd state=PENDING_OPEN, ts=1427362536201, server=VM1,16040,1427362531818} to {8f62e819b356736053e06240f7f7c6fd state=PENDING_CLOSE, ts=1427362536244, server=VM1,16040,1427362531818}2015-03-26 15:05:36,244 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStateStore: Updating row t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd. with state=PENDING_CLOSE2015-03-26 15:05:36,248 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=1 of 102015-03-26 15:05:36,248 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=2 of 102015-03-26 15:05:36,249 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=3 of 102015-03-26 15:05:36,249 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=4 of 102015-03-26 15:05:36,249 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=5 of 102015-03-26 15:05:36,250 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=6 of 102015-03-26 15:05:36,250 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=7 of 102015-03-26 15:05:36,250 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=8 of 102015-03-26 15:05:36,251 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=9 of 102015-03-26 15:05:36,251 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.AssignmentManager: Server VM1,16040,1427362531818 returned java.nio.channels.ClosedChannelException for t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd., try=10 of 102015-03-26 15:05:36,251 WARN [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStates: Failed to open/close 8f62e819b356736053e06240f7f7c6fd on VM1,16040,1427362531818, set to FAILED_CLOSE2015-03-26 15:05:36,251 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStates: Transition {8f62e819b356736053e06240f7f7c6fd state=PENDING_CLOSE, ts=1427362536244, server=VM1,16040,1427362531818} to {8f62e819b356736053e06240f7f7c6fd state=FAILED_CLOSE, ts=1427362536251, server=VM1,16040,1427362531818}2015-03-26 15:05:36,251 INFO [VM2,16020,1427362216887-GeneralBulkAssigner-0] master.RegionStateStore: Updating row t1,55555555,1427362431330.8f62e819b356736053e06240f7f7c6fd. with state=FAILED_CLOSE</description>
      <version>2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.AsyncRpcClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-26 01:00:00" id="13339" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default Hadoop version to latest for master</summary>
      <description>Current default Hadoop version is getting a little long in the tooth. We should update to the latest version. The latest version is backwards compatible with 2.5.1's dfs and mr so this should be painless.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">dev-support.test-patch.properties</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-26 01:00:00" id="13342" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix incorrect interface annotations</summary>
      <description>Still some old annotations. Have slipped in. Lets remove them and add in a patch check for them.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.ReplayHLogKey.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.RegionReplicaFlushHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RegionStateListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceTableAndRegionInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.namespace.NamespaceStateManager.java</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-26 01:00:00" id="13344" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add enforcer rule that matches our JDK support statement</summary>
      <description>The ref guide gives a list of JDKs that we expect our hbase versions to work with at runtime.Let's add in the extra-enforcer-rules mojo and start using the bytecode version rule to make sure that the result of our builds on a given branch won't fail out because of a misconfigured target jdk version (or a dependency that targets a later jdk).</description>
      <version>2.0.0</version>
      <fixedVersion>0.94.28,0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-28 01:00:00" id="13355" opendate="2015-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QA bot reports checking javac twice</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-30 01:00:00" id="13364" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make using the default javac on by default</summary>
      <description>Errorprone doesn't work with java 8 and java 8 is becoming more and more standard everywhere.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="13366" opendate="2015-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Throw DoNotRetryIOException instead of read only IOException</summary>
      <description>Currently, the read only region just throws an IOException to the clients who send write requests to it. This will cause the clients retry for configured times or until operation timeout.Changing this exception to DoNotRetryIOException will make the client failed fast.Suggestions are welcomed~ Thanks</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-1 01:00:00" id="13372" opendate="2015-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit tests for SplitTransaction and RegionMergeTransaction listeners</summary>
      <description>We have new Listener interfaces in SplitTransaction and RegionMergeTransaction. There are no use cases for these yet, nor unit tests. We should have unit tests for these that do something just a bit nontrivial so as to provide a useful example.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionMergeTransaction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-1 01:00:00" id="13374" opendate="2015-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small scanners (with particular configurations) do not return all rows</summary>
      <description>I recently ran into a couple data loss issues with small scans. Similar to HBASE-13262, these issues only appear when scans are configured in such a way that the max result size limit is reached before the caching limit is reached. As far as I can tell, this issue affects branches 0.98+I should note that after investigation it looks like the root cause of these issues is not the same as HBASE-13262. Rather, these issue are caused by errors in the small scanner logic (I will explain in more depth below). Furthermore, I do know that the solution from HBASE-13262 has not made its way into small scanners (it is being addressed in HBASE-13335). As a result I made sure to test these issues with the patch from HBASE-13335 applied and I saw that they were still present.The following two issues have been observed (both lead to data loss):1. When a small scan is configured with a caching value of Integer.MAX_VALUE, and a maxResultSize limit that is reached before the region is exhausted, integer overflow will occur. This eventually leads to a preemptive skip of the regions.2. When a small scan is configured with a maxResultSize that is smaller than the size of a single row, the small scanner will jump between regions preemptively. This issue seems to be because small scanners assume that, unless a region is exhausted, at least 2 rows will be returned from the server. This assumption isn't clearly state in the small scanners but is implied through the use of skipRowOfFirstResult.Again, I would like to stress that the root cause of these issues is NOT related to the cause of HBASE-13262. These issues occur because of inappropriate assumption made in the small scanner logic. The inappropriate assumptions are:1. Integer overflow will not occur when incrementing caching2. At least 2 rows will be returned from the server unless the region has been exhaustedI am attaching a patch that contains tests to display these issues. If these issues should be split into separate JIRAs please let me know.</description>
      <version>1.0.0,1.1.0,0.98.13,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSmallReversedScanner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-3 01:00:00" id="13398" opendate="2015-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document HBase Quota</summary>
      <description>As part of this we should document HBASE-11598 and HBASE-8410</description>
      <version>None</version>
      <fixedVersion>1.1.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="13409" opendate="2015-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add categories to uncategorized tests</summary>
      <description>A couple tests without categories were flagged recently by TestCheckTestClasses in a precommit build.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.0.1,1.1.0,0.98.12,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.filter.TestLongComparator.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientExponentialBackoff.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-23 01:00:00" id="1341" opendate="2009-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HTable Pooler</summary>
      <description>A client class that takes care of properly pooling HTable references for use in multi-threaded, low-latency Java clients.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="13411" opendate="2015-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Misleading error message when request size quota limit exceeds</summary>
      <description>User will get the same error message when either number of requests exceeds or request size exceeds. So its better we differentiate them.Thanks to mbertozzi for confirming the same offline.</description>
      <version>2.0.0</version>
      <fixedVersion>1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.TimeBasedLimiter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.ThrottlingException.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="13412" opendate="2015-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region split decisions should have jitter</summary>
      <description>Whenever a region splits it causes lots of IO (compactions are queued for a while). Because of this it's important to make sure that well distributed tables don't have all of their regions split at exactly the same time.This is basically the same as our compaction jitter.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.13,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="13413" opendate="2015-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create an integration test for Replication</summary>
      <description>We want to have an end-to-end test for replication. it can write data into one cluster (with replication setup) and then read data from the other. The test should be capable of running for a long time and be reliant even under chaos monkey testing.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.13,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-8 01:00:00" id="13425" opendate="2015-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation nit in REST Gateway impersonation section</summary>
      <description>In section "55.8. REST Gateway Impersonation Configuration", there is another property that needs to be set (and thus documented).After this sentence ("To enable REST gateway impersonation, add the following to the hbase-site.xml file for every REST gateway."), we should add :&lt;property&gt; &lt;name&gt;hbase.rest.support.proxyuser&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;It not set, doing a curl call on the rest gateway gives the error "support for proxyuser is not configured".</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-4-15 01:00:00" id="13477" opendate="2015-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metrics on failed requests</summary>
      <description>Add a metric on how many requests failed/errored out.</description>
      <version>1.0.0,2.0.0</version>
      <fixedVersion>1.1.0,0.98.13,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestRpcMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServer.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.ipc.MetricsHBaseServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-15 01:00:00" id="13478" opendate="2015-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the change of default master ports being used .</summary>
      <description>In 1.0.x, master by default binds to the region server ports. But in 1.1 and 2.0 branches, we have undone this changes and brought back the usage of old master ports to make the migration from 0.98 -&gt; 1.1 hassle free. Please see the parent jira for more background.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-15 01:00:00" id="13483" opendate="2015-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[docs] onheap is not a valid bucket cache IO engine.</summary>
      <description>From the HBase book: http://hbase.apache.org/book.html#hbase_default_configurations:hbase.bucketcache.ioengineDescriptionWhere to store the contents of the bucketcache. One of: *onheap*, offheap, or file. If a file, set it to file:PATH_TO_FILE. See https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/io/hfile/CacheConfig.html for more information.Instead of onheap it should be heap.</description>
      <version>2.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-16 01:00:00" id="13487" opendate="2015-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Doc KEEP_DELETED_CELLS</summary>
      <description>Let me doc this nice feature that has been around a long time but is not explained other than in Lars lectures. I was talking to someone and could not explain this feature myself until Lars set me straight.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.create.rb</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-4-24 01:00:00" id="13554" opendate="2015-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book clarifying API stability guarantees</summary>
      <description>From the "Clarifying interface evolution freedom in patch releases" thread on dev@h.a.oSeems we have consensus that "HBase uses Semantic Versioning" isn't quite correct (or desired) at the moment. Update the documentation to make sure we're not misrepresenting any guarantees to users.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-25 01:00:00" id="13563" opendate="2015-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing table owner to AC tests.</summary>
      <description>As per the description.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-28 01:00:00" id="13586" opendate="2015-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update book on Hadoop and Java supported versions for 1.1.x</summary>
      <description>Should update http://hbase.apache.org/book.html#basic.prerequisites with the latest info.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-30 01:00:00" id="1359" opendate="2009-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>After a large truncating table HBase becomes unresponsive</summary>
      <description>If you see **** I removed and ip or something for security reasonsOnce I truncate the table, hbase freaks out for about 10 seconds and all the thrift servers die.Thrift server log:2009-04-02 12:09:08,971 INFO org.apache.hadoop.ipc.HBaseClass: Retrying connect to server: /*****:60020. Already tried 0 time(s).You see this a bunch of times and then it times outThe hbase shellnhbase(main):001:0&gt; truncate 't2'09/04/30 13:01:08 INFO zookeeper.ZooKeeperWrapper: Quorum servers: ****Truncating t2; it may take a whileDisabling table...09/04/30 13:01:19 INFO client.HBaseAdmin: Disabled t20 row(s) in 10.3417 secondsDropping table...09/04/30 13:01:19 INFO client.HBaseAdmin: Deleted t20 row(s) in 0.1592 secondsCreating table...0 row(s) in 14.7567 secondshbase(main):002:0&gt; lsitNameError: undefined local variable or method `lsit' for #&lt;Object:0x3bbe9a50&gt; from (hbase):3hbase(main):003:0&gt; lsitNameError: undefined local variable or method `lsit' for #&lt;Object:0x3bbe9a50&gt; from (hbase):4hbase(main):004:0&gt; listNativeException: java.lang.NullPointerException: null from org/apache/hadoop/hbase/client/HConnectionManager.java:344:in `processRow' from org/apache/hadoop/hbase/client/MetaScanner.java:64:in `metaScan' from org/apache/hadoop/hbase/client/MetaScanner.java:29:in `metaScan' from org/apache/hadoop/hbase/client/HConnectionManager.java:351:in `listTables' from org/apache/hadoop/hbase/client/HBaseAdmin.java:121:in `listTables' from sun/reflect/NativeMethodAccessorImpl.java:-2:in `invoke0' from sun/reflect/NativeMethodAccessorImpl.java:39:in `invoke' from sun/reflect/DelegatingMethodAccessorImpl.java:25:in `invoke' from java/lang/reflect/Method.java:597:in `invoke' from org/jruby/javasupport/JavaMethod.java:298:in `invokeWithExceptionHandling' from org/jruby/javasupport/JavaMethod.java:259:in `invoke' from org/jruby/java/invokers/InstanceMethodInvoker.java:36:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCall' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from org/jruby/ast/CallNoArgNode.java:61:in `interpret' from org/jruby/ast/ForNode.java:101:in `interpret'... 113 levels... from org/jruby/internal/runtime/methods/DynamicMethod.java:226:in `call' from org/jruby/internal/runtime/methods/CompiledMethod.java:216:in `call' from org/jruby/internal/runtime/methods/CompiledMethod.java:71:in `call' from org/jruby/runtime/callsite/CachingCallSite.java:260:in `cacheAndCall' from org/jruby/runtime/callsite/CachingCallSite.java:75:in `call' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:441:in `_file_' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `_file_' from home/fds/ts/hadoop/hbase/bin/$dot_dot/bin/hirb.rb:-1:in `load' from org/jruby/Ruby.java:564:in `runScript' from org/jruby/Ruby.java:467:in `runNormally' from org/jruby/Ruby.java:340:in `runFromMain' from org/jruby/Main.java:214:in `run' from org/jruby/Main.java:100:in `run' from org/jruby/Main.java:84:in `main' from /home/fds/ts/hadoop/hbase/bin/../bin/hirb.rb:300:in `list' from (hbase):5hbase(main):005:0&gt;hbase(main):006:0*</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-5-15 01:00:00" id="13694" opendate="2015-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CallQueueSize is incorrectly decremented until the response is sent</summary>
      <description>We should decrement the CallQueueSize as soon as we no longer need the call around, e.g. after RpcServer.CurCall.set(null) otherwise we will be only pushing back other client requests while we send the response back to the client that originated the call.</description>
      <version>1.1.0,0.98.12,1.0.2,1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-15 01:00:00" id="13699" opendate="2015-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expand information about HBase quotas</summary>
      <description>See HBASE-13398 and http://blog.cloudera.com/blog/2014/12/new-in-cdh-5-2-improvements-for-running-multiple-workloads-on-a-single-hbase-cluster/.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-1-5 01:00:00" id="1373" opendate="2009-5-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to use compact/framed protocol</summary>
      <description>TCompactProtocol/TFramedTransport and nonblocking server option promises better efficiency and performance improvements. Consider moving HBase Thrift bits to this when full platform support is ready for TCompactProtocol.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.package.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-22 01:00:00" id="13746" opendate="2015-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>list_replicated_tables command is not listing table in hbase shell.</summary>
      <description>IN HBase shell prompt execute the following commandlist_replicated_tableshbase(main):014:0&gt; list_replicated_tablesTABLE:COLUMNFAMILY ReplicationTypeERROR: undefined method `TNAME' for Java::OrgApacheHadoopHbaseClientReplication::ReplicationAdmin:ClassHere is some help for this command:List all the tables and column families replicated from this cluster hbase&gt; list_replicated_tables hbase&gt; list_replicated_tables 'abc.*' list.select {|s| pattern.match(s.get(ReplicationAdmin.TNAME))}</description>
      <version>1.1.0,0.98.13,1.0.2,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.replication.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-24 01:00:00" id="13760" opendate="2015-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup Findbugs keySet iterator warnings</summary>
      <description>Cleanup Findbugs keySet iterator warnings</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-25 01:00:00" id="13767" opendate="2015-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ZKAclReset to set and not just clear ZK ACLs</summary>
      <description>The ZKAclReset tool allows to clear ZK ACLs, which is useful if you are migrating from a secure to unsecure cluster setup.If you want to make sure that your znode ACLs are correct, a -set-acls option, which allows to enforce the proper ACLs on the znodes in a secure setup, can be useful too.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZkAclReset.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-25 01:00:00" id="13768" opendate="2015-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znodes are bootstrapped with insecure ACLs in a secure configuration</summary>
      <description>A logic error causes HBase in most secure configuration deployments to handle its coordination state in ZooKeeper via insecure ACLs. Anyone with remote unauthenticated network access to the ZooKeeper quorum, which by definition includes all HBase clients, can make use of this opening to violate the operational integrity of the system. For example, critical znodes can be deleted, causing outages. It is possible to introduce rogue replication endpoints. It is possible to direct the distributed log splitting facility to split arbitrary files in HDFS.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,0.98.12.1,1.0.1.1,1.1.0.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-6 01:00:00" id="1377" opendate="2009-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS address is null in master web UI</summary>
      <description>My patch in HBASE-1279 was targeted for branch 0.19 and was missing a line to make it work for trunk in the copy constructor of HSI.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-26 01:00:00" id="13776" opendate="2015-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Setting illegal versions for HColumnDescriptor does not throw IllegalArgumentException</summary>
      <description>HColumnDescriptor hcd = new HColumnDescriptor( new HColumnDescriptor(HConstants.CATALOG_FAMILY) .setInMemory(true) .setScope(HConstants.REPLICATION_SCOPE_LOCAL) .setBloomFilterType(BloomType.NONE) .setCacheDataInL1(true)); final int minVersions = 123; final int maxVersions = 234; hcd.setMaxVersions(minVersions); hcd.setMinVersions(maxVersions);//no exception throw</description>
      <version>0.98.14,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-28 01:00:00" id="13801" opendate="2015-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop src checksum is shown instead of HBase src checksum in master / RS UI</summary>
      <description>Simple bug. We are showing the Hadoop's source MD5 checksum in the master UI instead of the HBase's one.</description>
      <version>None</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-6-2 01:00:00" id="13826" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unable to create table when group acls are appropriately set.</summary>
      <description>Steps for reproducing the issue. Create user 'test' and group 'hbase-admin'. Grant global create permissions to 'hbase-admin'. Add user 'test' to 'hbase-admin' group. Create table operation for 'test' user will throw ADE.</description>
      <version>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</version>
      <fixedVersion>0.98.13,1.0.2,1.2.0,1.1.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.TableAuthManager.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-8 01:00:00" id="13866" opendate="2015-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add endpoint coprocessor to the section hbase.coprocessor.region.classes in HBase book</summary>
      <description>hbase.coprocessor.region.classes Description A comma-separated list of Coprocessors that are loaded by default on all tables. For any override coprocessor method, these classes will be called in order. After implementing your own Coprocessor, just put it in HBases classpath and add the fully qualified class name here. A coprocessor can also be loaded on demand by setting HTableDescriptor.This must be more specific: not Coprocessors, but Region observers and endpoint coprocessors.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-8 01:00:00" id="13867" opendate="2015-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add endpoint coprocessor guide to HBase book</summary>
      <description>Endpoint coprocessors are very poorly documented.Coprocessor section of HBase book must be updated either with its own endpoint coprocessors HOW-TO guide or, at least, with the link(s) to some other guides. There is good description here:http://www.3pillarglobal.com/insights/hbase-coprocessors</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-8 01:00:00" id="13868" opendate="2015-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct "Disable automatic splitting" section in HBase book</summary>
      <description>This recommendation is not correct for IncreasingToUpperBoundRegionSplitPolicy (which is default now)Disable Automatic SplittingTo disable automatic splitting, set hbase.hregion.max.filesize to a very large value, such as 100 GB It is not recommended to set it to its absolute maximum value of Long.MAX_VALUE.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-9 01:00:00" id="13869" opendate="2015-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo in HBase book</summary>
      <description>Typo in section's title:42.1.4. Variangle Length or Fixed Length Rowkeys</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-7 01:00:00" id="1387" opendate="2009-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Before release verify all object sizes using Ryans&amp;#39; instrumented JVM trick</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLogKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.HeapSize.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-10 01:00:00" id="13883" opendate="2015-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Memstore Flush section in HBase book</summary>
      <description>65.7.2. MemStore FlushA MemStore flush can be triggered under any of the conditions listed below. The minimum flush unit is per region, not at individual MemStore level. // SKIPPED 3. When the number of WAL per region server reaches the value specified in hbase.regionserver.max.logs, MemStores from various regions will be flushed out to disk to reduce WAL count. The flush order is based on time. Regions with the oldest MemStores are flushed first until WAL count drops below hbase.regionserver.max.logs.Section 3. requires clarification (reference to HBase version which supports this). Is it MultiWAL feature in 1.0?</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="13884" opendate="2015-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Compactions section in HBase book</summary>
      <description>http://hbase.apache.org/book.html#_compactionBeing StuckWhen the MemStore gets too large, it needs to flush its contents to a StoreFile. However, a Store can only have hbase.hstore.blockingStoreFiles files, so the MemStore needs to wait for the number of StoreFiles to be reduced by one or more compactions. However, if the MemStore grows larger than hbase.hregion.memstore.flush.size, it is not able to flush its contents to a StoreFile. If the MemStore is too large and the number of StoreFiles is also too high, the algorithm is said to be "stuck". The compaction algorithm checks for this "stuck" situation and provides mechanisms to alleviate it.According to source code, this "stuck" situation has nothingg to do with MemStore size. // Stuck and not compacting enough (estimate). It is not guaranteed that we will be // able to compact more if stuck and compacting, because ratio policy excludes some // non-compacting files from consideration during compaction (see getCurrentEligibleFiles). int futureFiles = filesCompacting.isEmpty() ? 0 : 1; boolean mayBeStuck = (candidateFiles.size() - filesCompacting.size() + futureFiles) &gt;= storeConfigInfo.getBlockingFileCount();If the number of store files which are not being compacted yet exceeds blocking file count +(potentially)1 - we say that compaction may be stuck.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-14 01:00:00" id="13899" opendate="2015-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jacoco instrumentation fails under jdk8</summary>
      <description>Moving the post-commit build for master to also cover jdk8 shows failures when attempting to instrument test for jacoco coverage.example: Exception in thread "main" java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:483) at sun.instrument.InstrumentationImpl.loadClassAndStartAgent(InstrumentationImpl.java:386) at sun.instrument.InstrumentationImpl.loadClassAndCallPremain(InstrumentationImpl.java:401)Caused by: java.lang.RuntimeException: Class java/util/UUID could not be instrumented. at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:138) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:99) at org.jacoco.agent.rt.internal_5d10cad.PreMain.createRuntime(PreMain.java:51) at org.jacoco.agent.rt.internal_5d10cad.PreMain.premain(PreMain.java:43) ... 6 moreCaused by: java.lang.NoSuchFieldException: $jacocoAccess at java.lang.Class.getField(Class.java:1695) at org.jacoco.agent.rt.internal_5d10cad.core.runtime.ModifiedSystemClassRuntime.createFor(ModifiedSystemClassRuntime.java:136) ... 9 moreFATAL ERROR in native method: processing of -javaagent failedAborted</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="13913" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RAT exclusion list missing asciidoctor support files</summary>
      <description>Add src/main/asciidoc/asciidoctor.css to RAT exclusion list in POM</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="13915" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove EOL HBase versions from java and hadoop prereq tables</summary>
      <description>We don't need to know what Hadoop and Java versions we recommend / test one EOL versions in the current docs. That information is still in the old releases.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="13916" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create MultiByteBuffer an aggregation of ByteBuffers</summary>
      <description>This is an aggregation of N ByteBuffers. The block when served directly by block cache buckets memory, we have the block data split across multiple byte buffers. This aggregate type (like ByteBuffer) will serve the HFileBlock data.This jira wil just provide the new data structure</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestByteBufferUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="13917" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove string comparison to identify request priority</summary>
      <description>We have a couple of if (methodName.equalsIgnoreCase("scan")) { ScanRequest req = (ScanRequest)parm; } we can replace that string comparison with an instanceof</description>
      <version>None</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AnnotationReadingPriorityFunction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RWQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-18 01:00:00" id="13930" opendate="2015-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude Findbugs packages from shaded jars</summary>
      <description>Looking at 1.1.1RC0 shaded artifacts, looks like classes from find bugs are under the edu prefix and are not shaded. We should exclude find bugs from the shaded builds, and/or shade shade the edu prefix as well.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-23 01:00:00" id="13950" opendate="2015-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a NoopProcedureStore for testing</summary>
      <description>Add a NoopProcedureStore and an helper in ProcedureTestingUtil to submitAndWait() a procedure without having to do anything else.This is useful to avoid extra code like in case of TestAssignmentManager.processServerShutdownHandler()</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-23 01:00:00" id="13956" opendate="2015-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add myself as 1.1 release manager</summary>
      <description>Just saw we have an RM section. Add myself.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-9 01:00:00" id="1399" opendate="2009-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cant delete tables since 1398</summary>
      <description>since my patch, I cant drop tables.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-2 01:00:00" id="14013" opendate="2015-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry when RegionServerNotYetRunningException rather than go ahead with assign so for sure we don&amp;#39;t skip WAL replay</summary>
      <description>Patches are copied from parent. They were done by enis +1 from. They continue the theme of the parent applying it to RegionServerNotYetRunningException as well as the new region aborting exception .. added in parent issue.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-7-14 01:00:00" id="14073" opendate="2015-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestRemoteTable.testDelete failed in the latest trunk code</summary>
      <description>TestRemoteTable.testDelete failed in the latest trunk code."excepted null, but was: &lt;B@615c4156&gt;"</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-7-15 01:00:00" id="14086" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-15 01:00:00" id="14087" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-7-16 01:00:00" id="14100" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix high priority findbugs warnings</summary>
      <description>See here:https://builds.apache.org/job/HBase-TRUNK/6654/findbugsResult/HIGH/We have 6 high priority findbugs warnings. A high priority findbugs warning is usually a bug.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.BloomFilterUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-16 01:00:00" id="14102" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add thank you to our thanks page for vectorportal.com</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-7-26 01:00:00" id="14156" opendate="2015-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test failure in TestOpenTableInCoprocessor</summary>
      <description>This is after HBASE-12295 went in</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-28 01:00:00" id="14158" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for Initial Release for HBase-Spark Module integration</summary>
      <description>Add documentation for Initial Release for HBase-Spark Module integration</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc.book.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-28 01:00:00" id="14162" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixing maven target for regenerating thrift classes fails against 0.9.2</summary>
      <description>HBASE-14045 updated the thrift version, but our enforcer rule is still checking 0.9.0.$ git checkout masterSwitched to branch 'master'Your branch is up-to-date with 'origin/master'.$ mvn compile -Pcompile-thrift -DskipTests[INFO] Scanning for projects...... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Building HBase - Thrift 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce) @ hbase-thrift ---[INFO] [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-thrift-version) @ hbase-thrift ---[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireProperty failed with message:--[FATAL] ==========================================================================================[FATAL] HBase Thrift requires the thrift generator version 0.9.0.[FATAL] Setting it to something else needs to be reviewed for wire and behavior compatibility.[FATAL] ==========================================================================================--[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] HBase .............................................. SUCCESS [ 2.897 s][INFO] HBase - Checkstyle ................................. SUCCESS [ 0.554 s][INFO] HBase - Annotations ................................ SUCCESS [ 0.940 s][INFO] HBase - Protocol ................................... SUCCESS [ 15.454 s][INFO] HBase - Common ..................................... SUCCESS [ 8.984 s][INFO] HBase - Procedure .................................. SUCCESS [ 1.982 s][INFO] HBase - Client ..................................... SUCCESS [ 6.805 s][INFO] HBase - Hadoop Compatibility ....................... SUCCESS [ 0.202 s][INFO] HBase - Hadoop Two Compatibility ................... SUCCESS [ 1.393 s][INFO] HBase - Prefix Tree ................................ SUCCESS [ 1.233 s][INFO] HBase - Server ..................................... SUCCESS [ 13.841 s][INFO] HBase - Testing Util ............................... SUCCESS [ 2.979 s][INFO] HBase - Thrift ..................................... FAILURE [ 0.234 s][INFO] HBase - Shell ...................................... SKIPPED[INFO] HBase - Integration Tests .......................... SKIPPED[INFO] HBase - Examples ................................... SKIPPED[INFO] HBase - Rest ....................................... SKIPPED[INFO] HBase - Assembly ................................... SKIPPED[INFO] HBase - Shaded ..................................... SKIPPED[INFO] HBase - Shaded - Client ............................ SKIPPED[INFO] HBase - Shaded - Server ............................ SKIPPED[INFO] Apache HBase - Spark ............................... SKIPPED[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 01:00 min[INFO] Finished at: 2015-07-28T12:36:15-05:00[INFO] Final Memory: 84M/1038M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.apache.maven.plugins:maven-enforcer-plugin:1.3.1:enforce (enforce-thrift-version) on project hbase-thrift: Some Enforcer rules have failed. Look above for specific messages explaining why the rule failed. -&gt; [Help 1][ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR] [ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :hbase-thrift</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-11 01:00:00" id="14208" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove yarn dependencies on -common and -client</summary>
      <description>They aren't really needed since MR can't be used without server.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.security.User.java</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.token.TokenUtil.java</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-11 01:00:00" id="14212" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add IT test for procedure-v2-based namespace DDL</summary>
      <description>Integration test for proc-v2-based table DDLs was created in HBASE-12439 during HBASE 1.1 release. With HBASE-13212, proc-v2-based namespace DDLs are introduced. We need to enhanced the IT from HBASE-12429 to include namespace DDLs.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestDDLMasterFailover.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-19 01:00:00" id="14249" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14250" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14251" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-19 01:00:00" id="14253" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update docs + build for maven 3.0.4+</summary>
      <description>our new hbase-spark module raises our minimum maven version from 3.0.0 (though I've only tried 3.0.3) to 3.0.4:[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.0:add-source (scala-compile-first) on project hbase-spark: The plugin net.alchim31.maven:scala-maven-plugin:3.2.0 requires Maven version 3.0.4 -&gt; [Help 1]Update the docs to call out 3.0.4 and add an enforcer rule so that this failure can happen at the start of a build rather than 15 minutes in.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14260" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t build javadocs for hbase-protocol module</summary>
      <description>I'm not sure I have all the affected versions, but it seems that something is amiss in making our javadocs: mvn -Papache-release -Prelease -DskipTests clean package... SNIP ...[INFO] ------------------------------------------------------------------------[INFO] Reactor Summary:[INFO] [INFO] Apache HBase ....................................... SUCCESS [ 11.149 s][INFO] Apache HBase - Checkstyle .......................... SUCCESS [ 1.249 s][INFO] Apache HBase - Resource Bundle ..................... SUCCESS [ 0.539 s][INFO] Apache HBase - Annotations ......................... SUCCESS [ 4.438 s][INFO] Apache HBase - Protocol ............................ SUCCESS [10:15 min][INFO] Apache HBase - Common .............................. SUCCESS [ 48.465 s][INFO] Apache HBase - Procedure ........................... SUCCESS [ 14.375 s][INFO] Apache HBase - Client .............................. SUCCESS [ 45.187 s][INFO] Apache HBase - Hadoop Compatibility ................ SUCCESS [ 6.998 s][INFO] Apache HBase - Hadoop Two Compatibility ............ SUCCESS [ 14.891 s][INFO] Apache HBase - Prefix Tree ......................... SUCCESS [ 14.214 s][INFO] Apache HBase - Server .............................. SUCCESS [02:01 min][INFO] Apache HBase - Testing Util ........................ SUCCESS [ 12.779 s][INFO] Apache HBase - Thrift .............................. SUCCESS [01:15 min][INFO] Apache HBase - Shell ............................... SUCCESS [ 6.649 s][INFO] Apache HBase - Integration Tests ................... SUCCESS [ 6.429 s][INFO] Apache HBase - Examples ............................ SUCCESS [ 13.200 s][INFO] Apache HBase - Rest ................................ SUCCESS [ 27.831 s][INFO] Apache HBase - Assembly ............................ SUCCESS [ 19.400 s][INFO] Apache HBase - Shaded .............................. SUCCESS [ 0.419 s][INFO] Apache HBase - Shaded - Client ..................... SUCCESS [ 23.707 s][INFO] Apache HBase - Shaded - Server ..................... SUCCESS [ 43.654 s][INFO] Apache HBase - Spark ............................... SUCCESS [02:22 min][INFO] ------------------------------------------------------------------------[INFO] BUILD SUCCESS[INFO] ------------------------------------------------------------------------[INFO] Total time: 21:13 min[INFO] Finished at: 2015-08-19T15:48:00-05:00[INFO] Final Memory: 181M/1513M[INFO] ------------------------------------------------------------------------</description>
      <version>0.98.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-20 01:00:00" id="14271" opendate="2015-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Nexus staging instructions</summary>
      <description>Refine the Nexus staging instructions a bit. (A promise I made a long time ago.)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-22 01:00:00" id="14292" opendate="2015-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Call Me Maybe HBase links haved moved</summary>
      <description>The links to the Yammer engineering blog have moved.Please use the following links in section 83.5. Network Consistency and Partition Tolerancehttp://old.eng.yammer.com/call-me-maybe-hbase/http://old.eng.yammer.com/call-me-maybe-hbase-addendum/Thanks</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-27 01:00:00" id="14325" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add snapshotinfo command to hbase script</summary>
      <description>Since we already have commands like hbck, hfile, wal etc. that are used for getting various types of information about HBase components it make sense to me to add SnapshotInfo tool to collection. If nobody objects i would add patch for this.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,0.98.15,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-27 01:00:00" id="14326" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book: fix definition of max min size to compact</summary>
      <description>I think we need to change wording/definition of these config parameters in HBase book, they are misleading:hbase.hstore.compaction.min.sizeDescriptionA StoreFile smaller than this size will always be eligible for minor compaction. HFiles this size or larger are evaluated by hbase.hstore.compaction.ratio to determine if they are eligible. Because this limit represents the "automatic include"limit for all StoreFiles smaller than this value, this value may need to be reduced in write-heavy environments where many StoreFiles in the 1-2 MB range are being flushed, because every StoreFile will be targeted for compaction and the resulting StoreFiles may still be under the minimum size and require further compaction. If this parameter is lowered, the ratio check is triggered more quickly. This addressed some issues seen in earlier versions of HBase but changing this parameter is no longer necessary in most situations. Default: 128 MB expressed in bytes.Default134217728hbase.hstore.compaction.max.sizeDescriptionA StoreFile larger than this size will be excluded from compaction. The effect of raising hbase.hstore.compaction.max.size is fewer, larger StoreFiles that do not get compacted often. If you feel that compaction is happening too often without much benefit, you can try raising this value. Default: the value of LONG.MAX_VALUE, expressed in bytes.hbase.hstore.compaction.ratioDescriptionFor minor compaction, this ratio is used to determine whether a given StoreFile which is larger than hbase.hstore.compaction.min.size is eligible for compaction. Its effect is to limit compaction of large StoreFiles. The value of hbase.hstore.compaction.ratio is expressed as a floating-point decimal. A large ratio, such as 10, will produce a single giant StoreFile. Conversely, a low value, such as .25, will produce behavior similar to the BigTable compaction algorithm, producing four StoreFiles. A moderate value of between 1.0 and 1.4 is recommended. When tuning this value, you are balancing write costs with read costs. Raising the value (to something like 1.4) will have more write costs, because you will compact larger StoreFiles. However, during reads, HBase will need to seek through fewer StoreFiles to accomplish the read. Consider this approach if you cannot take advantage of Bloom filters. Otherwise, you can lower this value to something like 1.0 to reduce the background cost of writes, and use Bloom filters to control the number of StoreFiles touched during reads. For most cases, the default value is appropriate.Default1.2FFor details, see HBASE-14263.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-28 01:00:00" id="14332" opendate="2015-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the table state when we encounter exception while disabling / enabling table</summary>
      <description>This patch is a advice for good user experiencereason:When we disable a table and the table is not enabled,we receive a exception,but the exception is too brief,some time we want to know what state is the table in,so that we can know why the table can't be disable.For example,I once encountered a problem the table is neither disable nor enable when my region server crash down,if we give the table state,I will find the problem more quickly .</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableStateManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-30 01:00:00" id="14338" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-30 01:00:00" id="14340" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-31 01:00:00" id="14346" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in FamilyFilter</summary>
      <description>I think there's a typo. "qualifier name" should read "column family name"Family Filter This filter takes a compare operator and a comparator. It compares each qualifier name with the comparator using the compare operator and if the comparison returns true, it returns all the key-values in that column.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.thrift.filter.language.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="14348" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-1 01:00:00" id="14349" opendate="2015-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-19 01:00:00" id="1438" opendate="2009-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-1421 broke the build (#602 up on hudson).</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-8 01:00:00" id="14380" opendate="2015-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct data gets skipped along with bad data in importTsv bulk load thru TsvImporterTextMapper</summary>
      <description>Cosider the input data is as below ROWKEY, TIEMSTAMP, Col_Valuer1,1,v1 &gt;&gt; Correct liner1 &gt;&gt; Bad liner1,3,v3 &gt;&gt; Correct liner1,4,v4 &gt;&gt; Correct lineWhen data is bulk loaded using importTsv with mapper as TsvImporterTextMapper , All the lines are getting ignored even though skipBadLines is set to true.</description>
      <version>2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TextSortReducer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-9 01:00:00" id="14385" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close the sockets that is missing in connection closure.</summary>
      <description>As per heading. Due credit to one of our awesome customers for digging into this and helping me craft the unit test.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.RpcClientImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-9 01:00:00" id="14387" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction improvements: Maximum off-peak compaction size</summary>
      <description>Make max compaction size for peak and off peak separate config options.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerOnlineConfigChange.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.ExploringCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-10 01:00:00" id="14398" opendate="2015-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create the fake keys required in the scan path to avoid copy to byte[]</summary>
      <description>Already we have created some fake keys for the ByteBufferedCells so that we can avoid the copy requried to create fake keys. This JIRA aims to fill up all such places so that the Offheap BBs are not copied to onheap byte[].</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-11 01:00:00" id="14406" opendate="2015-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The dataframe datasource filter is wrong, and will result in data loss or unexpected behavior</summary>
      <description>Following condition will result in the same filter. It will have data loss with the current filter construction.col1 &gt; 4 &amp;&amp; col2 &lt; 3col1 &gt; 4 || col2 &lt; 3</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.HBaseDStreamFunctionsSuite.scala</file>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.java.org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter.java</file>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-protocol.src.main.protobuf.Filter.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.FilterProtos.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-22 01:00:00" id="14462" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>rolling_restart.sh --master-only throws "line 142: test: 0: unary operator expected"</summary>
      <description>I was trying to run this on distributed cluster (master branch build) and ended up with this error. Looking in to this,</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.rolling-restart.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-22 01:00:00" id="14464" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Removed unused fs code</summary>
      <description>remove unused code related to fs</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSVisitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNamespace.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotLogCleaner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSVisitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotReferenceUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotManifestV2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotManifestV1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotManifest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.SnapshotInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.ExportSnapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.snapshot.SnapshotLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2009-6-26 01:00:00" id="1449" opendate="2009-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to latest ZooKeeper trunk</summary>
      <description>The latest ZooKeeper trunk has many improvements and is much more clean. Since we are working on integrating better with ZooKeeper (e.g. shell and JNI) we should run off their latest and greatest.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.zookeeper.HQuorumPeerTest.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">lib.zookeeper-3.1.0-hbase-1241.jar</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-9-28 01:00:00" id="14500" opendate="2015-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove load of deprecated MOB ruby scripts after HBASE-14227</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.shell.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-1 01:00:00" id="14532" opendate="2015-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[book] dfs.client.read.shortcircuit is referenced as hbase-site.xml config and not described in section 7</summary>
      <description>After trying to figure out whether shortcircuit reads would work on my system, I studied the book and found conflicting information.It's suggested in section 92.2, that dfs.client.read.shortcircuit is an option in hbase-site.xml, but the supposedly complete default configuration in section 7 does not include this setting. This leads to confusion on whether it's sufficient to enable this setting in hdfs-site.xml, or whether it needs to be added to both configurations.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-6 01:00:00" id="14558" opendate="2015-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document ChaosMonkey enhancements from HBASE-14261</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-8 01:00:00" id="14581" opendate="2015-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Znode cleanup throws auth exception in secure mode</summary>
      <description>When the master process or region server process dies on Linux, we invoke: if [ -f ${HBASE_ZNODE_FILE} ]; then if [ "$command" = "master" ]; then $bin/hbase master clear &gt; /dev/null 2&gt;&amp;1 else #call ZK to delete the node ZNODE=`cat ${HBASE_ZNODE_FILE}` $bin/hbase zkcli delete ${ZNODE} &gt; /dev/null 2&gt;&amp;1 fi rm ${HBASE_ZNODE_FILE} fiWe delete its znode from the process which started the server JVM for faster crash recovery.In secure deployment however, the second process does not authenticate to zookeeper properly and fails to delete the znode: 2015-06-11 11:05:06,238 WARN [main-SendThread(ip-172-31-32-230.ec2.internal:2181)] client.ZooKeeperSaslClient: Could not login: the client is being asked for a password, but the Zookeeper client code does not currently support obtaining a password from the user. Make sure that the client is configured to use a ticket cache (using the JAAS config2015-06-11 11:05:06,248 WARN [main-SendThread(ip-172-31-32-230.ec2.internal:2181)] zookeeper.ClientCnxn: SASL configuration failed: javax.security.auth.login.LoginException: No password provided Will continue connection to Zookeeper server without SASL authentication, if Zookeeper server allows it.2015-06-11 11:05:06,251 INFO [main-SendThread(ip-172-31-32-230.ec2.internal:2181)] zookeeper.ClientCnxn: Opening socket connection to server ip-172-31-32-230.ec2.internal/172.31.32.230:21812015-06-11 11:05:06,263 INFO [main-SendThread(ip-172-31-32-230.ec2.internal:2181)] zookeeper.ClientCnxn: Socket connection established to ip-172-31-32-230.ec2.internal/172.31.32.230:2181, initiating session2015-06-11 11:05:06,294 INFO [main-SendThread(ip-172-31-32-230.ec2.internal:2181)] zookeeper.ClientCnxn: Session establishment complete on server ip-172-31-32-230.ec2.internal/172.31.32.230:2181, sessionid = 0x14de1dd0f3200cf, negotiated timeout = 400002015-06-11 11:05:06,664 WARN [main] util.HeapMemorySizeUtil: hbase.regionserver.global.memstore.upperLimit is deprecated by hbase.regionserver.global.memstore.size2015-06-11 11:05:09,070 WARN [main] zookeeper.ZooKeeperNodeTracker: Can't get or delete the master znodeorg.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /hbase-secure/master at org.apache.zookeeper.KeeperException.create(KeeperException.java:113) at org.apache.zookeeper.KeeperException.create(KeeperException.java:51) at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:179) at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:1345) at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.deleteIfEquals(MasterAddressTracker.java:270) at org.apache.hadoop.hbase.ZNodeClearer.clear(ZNodeClearer.java:149)This is due to REGIONSERVER_OPTS / HBASE_MASTER_OPTS not being passed for invoking the zkcli command.Thanks to Enis who spotted the issue.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,0.98.16,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-8 01:00:00" id="14582" opendate="2015-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver status webpage bucketcache list can become huge</summary>
      <description>The regionserver status page, such as http://127.0.0.1:60030/rs-status always downloads information about every bucket in the cache. In some cases this can be hundreds of thousands of buckets, causing megabytes of info to need to be downloaded and significant browser instability and memory usage:wc -l HBase-Region-Server-hb22.html2010116 HBase-Region-Server-hb22.htmlls -lah HBase-Region-Server-hb22.html32M Oct 6 19:23 HBase-Region-Server-hb22.htmlFirefox "about:memory":1,330.18 MB (48.22%) &amp;#8211; top(http://hb22:60030/rs-status#bc_l2, id=2010)1,329.61 MB (48.20%) &amp;#8211; active/window(http://hb22:60030/rs-status#bc_l2)</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-10-16 01:00:00" id="14627" opendate="2015-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Stargate docs to Ref Guide</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-16 01:00:00" id="14633" opendate="2015-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Try fluid width UI</summary>
      <description>Our UI is often too long. Lets give it more room if available.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-rest.src.main.resources.hbase-webapps.rest.rest.jsp</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-19 01:00:00" id="14638" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Jython info from the Wiki to the Ref Guide</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-19 01:00:00" id="14639" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Scala info from Wiki to Ref Guide</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-19 01:00:00" id="14640" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Cascading info from Wiki to Ref Guide</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.mapreduce.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-19 01:00:00" id="14641" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move JDO example from Wiki to Ref Guide</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-19 01:00:00" id="14643" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid Splits from once again opening a closed reader for fetching the first and last key</summary>
      <description>Currently split flow is such that we close the parent region and all its store file readers are also closed. After that inorder to split the reference files we need the first and last keys for which once again open the readers on those store files. This could be costlier operation considering the fact that it has to contact the HDFS for this close and open operation. This JIRA is to see if we can improve this.</description>
      <version>2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-19 01:00:00" id="14644" opendate="2015-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region in transition metric is broken</summary>
      <description>ritCount stays 0 no matter what</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsAssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-21 01:00:00" id="14669" opendate="2015-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused import and fix javadoc</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.FilterTestingCluster.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandlerWithLabels.java</file>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestRecoverableZooKeeper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestSecureWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestBoundedRegionGroupingStrategy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.FaultyFSLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestRegionSplitter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMiniClusterLoadSequential.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMergeTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSVisitor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestFSTableDescriptors.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.ProcessBasedLocalHBaseCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedWriterWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedReaderWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.BaseTestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.trace.TestHTraceHooks.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestTableDescriptor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNamespace.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestMetaTableAccessor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestHBaseOnOtherDfsCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestGlobalMemStoreSize.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobRestoreSnapshotHelper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobRestoreFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobFlushSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestMobExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.TestExportSnapshot.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.SnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.MobSnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestVisibilityLabels.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.visibility.TestEnforcingScanLabelGenerator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestTablePermissions.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationWALEntryFilters.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationSource.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationKillRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMultiSlaveReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestReadOldRootAndMetaEdits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollPeriod.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestServerCustomProtocol.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSCVFWithMiniCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerWithBulkload.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerRetriableFailure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerReportForDuty.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerOnlineConfigChange.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerNoMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerHostname.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionFavoredNodes.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRecoveredEdits.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestPerColumnFamilyFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestJoinedScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHMobStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHeapMemoryManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.TestCompactionWithThroughputController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.compactions.MockStoreFileGenerator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaThrottle.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaState.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaAdmin.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestTableLockManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRollingRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionStates.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterRestartAfterDisablingTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestGetLastFlushedSequenceId.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.snapshot.TestSnapshotFileCache.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.handler.TestEnableTableHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestSnapshotFromMaster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduceBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScanBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestSyncTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestRowCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultithreadedTableMapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestMultiTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFilesSplitRecovery.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHashTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCopyTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestCellCounter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.MultiTableInputFormatTestBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestGlobalEventLoopGroup.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockCompatibility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestChecksum.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCacheConfig.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.CacheTestUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestLoadAndSwitchEncodeOnDisk.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.TestHttpServerLifecycle.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWrapper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientSimpleScanner.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.client.TestClientExponentialBackoff.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ProcedureInfo.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.CategoryBasedTimeout.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.codec.TestCellCodec.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestChoreService.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestTimeout.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestBulkDeleteProtocol.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestRowCountEndpoint.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.coprocessor.example.TestZooKeeperScanPolicyObserver.java</file>
      <file type="M">hbase-examples.src.test.java.org.apache.hadoop.hbase.types.TestPBCell.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.regionserver.TestMetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.test.MetricsAssertHelperImpl.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.RestartRsHoldingTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.actions.TruncateTableAction.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestLazyCfLoading.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestMetaReplicas.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestTimeBoundedMultiGetRequestsWithRegionReplicas.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestTimeBoundedRequestsWithRegionReplicas.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestYieldProcedures.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.package-info.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactionRequestor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.snapshot.FlushSnapshotSubprocedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSizeCalculator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DisabledWALProvider.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestClientTimeouts.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestFastFail.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMobRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMobSnapshotCloneIndependence.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMobSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestReplicaWithCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestResultSizeEstimation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.codec.TestCellMessageCodec.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.constraint.TestConstraint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestAggregateProtocol.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestBatchCoprocessorEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestBigDecimalColumnInterpreter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorEndpoint.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorHost.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestDoubleColumnInterpreter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionObserverBypass.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithRemove.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-22 01:00:00" id="14681" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Checkstyle plugin to 2.16</summary>
      <description>We are getting a NPE in checkstyle when running mvn:site. It seems to be MCHECKSTYLE-288 or MCHECKSTYLE-250. Updating maven-checkstyle-plugin from 2.13 to 2.16 seems to fix it.Or maybe not.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-24 01:00:00" id="14690" opendate="2015-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix css so there&amp;#39;s no left/right scroll bar</summary>
      <description>2 em of extra padding needs to be reomved.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.css.hbase.css</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.css.hbase.css</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-26 01:00:00" id="14696" opendate="2015-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting allowPartialResults in mapreduce Mappers</summary>
      <description>It is currently impossible to get partial results in mapreduce mapper jobs.When setting setAllowPartialResults(true) for scan jobs, they still fail with OOME on large rows.The reason is that Scan field allowPartialResults is lost during job creation: 1. User creates a Job and sets a scan object via TableMapReduceUtil.initTableMapperJob(table_name, scanObj,...) -&gt; which puts a result of TableMapReduceUtil.convertScanToString(scanObj) to the job config. 2. When the job starts - method TableInputFormat.setConfig retrieves a scan string from config and converts it to Scan object by calling TableMapReduceUtil.convertStringToScan - which results in a Scan object with a field allowPartialResults always set to false.I have tried to experiment and modify a TableInputFormat method setConfig() by forcing all scans to allow partial results and after this all jobs succeeded with no more OOME and I also noticed that mappers began to get partial results (Result.isPartial()).My use case is very simple - I just have large rows and expect a mapper to get them partially - to get same rowid several times with different key/value records.This would allow me not to worry about implementing my own result partitioning solution, which i would encounter in case the big amount of result key values could be transparently returned for a single large row.And from the other side - if a Scan object can return several records for the same rowid (partial results), perhaps the mapper should do the same.</description>
      <version>1.1.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.Client.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.ClientProtos.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-28 01:00:00" id="14712" opendate="2015-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MasterProcWALs never clean up</summary>
      <description>MasterProcWALs directory grows pretty much un-bounded. Because of that when master failover happens the NN is flooded with connections and everything grinds to a halt.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormatReader.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFile.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStoreTracker.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-29 01:00:00" id="14719" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metric for number of MasterProcWALs</summary>
      <description>Lets add monitoring to this so that we can see when it starts.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFile.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSource.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-11-4 01:00:00" id="14755" opendate="2015-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some broken links and HTML problems</summary>
      <description>Problems seen in https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Website%20Link%20Ckecker/3/artifact/link_report/index.html</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.xdoc.poweredbyhbase.xml</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-4 01:00:00" id="14764" opendate="2015-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop using post-site target</summary>
      <description>We don't really need to use the post-site target. This patch re-orders the POM so we don't need it. Otherwise, you have to run post-site before site:stage and that can be confusing.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-4 01:00:00" id="14765" opendate="2015-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snappy profile</summary>
      <description>Snappy is provided by hadoop and has been for, a long while.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-5 01:00:00" id="14766" opendate="2015-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In WALEntryFilter, cell.getFamily() needs to be replaced with the new low-cost implementation</summary>
      <description>Cell's getFamily() gets an array copy of the cell's family, while in the filter function, it just needs to peek into the family and do a compare. Replace Bytes.toString(cell.getFamily())with Bytes.toString(cell.getFamilyArray(), cell.getFamilyOffset(), cell.getFamilyLength())</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.TableCfWALEntryFilter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-2 01:00:00" id="1478" opendate="2009-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hbase master options from shell</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-6 01:00:00" id="14781" opendate="2015-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn per cf flushing on for ITBLL by default</summary>
      <description/>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestBigLinkedList.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-11 01:00:00" id="14797" opendate="2015-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Last round of CSS fix-ups</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.repo.org.apache.maven.skins.maven-fluido-skin.maven-metadata-local.xml</file>
      <file type="M">src.main.site.resources.css.site.css</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-12 01:00:00" id="14799" opendate="2015-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Commons-collections object deserialization remote command execution vulnerability</summary>
      <description>Read: http://foxglovesecurity.com/2015/11/06/what-do-weblogic-websphere-jboss-jenkins-opennms-and-your-application-have-in-common-this-vulnerability/TL;DR: If you have commons-collections on your classpath and accept and process Java object serialization data, then you probably have an exploitable remote command execution vulnerability. 0.94 and earlier HBase releases are vulnerable because we might read in and rehydrate serialized Java objects out of RPC packet data in HbaseObjectWritable using ObjectInputStream#readObject (see https://hbase.apache.org/0.94/xref/org/apache/hadoop/hbase/io/HbaseObjectWritable.html#714) and we have commons-collections on the classpath on the server.0.98 also carries some limited exposure to this problem through inclusion of backwards compatible deserialization code in HbaseObjectWritableFor96Migration. This is used by the 0.94-to-0.98 migration utility, and by the AccessController when reading permissions from the ACL table serialized in legacy format by 0.94. Unprivileged users cannot run the tool nor access the ACL table.Unprivileged users can however attack a 0.94 installation. An attacker might be able to use the method discussed on that blog post to capture valid HBase RPC payloads for 0.94 and prior versions, rewrite them to embed an exploit, and replay them to trigger a remote command execution with the privileges of the account under which the HBase RegionServer daemon is running.We need to make a patch release of 0.94 that changes HbaseObjectWritable to disallow processing of random Java object serializations. This will be a compatibility break that might affect old style coprocessors, which quite possibly may rely on this catch-all in HbaseObjectWritable for custom object (de)serialization. We can introduce a new configuration setting, "hbase.allow.legacy.object.serialization", defaulting to false.To be thorough, we can also use the new configuration setting "hbase.allow.legacy.object.serialization" (defaulting to false) in 0.98 to prevent the AccessController from falling back to the vulnerable legacy code. This turns out to not affect the ability to migrate permissions because TablePermission implements Writable, which is safe, not Serializable.</description>
      <version>None</version>
      <fixedVersion>0.94.28,1.2.0,1.3.0,1.0.3,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-4 01:00:00" id="1480" opendate="2009-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>compaction file not cleaned up after a crash/OOME server</summary>
      <description>We do not clean up compaction files after a crash/OOME of a region server.I am not sure how the compaction file naming is anymore if its not reproducable some how we should let the master or the server with the root region check every so often and delete old files say older then 24 hours in the compaction dir's of the tables</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-12 01:00:00" id="14801" opendate="2015-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the Spark-HBase connector catalog with json format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
      <file type="M">hbase-spark.src.main.java.org.apache.hadoop.hbase.spark.SparkSQLPushDownFilter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-17 01:00:00" id="14823" opendate="2015-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Ref Guide Refactoring</summary>
      <description>Refactor some tables, links, and other things that don't look quite right in the output.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.unit.testing.adoc</file>
      <file type="M">src.main.asciidoc..chapters.tracing.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.rpc.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.other.info.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbck.in.depth.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase.history.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.compression.adoc</file>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
      <file type="M">src.main.asciidoc..chapters.asf.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.acl.matrix.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-19 01:00:00" id="14849" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-19 01:00:00" id="14851" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test showing how to use TTL from thrift</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-5 01:00:00" id="1486" opendate="2009-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BLOCKCACHE always on even when disabled</summary>
      <description>Comes from Billy Pearson up on list.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-24 01:00:00" id="14877" opendate="2015-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven archetype: client application</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-24 01:00:00" id="14878" opendate="2015-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>maven archetype: client application with shaded jars</summary>
      <description>Add new archetype for generation of hbase-shaded-client dependent project.</description>
      <version>2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-archetypes.README.md</file>
      <file type="M">hbase-archetypes.pom.xml</file>
      <file type="M">hbase-archetypes.hbase-archetype-builder.pom.xml</file>
      <file type="M">hbase-archetypes.hbase-archetype-builder.installArchetypes.sh</file>
      <file type="M">hbase-archetypes.hbase-archetype-builder.createArchetypes.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-5 01:00:00" id="1488" opendate="2009-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>After 1304 goes in, fix and reenable test of thrift, mr indexer, and merge tool</summary>
      <description>In 1304 patch, these tests are disabled. This issue is about making them work again after 1304 goes in.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.mapred.DisabledTestTableIndex.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.util.DisabledTestMergeTool.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-30 01:00:00" id="14898" opendate="2015-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correct Bloom filter documentation in the book</summary>
      <description>In section 96.4. Bloom Filters: Since HBase 0.96, row-based Bloom filters are enabled by default. (HBASE-) --&gt; in HBASE-8450In section 94.4.3. Configuring Server-Wide Behavior of Bloom Filters: io.hfile.bloom.enabled --&gt; io.storefile.bloom.enabled Master switch to enable Bloom filtersio.hfile.bloom.max.fold --&gt; io.storefile.bloom.max.foldio.hfile.bloom.error.rate --&gt; io.storefile.bloom.error.rateio.storefile.bloom.block.size --&gt; default is 128*1024 = 131072These properties are probably not tuned usually, but should still be fixed in the doc.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-5 01:00:00" id="1490" opendate="2009-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update ZooKeeper library</summary>
      <description>ZooKeeper folks have committed ZOOKEEPER-431, which includes our updates from HBASE-1449. We should upgrade our ZK jar to their latest so we can remove the need for our patches.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.zookeeper-r780828-hbase-1449.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-4 01:00:00" id="14928" opendate="2015-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-12-8 01:00:00" id="14952" opendate="2015-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-assembly source artifact has some incorrect modules</summary>
      <description>After generating a tarball we noticed: that hbase-external-blockcache was missing. that hbase-spark is missing that there are duplicate hbase-shaded-{client,server} modules</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-15 01:00:00" id="14984" opendate="2015-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow memcached block cache to set optimze to false</summary>
      <description>In order to keep latency consistent it might not be good to allow the spy memcached client to optimize.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-external-blockcache.src.main.java.org.apache.hadoop.hbase.io.hfile.MemcachedBlockCache.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-16 01:00:00" id="14994" opendate="2015-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up some broken links and references to old APIs</summary>
      <description>Clean up broken links and references to old APIs found in https://builds.apache.org/job/HBase%20Website%20Link%20Ckecker/14/artifact/link_report/errorX.html</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-18 01:00:00" id="15015" opendate="2015-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checktyle plugin shouldn&amp;#39;t check Jamon-generated Java classes</summary>
      <description/>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-21 01:00:00" id="15021" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105  NullPointer Run 2: TestRSStatusServlet.testBasic:105  NullPointer Run 3: TestRSStatusServlet.testBasic:105  NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119  NullPointer Run 2: TestRSStatusServlet.testWithRegions:119  NullPointer Run 3: TestRSStatusServlet.testWithRegions:119  NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-23 01:00:00" id="15036" opendate="2015-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HBase Spark documentation to include bulk load with thin records</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-4 01:00:00" id="15068" opendate="2016-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metrics for region normalization plans</summary>
      <description>Currently there is no metric for region normalization plans.This JIRA would add metrics for the following:number of region split plans executednumber of region merge plans executedThese metrics would allow admin to know the impact of region normalization on production cluster</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-9 01:00:00" id="1507" opendate="2009-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>iCMS as default JVM</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-14 01:00:00" id="15106" opendate="2016-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Procedure Queue pass Procedure for better debuggability</summary>
      <description>Changes the various acquire/release methods to take the Procedure as argument.That allows better debuggability. (The patch it is just a refactor, it does not introduce any new thing)https://reviews.apache.org/r/42271/</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.TruncateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.EnableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DisableTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteColumnFamilyProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateTableProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.AddColumnFamilyProcedure.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-26 01:00:00" id="15172" opendate="2016-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting storage policy in bulkload</summary>
      <description>When using tiered HFile storage, we should be able to generating hfile with correct storage type during bulkload. This JIRA is targeting at making it possible.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-26 01:00:00" id="15174" opendate="2016-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client Public API should not have PB objects in 2.0</summary>
      <description>Some more cleanup for the parent jira. We have leaked some PB structs in Admin (and possible other places). We should clean up these API before 2.0.Examples include: AdminProtos.GetRegionInfoResponse.CompactionState getCompactionState(final TableName tableName) throws IOException; .... void snapshot(final String snapshotName, final TableName tableName, HBaseProtos.SnapshotDescription.Type type) throws IOException, SnapshotCreationException, IllegalArgumentException; .... MasterProtos.SnapshotResponse takeSnapshotAsync(HBaseProtos.SnapshotDescription snapshot) throws IOException, SnapshotCreationException;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Triple.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestInterfaceAudienceAnnotations.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionLoadStats.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="15200" opendate="2016-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ZooKeeper znode ACL checks should only compare the shortname</summary>
      <description>After HBASE-13768 we check at startup in secure configurations if our znodes have the correct ACLs. However when checking the ACL we compare the Kerberos fullname, which includes the host component. We should only compare the shortname, the principal. Otherwise in a multimaster configuration we will unnecessarily reset ACLs whenever any master running on a host other than the one that initialized the ACLs makes the check. You can imagine this happening multiple times in a rolling restart scenario.</description>
      <version>1.2.0,1.0.3,1.1.3,0.98.17,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,1.0.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-1 01:00:00" id="15201" opendate="2016-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbase-spark to hbase assembly</summary>
      <description>hbase-spark currently is missing from hbase assembly.We should add it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-6-10 01:00:00" id="15248" opendate="2016-2-10 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>BLOCKSIZE 4k should result in 4096 bytes on disk; i.e. fit inside a BucketCache &amp;#39;block&amp;#39; of 4k</summary>
      <description>Chatting w/ a gentleman named Daniel Pol who is messing w/ bucketcache, he wants blocks to be the size specified in the configuration and no bigger. His hardware set ups fetches pages of 4k and so a block that has 4k of payload but has then a header and the header of the next block (which helps figure whats next when scanning) ends up being 4203 bytes or something, and this then then translates into two seeks per block fetch.This issue is about what it would take to stay inside our configured size boundary writing out blocks.If not possible, give back better signal on what to do so you could fit inside a particular constraint.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-11 01:00:00" id="15255" opendate="2016-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pointer to linkedin blog on putting jvm logs on fast disk</summary>
      <description>Add pointer to linked in blog: https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-trafficIIRC, tsdb says to do similar.Also add into perf section note on native crc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-3-12 01:00:00" id="15261" opendate="2016-2-12 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Make Throwable t in DaughterOpener volatile</summary>
      <description>In the region split process, daughter regions are opened in different threads, Throwable t is set in these threads and it is checked in the calling thread. Need to make it volatile so the checking will not miss any exceptions from opening daughter regions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-15 01:00:00" id="15271" opendate="2016-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spark Bulk Load: Need to write HFiles to tmp location then rename to protect from Spark Executor Failures</summary>
      <description>With the current code if an executor failure before the HFile is close it will cause problems. This jira will have the files first write out to a file that starts with an underscore. Then when the HFile is complete it will be renamed and the underscore will be removed.The underscore is important because the load bulk functionality will skip files with an underscore.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-6-15 01:00:00" id="1528" opendate="2009-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure scanners work across memcache snapshot</summary>
      <description>We have hole in scanning where if a snapshot, we'll stop seeing in-memory results.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.MultiRegionTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestClient.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-17 01:00:00" id="15282" opendate="2016-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump hbase-spark to use Spark 1.6.0</summary>
      <description>The latest stable Spark is spark 1.6. &amp;#91;1&amp;#93; Let's bump the version.&amp;#91;1&amp;#93; http://spark.apache.org/news/spark-1-6-0-released.html</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-18 01:00:00" id="15289" opendate="2016-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add details about how to get usage instructions for Import and Export utilities</summary>
      <description>Some folks don't realize that you can specify options for the Import and Export commands, like limiting the column families or applying filters. Document how to see the usage.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-20 01:00:00" id="15298" opendate="2016-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing or wrong asciidoc anchors in the reference guide</summary>
      <description>There are some missing or wrong asciidoc anchors in the reference guide.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ycsb.adoc</file>
      <file type="M">src.main.asciidoc..chapters.unit.testing.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.preface.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.compression.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
      <file type="M">src.main.asciidoc..chapters.appendix.contributing.to.documentation.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="15302" opendate="2016-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reenable the other tests disabled by HBASE-14678</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-23 01:00:00" id="15310" opendate="2016-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-spark module has compilation failures with clover profile</summary>
      <description>running with a clover profile enabled will fail due to cross compilation ordering issues with the hbase-spark module. 21:07:47 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project hbase-spark: Compilation failure: Compilation failure:21:07:47 [ERROR] /data/jenkins/workspace/CDH5.7.0-HBase-1.2.0-Clover/hbase-spark/target/clover/src-instrumented/org/apache/hadoop/hbase/spark/example/hbasecontext/JavaHBaseBulkDeleteExample.java:[23,36] error: cannot find symbol21:07:47 [ERROR] symbol: class JavaHBaseContext21:07:47 [ERROR] location: package org.apache.hadoop.hbase.spark21:07:47 [ERROR] /data/jenkins/workspace/CDH5.7.0-HBase-1.2.0-Clover/hbase-spark/target/clover/src-instrumented/org/apache/hadoop/hbase/spark/example/hbasecontext/JavaHBaseDistributedScan.java:[27,36] error: cannot find symbol.... (many classes)Apparently this is a known issue and this page shows a remedy. https://confluence.atlassian.com/display/CLOVERKB/Java-+Scala+cross-compilation+error+-+cannot+find+symbol</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-17 01:00:00" id="1532" opendate="2009-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UI Visibility into ZooKeeper</summary>
      <description>Add ZooKeeper information/administration to UI.Discussion showed particular interest in a tree-viewer application, something like ZOOKEEPER-418.There was talk between Lars/JimK about how often the viewer should update its data.See HBASE-1329 for more information.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-25 01:00:00" id="15332" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to take advantage of HDFS-6133 in HBase</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-25 01:00:00" id="15334" opendate="2016-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add avro support for spark hbase connector</summary>
      <description>Avro is a popular format for hbase storage. User may want the support natively in the connector.With the support, user can save serialized avro into hbase table, and then query on top of it using spark sql. The conversion between avro and catalyst datatype will be handled automatically. This is one way of support complex data types. Otherwise, user has to define their own customized Serdes to support complex data types.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.spark.sql.datasources.hbase.HBaseTableCatalog.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Utils.scala</file>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-27 01:00:00" id="15356" opendate="2016-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused Imports</summary>
      <description>Remove unused Imports.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftHttpServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsckTwoRS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedUpdaterWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.MultiThreadedAction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.hbck.OfflineMetaRebuildTestCore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestNamespace.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.snapshot.MobSnapshotTestingUtils.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollPeriod.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestCustomWALCellCodec.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScannerRetriableFailure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactSplitThread.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaState.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestProcedureMember.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestBaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapred.TestTableInputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.TestHFileLink.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.encoding.TestChangingEncoding.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMultiParallel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestMobRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTableMultiplexerFlushCache.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.token.TokenUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestNamespacesInstanceModel.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestYieldProcedures.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.TestProcedureStoreTracker.java</file>
      <file type="M">hbase-prefix-tree.src.main.java.org.apache.hadoop.hbase.codec.prefixtree.PrefixTreeSeeker.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestBulkLoad.java</file>
      <file type="M">hbase-hadoop2-compat.src.test.java.org.apache.hadoop.hbase.metrics.TestBaseSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.test.java.org.apache.hadoop.hbase.master.TestMetricsMasterSourceFactory.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKConfig.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.CategoryBasedTimeout.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.security.TestEncryptionUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-10-17 01:00:00" id="1537" opendate="2009-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intra-row scanning</summary>
      <description>To continue scaling numbers of columns or versions in a single row, we need a mechanism to scan within a row so we can return some columns at a time. Currently, an entire row must come back as one piece.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestWideScanner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.InternalScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-1 01:00:00" id="15371" opendate="2016-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Completed support parent-child procedure</summary>
      <description>In Procedure-V2 Phase 1 (HBASE-14336), some infrastructure of supporting child procedure exists. However, there is no need in Phase 1 (master DDL) to have multi-level procedures. This JIRA implements adding child procedures to procedure execution list and execute them before parent procedure can make further progress.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureRecovery.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.StateMachineProcedure.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-6-17 01:00:00" id="1538" opendate="2009-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up zookeeper timeout from 10 seconds to 30 seconds to cut down on hbase-user traffic</summary>
      <description>See hbase-dev and search for topic "Should we up the zk default time out from default 10 seconds to twenty or so?" to see discussion that arrives at yes, we should up the default from 10 seconds to 30 seconds.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-9 01:00:00" id="15434" opendate="2016-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[findbugs] Exclude scala generated source and protobuf generated code in hbase-spark module</summary>
      <description>Findbugs against Scala compiler generated bytecode is known to generate false positives and spurious warnings. &amp;#91;1&amp;#93; Protobuf generated code has similar issues. This patch excludes these from the hbase-spark module&amp;#91;1&amp;#93; https://github.com/sbt/findbugs4sbt/issues/4</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-14 01:00:00" id="15461" opendate="2016-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ref guide has bad links to blogs originally posted on cloudera website</summary>
      <description>The ref guide section on "Secure Client Access to Apache HBase" starts with a link to a blog post from Matteo, but the link is broken.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.other.info.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-17 01:00:00" id="15473" opendate="2016-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation for the usage of hbase dataframe user api (JSON, Avro, etc)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.spark.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-21 01:00:00" id="15502" opendate="2016-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skeleton unit test to copy/paste</summary>
      <description>Add to our docs a skeleton unit test to copy/paste with all the vitals already filled out such as category and categorybasetimeout @Rule.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-4-26 01:00:00" id="15537" opendate="2016-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make multi WAL work with WALs other than FSHLog</summary>
      <description>The multi WAL should not be bound with FSHLog.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestBoundedRegionGroupingStrategy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.multiwal.TestReplicationSyncUpToolWithMultipleWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.multiwal.TestReplicationKillMasterRSCompressedWithMultipleWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.multiwal.TestReplicationEndpointWithMultipleWAL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.RegionGroupingProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AbstractFSWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.AbstractFSWAL.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-1 01:00:00" id="15586" opendate="2016-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unify human readable numbers in the web UI</summary>
      <description>I was looking at something else in the regionserver web ui trying to understand the WAL size, and we are reporting that as raw bytes, not in MB / GB range. Did a quick sweep to unify all the human-readable representations in the UI.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.BlockCacheTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-6 01:00:00" id="15597" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up configuration keys used in hbase-spark module</summary>
      <description>This should be considered a blocker for backport to branch-1 since it will impact our compatibility.The constants we expose in configuration should all start with "hbase". Since our configurations keys for the spark integration all relate to that system, the prefix for all configuration keys (excluding those cases where we need to do something special due to restrictions in how properties are handled by e.g. spark) should be "hbase.spark".Before publishing a public api labeled version of our spark integration we should review all of our configuration keys to make sure they either conform to the "hbase.spark" prefix or they have a comment documenting why they need to be otherwise.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.PartitionFilterSuite.scala</file>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.HBaseTestSource.scala</file>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DynamicLogicExpressionSuite.scala</file>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseConnectionCache.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-5-7 01:00:00" id="15613" opendate="2016-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNamespaceCommand times out</summary>
      <description>We think that the root cause maybe HBASE-15295. Will inspect more. Seehttps://issues.apache.org/jira/browse/HBASE-15537?page=com.atlassian.jira.plugin.system.issuetabpanels:all-tabpanel</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.5,1.2.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.resources.hbase-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-9 01:00:00" id="15623" opendate="2016-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update refguide to change hadoop &lt;= 2.3.x from NT to X for hbase-1.2.x</summary>
      <description>This issue is about updating our hadoop supported versions grid in the prerequisites section of refguide. Here is thread proposing this change up on dev list: http://osdir.com/ml/general/2016-04/msg09194.html</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-13 01:00:00" id="15646" opendate="2016-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some docs about exporting and importing snapshots using S3</summary>
      <description>Got a request to add this and wrote something up using the info I could scrounge together.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-17 01:00:00" id="15664" opendate="2016-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Long.MAX_VALUE instead of HConstants.FOREVER in CompactionPolicy</summary>
      <description>The TTL per CF is in seconds, we will convert it to milliseconds when construct HStore. And if it is HConstants.FOREVER, we will set it to Long.MAX_VALUE.HStore.java public static long determineTTLFromFamily(final HColumnDescriptor family) { // HCD.getTimeToLive returns ttl in seconds. Convert to milliseconds. long ttl = family.getTimeToLive(); if (ttl == HConstants.FOREVER) { // Default is unlimited ttl. ttl = Long.MAX_VALUE; } else if (ttl == -1) { ttl = Long.MAX_VALUE; } else { // Second -&gt; ms adjust for user data ttl *= 1000; } return ttl; }</description>
      <version>1.3.0,0.98.19,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,0.98.19,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.RatioBasedCompactionPolicy.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-21 01:00:00" id="15685" opendate="2016-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in REST documentation</summary>
      <description>The Chapter - REST of HBase Book has a few typo in the provided example links, like "http://example.com:8000&lt;table&gt;/&lt;row&gt;/&lt;column&gt;:&lt;qualifier&gt;?v=&lt;num-versions&gt;" which misses a forward slash between the port number 8000 and table name.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-5-27 01:00:00" id="15729" opendate="2016-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove old JDiff wrapper scripts in dev-support</summary>
      <description>Since HBASE-12808, we've been using the Java API Compliance Checker instead of JDiff to look at API compatibility. Probably makes sense to remove the old wrapper scripts that aren't being used anymore.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,0.98.21,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-28 01:00:00" id="15732" opendate="2016-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-rsgroups should be in the assembly</summary>
      <description>hbase-rsgroup is a new module that does not appear in the assembly. The binary tarball still contains the jars through dependencies, but we need the test-jar as well for running the IntegrationTestRSGroup. toffer can you take a quick look.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.src.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.components.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-4 01:00:00" id="15767" opendate="2016-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade httpclient dependency</summary>
      <description>Currently commons-httpclient 3.1 is used.This is already end-of-life by apache.We should move to 4.3.6 or later.Details:https://issues.apache.org/jira/browse/HADOOP-12767https://issues.apache.org/jira/browse/HADOOP-10105https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2015-5262 : http/conn/ssl/SSLConnectionSocketFactory.java in Apache HttpComponents HttpClient before 4.3.6 ignores the http. socket.timeout configuration setting during an SSL handshake, which allows remote attackers to cause a denial of service (HTTPS call hang) via unspecified vectors.https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-6153https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-5783Apache Commons HttpClient 3.x, as used in Amazon Flexible Payments Service (FPS) merchant Java SDK and other products, does not verify that the server hostname matches a domain name in the subject's Common Name (CN) or subjectAltName field of the X.509 certificate, which allows man-in-the-middle attackers to spoof SSL servers via an arbitrary valid certificate.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-4 01:00:00" id="15768" opendate="2016-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix capitalization of ZooKeeper usage</summary>
      <description>Apache ZooKeeper is a proper name (ref), figured I'd fix mentions of "Zookeeper" that appear throughout the codebase and documentation.Ignoring camelcase function names, focused primarily on visible errors, logs, comments, and UI strings.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZooKeeperACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIPv6NIOServerSocketChannel.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFilterWrapper.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.FilterTestingCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterCoprocessorExceptionWithRemove.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ZNodeClearer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ZKNamespaceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.visibility.ZKVisibilityLabelWatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.ZKPermissionWatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSyncUp.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepJobNodeTracker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mob.mapreduce.SweepJob.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ModifyNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.DeleteNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.CreateNamespaceProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.test.IntegrationTestZKAndFSPermissions.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.HBaseClusterManager.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.DistributedHBaseCluster.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSource.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationTrackerZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ZooKeeperRegistry.java</file>
      <file type="M">hbase-archetypes.hbase-shaded-client-project.src.main.java.org.apache.hbase.archetypes.exemplars.shaded.client.HelloHBase.java</file>
      <file type="M">hbase-archetypes.hbase-client-project.src.main.java.org.apache.hbase.archetypes.exemplars.client.HelloHBase.java</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.hbase-env.cmd</file>
      <file type="M">bin.hbase.cmd</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-23 01:00:00" id="1577" opendate="2009-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move memcache to ConcurrentSkipListMap from ConcurrentSkipListSet</summary>
      <description>The CSLM will replace old entry with a new when you put. The CSLS will NOT replace if existent key making for a test, and if present, remove semantic which to be safe needs synchronizing (Replacement is a Ryan Rawson suggestion).</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestOldAPIGetRowVersions.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestMemcache.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Memcache.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-5-7 01:00:00" id="15796" opendate="2016-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMetaCache fails after HBASE-15745</summary>
      <description>HBASE-15745 broke TestMetaCache. Seems I made a mistake by committing the AsyncRpcChannelImpl of the patch version of the older HBASE-13784 instead of basing in on current code.(Checked the other abstracted classes and did not make the same mistake with the others)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AbstractRegionServerCallable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-9 01:00:00" id="15801" opendate="2016-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle for all branches</summary>
      <description>We should use the same checkstyle for all branches.</description>
      <version>1.3.0,1.2.1,1.0.3,0.98.19,1.4.0,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.0.4,1.2.2,0.98.20,1.1.6,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-6-24 01:00:00" id="1581" opendate="2009-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run major compaction on .META. when table is dropped or truncated</summary>
      <description>Add to shell major compacting of .META. on drop or truncate. Implementing this workaround in scripts will narrow the number exposed to the issue where on recreate of a a just-dropped table, they can get erratic results querying meta.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-10-31 01:00:00" id="15921" opendate="2016-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add first AsyncTable impl and create TableImpl based on it</summary>
      <description>First we create an AsyncTable interface with implementation without the Scan functionality. Those will land in a separate patch since they need a refactor of existing scans.Also added is a new TableImpl to replace HTable. It uses the AsyncTableImpl internally and should be a bit faster because it does jump through less hoops to do ProtoBuf transportation. This way we can run all existing tests on the AsyncTableImpl to guarantee its quality.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ReflectionUtils.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.CollectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RpcRetryingCallerImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-2 01:00:00" id="15938" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>submit-patch.py: Don&amp;#39;t crash if there are tests with same name. Refactor: Split out html template to separate file.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.report-flakies.py</file>
      <file type="M">dev-support.findHangingTests.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-2 01:00:00" id="15943" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add page displaying JVM process metrics</summary>
      <description>It would be useful to have page displaying some JVM metrics like PID, process owner. threads info, GC info, etc. This ticked will create two jsp pages (for master and rs) displaying stats listed above.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.region.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.procedures.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-2 01:00:00" id="15944" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Spark test flooding mvn output. Redirect test logs to file.</summary>
      <description>INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ hbase-spark ---[INFO] Building jar: /Users/appy/apache/hbase/hbase-spark/target/hbase-spark-2.0.0-SNAPSHOT.jar[INFO][INFO] --- scalatest-maven-plugin:1.0:test (integration-test) @ hbase-spark ---Discovery starting.Discovery completed in 2 seconds, 434 milliseconds.Run starting. Expected test count is: 78HBaseDStreamFunctionsSuite:2016-06-02 01:04:05,208 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(1029): Starting up minicluster with 1 master(s) and 1 regionserver(s) and 1 datanode(s)2016-06-02 01:04:05,216 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(512): Created new mini-cluster data directory: /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/dfscluster_d7a5357d-4269-4451-afc6-f456a2392469, deleteOnExit=true2016-06-02 01:04:05,217 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(759): Setting test.cache.data to /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/cache_data in system properties and HBase conf2016-06-02 01:04:05,217 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(759): Setting hadoop.tmp.dir to /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/hadoop_tmp in system properties and HBase conf2016-06-02 01:04:05,218 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(759): Setting hadoop.log.dir to /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/hadoop_logs in system properties and HBase conf2016-06-02 01:04:05,218 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(759): Setting mapreduce.cluster.local.dir to /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/mapred_local in system properties and HBase conf2016-06-02 01:04:05,219 INFO [ScalaTest-main-running-DiscoverySuite] hbase.HBaseTestingUtility(759): Setting mapreduce.cluster.temp.dir to /Users/appy/apache/hbase/hbase-spark/target/test-data/a075b24a-36fe-48a9-80c5-31b983a29a8e/mapred_temp in system properties and HBase conf</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.resources.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-6 01:00:00" id="15971" opendate="2016-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression: Random Read/WorkloadC slower in 1.x than 0.98</summary>
      <description>branch-1 is slower than 0.98 doing YCSB random read/workloadC. It seems to be doing about 1/2 the throughput of 0.98.In branch-1, we have low handler occupancy compared to 0.98. Hacking in reader thread occupancy metric, is about the same in both. In parent issue, hacking out the scheduler, I am able to get branch-1 to go 3x faster so will dig in here.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-6-14 01:00:00" id="16023" opendate="2016-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fastpath for the FIFO rpcscheduler</summary>
      <description>This is an idea copied from kudu where we skip queuing a request if there is a handler ready to go; we just do a direct handoff from reader to handler.Makes for close to a %20 improvement in random read workloadc testing moving the bottleneck to HBASE-15716 and to returning the results.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BalancedQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoWithFastPathBalancedQueueRpcExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-15 01:00:00" id="16035" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested AutoCloseables might not all get closed</summary>
      <description>Subtle problem in HBASE-15891:try (A myA = new A(new B()))An exception thrown between B starting to open an A finishing initialization may not result in B being closed. A safer syntax would be:try(B myB = new B(); A myA = newA(myB))</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.log.LogLevel.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-17 01:00:00" id="16056" opendate="2016-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - fix master crash for FileNotFound</summary>
      <description>syuanjiang and tedyu reported a backup master not able to start with FileNotFound during proc-v2 lease recovery. (another restart should have solved the problem)FileNotFoundException: File does not exist: /hbase/MasterProcWALs/state-000001.lognamenode.INodeFile.valueOf(INodeFile.java:61) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease(FSNamesystem.java:2877) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.recoverLease(NameNodeRpcServer.java:753) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.recoverLease(ClientNamenodeProtocolServerSideTranslatorPB.java:671) this may happen when the other master is still active (e.g. GC) and tries to remove files while the other master tries to become active. This operation is retryable so the code should able to handle that.</description>
      <version>1.3.0,1.2.1,1.1.5,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,1.1.6,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-20 01:00:00" id="16068" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - use consts for conf properties in tests</summary>
      <description>replace the hardcoded properties string conf.set("foo.key", v) in the tests with the use of the configuration property constants that we already have</description>
      <version>1.3.0,1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,1.2.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestWALProcedureStoreOnHDFS.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureEvents.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterFailoverWithProcedures.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureReplayOrder.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-20 01:00:00" id="16069" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo "trapsparently" in item 3 of chapter 87.2 of Reference Guide</summary>
      <description>In Chapter 87.2. Coprocessor Implementation Overview...3. Call the coprocessor from your client-side code. HBase handles the coprocessor trapsparently....Correct "trapsparently" into "transparently"</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-6-22 01:00:00" id="16089" opendate="2016-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add on FastPath for CoDel</summary>
      <description>If this is all that awesome, so we should have it on CoDel too.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoWithFastPathBalancedQueueRpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.AdaptiveLifoCoDelCallQueue.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-24 01:00:00" id="16111" opendate="2016-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate preserve shell command is broken</summary>
      <description>On a recent version of master I get this:hbase(main):001:0&gt; truncate_preserve 'TestTable'ERROR: undefined local variable or method `table' for #&lt;Hbase::Admin:0x2fdf17dc&gt;Here is some help for this command: Disables, drops and recreates the specified table while still maintaing the previous region boundaries.Took 0.0290 secondshbase(main):002:0&gt; truncate 'TestTable'Truncating 'TestTable' table (it may take a while):Disabling table...Truncating table...Took 10.0040 seconds</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-27 01:00:00" id="16119" opendate="2016-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Reimplement merge</summary>
      <description>use the proc-v2 state machine for merge. also update the logic to have a single meta-writer.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestSerialReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.namespace.TestNamespaceAuditor.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockNoopMasterServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestRegionServerObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionFileSystem.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterCoprocessorHost.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseMasterObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseMasterAndRegionObserver.java</file>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Master.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Admin.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProtos.java</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.MasterProcedureProtos.java</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.AdminProtos.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ResponseConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-7-1 01:00:00" id="16159" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>OutOfMemory exception when using AsyncRpcClient with encryption to read rpc response</summary>
      <description>Test the Get with encryption AsyncRpcClient with in infinity loop, will get the OOM exception. The root cause is the same as HBASE-16054.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.SaslClientHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-6 01:00:00" id="1616" opendate="2009-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test of compacting referenced StoreFiles</summary>
      <description>We need a unit test for compactions of referenced StoreFiles. This broke in trunk but was not found in any existing tests.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestForceSplit.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2009-1-7 01:00:00" id="1621" opendate="2009-7-7 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>merge tool should work on online cluster</summary>
      <description>taking down the entire cluster to merge 2 regions is a pain, i dont see why the table or regions specifically couldnt be taken offline, then merged then brought back up.this might need a new API to the regionservers so they can take direction from not just the master.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-10-27 01:00:00" id="16290" opendate="2016-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dump summary of callQueue content; can help debugging</summary>
      <description>Being able to get a clue what is in a backedup callQueue could give insight on what is going on on a jacked server. Just needs to summarize count, sizes, call types. Useful debugging. In a servlet?</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestSimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.DelegatingRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSDumpServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcExecutor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.FifoRpcScheduler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-1 01:00:00" id="16312" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jquery version</summary>
      <description>the jquery version we bundle for our web ui is EOM. update to latest jquery 3.y.we can use the jquery migrate plugin to help update APIs.</description>
      <version>1.4.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-thrift.src.main.resources.hbase-webapps.static.js.jquery.min.js</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.thrift.thrift.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.js.jquery.min.js</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.storeFile.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.region.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.processRS.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshotsStats.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.snapshot.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.processRS.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.processMaster.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.procedures.jsp</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-rest.src.main.resources.hbase-webapps.rest.rest.jsp</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-1 01:00:00" id="16315" opendate="2016-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RegionSizeCalculator prints region names as binary without escapes</summary>
      <description>Region start key is not escaped: 2016-05-20 01:35:04,646|beaver.machine|INFO|8984|140719733470976|MainThread|2016-05-20 01:35:04,637 DEBUG [main] util.RegionSizeCalculator: Region IntegrationTestBigLinkedList,,1463706056389.609c5d0e2a3a03eb3d93d608b9722fb9. has size 1300234242016-05-20 01:35:04,647|beaver.machine|INFO|8984|140719733470976|MainThread|2016-05-20 01:35:04,637 DEBUG [main] util.RegionSizeCalculator: Region IntegrationTestBigLinkedList,"""""" ,1463706056389.6b2d56bc9f04339156a858595b237614. has size 1300234242016-05-20 01:35:04,647|beaver.machine|INFO|8984|140719733470976|MainThread|2016-05-20 01:35:04,637 DEBUG [main] util.RegionSizeCalculator: Region IntegrationTestBigLinkedList,,1463706056389.4eab8c5beba325cb8a2731151f8bbe77. has size 1342177282016-05-20 01:35:04,647|beaver.machine|INFO|8984|140719733470976|MainThread|2016-05-20 01:35:04,637 DEBUG [main] util.RegionSizeCalculator: Region IntegrationTestBigLinkedList,wwwwwwh,1463706056389.406e8bbe17eabb4aca2b246d1242013c. has size 131072000</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.1.6,1.2.3,0.98.22,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionLoad.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-9 01:00:00" id="1632" opendate="2009-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write documentation for configuring/managing ZooKeeper with HBase</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.overview.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-9 01:00:00" id="1633" opendate="2009-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t delete in TRUNK shell; makes it hard doing admin repairs</summary>
      <description>Because shell uses old API, it runs into the "Can't add deletes to a BatchUpdate" issue. Add new API to do shell delete and deleteAll. Just a few lines.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-3 01:00:00" id="16347" opendate="2016-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unevaluated expressions in book</summary>
      <description>Have a look at the quickstart guide, step two$ tar xzvf hbase-&lt;?eval ${project.version}?&gt;-bin.tar.gz$ cd hbase-&lt;?eval ${project.version}?&gt;/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-17 01:00:00" id="16426" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove company affiliations from committer list</summary>
      <description>An email thread on the dev mailing list came to the consensus that company affiliations in the committer list are difficult to keep up to date and not worth it. This JIRA is to track removing them.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="16434" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve flaky dashboard</summary>
      <description>Sort list of tests in descending order of flakyness add date and flaky tests count local hrefs to job results</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.report-flakies.py</file>
      <file type="M">dev-support.flaky-dashboard-template.html</file>
      <file type="M">dev-support.findHangingTests.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-19 01:00:00" id="16451" opendate="2016-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Test WAL protobuf entry size limit</summary>
      <description>Add a test to make sure that we are able to read/write procedures with a big "data" size.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestStressWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.util.ByteSlot.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-8-28 01:00:00" id="16510" opendate="2016-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reset RpcController before retry</summary>
      <description>Later we will use RpcController to pass exceptions to upper layer, so we need to clear the exception info before retry.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRpcControllerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.NoncedRegionServerCallable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MasterCallable.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-8-30 01:00:00" id="16532" opendate="2016-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure-V2: Enforce procedure ownership at submission</summary>
      <description>HBASE-16520 (for TableBackupProcedure) and HBASE-16528 (for ServerCrashProcedure) were two recent JIRAs where procedure ownership is set at time of construction.This JIRA continues the discussion toward the end of HBASE-16520.Proposal is to enforce procedure ownership for all the procedures.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.resources.hbase-site.xml</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-30 01:00:00" id="16533" opendate="2016-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Extract chore from the executor</summary>
      <description>At the moment we have the CompletedProcedureCleaner chore as a special case in the executor. let's extract that and allow to have other chores. (I want to use it for the AM)</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.util.TestTimeoutBlockingQueue.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.util.TimeoutBlockingQueue.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-30 01:00:00" id="16534" opendate="2016-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - Perf Tool for Scheduler</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-31 01:00:00" id="16535" opendate="2016-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use regex to exclude generated classes for findbugs</summary>
      <description>As I tried in HBASE-16526, &lt;Match&gt; &lt;Package name="org.apache.hadoop.hbase.ipc.protobuf.generated"/&gt; &lt;/Match&gt;This does not work.So I propose that we can use regex to match the class name to exclude the generated classes.</description>
      <version>1.3.0,1.4.0,1.1.6,0.98.21,1.2.3,2.0.0</version>
      <fixedVersion>1.3.0,0.98.22,1.1.7,1.2.4,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-1 01:00:00" id="16547" opendate="2016-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-archetype-builder shell scripts assume bash is installed in /bin</summary>
      <description>There's no guarantee UNIX systems will have bash installed in /bin. HBase builds fail for me on FreeBSD. The hbase-archetype-builder scripts do not use any bash features so let's specify '/bin/sh' as interpreter instead.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-archetypes.hbase-archetype-builder.installArchetypes.sh</file>
      <file type="M">hbase-archetypes.hbase-archetype-builder.createArchetypes.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-14 01:00:00" id="1656" opendate="2009-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>loadZooConfig can mask true error</summary>
      <description>try { properties.load(inputStream);} catch (IOException e) { String msg = "fail to read properties from " + ZOOKEEPER_CONFIG_NAME; LOG.fatal(msg); throw new IOException(msg);}This masks the actual error, if there is one.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-7 01:00:00" id="16574" opendate="2016-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add backup / restore feature to refguide</summary>
      <description>This issue is to add backup / restore feature description to hbase refguide.The description should cover:scenarios where backup / restore is usedbackup / restore commands and sample usageconsiderations in setup</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc.book.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-14 01:00:00" id="1658" opendate="2009-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove UI refresh -- its annoying</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.webapps.regionserver.regionserver.jsp</file>
      <file type="M">src.webapps.master.table.jsp</file>
      <file type="M">src.webapps.master.master.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-12 01:00:00" id="16614" opendate="2016-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use daemon thread for netty event loop</summary>
      <description>As always use daemon thread in rpc implementation.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.NettyRpcClient.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ipc.DefaultNettyEventLoopConfig.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-13 01:00:00" id="16622" opendate="2016-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some issues with the HBase reference guide</summary>
      <description>1. if (admin.tableExists(tableName)) { System.out.println("Table does not exist."); System.exit(-1); }This should be if (!admin.tableExists(tableName)) {2. SNAPPY is not suitable for begginer. They may get exceptions like Caused by: org.apache.hadoop.hbase.ipc.RemoteWithExtrasException(org.apache.hadoop.hbase.DoNotRetryIOException): org.apache.hadoop.hbase.DoNotRetryIOException: Compression algorithm 'snappy' previously failed test. Set hbase.table.sanity.checks to false at conf or table descriptor if you want to bypass sanity checks at org.apache.hadoop.hbase.master.HMaster.warnOrThrowExceptionForFailure(HMaster.java:1701) at org.apache.hadoop.hbase.master.HMaster.sanityCheckTableDescriptor(HMaster.java:1569) at org.apache.hadoop.hbase.master.HMaster.createTable(HMaster.java:1491) at org.apache.hadoop.hbase.master.MasterRpcServices.createTable(MasterRpcServices.java:462) at org.apache.hadoop.hbase.protobuf.generated.MasterProtos$MasterService$2.callBlockingMethod(MasterProtos.java:55682) at org.apache.hadoop.hbase.ipc.RpcServer.call(RpcServer.java:2178) at org.apache.hadoop.hbase.ipc.CallRunner.run(CallRunner.java:112) at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop(RpcExecutor.java:133) at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run(RpcExecutor.java:108) at java.lang.Thread.run(Thread.java:745)So the code belowtable.addFamily(new HColumnDescriptor(CF_DEFAULT).setCompressionType(Algorithm.SNAPPY));it better to change intotable.addFamily(new HColumnDescriptor(CF_DEFAULT).setCompressionType(Algorithm.NONE));3.Before modify column family , get the table from connectionChangeHTableDescriptor table = new HTableDescriptor(tableName);intoTable table = connection.getTable(TableName.valueOf(tablename));4.In 143.1.1. Code Formattingit just saidStill in Preferences, click . Be sure the following options are selected:Apache HBase  Reference GuideBut nothing after click. It should be Java-&gt;Editor-&gt;Save Actions</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase.apis.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-16 01:00:00" id="1663" opendate="2009-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Request compaction only once instead of every time 500ms each time we cycle the hstore.getStorefilesCount() &gt; this.blockingStoreFilesNumber loop</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-10-20 01:00:00" id="16657" opendate="2016-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose per-region last major compaction timestamp in RegionServer UI</summary>
      <description>HBASE-12859 added some tracking for the last major compaction completed for each region. However, this is currently only exposed through the cluster status reporting and the Admin API. Since the regionserver is already reporting this information, it would be nice to fold it in somewhere to the region listing in the regionserver UI.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-7-17 01:00:00" id="1670" opendate="2009-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>transactions / indexing fixes: trx deletes not handeled, index scan can&amp;#39;t specify stopRow</summary>
      <description>couple of things I missed in api refactor</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.contrib.transactional.src.test.org.apache.hadoop.hbase.client.transactional.TestTransactions.java</file>
      <file type="M">src.contrib.transactional.src.test.org.apache.hadoop.hbase.client.tableindexed.TestIndexedTable.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionState.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegionServer.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.TransactionalRegion.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.transactional.THLog.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexMaintenanceUtils.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.regionserver.tableindexed.IndexedRegion.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.transactional.JtaXAResource.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTableDescriptor.java</file>
      <file type="M">src.contrib.transactional.src.java.org.apache.hadoop.hbase.client.tableindexed.IndexedTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-24 01:00:00" id="16700" opendate="2016-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow for coprocessor whitelisting</summary>
      <description>Today one can turn off all non-system coprocessors with hbase.coprocessor.user.enabled however, this disables very useful things like Apache Phoenix's coprocessors. Some tenants of a multi-user HBase may also need to run bespoke coprocessors. But as an operator I would not want wanton coprocessor usage. Ideally, one could do one of two things: Allow coprocessors defined in hbase-site.xml &amp;#8211; this can only be administratively changed in most cases Allow coprocessors from table descriptors but only if the coprocessor is whitelisted</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-27 01:00:00" id="16711" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hadoop-3.0 profile compile</summary>
      <description>The -Dhadoop.profile=3.0 build is failing currently due to code deprecated in hadoop2 and removed in hadoop3.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.IncrementCoalescer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBulkLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-27 01:00:00" id="16712" opendate="2016-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix hadoop-3.0 profile mvn install</summary>
      <description>After the compile is fixed, mvn install fails due to transitive dependencies coming from hadoop3.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.NOTICE.vm</file>
      <file type="M">hbase-resource-bundle.src.main.resources.META-INF.LICENSE.vm</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-28 01:00:00" id="16722" opendate="2016-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document: Broken link in CatalogJanitor</summary>
      <description>A link in CatalogJanitor is broken. This should be linked to https://hbase.apache.org/book.html#arch.catalog.meta</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-29 01:00:00" id="16730" opendate="2016-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude junit as a transitive dependency from hadoop-common</summary>
      <description>add exclusion to the hadoop-common dependency in hbase-client: exclude junit</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-30 01:00:00" id="16733" opendate="2016-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add hadoop 3.0.0-alpha1 to precommit checks</summary>
      <description>Been working on getting hadoop3 related build up and running and woudl ike to add a precommit check so that new commits don't break the mvn compile/install.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-10-4 01:00:00" id="16763" opendate="2016-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unintentional dependency on net.sf.ehcache.search.Results</summary>
      <description>HBASE-15638 introduced what looks like an inadvertent dependency on net.sf.ehcache.search.Results. This removes it.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockRegionServer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-10-6 01:00:00" id="16781" opendate="2016-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix flaky TestMasterProcedureWalLease</summary>
      <description>Attempt to fix the flaky TestMasterProcedureWalLease.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockNoopMasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-9 01:00:00" id="16793" opendate="2016-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude shaded protobuf files from rat check</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-10 01:00:00" id="16802" opendate="2016-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - group procedure cleaning</summary>
      <description>group the cleaning of the evicted procedures</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.TestWALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.NoopProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-10-14 01:00:00" id="16837" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement checkAndPut and checkAndDelete</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTable.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-22 01:00:00" id="1685" opendate="2009-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[stargate] package and wiki documentation</summary>
      <description>Write package and wiki documentation for the Stargate contrib. Convert spec and transaction examples into package javadoc. Render package javadoc into wiki markup. Write up instructions for starting the service in standalone mode and as a servlet.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-18 01:00:00" id="16867" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure V2 - Check ACLs for remote HBaseLock</summary>
      <description>HBaseLock was added in HBASE-16744 to allow clients to take locks on namespace/table/regions. Check acls for the 2 rpcs that were added.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-18 01:00:00" id="16869" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in "Disabling Blockcache" doc</summary>
      <description>The Disabling Blockcache section of the documentation refers to hbase.block.cache.size. Should it be hfile.block.cache.size?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-18 01:00:00" id="16871" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - add waiting procs back to the queue after restart</summary>
      <description>Procs in WAITING_TIMEOUT state don't get re-added to the queue after restart.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.TestProcedureEvents.java</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.ProcedureTestingUtility.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-18 01:00:00" id="16875" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changed try-with-resources in the docs to recommended way</summary>
      <description>In a number of places, we show examples that lend themselves to using Java 7's try-with-resources statement, but we use the statement in a less-than-ideal nested way. Let's change our docs throughout to do it the recommended way.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-20 01:00:00" id="16884" opendate="2016-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add HBase-2.0.x to the hadoop version support matrix in our documentation</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-20 01:00:00" id="16887" opendate="2016-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow setting different hadoopcheck versions in precommit for different branches</summary>
      <description>http://hbase.apache.org/book.html#hadoopThe supportted hadoop versions are different for different HBase versions.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-23 01:00:00" id="1693" opendate="2009-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE close_region ".META." in shell</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-24 01:00:00" id="1694" opendate="2009-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TOC to &amp;#39;Getting Started&amp;#39;, add references to THBase and ITHBase</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-25 01:00:00" id="16941" opendate="2016-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FavoredNodes - Split/Merge code paths</summary>
      <description>This jira is to deal with the split/merge logic discussed as part of HBASE-15532. The design document can be seen at HBASE-15531. The specific changes are:Split and merged regions should inherit favored node information from parent regions. For splits also include some randomness so even if there are subsequent splits, the regions will be more or less distributed. For split, we include 2 FN from the parent and generate one random node.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement2.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.MockNoopMasterServices.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestFavoredNodeAssignmentHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.SnapshotOfRegionAssignmentFromMeta.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionPlacementMaintainer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodeAssignmentHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentVerificationReport.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-26 01:00:00" id="16949" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix RAT License complaint about the hbase-protocol-shaded/src/main/patches content</summary>
      <description>Noticed by @duo zhang over on HBASE-16835. Let me exclude the patches dir from rat check.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-protocol.README.txt</file>
      <file type="M">hbase-protocol-shaded.README.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-27 01:00:00" id="16955" opendate="2016-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixup precommit protoc check to do new distributed protos and pb 3.1.0 build</summary>
      <description>HBASE-15638 Shade protobuf and a follow-ons changed how we do protobufs. One, protobufs are in the module they pertain to so distributed throughout the modules and secondly, we do 2.5.0 pb for externally consumed protobuf &amp;#8211; e.g. Coprocessor Endpoints &amp;#8211; but internally we use protobuf 3.1.0.A precommit check looks to see if any proto changes break protoc compile. This task is about updating the precommit to accommodate the changes brought about by HBASE-15638.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-27 01:00:00" id="16956" opendate="2016-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor FavoredNodePlan to use regionNames as keys</summary>
      <description>We would like to rely on the FNPlan cache whether a region is offline or not. Sticking to regionNames as keys makes that possible.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRegionPlacement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionPlacementMaintainer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.FavoredNodesPlan.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-31 01:00:00" id="16974" opendate="2016-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update os-maven-plugin to 1.4.1.final+ for building shade file on RHEL/CentOS</summary>
      <description>The current os-maven-plugin may export the profile with quote on certain versions of centos/RHEL, and it introduces the error when building shade file. The error message is shown below. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.3:shade (default) on project hbase-protocol-shaded: Error creating shaded jar: The name "os.detected.release.like."centos"" is not legal for JDOM/XML elements: XML names cannot contain the character """. -&gt; [Help 1]The error is caused by the /etc/os-release which contains some quote. The os-maven-plugin 1.4.1.final+ had fixed it. Therefore, we ought to update the os-maven-plugin to 1.4.1.final+ for the user who cant change the content of the /etc/os-release.Any comment? Thanks.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-protocol-shaded.README.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-31 01:00:00" id="16976" opendate="2016-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-protocol-shaded module generates classes to wrong directory</summary>
      <description>I've been fighting with getting master to import/build normally inside of Eclipse (a bit seems to have come in as a part of the shaded protocol work).Once thing I see definitely wrong is that Eclipse is balking at the source files being compiled in target/ instead of target/classes. This looks like an omission since there is no good reason to not use the standard convention of target/classes.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-1 01:00:00" id="16986" opendate="2016-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note on how scanner caching has changed since 0.98 to refguid</summary>
      <description>Add note on how scanner caching config changed from 0.98 to the refguide (see parent issue for discussion but basics are we used to have default of 100 but not have unlimited as default)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-4 01:00:00" id="17017" opendate="2016-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the current per-region latency histogram metrics</summary>
      <description/>
      <version>1.3.0,1.4.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-8 01:00:00" id="17044" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix merge failed before creating merged region leaves meta inconsistent</summary>
      <description>Similar to HBASE-16093. Rollback from failed merge should not offline the target region for the merge if it hasn't been created.</description>
      <version>None</version>
      <fixedVersion>1.3.0,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-8 01:00:00" id="17047" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an API to get HBase connection cache statistics</summary>
      <description>This patch will add a function "getStat" for the user to get the statistics of the HBase connection cache.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.HBaseConnectionCacheSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseConnectionCache.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-25 01:00:00" id="1705" opendate="2009-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server: deletes in mutateRow/s don&amp;#39;t delete.</summary>
      <description>Simple bugs:In mutateRow we don't check the isDelete flag, it always assumes a put.In mutateRows we don't check if the delete has only the family specified.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-8 01:00:00" id="17050" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache CLI version from 1.2 to 1.3.1</summary>
      <description>http://mail-archives.apache.org/mod_mbox/hbase-dev/201611.mbox/%3CCAGHyZ6KNjTH6qnVN%2B%3Dd_rC%3DtkLEjELUCQf9pmqQ-ixJj%2BfbrOA%40mail.gmail.com%3E</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-9 01:00:00" id="17052" opendate="2016-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>compile-protobuf profile does not compile protobufs in some modules anymore</summary>
      <description>Due to recent changes, we are not compiling the protobuf files in hbase-endpoint, hbase-rsgroup, etc anymore. [INFO] --- protobuf-maven-plugin:0.5.0:compile (compile-protoc) @ hbase-rsgroup ---[INFO] /Users/enis/projects/hbase-sal/hbase-rsgroup/src/main/protobuf/,/Users/enis/projects/hbase-sal/hbase-rsgroup/../hbase-protocol/src/main/protobuf does not exist. Review the configuration or consider disabling the plugin.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.generated.BulkDeleteProtos.java</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-12-10 01:00:00" id="17068" opendate="2016-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - inherit region locks</summary>
      <description>Add support for inherited region locks. e.g. Split will have Assign/Unassign as child which will take the lock on the same region split is running on</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-11 01:00:00" id="17073" opendate="2016-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the max number of buffers in ByteBufferPool</summary>
      <description>Before the HBASE-15525 issue fix, we had variable sized buffers in our buffer pool. The max size upto which one buffer can grow was 2 MB. Now we have changed it to be a fixed sized BBPool. By default 64 KB is the size of each buffer. But the max number of BBs allowed to be in the pool was not changed. ie. twice the number of handlers. May be we should be changing increasing it now? To make it equal to the way like 2 MB, we will need 32 * 2 * handlers. There is no initial #BBs any way. 2 MB is the default max response size what we have. And write reqs also, when it is Buffered mutator 2 MB is the default flush limit. We can make it to be 32 * #handlers as the def max #BBs I believe.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-11 01:00:00" id="17074" opendate="2016-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit job always fails because of OOM</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/4434/artifact/patchprocess/patch-unit-hbase-server.txtException in thread "Thread-2369" java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf(Arrays.java:3332) at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:124) at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:596) at java.lang.StringBuffer.append(StringBuffer.java:367) at java.io.BufferedReader.readLine(BufferedReader.java:370) at java.io.BufferedReader.readLine(BufferedReader.java:389) at org.apache.maven.surefire.shade.org.apache.maven.shared.utils.cli.StreamPumper.run(StreamPumper.java:66)Exception in thread "Thread-2357" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2365" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.filter.TestFuzzyRowFilterEndToEndRunning org.apache.hadoop.hbase.filter.TestFilterListOrOperatorWithBlkCntException in thread "Thread-2383" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2397" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2401" java.lang.OutOfMemoryError: Java heap spaceRunning org.apache.hadoop.hbase.TestHBaseTestingUtilityException in thread "Thread-2407" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2411" java.lang.OutOfMemoryError: Java heap spaceException in thread "Thread-2413" java.lang.OutOfMemoryError: Java heap spaceThe OOM happens in the surefire plugin when reading the stdout or stderr of the running test...</description>
      <version>1.3.0,1.4.0,1.1.7,0.98.23,1.2.4,2.0.0</version>
      <fixedVersion>1.3.0,1.2.5,0.98.24,1.1.8,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-12 01:00:00" id="17079" opendate="2016-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase build fails on windows, hbase-archetype-builder is reason for failure</summary>
      <description>HBase buid fails on windows, hbase-archetype-builder is reason for failure. Cygwin is installed so the shell scripts should execute successfully.Here is build failure log[INFO] Apache HBase - Archetype builder ................... FAILURE [ 1.014 s][INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 06:13 min[INFO] Finished at: 2016-11-12T18:12:26+05:30[INFO] Final Memory: 235M/1012M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.4.0:exec (make-scripts-executable) on project hbase-archetype-builder: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException[ERROR][ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :hbase-archetype-builder</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-archetypes.hbase-archetype-builder.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-12 01:00:00" id="17082" opendate="2016-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ForeignExceptionUtil isnt packaged when building shaded protocol with -Pcompile-protobuf</summary>
      <description>The source folder will be replaced from src/main/java to project.build.directory/protoc-generated-sources when building shaded protocol with -Pcompile-protobuf, but we do not copy the ForeignExceptionUtil. So the final jar lacks the ForeignExceptionUtil and it causes the test error for hbase-client and hbase-server.[ERROR] /testptch/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:[169,36] cannot find symbol symbol: class ForeignExceptionUtil location: package org.apache.hadoop.hbase.util[ERROR] /testptch/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:[100,36] cannot find symbol symbol: class ForeignExceptionUtil location: package org.apache.hadoop.hbase.util[ERROR] /testptch/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegionServer.java:[2144,17] cannot find symbol symbol: variable ForeignExceptionUtil location: class org.apache.hadoop.hbase.regionserver.HRegionServer[ERROR] /testptch/hbase/hbase-server/src/main/java/org/apache/hadoop/hbase/master/MasterRpcServices.java:[938,32] cannot find symbol symbol: variable ForeignExceptionUtil location: class org.apache.hadoop.hbase.master.MasterRpcServicesThis bug blocks the patches which are against the hbase-protocol-shaded module.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.README.txt</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-14 01:00:00" id="17089" opendate="2016-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc on experience running with hedged reads</summary>
      <description>Add to the refguide carp84's useful experience running with hedged reads.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.protobuf.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-27 01:00:00" id="1709" opendate="2009-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift getRowWithColumns doesn&amp;#39;t accept column-family only</summary>
      <description>In HBase 0.19, it was possible to give just a column-family name to getRowWithColumns and it would include all columns of that family in the result.In 0.20-r798074, this does not work anymore.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Get.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-14 01:00:00" id="17090" opendate="2016-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure v2 - fast wake if nothing else is running</summary>
      <description>We wait Nmsec to see if we can batch more procedures, but the pattern that we have allows us to wait only for what we know is running and avoid waiting for something that will never get there.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-procedure.src.test.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALPerformanceEvaluation.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.WALProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.ProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.NoopProcedureStore.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-22 01:00:00" id="17160" opendate="2016-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undo unnecessary inter-module dependency; spark to hbase-it and hbase-it to shell</summary>
      <description>Very minor untangling.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-11-23 01:00:00" id="17169" opendate="2016-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Cell variants with ShareableMemory</summary>
      <description>As asked by Stack in review comment of other sub tasks of the parent.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.AbstractMemStore.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.OffheapKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.NoTagsKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.BufferedDataBlockEncoder.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ExtendedCell.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.KeyValueCodecWithTags.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.codec.KeyValueCodec.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-11-28 01:00:00" id="17183" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle ByteBufferCell while making TagRewriteCell</summary>
      <description>TagRewriteCell is the normal ExtendedCell. When it wraps a ByteBufferCell, we need a new TagRewriteCell type of type ByteBufferCell.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.CellUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-29 01:00:00" id="17194" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assign the new region to the idle server after splitting</summary>
      <description>The new regions are assigned to the random servers after splitting, but there always are some idle servers which dont be assigned any regions on the new cluster. It is a bad start of load balance, hence we should give priority to the idle servers for assignment.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.balancer.TestBaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-29 01:00:00" id="17198" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FN updates during region merge (follow up to Procedure v2 merge)</summary>
      <description>As mentioned in https://reviews.apache.org/r/53242/ (HBASE-16941), since the procedure v2 merge changes are in development, there is a follow up optimization/cleanup that can be done for favored nodes during merge. This jira will be taken up once HBASE-16119 is complete.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-30 01:00:00" id="17207" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrays.asList() with too few arguments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RackManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutputSaslHelper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.MultiAction.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.BufferedMutatorImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-30 01:00:00" id="17216" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A Few Fields Can Be Safely Made Static</summary>
      <description>Automated Test...</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.HFileReplicator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.asyncfs.FanOutOneBlockAsyncDFSOutput.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.encoding.FastDiffDeltaEncoder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncProcess.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-2 01:00:00" id="17235" opendate="2016-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement in creation of CIS for onheap buffer cases</summary>
      <description>if (buf.hasArray()) { cis = CodedInputStream.newInstance(buf.array(), offset, buf.limit()); } else {Currently we do this for onheap buffers incase there is no reservoir or the size is less than the minSizeforReservoir. I could see that even if reservoir is there there are requests which goes with the above way of creating CIS. This could be made efficient to avoid underlying copies by just doing thiscis = UnsafeByteOperations.unsafeWrap(buf.array(), offset, buf.limit()).newCodedInput();</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-4 01:00:00" id="17251" opendate="2016-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a timeout parameter when locating region</summary>
      <description>Now we always use the default timeout configured for zk and meta.I think it is reasonable to always use the same timeout when accessing zk and meta as the result will be shared by lots of threads. If we could do a successful fetching then the result will be in cache so it does not make sense to set the timeout to a very small value when fetching.But I think it is also important to let the user request finish in time even if the user can only get a timeout exception. We should not block a user request longer than operation timeout.So I think we could add a timeout parameter to the region locate method, and if the location can not be fetched in time, we will just finished the returned CompletableFuture with a timeout exception, but the actual fetching operation can still go on and use its own timeout config.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncSingleRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableRegionLocatorImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncSingleRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-5-6 01:00:00" id="17263" opendate="2016-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Netty based rpc server impl</summary>
      <description>An RPC server with Netty4 implementation, which provide better performance.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.token.TestTokenAuthentication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.TestSecureIPC.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestRpcHandlerException.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.TestProtoBufRpc.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.AbstractTestIPC.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.SimpleRpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.RpcServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.BufferChain.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-30 01:00:00" id="1727" opendate="2009-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTD and HCD versions need update</summary>
      <description>HCD is at version 7 but there's no comment as to why.There is a fixme in HTD indicating the version should be bumped to 5 after indexes have been removed. Indexes have been removed, but the version is still at 4.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-7 01:00:00" id="17271" opendate="2016-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase-thrift QA tests only run one test</summary>
      <description>From https://builds.apache.org/job/PreCommit-HBASE-Build/4822/artifact/patchprocess/patch-unit-hbase-thrift.txt , it is pretty clear that only one test was run - TestCallQueueEven though the patch contains modified TestThriftHBaseServiceHandler.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-9 01:00:00" id="17282" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the redundant requests to meta table</summary>
      <description>This usually happens at startup when the meta cache is empty. There will be a lot of locating requests, but most of will have same results. Things become worse if we do batch operations with AsyncTable as we will send a locating request for each operation concurrently.We need to reduce the redundant requests to meta table.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableNoncedRetry.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTable.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionLocatorTimeout.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionLocator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncGetMultiThread.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-10 01:00:00" id="17286" opendate="2016-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LICENSE.txt in binary tarball contains only ASL text</summary>
      <description>Noticed this one today because I needed to make sure LICENSE was getting updated for a patch-in-progress.What I'm presently seeing after invoking mvn clean package assembly:single -DskipTests -Drat.skip -Prelease on master is that the LICENSE.txt file contains only the ASL text (which I know for certain it should contain BSD and MIT as well).I checked branch-1.2 which has lots of extra greatness, so it seems like something happened in master which broke this. Filing this now so we can try to bisect and figure out what happened.FYI, this is the one I was chatting you about busbey.</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-12-14 01:00:00" id="17316" opendate="2016-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addendum to HBASE-17294, &amp;#39;External Configuration for Memory Compaction&amp;#39;</summary>
      <description>Updating 2 tests that failed during the commit of HBASE-17294</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestSnapshotFromClient.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-12-19 01:00:00" id="17333" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-17294 always ensures CompactingMemstore is default</summary>
      <description>Was the purpose of HBASE-17294 is to make Compacting Memstore as default? Am not sure on that. But that patch makes DefaultMemstore as a Noop. This JIRA is to discuss and revert back to default memstore only if the family is not configured for in memory flush/compaction.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-5-20 01:00:00" id="17343" opendate="2016-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Compacting Memstore default in 2.0 with BASIC as the default type</summary>
      <description>FYI anastas, eshcar and ebortnik.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactingMemStore.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-20 01:00:00" id="17349" opendate="2016-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add doc for regionserver group-based assignment</summary>
      <description>Currently, this feature has no doc. I tried to use it last night and it took reading unit tests to figure how to get it going and how to use it. I added a bit to the release notes in the parent.Marking as a critical on 2.0.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-21 01:00:00" id="17352" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hbase-assembly build with bash 4</summary>
      <description>hbase-assembly fails to build with bash 4.[DEBUG] Executing command line: [env, bash, -c, cat maven-shared-archive-resources/META-INF/NOTICE \ `find /Users/jg/github/hbase/hbase-assembly/target/dependency -iname NOTICE -or -iname NOTICE.txt` \][ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.4.0:exec (concat-NOTICE-files) on project hbase-assembly: Command execution failed. Process exited with an error: 1 (Exit value: 1) -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.codehaus.mojo:exec-maven-plugin:1.4.0:exec (concat-NOTICE-files) on project hbase-assembly: Command execution failed.The error is caused by the trailing backslash in the bash command for concat-NOTICE-files. You can see the behavioral difference between bash 3 and 4 with the following snippet.$ # Using bash 3$ /bin/bash -c 'cat &lt;(echo foo) \' &amp;&amp; echo good || echo badfoogood$ # Using bash 4$ /usr/local/bin/bash -c 'cat &lt;(echo foo) \' &amp;&amp; echo good || echo badfoocat: \: No such file or directorybad</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-30 01:00:00" id="17396" opendate="2016-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add first async admin impl and implement balance methods</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncSingleRequestRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCallerFactory.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnectionImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-9 01:00:00" id="17441" opendate="2017-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>precommit test "hadoopcheck" not properly testing Hadoop 3 profile</summary>
      <description>HBASE-14061 made a change that caused building against hadoop 3 to fail, but the hadoopcheck precommit test gave the change a +1.</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-4,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-1-18 01:00:00" id="17480" opendate="2017-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove split region code from Region Server</summary>
      <description>HBASE-14551 moves the split region logic to the master-side. With the code in HBASE-14551, the split transaction code from region server side became un-used. There is no need to keep region_server-side split region code. We should remove them to avoid code duplication.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionWithInMemoryFlush.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestCoprocessorInterface.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransactionFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-19 01:00:00" id="17488" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WALEdit should be lazily instantiated</summary>
      <description>Some trivial improvement. create the WALEdit on step 3 instead of step 2 count the cells from coprocessor dont count the mutations which contain the Durability.SKIP_WAL property</description>
      <version>None</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-23 01:00:00" id="17511" opendate="2017-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement enable/disable table methods</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-23 01:00:00" id="17516" opendate="2017-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table quota not taking precedence over namespace quota</summary>
      <description>Romil Choksi found a bug in the current patch-set where a more restrictive table quota did not take priority over a less-restrictive namespace quota.Turns out some of the logic to handle this case was incorrect.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="17518" opendate="2017-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Reference Guide has a syntax error</summary>
      <description>The image of "HFile Version 2 Structure" in Appendix F of HBase Reference Guide (pdf) is missing because of a wrong asciidoc syntax:image:hfilev2.png&amp;#91;HFile Version 2&amp;#93;modified as:image::hfilev2.png&amp;#91;HFile Version 2&amp;#93;it should be a double colon instead of single one</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.appendix.hfile.format.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-8-8 01:00:00" id="1754" opendate="2009-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use TCP keepalives</summary>
      <description>If a regionserver crashes while the client is engaged in IPC with it at a vulnerable point in the TCP FSM (ESTABLISHED, no outstanding data to send), the IPC will be stuck waiting "forever" (&gt; 12 hours, etc.). This hoses the client, especially if it is trying to look up a region in META. Worse, it is not possible to restart the regionserver if the hung client is colocated with it on the same host, because the OS will consider port 60020 bound and in use, unless the client is forcibly killed. Killing some types of applications &amp;#8211; especially long running processes which can't redo work from a checkpoint but must start over from the beginning &amp;#8211; can be very painful. Investigate if TCP keepalives can be enabled at the IPC level.</description>
      <version>None</version>
      <fixedVersion>0.20.0,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-27 01:00:00" id="17562" opendate="2017-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove documentation for coprocessor execution times after HBASE-14205</summary>
      <description>Thanks, Steen Manniche for reporting. Opened up a subtask. Feel free to pick it up if you want to patch it yourself. Otherwise, I can do a quick patch.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.images.coprocessor.stats.png</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-30 01:00:00" id="17566" opendate="2017-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jetty upgrade fixes</summary>
      <description>There are some issues with Jetty 9.2.6 upgrade: static contents are not available, therefore html pages do not show correctly, logs are not accessible.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.TestServletFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.TestHttpServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.resource.JerseyResource.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.HttpServerFunctionalTest.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.http.conf.TestConfServlet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.locking.TestEntityLocks.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.DumpReplicationQueues.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.SslSocketConnectorSecure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.HttpServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.http.AdminAuthorizedServlet.java</file>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.client.Response.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-31 01:00:00" id="17569" opendate="2017-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase-Procedure module need to support mvn clean test -PskipProcedureTests to skip unit test</summary>
      <description>From Reference guide, we know that To skip the tests in the hbase-server module, you would run:mvn clean test -PskipServerTestsWe can also support this command in hbase-procedure module.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-10 01:00:00" id="1757" opendate="2009-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST server runs out of fds</summary>
      <description>Using the REST server at pset, we ran out of descriptors. Small issue w/ how the new HTables were being made (new one each time rather than resuse).</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.TimestampModel.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.TableModel.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.ScannerModel.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.RowModel.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.rest.AbstractModel.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-2 01:00:00" id="17581" opendate="2017-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn clean test -PskipXXXTests does not work properly for some modules</summary>
      <description>mvn clean test -PskipXXXTests will skip all tests in XXX module, it works for hbase-server, when we use mvn clean test -PskipServerTests, it will skip all small and medium tests in hbase-server module. However for some module like hbase-common, this command only skip small tests, the medium tests are still executed.Need to ensure all components work in the same way</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-prefix-tree.pom.xml</file>
      <file type="M">hbase-metrics.pom.xml</file>
      <file type="M">hbase-metrics-api.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-hadoop-compat.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-2 01:00:00" id="17588" opendate="2017-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused imports brought in by HBASE-17437</summary>
      <description>During the review process of https://issues.apache.org/jira/browse/HBASE-17437, some unused imports (or unused code) was missed. This patch removes those.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALActionsListener.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRollAbort.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.AbstractTestFSWAL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestWALObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.DisabledWALProvider.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSHLog.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.fs.HFileSystem.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-4 01:00:00" id="17596" opendate="2017-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement add/delete/modify column family methods</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="17600" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement get/create/modify/delete/list namespace admin operations</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.RequestConverter.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRpcRetryingCaller.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-11 01:00:00" id="17634" opendate="2017-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up the usage of Result.isPartial</summary>
      <description>We have marked Result.isPartial as deprecated in HBASE-17599. This issue aims to remove the isPartial usage in our code base.</description>
      <version>1.4.0,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPartialResultsFromClientSide.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-14 01:00:00" id="17649" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST API for scan should return 410 when table is disabled</summary>
      <description>Background:When scanning using REST API, if the table is disabled in the middle of the scan, code 410 (Gone) should be returned.Currently in 1.3 and lower releases, ScannerResultGenerator#next() doesn't handle TableNotEnabledException, leading to the exception being ignored and code 204 is returned to caller.This was first spotted by &amp;#91;~Apache9&amp;#93; when investigating HBASE-17603.Patch for HBASE-17603 contains fix for this problem.This issue fixes the bug for 1.3 and lower branches and adds test for branch-1 and master branch.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-8-14 01:00:00" id="1767" opendate="2009-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test zookeeper broken in trunk and 0.20 branch; broken on hudson too</summary>
      <description>Anyone want to take a look at this?</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-21 01:00:00" id="17673" opendate="2017-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Monitored RPC Handler not shown in the WebUI</summary>
      <description>This issue has been fixed once in HBASE-14674. But, I noticed that almost all RS in our production environment still have this problem. Strange thing is that newly started servers seems do not affected. Digging for a while, then I realize the CircularFifoBuffer introduced by HBASE-10312 is the root cause. The RPC hander's monitoredTask only create once, if the server is flooded with tasks, RPC monitoredTask can be purged by CircularFifoBuffer, and then never visible in WebUI.So my solution is creating a list for RPC monitoredTask separately. It is OK to do so since the RPC handlers remain in a fixed number. It won't increase or decrease during the lifetime of the server.</description>
      <version>3.0.0-alpha-1,1.2.4,1.1.8,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.monitoring.TestTaskMonitor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.monitoring.TaskMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-22 01:00:00" id="17677" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ServerName parsing from directory name should be more robust to errors from guava&amp;#39;s HostAndPort</summary>
      <description>our internal Address facade over HostAndPort directly passes on any failures from the underlying Guava implementation. Right now when we parse a ServerName from a WAL directory we properly handle if guava gives us an IllegalArgumentException but we do not handle if it gives us a IllegalStateException. e.g. it uses the former for "I couldn't parse anything out of this string" and the latter for "I parsed a host name but didn't see a port".</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,1.3.1,1.2.5,1.1.10,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALMethods.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.AbstractFSWALProvider.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-25 01:00:00" id="17699" opendate="2017-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestLockProcedure</summary>
      <description>TestLockProcedure is failing consistently after HBASE-17605. It's interesting that HadoopQA didn't report any test failures on that jira. Anyways, need to fix the test now.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.SimpleProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureExecutor.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.AbstractProcedureScheduler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-5 01:00:00" id="17736" opendate="2017-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some options can&amp;#39;t be configured by the shell</summary>
      <description>The lost options for table are shown below. setFlushPolicyClassName setPriority setRegionMemstoreReplication setRegionSplitPolicyClassName</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="17737" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift2 proxy should support scan timeRange per column family</summary>
      <description>see discussion in HBASE-14872 and HBASE-14355</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-7 01:00:00" id="17745" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support short circuit connection for master services</summary>
      <description>As titled, now we have short circuit connection, but no short circuit for master services, and we propose to support it in this JIRA.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-8-18 01:00:00" id="1776" opendate="2009-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make rowcounter enum public</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-8 01:00:00" id="17760" opendate="2017-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HDFS Balancer doc is misleading</summary>
      <description>HBASE-15332 added a doc note about how to use HDFS-6133, but the steps it adds are incorrect. The specific balancer command provided in the doc note is incorrect and not required.Since HBase uses favored nodes features internally (HBASE-7932), and HBASE-7942 extended that information to cover HDFS hinting too, the only step required in the doc note is to enable the pinning feature DN-side.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-18 01:00:00" id="17803" opendate="2017-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE always re-creates table when we specify the split policy</summary>
      <description>I find this bug when i run the tests for HBASE-17623The critical code is shown below.if ((exists &amp;&amp; opts.presplitRegions != DEFAULT_OPTS.presplitRegions) || (!isReadCmd &amp;&amp; desc != null &amp;&amp; desc.getRegionSplitPolicyClassName() != opts.splitPolicy) || (!isReadCmd &amp;&amp; desc != null &amp;&amp; desc.getRegionReplication() != opts.replicas)) {</description>
      <version>1.4.0,2.0.0</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="17807" opendate="2017-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct the value of zookeeper.session.timeout in hbase doc</summary>
      <description>I met a regionserver gc problem, and the regionserver log show me to read the dochttp://hbase.apache.org/book.html#trouble.rs.runtime.zkexpiredIf you wish to increase the session timeout, add the following to your hbase-site.xml to increase the timeout from the default of 60 seconds to 120 seconds.&lt;property&gt; &lt;name&gt;zookeeper.session.timeout&lt;/name&gt; &lt;value&gt;1200000&lt;/value&gt;&lt;/property&gt;the value should be 120000120s) instead of 1200000(1200s</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-23 01:00:00" id="17826" opendate="2017-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backup: submit M/R job to a particular Yarn queue</summary>
      <description>We need this to be configurable. Currently, all M/R jobs are submitted to a default queue.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.RestoreDriver.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.impl.BackupCommands.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupRestoreConstants.java</file>
      <file type="M">hbase-backup.src.main.java.org.apache.hadoop.hbase.backup.BackupRequest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-29 01:00:00" id="17847" opendate="2017-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update documentation to include positions on recent Hadoop releases</summary>
      <description>per dev@hbase discussion on how to handle the most recent round of Hadoop releases, get our docs updated.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-29 01:00:00" id="17849" opendate="2017-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PE tool random read is not totally random</summary>
      <description>Recently we were using the PE tool for doing some bucket cache related performance tests. One thing that we noted was that the way the random read works is not totally random.Suppose we load 200G of data using --size param and then we use --rows=500000 to do the randomRead. The assumption was among the 200G of data it could generate randomly 500000 row keys to do the reads.But it so happens that the PE tool generates random rows only on those set of row keys which falls under the first 500000 rows. This was quite evident when we tried to use HBASE-15314 in our testing. Suppose we split the bucket cache of size 200G into 2 files each 100G the randomReads with --rows=500000 always lands in the first file and not in the 2nd file. Better to make PE purely random.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestPerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-31 01:00:00" id="17858" opendate="2017-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update refguide about the IS annotation if necessary</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-4 01:00:00" id="17873" opendate="2017-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the IA.Public annotation to IA.Private for unstable API</summary>
      <description>As discussed in mailing list and HBASE-17857.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-4 01:00:00" id="17875" opendate="2017-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document why objects over 10MB are not well-suited for hbase</summary>
      <description>A new-user who has patently done their homework is unable to find in doc why objects above 10MB are not recommended for hbase. Lets add explanation that has a link from MOB on what happens when objects this size are requested form HBase.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.hbase.mob.adoc</file>
      <file type="M">src.main.asciidoc..chapters.faq.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-4 01:00:00" id="17877" opendate="2017-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve HBase&amp;#39;s byte[] comparator</summary>
      <description>vik.karma did some extensive tests and found that Hadoop's version is faster - dramatically faster in some cases.Patch forthcoming.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">NOTICE.txt</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.ByteBufferUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-5 01:00:00" id="17881" opendate="2017-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the ByteBufferCellImpl</summary>
      <description>We should substitute ByteBufferKeyValue for ByteBufferCellImpl</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestSingleColumnValueFilter.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestCellUtil.java</file>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.TestCellComparator.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.filter.TestKeyOnlyFilter.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.filter.TestComparators.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-10 01:00:00" id="17898" opendate="2017-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update dependencies</summary>
      <description>General issue to cover updating old, stale dependencies for hbase2 release. Lets make subissues doing each.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-11 01:00:00" id="17903" opendate="2017-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Corrected the alias for the link of HBASE-6580</summary>
      <description>Previous versions of this guide discussed `HTablePool`, which was deprecated in HBase 0.94, 0.95, and 0.96, and removed in 0.98.1, by link:https://issues.apache.org/jira/browse/HBASE-6580[HBASE-6500], or `HConnection`, which is deprecated in HBase 1.0 by `Connection`.Please use link:http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Connection.html[Connection] instead.6500 -&gt; 6580</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-11 01:00:00" id="17905" opendate="2017-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbase-spark] bulkload does not work when table not exist</summary>
      <description>when using HBase-Spark bulkload api, an argument of tablename is needed, the bulkload can run successfully only if table exist in HBase. If table not exist, the bulkload can not run successfully and it even do not report any errors or throw exception.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-14 01:00:00" id="17917" opendate="2017-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use pread by default for all user scan and switch to streaming read if needed</summary>
      <description>As said in the parent issue. We need some benchmark here first.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorScanPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestIOFencing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestReversibleScanners.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMajorCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultCompactSelection.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBlocksScanned.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.querymatcher.TestUserScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.querymatcher.TestCompactionScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MockStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.DelegatingKeyValueScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlock.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileReader.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SegmentScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ReversedStoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.NonLazyKeyValueScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-14 01:00:00" id="17918" opendate="2017-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>document serial replication</summary>
      <description>It looks like HBASE-9465 addresses one of the major flaws in our existing replication (namely that order of delivery is not assured). All I see in the reference guide is a note on hbase.serial.replication.waitingMs. Instead we should cover this in the replication section, especially given that we call out the order of delivery limitation.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-14 01:00:00" id="17925" opendate="2017-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn assembly:single fails against hadoop3-alpha2</summary>
      <description>generally to build tarballs against hadoop3 alpha2 we'd use mvn -Dhadoop.profile=3.0 -Dhadoop.three-version=3.0.0-alpha2 assembly:single -DskipTestsThis currently fails.</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,1.1.10,1.2.6,1.3.2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-17 01:00:00" id="17929" opendate="2017-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more options for PE tool</summary>
      <description>For better evaluating scan performance.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-4-24 01:00:00" id="17950" opendate="2017-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write the chunkId also as Int instead of long into the first byte of the chunk</summary>
      <description>I think some issue happened while updating the patch. The chunkId was converted to int every where but while writing it has been written as a long. Will push a small patch to change it.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemstoreLABWithoutPool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStoreLAB.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStoreChunkPool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestDefaultMemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.OnheapChunk.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.OffheapChunk.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Chunk.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="17952" opendate="2017-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The new options for PE tool do not work</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-24 01:00:00" id="17954" opendate="2017-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch findbugs implementation to spotbugs</summary>
      <description>The Hadoop folks got some nice finds out of switching their findbugs plugin to use the new spotbugs fork instead, let's try it out too. (ref HADOOP-14316 for details)</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-24 01:00:00" id="17955" opendate="2017-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Commit Reviewboard comments from Vlad</summary>
      <description>Need to commit the local changes from Vlad's review on https://reviews.apache.org/r/58364/</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionServerRegionSpaceUseReport.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestTableSpaceQuotaViolationNotifier.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestSpaceQuotas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestRegionServerSpaceQuotaManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaStatusRPCs.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreWithMiniCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestQuotaObserverChoreRegionReports.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestMasterSpaceQuotaObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.TestActivePolicyEnforcement.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.SpaceQuotaHelperForTests.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.quotas.policies.TestBulkLoadCheckingViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.RSRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.TableQuotaSnapshotStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.SpaceViolationPolicyEnforcementFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.SpaceQuotaRefresherChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.SpaceLimitingException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.RegionServerSpaceQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.QuotaObserverChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.policies.NoWritesViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.policies.NoInsertsViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.policies.DisableTableViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.policies.BulkLoadVerifyingViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.policies.AbstractViolationPolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.NamespaceQuotaSnapshotStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterSpaceQuotaObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.MasterQuotaManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.FileSystemUtilizationChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.quotas.ActivePolicyEnforcement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-protocol.src.main.protobuf.Quota.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.QuotaProtos.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.RegionServerStatus.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.Quota.proto</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.RegionServerStatusProtos.java</file>
      <file type="M">hbase-protocol-shaded.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.generated.QuotaProtos.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterQuotaSourceImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterQuotaSourceFactoryImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerQuotaSource.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterQuotaSource.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.SpaceViolationPolicy.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.SpaceQuotaSnapshot.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.SpaceLimitSettings.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.QuotaTableUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.QuotaSettingsFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-25 01:00:00" id="17956" opendate="2017-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Raw scan should ignore TTL</summary>
      <description>For now we will also test TTL to check if a cell is expired for raw scan. Since we can even return delete marker for a raw scan, I think it is also reasonable to eliminate the TTL check.</description>
      <version>1.4.0,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestScannersFromClientSide.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-5-28 01:00:00" id="17976" opendate="2017-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove stability annotation from public audience-marked classes</summary>
      <description>Looks like I grabbed HBASE-17857 in the last rebase, but missed this test failing.</description>
      <version>None</version>
      <fixedVersion>HBASE-16961,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.quotas.SpaceViolationPolicy.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-6-2 01:00:00" id="17982" opendate="2017-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Is the word "occured" should be "occurred ?</summary>
      <description>I have find some spelling may have a problem</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestJMXConnectorServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestMasterReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileBlockIndex.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestComparatorSerialization.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.DeletionListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.backup.HFileArchiver.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.policies.PeriodicRandomActionPolicy.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.chaos.policies.DoActionsOncePolicy.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ClientIdGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-2 01:00:00" id="17985" opendate="2017-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inline package manage updates with package installation in Yetus Dockerfile</summary>
      <description>Context: https://lists.apache.org/thread.html/d34093557cc510bb8b1dc4b37f8a729b74577c7d4eaecdc3f1badea1@%3Cdev.hbase.apache.org%3EThe way Docker images are built for the Yetus-based PreCommit, we may accidentally use a pre-built image that has a stale package-manager cache. If the distribution updates their published packages (removing an older version, adding a new one), our (stale) client will try to pull the older version which is missing, failing.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.2.6,1.3.2,1.1.11,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-4 01:00:00" id="17991" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more details about compaction queue on /dump</summary>
      <description>RS dump information as followsRS Queue:===========================================================Compaction/Split Queue summary: compaction_queue=(20:0), split_queue=0, merge_queue=0Compaction/Split Queue dump: LargeCompation Queue: Request = regionName=usertable,user4180497275766179957,1491904095205.1d4085e4438752f3611f66e7b043fe44., storeName=0, fileCount=1, fileSize=1.9 G (1.9 G), priority=1, time=21697920409804647 Request = regionName=usertable,user4568009753557153251,1491904099262.95bf004e3c9b35a58c60ca5d5b11d190., storeName=0, fileCount=1, fileSize=1.9 G (1.9 G), priority=1, time=21697920413223800 SmallCompation Queue: Store = b, pri = 108 Store = b, pri = 108 Store = b, pri = 108 Store = b, pri = 108 Store = b, pri = 108 Store = b, pri = 109Compaction queue information will be displayed on page /dump.If compation has selected the file, it will print the details information of the compation, otherwise only print storename and priority(Store = b, pri = 108) which is useless for us.So, we should also print more detailed information, such as regionName, starttime etc.</description>
      <version>2.0.0</version>
      <fixedVersion>1.4.0,1.3.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  <bug fixdate="2017-5-15 01:00:00" id="18049" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>It is not necessary to re-open the region when MOB files cannot be found</summary>
      <description>In HBASE-17712, we try to re-open the region when store files cannot be found. This is useful for store files in a region, but is not necessary when the MOB files cannot be found, because the store files in a region only contain the references to the MOB files and a re-open of a region doesn't help the lost MOB files.In this JIRA, we will directly throw DNRIOE only when the MOB files are not found in MobStoreScanner and ReversedMobStoreScanner. Other logics keep the same.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HMobStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-16 01:00:00" id="18052" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add document for async admin</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-16 01:00:00" id="18056" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change CompactingMemStore in BASIC mode to merge multiple segments in pipeline</summary>
      <description>Under HBASE-16417 it was decided that CompactingMemStore in BASIC mode should merge multiple ImmutableSegments in CompactionPipeline. Basic+Merge actually demonstrated reduction in GC, alongside improvement in other metrics.However, the limit on the number of segments in pipeline is still set to 30. Under this JIRA it should be changed to 1, as it was tested under HBASE-16417.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreCompactor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-9-1 01:00:00" id="1809" opendate="2009-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE thrown in BoundedRangeFileInputStream</summary>
      <description>NPE is thrown in BoundedRangeFileInputStream.read when attempting to synchronize on 'in' (line 97).This probably means the BRFIS was created with a null FSDIS.</description>
      <version>None</version>
      <fixedVersion>0.20.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-22 01:00:00" id="18091" opendate="2017-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add API for who currently holds a lock on namespace/ table/ region and log when state is LOCK_EVENT_WAIT</summary>
      <description>Add API for who currently holds a lock on namespace/ table/ region and log message when state is LOCK_EVENT_WAIT</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.SimpleProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.ProcedureScheduler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-24 01:00:00" id="18104" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[AMv2] Enable aggregation of RPCs (assigns/unassigns, etc.)</summary>
      <description>Machinery is in place to coalesce AMv2 RPCs (Assigns, Unassigns). It needs enabling and verification. From '6.3 We dont do the aggregating of Assigns' of https://docs.google.com/document/d/1eVKa7FHdeoJ1-9o8yZcOTAQbv0u0bblBlCCzVSIn69g/edit#heading=h.uuwvci2r2tz4</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.RSProcedureDispatcher.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-25 01:00:00" id="18108" opendate="2017-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure WALs are archived but not cleaned; fix</summary>
      <description>The Procedure WAL files used to be deleted when done. HBASE-14614 keeps them around in case issue but what is missing is a GC for no-longer-needed WAL files. This one is pretty important.From WALProcedureStore Cleaner TODO in https://docs.google.com/document/d/1eVKa7FHdeoJ1-9o8yZcOTAQbv0u0bblBlCCzVSIn69g/edit#heading=h.r2pc835nb7vi</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.cleaner.TestLogsCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.TimeToLiveLogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.LogCleaner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.CleanerChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.cleaner.BaseFileCleanerDelegate.java</file>
      <file type="M">hbase-common.src.main.resources.hbase-default.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-5-26 01:00:00" id="18118" opendate="2017-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default storage policy if not configured cannot be "NONE"</summary>
      <description>HBase can't use 'NONE' as default storage policy if not configured because HDFS supports no such policy. This policy name was probably available in a precommit or early version of the HDFS side support for heterogeneous storage. Now the best default is 'HOT'.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-27 01:00:00" id="18129" opendate="2017-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>truncate_preserve fails when the truncate method doesn&amp;#39;t exists on the master</summary>
      <description>Recently, I runs a rolling upgrade from HBase 0.98.x to HBase 1.2.5. During the master hasn't been upgraded yet, I truncate a table by the command truncate_preserve of 1.2.5, but failed.hbase(main):001:0&gt; truncate_preserve 'cf_logs'Truncating 'cf_logs' table (it may take a while): - Disabling table... - Truncating table... - Dropping table... - Creating table with region boundaries...ERROR: no method 'createTable' for arguments (org.apache.hadoop.hbase.HTableDescriptor,org.jruby.java.proxies.ArrayJavaProxy) on Java::OrgApacheHadoopHbaseClient::HBaseAdminAfter checking code and commit history, I found it's HBASE-12833 which causes this bug.so we should fix it.</description>
      <version>1.2.5,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-1 01:00:00" id="18147" opendate="2017-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly job to check health of active branches</summary>
      <description>We should set up a job that runs Apache Yetus Test Patch's nightly mode. Essentially, it produces a report that considers how the branch measures up against the things we check in our precommit checks.</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-2,1.1.12,2.0.0,1.2.7</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="18149" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The setting rules for table-scope attributes and family-scope attributes should keep consistent</summary>
      <description>I use the following command to create a table.hbase(main):030:0&gt; create 't3',{NAME =&gt; 'f2', BLOCKCACHE =&gt; false}, {COMPACTION_ENABLED =&gt; false}An argument ignored (unknown or overridden): COMPACTION_ENABLED0 row(s) in 1.1390 secondshbase(main):031:0&gt; describe 't3'Table t3 is ENABLEDt3 COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'f2', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'false', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'}1 row(s) in 0.0720 secondsBLOCKCACHE was in effect but COMPACTION_ENABLED didn't take effect.After checking code, I found that if the table-scope attributes value is false, you need to enclose 'false' in single quotation marks while family-scope is not required.so we should keep the consistent logic for table-scope and family-scope.the command alter also have the same problem.</description>
      <version>1.2.5,2.0.0</version>
      <fixedVersion>1.4.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-12 01:00:00" id="18209" opendate="2017-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include httpclient / httpcore jars in build artifacts</summary>
      <description>We need httpclient &amp; httpcore jars to be present when rootdir is placed on s3(a).Attempts to move to the fully shaded amazon-SDK JAR caused problems of its own. (according to steve_l)Here are the versions we should use:&lt;commons.httpclient.version&gt;4.5.2&lt;/commons.httpclient.version&gt;&lt;commons.httpcore.version&gt;4.4.4&lt;/commons.httpcore.version&gt;Currently they are declared test dependency.This JIRA is to move to compile time dependency so that the corresponding jars are bundled in lib directory.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-12-18 01:00:00" id="18232" opendate="2017-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add variable size chunks to the MSLAB</summary>
      <description>Add possibility to create a variable size chunks of memory, so any cell (of any size) can reside on a chunk.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingToCellFlatMapMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompactingMemStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestCellFlatSet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreLABImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreLAB.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ImmutableMemStoreLAB.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ChunkCreator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Chunk.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-26 01:00:00" id="18269" opendate="2017-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jython docs out of date</summary>
      <description>The documentation describing how to launch Jython + HBase is out of date. - https://hbase.apache.org/book.html#jythonFirst, we would set the classpath differently:HBASE_CLASSPATH=/home/hbase/jython.jar bin/hbase org.python.util.jythonThen, the actual code example is out of date too:&gt;&gt;&gt; desc = HTableDescriptor(tablename)&gt;&gt;&gt; desc.addFamily(HColumnDescriptor("content:"))Traceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt; at org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(HColumnDescriptor.java:566) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:470) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:425) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:390) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:338) at org.apache.hadoop.hbase.HColumnDescriptor.&lt;init&gt;(HColumnDescriptor.java:327) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at org.python.core.PyReflectedConstructor.constructProxy(PyReflectedConstructor.java:211)We should make sure that the examples we claim are runnable actually are.</description>
      <version>1.3.1,1.2.6,1.5.0,1.4.2,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-28 01:00:00" id="18290" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestAddColumnFamilyProcedure and TestDeleteTableProcedure</summary>
      <description>These two tests don't pass. Turns out the cause was interesting.We added a workaround for case where procedure WAL could have procs out of order.HBASE-18216 &amp;#91;AMv2&amp;#93; Workaround for HBASE-18152, corrupt procedure WALIf we find a procedure that is not 'increasing' &amp;#8211; of a later timestamp or procid &amp;#8211; then we'd skip the application of the 'old' proc. The workaround was until we figure in what scenarios we can write procedures out of order (seems to be rare and high-concurrency... TBD).These two tests trip FAILs and ROLLBACKs (double delete of table or disable of an already disabled table). They are good tests. But procedures that get marked FAIL or ROLLEDBACK will have procids that are less than current. Makes it so we skipped adding the ROLLBACK and so finishing up the procedure.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALFormatReader.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-13 01:00:00" id="18588" opendate="2017-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify we&amp;#39;re using netty .so epolling on linux post HBASE-18271</summary>
      <description>This is a task to verify that indeed we are using .so native epoll on linux. This task is probably unnecessary since we'd fail on the linux build box if this was not in place but verify that our relocation is indeed finding the native code. Assigned myself.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-23 01:00:00" id="18667" opendate="2017-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable error-prone for hbase-protocol-shaded</summary>
      <description>This is all generated code that we shouldn't be running extra analysis on because it adds a lot of noise to the build, and also takes a very long time (15 minutes on my machine). Let's make it fast and simple.Even when we run with error-prone enabled for the rest of the build, it should not apply here.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-11 01:00:00" id="18791" opendate="2017-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE_HOME/lib does not contain hbase-mapreduce-${project.version}-tests.jar</summary>
      <description>hbase peError: Could not find or load main class org.apache.hadoop.hbase.PerformanceEvaluationhbase lttError: Could not find or load main class org.apache.hadoop.hbase.util.LoadTestToolThose class are in hbase-mapreduce-test.jar</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.components.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-12-11 01:00:00" id="18988" opendate="2017-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add release managers to reference guide</summary>
      <description>Reference guide lists release managers only up to version 1.3. We should have a complete list there.http://hbase.apache.org/book.html#_release_managers</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-10-9 01:00:00" id="1899" opendate="2009-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use scanner caching in shell count</summary>
      <description>Since the shell count now uses the FirstKeyOnlyFilter, it's safe to combine it with scanner caching for huge count speedups.</description>
      <version>None</version>
      <fixedVersion>0.20.2,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-11-9 01:00:00" id="19227" opendate="2017-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly jobs should archive JVM dumpstream files</summary>
      <description>came up on dev@ discussion about some of our current nightly test failures. when surefire fails to launch a test JVM instance, the details go into a file that we currently don't archive:&amp;#91;ERROR&amp;#93; Please refer to dump files (if any exist) &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dump, &amp;#91;date&amp;#93;.dumpstream and &amp;#91;date&amp;#93;-jvmRun&amp;#91;N&amp;#93;.dumpstream.Add them to the default archive pattern.</description>
      <version>None</version>
      <fixedVersion>1.0.4,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-9 01:00:00" id="19228" opendate="2017-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly job should gather machine stats.</summary>
      <description>leverage the script added in HBASE-19189 to get machine stats when running nightly</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.gather.machine.environment.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-9 01:00:00" id="19229" opendate="2017-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly script to check source artifact should not do a destructive git operation without opt-in</summary>
      <description>right now we have a "git please destroy all this stuff" command in the check of the source artifact. we shouldn't do this unless the person invoking the script has indicated that's okay (e..g through a cli flag).</description>
      <version>None</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-beta-1,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.source-artifact.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-17 01:00:00" id="19290" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce zk request when doing split log</summary>
      <description>We observe once the cluster has 1000+ nodes and when hundreds of nodes abort and doing split log, the split is very very slow, and we find the regionserver and master wait on the zookeeper response, so we need to reduce zookeeper request and pressure for big cluster.(1) Reduce request to rsZNode, every time calculateAvailableSplitters will get rsZNode's children from zookeeper, when cluster is huge, this is heavy. This patch reduce the request. (2) When the regionserver has max split tasks running, it may still trying to grab task and issue zookeeper request, we should sleep and wait until we can grab tasks again.</description>
      <version>3.0.0-alpha-1,1.5.0,2.0.0</version>
      <fixedVersion>1.5.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coordination.ZkSplitLogWorkerCoordination.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-30 01:00:00" id="19390" opendate="2017-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert to older version of Jetty 9.3</summary>
      <description>As discussed in HBASE-19256 we will have to temporarily revert to Jetty 9.3 due existing issues with 9.4 and Hadoop3. Once HBASE-19256 is resolved we can revert to 9.4.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-12-15 01:00:00" id="19525" opendate="2017-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RS side changes for moving peer modification from zk watcher to procedure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationTrackerZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.replication.TestDummyModifyPeerProcedure.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.replication.DummyModifyPeerProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.Replication.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RefreshPeerCallable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.BaseReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ReplicationSourceService.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.handler.RSProcedureHandler.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationTrackerZKImpl.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeerZKImpl.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeersZKImpl.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationPeer.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationListener.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.shaded.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-15 01:00:00" id="19526" opendate="2017-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hadoop version to 3.0 GA</summary>
      <description>We're still building against hadoop 3.0-beta1, while GA is recently released. We should update, hopefully no surprises.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-12-17 01:00:00" id="19536" opendate="2017-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client side changes for moving peer modification from zk watcher to procedure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-12-20 01:00:00" id="19564" opendate="2017-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Procedure id is missing in the response of peer related operations</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.ModifyPeerProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-14 01:00:00" id="19996" opendate="2018-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some nonce procs might not be cleaned up (follow up HBASE-19756)</summary>
      <description>Follow up toHBASE-19756which dealt with NPEs during proc cleanup. Unfortunately, the patch for branch-1might notremove some valid procs too. Thebranch-2 patchdoesn't have this problem.This fixes the branch-1 bug and also adds another test to branch-2. Thanks to toffer for flagging this internally.</description>
      <version>None</version>
      <fixedVersion>1.3.2,2.0.0-beta-2,1.4.2,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.procedure.TestFailedProcCleanup.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-14 01:00:00" id="20000" opendate="2018-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the quantum logic in FairQueue, always put high priority queue in front</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureSchedulerConcurrency.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestMasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureSchedulerPerformanceEvaluation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureEnv.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AvlUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-9 01:00:00" id="20164" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>failed hadoopcheck should add footer link</summary>
      <description>thought for sure this already had an issue, busbey, but I can't find it.</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.3,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-9 01:00:00" id="20165" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell command to make a normal peer to be a serial replication peer</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.replication.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.list.peers.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.replication.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-11-30 01:00:00" id="2017" opendate="2009-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set configurable max value size check to 10MB</summary>
      <description>Make the user think about whether storing larger values than 10MB is a good idea.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-13 01:00:00" id="20182" opendate="2018-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can not locate region after split and merge</summary>
      <description>When implementing serial replication feature in HBASE-20046, I found that when splitting a region, we will not remove the parent region, instead we will mark it offline.And when locating a region, we will only scan one row so if we locate to the offlined region then we are dead.This will not happen for splitting, since one of the new daughter regions have the same start row with the parent region, and the timestamp is greater so when doing reverse scan we will always hit the daughter first.But if we also consider merge then bad things happen. Consider we have two regions A and B, we split B to C and D, and then merge A and C to E, then ideally the regions should be E and D, but actually the regions in meta will be E, B and D, and they all have different start rows. If you use a row within the range of old region C, then we will always locate to B and throw exception.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncNonMetaRegionLocatorConcurrenyLimit.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionInfoBuilder.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncNonMetaRegionLocator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-13 01:00:00" id="20189" opendate="2018-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in Required Java Version error message while building HBase.</summary>
      <description>Change 'requirs' to 'requires'. See below:$ mvn clean install -DskipTests...[WARNING] Rule 2: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:Java is out of date. HBase requirs at least version 1.8 of the JDK to properly build from source. You appear to be using an older version. You can use either "mvn -version" or "mvn enforcer:display-info" to verify what version is active. See the reference guide on building for more information: http://hbase.apache.org/book.html#build</description>
      <version>None</version>
      <fixedVersion>1.3.2,1.4.3,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-1 01:00:00" id="2019" opendate="2009-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[EC2] Prompt for and remember credentials if not configured</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.contrib.ec2.bin.image.create-hbase-image-remote</file>
      <file type="M">src.contrib.ec2.bin.hbase-ec2-env.sh</file>
      <file type="M">src.contrib.ec2.bin.create-hbase-image</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-14 01:00:00" id="20190" opendate="2018-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix default for MIGRATE_TABLE_STATE_FROM_ZK_KEY</summary>
      <description>All works but the flag name will confuse: name is MIGRATE_TABLE_STATE_FROM_ZK_KEY but you'd set it to true to NOT migrate from zk. Found by tedyu in the parent issue.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.TableStateManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-15 01:00:00" id="20207" opendate="2018-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update doc on the steps to revert RegionServer groups feature</summary>
      <description>Reverting the rsgroup feature from a hbase cluster involves additional steps on top of removing the changes to hbase-site.xml. Documenting it will help cluster admins to be aware of them when rsgroup feature is being enabled.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-15 01:00:00" id="20210" opendate="2018-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Note in refguide that RSGroups API is private, not for public consumption; shell is only access</summary>
      <description>Came up yesterday in an internal conversation. Mike Drob noticed that the CPEP for RSGroups is marked audience Private which sort of makes sense given this an evolving feature. The refguide though makes it sound as though you can drive RSGroups from shell or API. Let me shutdown the talk of API being public.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-22 01:00:00" id="20253" opendate="2018-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error message is missing for restore_snapshot</summary>
      <description>When the table is not disabled and restore_snapshot executed the error message is useless, only displays the table name.hbase(main):007:0&gt; restore_snapshot 'tsnap'ERROR: tRestore a specified snapshot.The restore will replace the content of the original table,bringing back the content to the snapshot state.The table must be disabled.Examples: hbase&gt; restore_snapshot 'snapshotName'Following command will restore all acl from snapshot table into the table. hbase&gt; restore_snapshot 'snapshotName', {RESTORE_ACL=&gt;true}Took 0.1044 seconds</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-22 01:00:00" id="20256" opendate="2018-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document commands that do not work with 1.2 shell</summary>
      <description>Some commands do not work from 1.2 shell when running against 2.0 server. Add a section to the reference guide mentioning the incompatibilities.Some of these are collected in this document:https://docs.google.com/document/d/1l2ad5G_GUk0WQ02xKeKO6mTbpdyZjU1NGuPPI42Wbf4/edit</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-23 01:00:00" id="20264" opendate="2018-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Java prerequisite section with LTS rec and status of current GA JDKs</summary>
      <description>Per the thread [DISCUSS] strategy on Java versions Add Java 9 and Java 10 to the support matrix as NT Add a NOTE to Java prereqs about "use a LTS version"For now, leave out talk about planning for timelines on LTS additions or dropping older JDK support. Once we get over the initial hurdle of prepping for Java 11 we'll hopefully have enough info to know how realistic the things talked about in the thread are and we can include a writeup.</description>
      <version>1.5.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-24 01:00:00" id="20275" opendate="2018-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] clarify impact to hfile command from HBASE-17197</summary>
      <description>Feedback on HBASE-19158 from mdrobTools:HBASE-17197It's not clear to me from the patch on HBASE-17197 if this was actually a change that needs to be called out. So tasks:1) Figure out if the hfile command args from HBase 1.y still works2) Update the title of HBASE-17197 to match what the change in the jira ended up being3) If hfile changed in an incompatible way, add it to the upgrade section and make sure the refguide section "hfile_tool" is up to date.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-26 01:00:00" id="20285" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete all last pushed sequence ids when removing a peer or removing the serial flag for a peer</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestSerialReplication.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestAddToSerialReplicationPeer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.SerialReplicationTestBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.UpdatePeerConfigProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.ReplicationPeerManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.RemovePeerProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.EnablePeerProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.DisablePeerProcedure.java</file>
      <file type="M">hbase-replication.src.test.java.org.apache.hadoop.hbase.replication.TestZKReplicationQueueStorage.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ZKReplicationQueueStorage.java</file>
      <file type="M">hbase-replication.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueueStorage.java</file>
      <file type="M">hbase-protocol-shaded.src.main.protobuf.MasterProcedure.proto</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-26 01:00:00" id="20288" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] upgrade section needs to call out DLR</summary>
      <description>copied out of HBASE-13428. I think from stack Make sure Distributed Log Replay is not enabled on your hbase1 cluster; it was problematic in hbase1 and does not work at all in hbase2.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="20289" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparator for NormalizationPlan breaks comparator&amp;#39;s convention</summary>
      <description>Comparator must meet the condition: sign(comparator(plan1, plan2)) = - sign(comparator(plan2, plan1)).Current implementation breaks above condition.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.normalizer.TestSimpleRegionNormalizer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-2 01:00:00" id="20328" opendate="2018-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix local backup master start command in documentation</summary>
      <description>In "2.3. Pseudo-Distributed Local Install" section of documentation, a command for starting backup masters lacks "start" argument.$ ./bin/local-master-backup.sh 2 3 5should$ ./bin/local-master-backup.sh start 2 3 5</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-2 01:00:00" id="20329" opendate="2018-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note for operators to refguide on AsyncFSWAL</summary>
      <description>Need a few notes in refguide on this new facility.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-3 01:00:00" id="20332" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded mapreduce module shouldn&amp;#39;t include hadoop</summary>
      <description>AFAICT, we should just entirely skip including hadoop in our shaded mapreduce module1) Folks expect to run yarn / mr apps via hadoop jar / yarn jar2) those commands include all the needed Hadoop jars in your classpath by default (both client side and in the containers)3) If you try to use "user classpath first" for your job as a workaround (e.g. for some library your application needs that hadoop provides) then our inclusion of some but not all hadoop classes then causes everything to fall over because of mixing rewritten and non-rewritten hadoop classes4) if you don't use "user classpath first" then all of our non-relocated-but-still-shaded hadoop classes are ignored anyways so we're just wasting space</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.src.test.resources.ensure-jars-have-correct-contents.sh</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rsgroup.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-hadoop2-compat.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle.xml</file>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-3 01:00:00" id="20333" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>break up shaded client into one with no Hadoop and one that&amp;#39;s standalone</summary>
      <description>there are contexts where we want to stay out of our downstream users way wrt dependencies, but they need more Hadoop classes than we provide. i.e. any downstream client that wants to use both HBase and HDFS in their application, or any non-MR YARN application.Now that Hadoop also has shaded client artifacts for Hadoop 3, we're also providing less incremental benefit by including our own rewritten Hadoop classes to avoid downstream needing to pull in all of Hadoop's transitive dependencies.right now those users need to ensure that any jars from the Hadoop project are loaded in the classpath prior to our shaded client jar. This is brittle and prone to weird debugging trouble.instead, we should have two artifacts: one that just lists Hadoop as a prerequisite and one that still includes the rewritten-but-not-relocated Hadoop classes.We can then use docs to emphasize when each of these is appropriate to use.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-check-invariants.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-3 01:00:00" id="20334" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a test that expressly uses both our shaded client and the one from hadoop 3</summary>
      <description>Since we're making a shaded client that bleed out of our namespace and into Hadoop's, we should ensure that we can show our clients coexisting. Even if it's just an IT that successfully talks to both us and HDFS via our respective shaded clients, that'd be a big help in keeping us proactive.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.source-artifact.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-3 01:00:00" id="20335" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly jobs no longer contain machine information</summary>
      <description>something is up with nightly jobs. they no longer have the machine information from HBASE-19228.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,1.4.4,2.0.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase.nightly.yetus.sh</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.gather.machine.environment.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="20343" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] fix log directory paths</summary>
      <description>The documentation refers to the log directories as .logs, splitlog, etc. These references should be changed to WALs, splitWAL, etc.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase-default.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="20344" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix asciidoc warnings</summary>
      <description>IntelliJ shows some warnings for asciidoc files.1. Markdown Style Heading:&amp;#35;## Required properties2. Asciidoc Old Style Heading:Creating a New Table with Compression On a ColumnFamily ==== &amp;#45;---hbase&gt; create 'test2', { NAME =&gt; 'cf2', COMPRESSION =&gt; 'SNAPPY' } &amp;#45;--- ====3. Warning during buildasciidoctor: WARNING: _chapters/troubleshooting.adoc: line 105: invalid style for listing block: NOTE</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.shell.adoc</file>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc..chapters.hbase.mob.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">src.main.asciidoc..chapters.compression.adoc</file>
      <file type="M">src.main.asciidoc..chapters.backup.restore.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="20346" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] document change to shell tests</summary>
      <description>HBASE-19903 changed how the shell tests are organized and executed, but it missed updating the section on the ref guide that talks about the shell tests.bring it up to date so that folks don't miss a bunch of the tests or add new ones in the wrong place.</description>
      <version>2.0.0-beta-2,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-4 01:00:00" id="20349" opendate="2018-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] upgrade guide should call out removal of prefix-tree data block encoding</summary>
      <description>See HBASE-19179. Needs to be in the upgrade section. Right now there's just an offhand mention in Appendix E about the removal.Since we can no longer read data encoded with prefix tree, we should include instructions on rewriting data.Also ensure we don't have spurious references to it. (the help from ltt that's in the ref guide still lists it, for example).</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.troubleshooting.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-5 01:00:00" id="20355" opendate="2018-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade section incorrectly attempts to link to sections</summary>
      <description>just figured out that I've been linking to things incorrectly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-9 01:00:00" id="20369" opendate="2018-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document incompatibilities between HBase 1.x and HBase 2.0</summary>
      <description>Hi,I compiled a draft document for the HBase incompatibilities from the raw source content that was available in HBase Beta 1 site. Can someone please review and provide a feedbackorshare your comments on this document?Appreciate your support and time.Best Regards,Triguna</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc.book.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-9 01:00:00" id="20371" opendate="2018-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>website landing page should draw attention to CFP</summary>
      <description>I missed that the CFP for HBaseCon NA West 2018 is still open because I didn't click the link to the conf page. The "News" section should mention that CFP is open until it is not.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.index.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-10 01:00:00" id="20384" opendate="2018-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[AMv2] Logging format improvements; use encoded name rather than full region name marking transitions</summary>
      <description>We use encoded name near everywhere. Makes logging regular-looking at least and eases tracing. In a few places we still do full region name. Let me fix (ran into it trying to debug...)</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionPlan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-11 01:00:00" id="20387" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flaky infrastructure should work for all branches</summary>
      <description>We need a flaky list per-branch, since what does/does not work reliably on master isn't really relevant to our older maintenance release lines.We should just make the invocation a step in the current per-branch nightly jobs, prior to when we need the list in the stages that run unit tests. We can publish it in the nightly job as well so that precommit can still get it. (and can fetch it per-branch if needed)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.2.7,2.1.1,2.0.2,1.4.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.report-flakies.py</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.yetus.sh</file>
      <file type="M">dev-support.flaky-dashboard-template.html</file>
      <file type="M">dev-support.findHangingTests.py</file>
      <file type="M">dev-support.Dockerfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-11 01:00:00" id="20388" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>nightly tests running on a feature branch should only comment on that feature branch&amp;#39;s jira</summary>
      <description>It would help improve our signal-to-noise ratio from nightly tests if feature branch runs stopped commenting on all the jiras that got covered by a rebase / merge.should be straight forward to have the commenting bit check the current branch against our feature branch naming convention.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-11 01:00:00" id="20389" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move website building flags into a profile</summary>
      <description>we have some "magic" in our website building right now. The script that's used bout our automated website build + publish mechanism manually sets a bunch of stuff on the maven command line.It'd be better to reflect those settings in a maven profile, so that folks are less likely to be surprised e.g. when trying to debug a failure in the site goal happens.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-spark.pom.xml</file>
      <file type="M">dev-support.jenkins-scripts.generate-hbase-website.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-11 01:00:00" id="20391" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>close out stale or finished PRs on github</summary>
      <description>Time to do another round of closing PRs via empty commit. #51 - &gt; 1 month since notification #52 - &gt; 1 month since notification #61 - HBASE-18928 has already closed #62 - HBASE-18929 has already closed #64 -HBASE-18901 has already closed #67 - HBASE-19386 has already closed #68 - HBASE-19387 has already closed</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-12 01:00:00" id="20393" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operational documents for synchronous replication.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
      <file type="M">src.main.asciidoc.book.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-12 01:00:00" id="20397" opendate="2018-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it more explicit that monkey.properties is found on CLASSPATH</summary>
      <description>Underline in the refguide that IT tests look for monkey properties on CLASSPATH. I missed this and it caused me a bit of confusion.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-11 01:00:00" id="2040" opendate="2009-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes to group commit</summary>
      <description>Two (somewhat) major things.First when the LogSyncer thread is created it's expecting optionalFlushInterval but we pass logflushentries. What it means is that it will run every 100ms by default.Also when the optional flush is running (meaning that no entries came in for that interval) and that logflushentries&gt;1 then it won't do the hflush because we don't enforce it.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-13 01:00:00" id="20410" opendate="2018-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade protoc compiler to 3.5.1-1</summary>
      <description>See HBASE-20356After doing the cleanup there, I was informed that there's a 3.5.1-1 version of the compiler binaries that work on rhel6, so let's just go to that. Wish I knew about it beforehand.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-17 01:00:00" id="20438" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an HBase antipattern check for reintroducing commons-logging</summary>
      <description>We moved to slf4j in HBASE-10092, but looking at our source tree we've had some regression back to commons-logging:$ git grep -E "org.apache.commons.logging.Log(Factory|;)"hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:import org.apache.commons.logging.Log;hbase-server/src/main/java/org/apache/hadoop/hbase/master/zksyncer/ClientZKSyncer.java:import org.apache.commons.logging.LogFactory;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java:import org.apache.commons.logging.Log;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/FileArchiverNotifierImpl.java:import org.apache.commons.logging.LogFactory;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java:import org.apache.commons.logging.Log;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeReportingChore.java:import org.apache.commons.logging.LogFactory;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java:import org.apache.commons.logging.Log;hbase-server/src/main/java/org/apache/hadoop/hbase/quotas/RegionSizeStoreImpl.java:import org.apache.commons.logging.LogFactory;hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java:import org.apache.commons.logging.Log;hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/throttle/StoreHotnessProtector.java:import org.apache.commons.logging.LogFactory;hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java:import org.apache.commons.logging.Log;hbase-server/src/test/java/org/apache/hadoop/hbase/TestClusterPortAssignment.java:import org.apache.commons.logging.LogFactory;hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java:import org.apache.commons.logging.Log;hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestFlushFromClient.java:import org.apache.commons.logging.LogFactory;hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:import org.apache.commons.logging.Log;hbase-server/src/test/java/org/apache/hadoop/hbase/client/TestSeparateClientZKCluster.java:import org.apache.commons.logging.LogFactory;hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java:import org.apache.commons.logging.Log;hbase-server/src/test/java/org/apache/hadoop/hbase/procedure/TestFailedProcCleanup.java:import org.apache.commons.logging.LogFactory;hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java:import org.apache.commons.logging.Log;hbase-server/src/test/java/org/apache/hadoop/hbase/wal/TestDisabledWAL.java:import org.apache.commons.logging.LogFactory;We should do the same kind of check that we do to avoid e.g. the Hadoop annotations</description>
      <version>3.0.0-alpha-1,2.1.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-26 01:00:00" id="20494" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade com.yammer.metrics dependency</summary>
      <description>The current com.yammer.metrics version is quite outdated. Please consider upgrading to the latest io.dropwizard.metrics</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-27 01:00:00" id="20501" opendate="2018-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the Hadoop minimum version to 2.7.1</summary>
      <description>See discussion thread on dev@ "&amp;#91;DISCUSS&amp;#93; Branching for HBase 1.5 and Hadoop minimum version update (to 2.7)"Consensus This is a needed change due to the practicalities of having Hadoop as a dependency Let's move up the minimum supported version of Hadoop to 2.7.1. Update documentation (support matrix, compatibility discussion) to call this out. Be sure to call out this change in the release notes. Take the opportunity to remind users about our callout "Replace the Hadoop Bundled With HBase!" recommending users upgrade their Hadoop if &lt; 2.7.1.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.5.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-27 01:00:00" id="20502" opendate="2018-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document HBase incompatible with Yarn 2.9.0 and 3.0.x due to YARN-7190</summary>
      <description>We need to call out hadoop-yarn 2.9.0 and the entire 3.0.x line as explicitly unsupported due to needing YARN-7190 fixed in versions that have ATS available.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-1 01:00:00" id="20512" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>document change to running tests on secure clusters</summary>
      <description>We should document the change to authentication handling in HBASE-16231 in the upgrade section of the reference guide.It's surprising to folks that have existing automated testing that's been working on our prior stable release lines. We should give a warning to those updating. The release note is probably suitable for a first pass.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.0.2</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-1 01:00:00" id="20516" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Offheap read-path needs more detail</summary>
      <description>Needs notes on what an operator should look for to see that all is on and what to monitor in a running cluster.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.offheap.read.write.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-14 01:00:00" id="20581" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase book documentation wrong for REST operations on schema endpoints</summary>
      <description>On https://hbase.apache.org/book.html#_using_rest_endpointsThe documentation states that to update a table schema (the configuration for a column family), the PUT HTTP verb will update the current configuration with the "fragment" of configuration provided, while the POST HTTP verb will replace the current configuration with whatever is provided.In reality, the opposite is true: POST updates the configuration, PUT replaces. The old javadoc for the o.a.h.h.rest package got it right, but the entry on the HBase book transposed this.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-14 01:00:00" id="20582" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump up JRuby version because of some reported vulnerabilities</summary>
      <description>There are some vulnerabilities reported with two of the libraries used in HBase.Jruby(version:9.1.10.0):CVE-2009-5147CVE-2013-4363CVE-2014-4975CVE-2014-8080CVE-2014-8090CVE-2015-3900CVE-2015-7551CVE-2015-9096CVE-2017-0899CVE-2017-0900CVE-2017-0901CVE-2017-0902CVE-2017-0903CVE-2017-10784CVE-2017-14064CVE-2017-9224CVE-2017-9225CVE-2017-9226CVE-2017-9227CVE-2017-9228Tool somehow able to relate the vulnerability of Ruby with JRuby(Java implementation). (Jackson will be handled in a different issue.)Not all of them directly affects HBase but elserj suggested that it is better to be on the updated version to avoid issues during an audit in security sensitive organization.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-6-18 01:00:00" id="20602" opendate="2018-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase.master.quota.observer.ignore property seems to be not taking effect</summary>
      <description>From docsetting hbase.master.quota.observer.ignore property to truewill retain the space quota even after table is deleted. But doesn't seem to be the case i.e. whether the property is not defined which sets the value to false or set the property to true in site.xml, the quota gets removed when the corresponding table is dropped. Will verify whether it works in 1.x. Did a grep on the master source, did get a hit on the property in code.Steps to reproduce Add this property and restart hbase  &lt;property&gt;    &lt;name&gt;hbase.master.quota.observer.ignore&lt;/name&gt;    &lt;value&gt;true&lt;/value&gt;  &lt;/property&gt; Through hbase shell hbase(main):003:0&gt; set_quota TYPE =&gt; SPACE, TABLE =&gt; 't1', LIMIT =&gt; '1G', POLICY =&gt; NO_INSERTSTook 0.0317 seconds hbase(main):005:0&gt; create 't1','cf1'Created table t1Took 0.7904 seconds hbase(main):006:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t1 TYPE =&gt; SPACE, TABLE =&gt; t1, LIMIT =&gt; 1073741824, VIOLATION_POLICY =&gt; NO_INSERTS1 row(s) hbase(main):007:0&gt; disable 't1'Took 0.4909 secondshbase(main):008:0&gt; list_quotasOWNER QUOTASTABLE =&gt; t1 TYPE =&gt; SPACE, TABLE =&gt; t1, LIMIT =&gt; 1073741824, VIOLATION_POLICY =&gt; NO_INSERTS1 row(s)Took 0.0420 seconds hbase(main):009:0&gt; drop 't1'Took 0.1407 seconds hbase(main):010:0&gt; list_quotasOWNER QUOTAS0 row(s)Took 0.0307 seconds</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,2.0.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-22 01:00:00" id="20617" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade/remove jetty-jsp</summary>
      <description>jetty-jsp removed after jetty-9.2.x version. We use the 9.2 version. Research so far brings out that apache-jspmight be of interest to us in jetty-9.4.x version(as JettyJspServlet.class is in apache-jsp). Yet to figure out about jetty-9.3.x.Filing to track this along.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-23 01:00:00" id="20626" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the value of "Requests Per Second" on WEBUI</summary>
      <description>Now we use "totalRequestCount"(RSRpcServices#requestCount) to calculate requests per second. After HBASE-18469, "totalRequestCount" count only once for multi request.(Includes requests that are not serviced by regions.) When we have a large number of read and write requests, the value of "Requests Per Second" is very small which does not reflect the load of the cluster.Maybe it is more reasonable to use "totalRowActionRequestCount" to calculate RPS?</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.2.2,2.1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-31 01:00:00" id="20664" opendate="2018-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Variable shared across multiple threads</summary>
      <description>Some static analysis found a variable which was used across multiple threads without any synchronization that would allow race conditions.The variable does not need to be a member of the class, instead just made a local variable.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,1.3.3,2.0.1,1.4.5,1.2.6.1,1.3.2.1,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftHttpServlet.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-1 01:00:00" id="20672" opendate="2018-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>New metrics ReadRequestRate and WriteRequestRate</summary>
      <description>Hbase currently provides counter read/write requests (ReadRequestCount, WriteRequestCount). That said it is not easy to use counter that reset only after a restart of the service, we would like to expose 2 new metrics in HBase to provide ReadRequestRate and WriteRequestRate at region server level.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.3.3,2.2.0,1.4.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-6 01:00:00" id="20688" opendate="2018-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refguide has "HBase Backup" section and a chapter named "Backup and Restore"; neither refers to the other</summary>
      <description>The two backup sections are not connected or related. It'd be confusing to the user. Needs addressing.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-7 01:00:00" id="20702" opendate="2018-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Processing crash, skip ONLINE&amp;#39;ing empty rows</summary>
      <description>This patch comes from the parent issue. Parent issue identifies us ONLINE'ing a region though it has nothing in the row (in parent issue scenario, region info family was deleted in a merge region parent). We shouldn't do this.Committing patch from parent here in this subtask since the parent issue is still under investigation.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-15 01:00:00" id="20739" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add priority for SCP</summary>
      <description>As said in the design doc of HBASE-20708.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerQueue.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.SchemaLocking.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.FairQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-15 01:00:00" id="20741" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split of a region with replicas creates all daughter regions and its replica in same server</summary>
      <description>Generally it is better that the parent region when split creates the daughter region in the same target server. But for replicas also we do the same and all the replica regions are created in the same target server. We should ideally be doing a round robin and only the primary daughter region should be opened in the intended target server (where the parent was previously opened).huaxiang FYI.</description>
      <version>3.0.0-alpha-1,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestRegionReplicaSplit.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.BaseLoadBalancer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.SplitTableRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MergeTableRegionsProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManagerUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-18 01:00:00" id="20749" opendate="2018-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade our use of checkstyle to 8.6+</summary>
      <description>We should upgrade our checkstyle version to 8.6 or later so we can use the "match violation message to this regex" feature for suppression. That will allow us to make sure we don't regress on HTrace v3 vs v4 APIs (came up in HBASE-20332).We're currently blocked on upgrading to 8.3+ by checkstyle #5279, a regression that flags our use of both the "separate import groups" and "put static imports over here" configs as an error.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-checkstyle.src.main.resources.hbase.checkstyle-suppressions.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-20 01:00:00" id="20759" opendate="2018-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Please use HTTPS for KEYS</summary>
      <description>Please use HTTPS for the link to KEYS on download page(s)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-29 01:00:00" id="2076" opendate="2009-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Many javadoc warnings</summary>
      <description>We are about to release 0.20.3, it would be nice to get pretty and fix javadoc warnings.</description>
      <version>None</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.InternalScanner.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.package-info.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-7-29 01:00:00" id="20817" opendate="2018-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Infinite loop when executing ReopenTableRegionsProcedure</summary>
      <description>As discussed in HBASE-20792, it seems that a region's openSeqNum could remain the same after a sucessful reopen, which causes the RTRP loop infinitely.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.1.0,2.0.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.MoveRegionProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-31 01:00:00" id="2083" opendate="2009-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[EC2] HDFS DataNode no longer required on master</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.contrib.ec2.bin.hbase-ec2-init-remote.sh</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.contrib.ec2.bin.hbase-ec2-env.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-2 01:00:00" id="20830" opendate="2018-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document that region replica does not work well with AMv2</summary>
      <description>As we can see lots of TODOs for region replica feature in the AssignmentManager' code. And also, now we rely on the openSeqNum to determine whether a region has been successfully reopened, but the openSeqNum for a non-default replica region seems unstable...It is too late for the 2.1.0 release so let's just add a note in ref guide to say that region replica does not work well with AMv2 first, and then start to make it stable in follow on minor or major release lines.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-11 01:00:00" id="20873" opendate="2018-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update doc for Endpoint-based Export</summary>
      <description>The current documentation on the usage is a little vague. I'd like to take a stab at expanding it, based on my experience.</description>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1,2.0.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-4 01:00:00" id="2090" opendate="2010-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>findbugs issues</summary>
      <description>Findbugs issues/ fixes for a subset of them.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestWideScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-19 01:00:00" id="20909" opendate="2018-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.0 to the download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-20 01:00:00" id="20915" opendate="2018-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the commit column on our download page</summary>
      <description>As required by the ASF moderator.However first please fix the download page so it does not link to the Gitrepo.Please drop the commit id and the link; they are not appropriate for adownload page.Only approved releases should be linked from the download page.Whilst the commit itself may have been included in the VOTE email as partof the approval process, it is not a formal release artefact. And links torepos give access to code that has not been approved.Information on commit ids and git repos etc should appear on pages intendedfor the HBase developer community only.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-25 01:00:00" id="20943" opendate="2018-7-25 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add offline/online region count into metrics</summary>
      <description>We intensively use metrics to monitor the health of our HBase production cluster. We have seen some regions of a table stuck and cannot be brought online due to AWS issue which cause some log file corrupted. It will be good if we can catch this early. Although WebUI has this information, it is not useful for automated monitoring. Byadding this metric, we can easily monitor them with our monitoring system.</description>
      <version>2.0.0,1.2.6.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterMetricsWrapper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapperImpl.java</file>
      <file type="M">hbase-hadoop2-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSourceImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterWrapper.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.master.MetricsMasterSource.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-8 01:00:00" id="21026" opendate="2018-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Backup/Restore command usage bug in book</summary>
      <description/>
      <version>2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.backup.restore.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-10-12 01:00:00" id="21299" opendate="2018-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>List counts of actual region states in master UI tables section</summary>
      <description>It the tables panel, we list Open Regions, Offline Regions, Failed Regions, Split Region, and Other. This is not very useful. It is from a time before region states were edited down. Better to list OPEN, CLOSED, OPENING, CLOSING...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-29 01:00:00" id="21404" opendate="2018-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Master/RS navbar active state does not work</summary>
      <description>In master/rs web UI, the current active tab is not updated when user switches to any tab other than "Home" tab.For example: even though say if we are on "tabledetailed.jsp", the navbar does not update the active state of that tab. See master_before.png</description>
      <version>3.0.0-alpha-1,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.regionserver.footer.jsp</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.footer.jsp</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-30 01:00:00" id="21405" opendate="2018-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] Add Details about Output of "status &amp;#39;replication&amp;#39;"</summary>
      <description>Add more information about the meaning of each metric on to http://hbase.apache.org/book.html#_monitoring_replication_status.SOURCE: PeerID AgeOfLastShippedOp SizeOfLogQueue TimeStampsOfLastShippedOp Replication LagSINK AgeOfLastAppliedOp TimeStampsOfLastAppliedOp</description>
      <version>3.0.0-alpha-1,1.4.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-30 01:00:00" id="21411" opendate="2018-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to document the snapshot metric data that is shown in HBase Master Web UI</summary>
      <description>We need to add documentation into the Reference Guide for the work that was done inHBASE-15415.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-20 01:00:00" id="21502" opendate="2018-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update SyncTable section on RefGuide once HBASE-20586 is committed</summary>
      <description>SyncTable refguide section currently mentions limitation to run it on different kerberos realm. HBASE-20586 is ongoing to resolve this problem. This jira is to make sure RefGuide is updated accordingly once HBASE-20586 is resolved.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-8 01:00:00" id="21697" opendate="2019-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.2 to the download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.2</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-1 01:00:00" id="21976" opendate="2019-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deal with RetryImmediatelyException for batching request</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-2 01:00:00" id="21978" opendate="2019-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should close AsyncRegistry if we fail to get cluster id when creating AsyncConnection</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-21 01:00:00" id="22077" opendate="2019-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose sleep time as a command line argument of IntergationTestBackupRestore</summary>
      <description>Extend command line arguments of IntergationTestBackupRestore with a sleep time of chaos monkey options to be able to setup policy of region server restarts more granularly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBackupRestore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-10 01:00:00" id="2208" opendate="2010-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableServers # processBatchOfRows - converts from List to [ ] - Expensive copy</summary>
      <description>With autoFlush to false and a large write buffer on HTable, when we write bulk puts - TableServer # processBatchOfRows , convert the input (List) to an [ ] , before sending down the wire. With a write buffer as large as 20 MB , that becomes an expensive copy when we do - list.toArray(new T[ ] ). May be - should we change the wire protocol to support List as well , and then revisit this to prevent the bulk copy ?Batch b = new Batch(this) { @Override int doCall(final List&lt;Row&gt; currentList, final byte [] row, final byte [] tableName) throws IOException, RuntimeException { *final Put [] puts = currentList.toArray(PUT_ARRAY_TYPE);* return getRegionServerWithRetries(new ServerCallable&lt;Integer&gt;(this.c, tableName, row) { public Integer call() throws IOException { return server.put(location.getRegionInfo().getRegionName(), puts); } }); }</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-3 01:00:00" id="22152" opendate="2019-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a jenkins file for yetus to processing GitHub PR</summary>
      <description>I think we can just copy the jenkinsfile from the hadoop project.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,1.3.4,2.3.0,2.0.6,1.2.12,2.1.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-3 01:00:00" id="22160" opendate="2019-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add sorting functionality in regionserver web UI for user regions</summary>
      <description>Should be good to have the same sort of sorting functionality, like hmaster via HBASE-21207, in regionserver web UI for the list of regions too.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-4-21 01:00:00" id="22281" opendate="2019-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failed shell UTs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.table.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.hbase.rb</file>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-8 01:00:00" id="22379" opendate="2019-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Markdown for "Voting on Release Candidates" in book</summary>
      <description>The Markdown in the section "Voting on Release Candidates" of the HBase book seems to be broken. It looks like that there should be a quote, which isn't displayed correctly. Same is true for the formatting of the Maven RAT command.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-10-14 01:00:00" id="23172" opendate="2019-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Canary region success count metrics reflect column family successes, not region successes</summary>
      <description>HBase Canary reads once per column family per region. The current "region success count" should actually be "column family success count," which means we need another metric that actually reflects region success count. Additionally, the region read and write latencies only store the latencies of the last column family of the region read. Instead of a map of regions to a single latency value and success value, we should map each region to a list of such values.</description>
      <version>3.0.0-alpha-1,1.3.0,1.4.0,1.5.0,2.0.0,2.1.5,2.2.1</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.tool.TestCanaryTool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.tool.CanaryTool.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-5-11 01:00:00" id="2533" opendate="2010-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[EC2] ec2-describe-instances returning account number instead of bucket name</summary>
      <description>ec2-describe-instances is returning account number instead of bucket name now that our bucket names are apparently too long. Update launch scripts.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.ec2.bin.launch-hbase-zookeeper</file>
      <file type="M">contrib.ec2.bin.launch-hbase-slaves</file>
      <file type="M">contrib.ec2.bin.launch-hbase-master</file>
      <file type="M">contrib.ec2.bin.hbase-ec2-init-remote.sh</file>
      <file type="M">contrib.ec2.bin.hbase-ec2-env.sh</file>
      <file type="M">contrib.ec2.bin.create-hbase-image</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-19 01:00:00" id="2577" opendate="2010-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove &amp;#39;core&amp;#39; maven module; move core up a level</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">bin.hirb.rb</file>
      <file type="M">core.pom.xml</file>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">core.src.test.ruby.test.helper.rb</file>
      <file type="M">core.src.test.ruby.tests.runner.rb</file>
      <file type="M">core.src.test.ruby.shell.shell.test.rb</file>
      <file type="M">core.src.test.ruby.shell.formatter.test.rb</file>
      <file type="M">core.src.test.ruby.shell.commands.test.rb</file>
      <file type="M">core.src.test.ruby.hbase.table.test.rb</file>
      <file type="M">core.src.test.ruby.hbase.hbase.test.rb</file>
      <file type="M">core.src.test.ruby.hbase.admin.test.rb</file>
      <file type="M">core.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">core.src.test.resources.mapred-queues.xml</file>
      <file type="M">core.src.test.resources.log4j.properties</file>
      <file type="M">core.src.test.resources.hbase-site.xml</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.zookeeper.TestHQuorumPeer.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestRootPath.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestKeying.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestByteBloomFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.TestBase64.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.SoftValueSortedMapTest.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.util.DisabledTestMetaUtils.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TimestampTestBase.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestZooKeeper.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestSerialization.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestScanMultipleVersions.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestMultiParallelPut.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestMergeTable.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestMergeMeta.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestKeyValue.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestInfoServers.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestHMsg.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestFullLogReconstruction.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestEmptyMetaInfo.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.TestCompare.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestVersionResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestTableResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestStatusResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestSchemaResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestScannersWithFilters.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestScannerResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.TestRowResource.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.PerformanceEvaluation.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestVersionModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestTableSchemaModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestTableRegionModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestTableListModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestTableInfoModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestStorageClusterVersionModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestStorageClusterStatusModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestScannerModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestRowModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestColumnSchemaModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestCellSetModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.model.TestCellModel.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.HBaseRESTClusterTestBase.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.client.TestRemoteTable.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.rest.client.TestRemoteAdmin.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogRolling.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestLogActionsListener.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestWildcardColumnTracker.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestWideScanner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreReconstruction.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanWildcardColumnTracker.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanDeleteTracker.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestReadWriteConsistencyControl.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinorCompactingStoreScanner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestMemStore.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeyValueSkipListSet.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeyValueScanFixture.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegionInfo.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestGetDeleteTracker.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestExplicitColumnTracker.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestDeleteCompare.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.OOMERegionServer.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.KeyValueScanFixture.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.DisabledTestRegionServerExit.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluationCommons.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.PerformanceEvaluation.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.MultiRegionTable.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.metrics.TestMetricsMBeanBase.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestServerManager.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestRegionServerOperationQueue.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestRegionManager.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestOldLogsCleaner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestMinimumServerCount.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestMasterWithDisabling.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.TestMasterTransitions.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.master.OOMEHMaster.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapred.TestTableMapReduce.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTimeRangeMapRed.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableMapReduce.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapreduce.TestTableInputFormatScan.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapreduce.TestSimpleTotalOrderPartitioner.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.MapFilePerformanceEvaluation.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.KeyValueTestUtil.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.TestImmutableBytesWritable.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.TestHeapSize.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.TestHbaseObjectWritable.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekTo.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestLruBlockCache.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFileSeek.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFilePerformance.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.TestCachedBlockQueue.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.RandomSeek.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.RandomDistribution.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.NanoTimer.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.KVGenerator.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.io.hfile.KeySampler.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HFilePerformanceEvaluation.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HBaseTestCase.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.HBaseClusterTestCase.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestSingleColumnValueFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestSingleColumnValueExcludeFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestPrefixFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestPageFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestInclusiveStopFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestFilterList.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestDependentColumnFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.filter.TestColumnPaginationFilter.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.EmptyWatcher.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestTimestamp.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestShell.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestScannerTimeout.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestHTablePool.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestGetRowVersions.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">core.src.saveVersion.sh</file>
      <file type="M">core.src.main.ruby.shell.formatter.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.zk.dump.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.zk.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.version.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.truncate.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.status.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.split.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.shutdown.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.scan.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.put.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.major.compact.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.list.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.incr.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.get.counter.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.get.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.flush.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.exists.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.enable.region.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.enable.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.drop.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.disable.region.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.disable.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.describe.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.deleteall.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.delete.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.create.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.count.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.compact.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.close.region.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.alter.rb</file>
      <file type="M">core.src.main.ruby.shell.commands.rb</file>
      <file type="M">core.src.main.ruby.shell.rb</file>
      <file type="M">core.src.main.ruby.irb.hirb.rb</file>
      <file type="M">core.src.main.ruby.hbase.table.rb</file>
      <file type="M">core.src.main.ruby.hbase.hbase.rb</file>
      <file type="M">core.src.main.ruby.hbase.admin.rb</file>
      <file type="M">core.src.main.ruby.hbase.rb</file>
      <file type="M">core.src.main.resources.webapps.static.images.treeview-default.gif</file>
      <file type="M">core.src.main.resources.webapps.static.images.treeview-default-line.gif</file>
      <file type="M">core.src.main.resources.webapps.static.images.plus.gif</file>
      <file type="M">core.src.main.resources.webapps.static.images.minus.gif</file>
      <file type="M">core.src.main.resources.webapps.static.hbase.logo.med.gif</file>
      <file type="M">core.src.main.resources.webapps.static.hbase.css</file>
      <file type="M">core.src.main.resources.webapps.rest.WEB-INF.web.xml</file>
      <file type="M">core.src.main.resources.webapps.rest.META-INF.MANIFEST.MF</file>
      <file type="M">core.src.main.resources.webapps.regionserver.regionserver.jsp</file>
      <file type="M">core.src.main.resources.webapps.regionserver.index.html</file>
      <file type="M">core.src.main.resources.webapps.master.zk.jsp</file>
      <file type="M">core.src.main.resources.webapps.master.table.jsp</file>
      <file type="M">core.src.main.resources.webapps.master.master.jsp</file>
      <file type="M">core.src.main.resources.webapps.master.index.html</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.VersionMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.TableSchemaMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.TableListMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.TableInfoMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.StorageClusterStatusMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.ScannerMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.ColumnSchemaMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.CellSetMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.rest.protobuf.CellMessage.proto</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.mapred.RowCounter.Counters.properties</file>
      <file type="M">core.src.main.resources.org.apache.hadoop.hbase.mapreduce.RowCounter.Counters.properties</file>
      <file type="M">core.src.main.resources.hbase-default.xml</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperWrapper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKServerTool.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.WritableComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.VersionAnnotation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ValueOverMaxLengthException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Writables.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.VersionInfo.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Threads.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Strings.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.SoftValueSortedMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.SoftValueMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.SoftValue.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Pair.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.MurmurHash.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.MetaUtils.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Merge.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Keying.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.JvmVersion.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.JVMClusterUtil.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.JenkinsHash.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Hash.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.FileSystemVersionException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.DynamicByteBloomFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.CompressionTest.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.ClassSize.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.ByteBloomFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Base64.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.UnknownScannerException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.UnknownRowLockException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.ThriftUtilities.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.TableNotFoundException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.TableNotDisabledException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.VersionResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.StorageClusterVersionResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.StorageClusterStatusResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.SchemaResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ScannerResultGenerator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ScannerResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ScannerInstanceResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RowSpec.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RowResultGenerator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RowResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RootResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ResultGenerator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RESTServlet.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ResourceConfig.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ResourceBase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.RegionsResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.provider.producer.ProtobufMessageBodyProducer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.provider.producer.PlainTextMessageBodyProducer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.provider.JAXBContextResolver.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.provider.consumer.ProtobufMessageBodyConsumer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.VersionMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableSchemaMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableListMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.TableInfoMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.StorageClusterStatusMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.ScannerMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.ColumnSchemaMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.CellSetMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.protobuf.generated.CellMessage.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ProtobufMessageHandler.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.VersionModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.TableSchemaModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.TableRegionModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.TableModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.TableListModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.TableInfoModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterVersionModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.StorageClusterStatusModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.ScannerModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.RowModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.ColumnSchemaModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.CellSetModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.model.CellModel.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.metrics.RESTStatistics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.metrics.RESTMetrics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.ExistsResource.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.Constants.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.client.Response.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteHTable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.client.RemoteAdmin.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.client.Cluster.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.rest.client.Client.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.RemoteExceptionHandler.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.WrongRegionException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.WildcardColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.WALEdit.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.LogRollListener.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.LogActionsListener.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogKey.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FailedLogCloseException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFlusher.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFileGetScan.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ScanWildcardColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.RegionServerRunningException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ReadWriteConsistencyControl.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.QueryMatcher.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.NoSuchColumnFamilyException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerStatistics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerMetrics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.LruHashMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueSkipListSet.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueHeap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.InternalScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.GetDeleteTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.GetClosestRowBeforeTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.FlushRequester.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.DeleteTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.DeleteCompare.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.DebugPrint.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ColumnCount.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ChangedReadersObserver.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.RegionException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.NotServingRegionException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.MiniZooKeeperCluster.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.metrics.MetricsRate.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.metrics.MetricsMBeanBase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.metrics.file.TimeStampingFileContext.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ZKMasterAddressWatcher.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.TimeToLiveLogCleaner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.TableDelete.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RootScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RetryableMetaOperation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RegionServerOperationQueue.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RegionServerOperationListener.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RegionServerOperation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessRegionStatusChange.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessRegionClose.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.OldLogsCleaner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.NotAllMetaRegionsOnlineException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ModifyTableMeta.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ModifyColumn.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.metrics.MasterStatistics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.metrics.MasterMetrics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.MetaScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.MetaRegion.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.LogCleanerDelegate.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.InvalidColumnNameException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.DeleteColumn.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ColumnOperation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ChangeTableState.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.AddColumn.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.MasterNotRunningException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableSplit.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableReduce.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReader.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableMapReduceUtil.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormatBase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.TableInputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.RowCounter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.IdentityTableMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.HRegionPartitioner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.GroupingTableMap.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapred.Driver.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableSplit.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableReducer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReader.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableOutputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableOutputCommitter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableMapper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.SimpleTotalOrderPartitioner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.package-info.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.MultiTableOutputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.KeyValueSortReducer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.IdentityTableReducer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.IdentityTableMapper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.GroupingTableMapper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.Export.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.Driver.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.Leases.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.LeaseListener.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.LeaseException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HMasterRegionInterface.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HMasterInterface.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCStatistics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCProtocolVersion.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRpcMetrics.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCErrorHandler.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPC.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.TimeRange.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.Reference.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.ImmutableBytesWritable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.SimpleBlockCache.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.LruBlockCache.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.HFileScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.Compression.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.CachedBlockQueue.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.CachedBlock.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.BoundedRangeFileInputStream.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.HeapSize.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.HbaseObjectWritable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.HbaseMapWritable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.io.CodeToClassAndBack.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HServerAddress.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HRegionLocation.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HMsg.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HMerge.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HBaseConfTool.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.WritableByteArrayComparable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.WhileMatchFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.ValueFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SkipFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueExcludeFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.RowFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.QualifierFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.PrefixFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.PageFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.package-info.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.InvalidRowFilterException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.IncompatibleFilterException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.InclusiveStopFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.FirstKeyOnlyFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.FilterList.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.FilterBase.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.io.hfile.package.html</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.ipc.package.html</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.thrift.doc-files.Hbase.html</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.thrift.doc-files.index.html</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.thrift.doc-files.style.css</file>
      <file type="M">core.src.main.javadoc.org.apache.hadoop.hbase.thrift.package.html</file>
      <file type="M">core.src.main.javadoc.overview.html</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Delete.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Get.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTableFactory.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTableInterfaceFactory.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.MetaScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.MultiPut.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.MultiPutResponse.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.NoServerForRegionException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.package-info.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Put.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.RegionOfflineException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Result.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ResultScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Row.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.RowLock.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.Scan.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ScannerCallable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ScannerTimeoutException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ServerCallable.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ServerConnection.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.ServerConnectionManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHColumnDescriptor.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHRegionInfo.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ClusterStatus.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.ColumnNameParseException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.DoNotRetryIOException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.ColumnCountGetFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.ColumnPaginationFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.CompareFilter.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-20 01:00:00" id="2578" opendate="2010-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability for tests to override server-side timestamp setting (currentTimeMillis)</summary>
      <description>Many of our tests use client APIs which do not set explicit stamps. This creates weird timing issues with tests running on different systems because sometimes a set of operations happens in the same millisecond and other times they do not.We should have a way for a test to specify it's own way of generating the timestamps (for example, could always increment by 1 ensuring forward progression in time).</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestHRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-2 01:00:00" id="3727" opendate="2011-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MultiHFileOutputFormat</summary>
      <description>Like MultiTableOutputFormat, but outputting HFiles. Key is tablename as an IBW. Creates sub-writers (code cut and pasted from HFileOutputFormat) on demand that produce HFiles in per-table subdirectories of the configured output path. Does not currently support partitioning for existing tables / incremental update.</description>
      <version>2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-9 01:00:00" id="4362" opendate="2011-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SITE: Center logo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.vm</file>
      <file type="M">src.site.resources.css.site.css</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-19 01:00:00" id="4625" opendate="2011-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Convert @deprecated HBaseTestCase tests JUnit4 style tests</summary>
      <description>This will class has 47 references so separating out into a separate subtask.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestMergeTool.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestWideScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFileInfo.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreFile.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanWildcardColumnTracker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestScanDeleteTracker.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestQueryMatcher.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestKeyValueHeap.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestGetClosestAtOrBefore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBlocksScanned.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestBlocksRead.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestSeekTo.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestHFile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-19 01:00:00" id="4626" opendate="2011-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filters unnecessarily copy byte arrays...</summary>
      <description>Just looked at SingleCol and ValueFilter... And on every column compared they create a copy of the column and/or value portion of the KV.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.WritableByteArrayComparable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SubstringComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.SingleColumnValueFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.RegexStringComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.NullComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.CompareFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BitComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BinaryPrefixComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BinaryComparator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2013-9-1 01:00:00" id="7972" opendate="2013-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a configuration for the TCP backlog in the Thrift server</summary>
      <description>Once THRIFT-1868 goes in, we can start letting our users configure the TCP backlog.</description>
      <version>1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2008-11-21 01:00:00" id="947" opendate="2008-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Optimization] Major compaction should remove deletes as well as the deleted cell</summary>
      <description>Currently major compactions retains both deletes and the deleted cell. It should remove both.</description>
      <version>None</version>
      <fixedVersion>0.19.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>