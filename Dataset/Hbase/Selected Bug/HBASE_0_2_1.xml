<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  <bug fixdate="2008-8-22 01:00:00" id="762" opendate="2008-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteFamily takes timestamp, should only take row and family. Javadoc describes both cases but only implements the timestamp case.</summary>
      <description>The three version of deleteFamily in client.HTable (Text, String, byte[]) have varying descriptions about whether they take timestamps or not.public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family, long timestamp) throws IOException Delete all cells for a row with matching column family with timestamps less than or equal to timestamp. These will become:public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family) throws IOException Delete all cells for a row with matching column family at all timestamps.Per Jean-Daniel's comment, deleteAll should then not permit families. I'm unsure whether this is currently allowed or not, but the documentation must be updated either way.Will post patch after more thorough testing.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-10-5 01:00:00" id="8000" opendate="2013-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create integration/perf tests for stripe compactions</summary>
      <description>While writing tests I seem to be finding more errors with edge cases unrelated to what test actually tests compared to what is expected.Integration test will be needed... probably good for perf/compare too.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestIngest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-9-16 01:00:00" id="836" opendate="2008-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update thrift examples to work with changed IDL (HBASE-697)</summary>
      <description>Examples are now out of date since HBASE-697 went in.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.DemoClient.py</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-9-3 01:00:00" id="865" opendate="2008-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc warnings</summary>
      <description>There are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.2,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-10-29 01:00:00" id="8652" opendate="2013-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of compacting KVs is not reset at the end of compaction</summary>
      <description>Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).In DefaultCompactor#compact(), we have this at the beginning: this.progress = new CompactionProgress(fd.maxKeyCount);But progress.totalCompactingKVs is not reset at the end of compact().</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-6-3 01:00:00" id="8680" opendate="2013-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>distributedLogReplay performance regression</summary>
      <description>The JIRA is to check in changes to address performance issues found during my performance testing as following:1) When WALEdits belongs to a region which split/merged later, replay incurs extra waitUntilRegionOnline RPC call2) Using a single shared connection for replaying achieves better performance. Everytime creating a new connection, it incurs 4+ seconds to establish a connection to ZK.3) other small modifications to mitigate excessive logging</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.RegionStates.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-9-4 01:00:00" id="871" opendate="2008-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Major compaction periodicity should be specifyable at the column family level, not cluster wide</summary>
      <description>jon gray has a table of ten rows and a couple of columns that is constantly being updated. Has max versions of 2. This table is growing fast because all versions written are kept until a major compaction. The way this table is being used is different than that of others. Would be good if he could have major compactions run more often than the default once a day.</description>
      <version>0.2.1,0.18.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStoreFile.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-3-19 01:00:00" id="8770" opendate="2013-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deletes and puts with the same ts should be resolved according to mvcc/seqNum</summary>
      <description>This came up during HBASE-8721. Puts with the same ts are resolved by seqNum. It's not clear why deletes with the same ts as a put should always mask the put, rather than also being resolve by seqNum.What do you think?</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Brainstorming</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.datamodel.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-7-28 01:00:00" id="8826" opendate="2013-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-8695 is covered in Thrift 2</summary>
      <description>HBASE-8695 is about using the config file, make sure Thrift 2 is doing the same.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-7-28 01:00:00" id="8832" opendate="2013-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-4658 is supported by Thrift 2</summary>
      <description>HBASE-4658 adds support for "attributes" for certain operations. Make sure Thrift 2 supports them where ever available in the native API.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-18 01:00:00" id="889" opendate="2008-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.</summary>
      <description>The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>