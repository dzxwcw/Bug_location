<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  <bug fixdate="2018-7-14 01:00:00" id="21606" opendate="2018-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document use of the meta table load metrics added in HBASE-19722</summary>
      <description>HBASE-19722 added a great new tool for figuring out where cluster load is coming from. Needs a section in the ref guide When should I use this? Why shouldn't I use it all the time? What does using it look like? How do I use it?I think all the needed info for making something to answer these questions is in the discussion on HBASE-19722</description>
      <version>3.0.0-alpha-1,1.5.0,1.4.6,2.2.0,2.0.2,2.1.3</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-1 01:00:00" id="21976" opendate="2019-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Deal with RetryImmediatelyException for batching request</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncBatchRpcRetryingCaller.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-2 01:00:00" id="21978" opendate="2019-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should close AsyncRegistry if we fail to get cluster id when creating AsyncConnection</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-11 01:00:00" id="22033" opendate="2019-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update to maven-javadoc-plugin 3.2.0 and switch to non-forking aggregate goals</summary>
      <description>MJAVADOC-444 got into the 3.1.0 release of the maven-javadoc-plugin so now there are versions of the aggregate javadoc goals that don't include a forked build.update our build to make use of this new feature. (a before/after on build time would be nice to know as well)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.4.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-21 01:00:00" id="22077" opendate="2019-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose sleep time as a command line argument of IntergationTestBackupRestore</summary>
      <description>Extend command line arguments of IntergationTestBackupRestore with a sleep time of chaos monkey options to be able to setup policy of region server restarts more granularly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.IntegrationTestBackupRestore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-10 01:00:00" id="2208" opendate="2010-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TableServers # processBatchOfRows - converts from List to [ ] - Expensive copy</summary>
      <description>With autoFlush to false and a large write buffer on HTable, when we write bulk puts - TableServer # processBatchOfRows , convert the input (List) to an [ ] , before sending down the wire. With a write buffer as large as 20 MB , that becomes an expensive copy when we do - list.toArray(new T[ ] ). May be - should we change the wire protocol to support List as well , and then revisit this to prevent the bulk copy ?Batch b = new Batch(this) { @Override int doCall(final List&lt;Row&gt; currentList, final byte [] row, final byte [] tableName) throws IOException, RuntimeException { *final Put [] puts = currentList.toArray(PUT_ARRAY_TYPE);* return getRegionServerWithRetries(new ServerCallable&lt;Integer&gt;(this.c, tableName, row) { public Integer call() throws IOException { return server.put(location.getRegionInfo().getRegionName(), puts); } }); }</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-23 01:00:00" id="22299" opendate="2019-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation has incorrect default number of versions</summary>
      <description>Reference guide has this section under compaction.Compaction and VersionsWhen you create a Column Family, you can specify the maximum number of versions to keep, by specifying HColumnDescriptor.setMaxVersions(int versions). The default value is 3. If more versions than the specified maximum exist, the excess versions are filtered out and not written back to the compacted StoreFile.This is incorrect, the default value is 1.Additionally, HColumnDescriptor is deprecated and the example should use ColumnFamilyDescriptorBuilder$setMaxVersions(int) instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-25 01:00:00" id="22312" opendate="2019-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop 3 profile for hbase-shaded-mapreduce should like mapreduce as a provided dependency</summary>
      <description>the hadoop 3 profile currently misses declaring a provided dependency on the core mapreduce client module. that means we pick it up as a compile dependency from the hbase-mapreduce module, which means we include things in the shaded jar that we don't need to. (and expressly aren't supposed to include because they're supposed to come from Hadoop at runtime).</description>
      <version>2.1.0,2.2.0,2.1.1,2.1.2,2.1.3,2.1.4</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-7 01:00:00" id="22807" opendate="2019-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK Report showed wrong orphans regions on FileSystem</summary>
      <description>It showed .tabledesc and .tmp as orphans regions. Need to use FSUtils.getRegionDirs(fs, tableDir) instead of fs.listStatus(tableDir.getPath()).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestHbckChore.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.hbck.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HbckRegionInfo.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-7 01:00:00" id="22808" opendate="2019-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK Report showed the offline regions which belong to disabled table</summary>
      <description>For disabled table, the regions were offline and the info:server may be an unknownserver. The HBCK report should not show these regions in UI. Because these regions no need to fix.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.assignment.TestHbckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-7 01:00:00" id="22809" opendate="2019-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow creating table in group when rs group contains no live servers</summary>
      <description>This is for API consistency. In general, a rs group could have no live servers if all the region servers are dead, and then the regions in side this group can not online.So it is a bit strange that we do not allow new regions here since they are just the same with the old regions...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBasics.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsAdmin1.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminEndpoint.java</file>
    </fixedFiles>
  </bug>
</bugrepository>