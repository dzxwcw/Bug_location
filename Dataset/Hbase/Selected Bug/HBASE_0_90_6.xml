<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  
  <bug fixdate="2011-10-14 01:00:00" id="3444" opendate="2011-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to prove Bytes.toBytesBinary and Bytes.toStringBinary() is reversible</summary>
      <description>Bytes.toStringBinary() doesn't escape \.Otherwise the transformation isn't reversiblebyte[] a = {'\', 'x' , '0', '0'}Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-2-14 01:00:00" id="5397" opendate="2012-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[book] zookeeper quorum mistake</summary>
      <description>In Chapter 2, section 2.5 "ZooKeeper" under "How many ZooKeepers should I run?" there is the sentenceThere can be no quorum if the number of members is an even number.This is not true. In ZooKeeper, an even number of peers is supported, but it is normally not used because an even sized ensemble requires, proportionally, more peers to form a quorum than an odd sized ensemble requires. For example, an ensemble with 4 peers requires 3 to form a quorum, while an ensemble with 5 also requires 3 to form a quorum. Thus, an ensemble of 5 allows 2 peers to fail, and thus is more fault tolerant than the ensemble of 4, which allows only 1 down peer.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.performance.xml</file>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-4-3 01:00:00" id="559" opendate="2008-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR example job to count table rows</summary>
      <description>The Lars' import is a little messy; he's not sure how many records were imported. Running a select takes a couple of hours. He happens to have an idle MR cluster standing by. An example MR job that just did a count of records would be generally useful. Could even output row keys so you'd have a list of what made it in. Later, if this tool becomes popular with derivatives and similiars, we can bundle a jar of MR jobs to run against your tables that can answer common queries and that are amenable to subclassing/modification.</description>
      <version>None</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-16 01:00:00" id="5591" opendate="2012-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-21 01:00:00" id="5613" opendate="2012-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ThriftServer getTableRegions does not return serverName and port</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-26 01:00:00" id="5638" opendate="2012-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport to 0.90 and 0.92 - NPE reading ZK config in HBase</summary>
      <description/>
      <version>0.90.6,0.92.1</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZKConfig.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2012-4-10 01:00:00" id="5758" opendate="2012-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forward port "HBASE-4109 Hostname returned via reverse dns lookup contains trailing period if configured interface is not &amp;#39;default&amp;#39;"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.2,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Strings.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2012-6-27 01:00:00" id="5892" opendate="2012-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck] Refactor parallel WorkItem* to Futures.</summary>
      <description>This would convert WorkItem* logic (with low level notifies, and rough exception handling) into a more canonical Futures pattern.Currently there are two instances of this pattern (for loading hdfs dirs, for contacting regionservers for assignments, and soon &amp;#8211; for loading hdfs .regioninfo files).</description>
      <version>0.90.6,0.92.1,0.94.0</version>
      <fixedVersion>0.92.2,0.94.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-5-18 01:00:00" id="6044" opendate="2012-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>copytable: remove rs.* parameters</summary>
      <description>In discussion of HBASE-6013 it was suggested that we remove these arguments from 0.92+ (but keep in 0.90)</description>
      <version>None</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-6-11 01:00:00" id="6200" opendate="2012-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>KeyComparator.compareWithoutRow can be wrong when families have the same prefix</summary>
      <description>As reported by Desert Rose on IRC and on the ML, Result has a weird behavior when some families share the same prefix. He posted a link to his code to show how it fails, http://pastebin.com/7TBA1XGhBasically KeyComparator.compareWithoutRow doesn't differentiate families and qualifiers so "f:a" is said to be bigger than "f1:", which is false. Then what happens is that the KVs are returned in the right order from the RS but then doing Result.binarySearch it uses KeyComparator.compareWithoutRow which has a different sorting so the end result is undetermined.I added some debug and I can see that the data is returned in the right order but Arrays.binarySearch returned the wrong KV, which is then verified agains the passed family and qualifier which fails so null is returned.I don't know how frequent it is for users to have families with the same prefix, but those that do have that and that use those families at the same time will have big correctness issues. This is why I mark this as a blocker.</description>
      <version>0.90.6,0.92.1,0.94.0</version>
      <fixedVersion>0.92.2,0.94.1,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-20 01:00:00" id="630" opendate="2008-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default hbase.rootdir is garbage</summary>
      <description>Always writes to '/tmp/hbase-'.</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-4 01:00:00" id="6327" opendate="2012-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HLog can be null when create table</summary>
      <description>As HBASE-4010 discussed, the HLog can be null.We have meet createTable failed because the no use hlog.When createHReagion, the HLog.LogSyncer is run sync(), in under layer it call the DFSClient.DFSOutputStream.sync(). Then the hlog.closeAndDelete() was calledï¼Œfirstly the HLog.close() will interrupt the LogSyncer, and interrupt DFSClient.DFSOutputStream.sync().The DFSClient.DFSOutputStream will store the exception and throw it when we called DFSClient.close(). The HLog.close() call the writer.close()/DFSClient.close() after interrupt the LogSyncer. And there is no catch exception for the close().So the Master throw exception to the client. There is no need to throw this exception, furtherï¼Œ the hlog is no use.Our cluster is 0.90, the logs is attached, after "closing hlog writer", there is no log for the createTable().The trunk and 0.92, 0.94, we used just one hlog, and if the exception happends, the client will got createTable failed, but indeed ,we expect all the regions for the table can also be assigned.I will give the patch for this later.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-2 01:00:00" id="6917" opendate="2012-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk jdk7 build broke because we moved to zk 3.4.4</summary>
      <description>Chatted w/ Mahadev and he confirmed issues running 3.4.4 w/ jdk7. Will be fixed in zk3.4.5.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-12-5 01:00:00" id="6957" opendate="2012-10-5 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>TestRowCounter consistently fails against hadoop-2.0</summary>
      <description>In https://builds.apache.org/job/HBase-TRUNK-on-Hadoop-2.0.0/210/testReport/org.apache.hadoop.hbase.mapreduce/TestRowCounter/testRowCounterHiddenColumn/ , we can see:java.lang.AssertionError at org.junit.Assert.fail(Assert.java:92) at org.junit.Assert.assertTrue(Assert.java:43) at org.junit.Assert.assertTrue(Assert.java:54) at org.apache.hadoop.hbase.mapreduce.TestRowCounter.runRowCount(TestRowCounter.java:135) at org.apache.hadoop.hbase.mapreduce.TestRowCounter.testRowCounterHiddenColumn(TestRowCounter.java:118)...2012-10-05 11:24:17,355 WARN [ContainersLauncher #1] launcher.ContainerLaunch(246): Failed to launch container.java.lang.ArithmeticException: / by zero at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:355) at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:150) at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.getLogPathForWrite(LocalDirsHandlerService.java:268) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:126) at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:68) at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) at java.util.concurrent.FutureTask.run(FutureTask.java:138) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662)2012-10-05 11:24:17,356 WARN [DeletionService #1] nodemanager.DefaultContainerExecutor(276): delete returned false for path: [/home/jenkins/jenkins-slave/workspace/HBase-TRUNK-on-Hadoop-2.0.0/trunk/hbase-server/target/org.apache.hadoop.mapred.MiniMRCluster/org.apache.hadoop.mapred.MiniMRCluster-localDir-nm-1_0/usercache/jenkins/appcache/application_1349436189156_0003/container_1349436189156_0003_01_000002]</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-12 01:00:00" id="6987" opendate="2012-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port HBASE-6920 to trunk (?)</summary>
      <description>Need to investigate whether we need to port HBASE-6920 to trunk.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-2-6 01:00:00" id="7777" opendate="2013-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK check for lingering split parents should check for child regions</summary>
      <description>HBCK checks for split parent regions being in meta and hdfs, and reports this as a transient error. However, split parents, by design, linger around for some time, until its children stops referring to it. Instead we should check whether the children are there, and do not report anything if it is so.</description>
      <version>None</version>
      <fixedVersion>0.94.6,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestEndToEndSplitTransaction.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
</bugrepository>