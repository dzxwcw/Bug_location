<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  <bug fixdate="2010-6-8 01:00:00" id="2703" opendate="2010-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ui not working in distributed context</summary>
      <description>UI is not showing when you put hbase on a cluster; this is since we renamed webapps dir as hbase-webapps.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.bin.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-3-30 01:00:00" id="281" opendate="2008-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should allow deletions in .META. and -ROOT- tables</summary>
      <description>For administrative and debugging purposes, it would be nice to be able to delete rows from .META. via the shell. The alternative is writing custom java code to do such operations, which is just ridiculous. The reality of HBase's maturity is that from time to time we're going to have to reach into the .META. and ROOT tables to fix things, so I think the shell should be where that happens.Currently, attempting to delete from either table gives a "non-existant table" error.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-2-6 01:00:00" id="415" opendate="2008-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite leases to use DelayedBlockingQueue instead of polling</summary>
      <description>The current lease implementation is pretty broken. This change was previously included in HBASE-69, but should really be a separate patch.</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.Leases.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HMaster.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-3-9 01:00:00" id="433" opendate="2008-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>region server should deleted restore log after successfull restore</summary>
      <description>Currently we do not remove the restore log "oldlogfile.log" after we reopen a region after a crashed region server.Suggestion would be to remove after we successfully flush of all the edits to a mapfileso something like:replay log memcache flushdeleted log</description>
      <version>0.1.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-4-10 01:00:00" id="436" opendate="2008-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>website</summary>
      <description>Make the hbase website. Base it on hadoop forrest templates.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docs.src.documentation.content.xdocs.tabs.xml</file>
      <file type="M">build.xml</file>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">docs.linkmap.html</file>
      <file type="M">docs.index.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-12 01:00:00" id="4374" opendate="2011-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up default regions size from 256M to 1G</summary>
      <description>HBASE-4365 has some discussion of why we default for a table should tend to fewer bigger regions. It doesn't look like this issue will be done for 0.92. For 0.92, lets up default region size from 256M to 1G and talk up pre-split on table creation in manual.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-2-11 01:00:00" id="438" opendate="2008-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>XMLOutputter state should be initialized.</summary>
      <description>bash-3.00# bin/hbase shell --htmlHQL, 0.2.0-dev version.Copyright (c) 2008 by udanax, licensed to Apache Software Foundation.Type 'help;' for usage.hql &gt; show tables;&lt;table&gt; &lt;tr&gt; &lt;th&gt;Name &lt;/th&gt; &lt;th&gt;Descriptor &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;a &lt;/td&gt; &lt;td&gt;name: a, families: {b:={name: b, max versions: 3, compression: NONE, in memory: false, block cache enabled: false, max length: 2147483647, bloom filter: none}} &lt;/td&gt; &lt;/tr&gt;&lt;/table&gt;1 table(s) in set. (0.21 sec)hql &gt; show tables;Exception in thread "main" java.lang.IllegalStateException: getState() == DOCUMENT_ENDED at org.znerd.xmlenc.XMLOutputter.startTag(Unknown Source) at org.apache.hadoop.hbase.hql.formatter.HtmlTableFormatter.header(HtmlTableFormatter.java:125) at org.apache.hadoop.hbase.hql.ShowCommand.execute(ShowCommand.java:66) at org.apache.hadoop.hbase.hql.HQLClient.executeQuery(HQLClient.java:68) at org.apache.hadoop.hbase.Shell.main(Shell.java:114)bash-3.00#</description>
      <version>0.1.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-22 01:00:00" id="4461" opendate="2011-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose getRowOrBefore via Thrift</summary>
      <description>In order for fat Thrift-based clients to locate region locations they need to utilize the getRowOrBefore method.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-10-9 01:00:00" id="4561" opendate="2011-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Maven documentation in book</summary>
      <description>The maven documentation is a little out of date and has recently led to some confusion about tests. This would cleanup the maven documents in the book to be more explicit about how maven should be used.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-3-29 01:00:00" id="477" opendate="2008-2-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for an HBASE_CLASSPATH</summary>
      <description>We have mention of HBASE_CLASSPATH in hbase-env.sh but its not actually read anywhere. Make it work like HADOOP_CLASSPATH. See classpath discussion on this page, http://wiki.apache.org/hadoop/Hbase/Jython, for a use case.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-19 01:00:00" id="4830" opendate="2011-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regionserver BLOCKED on WAITING DFSClient$DFSOutputStream.waitForAckedSeqno running 0.20.205.0+</summary>
      <description>Running 0.20.205.1 (I was not at tip of the branch) I ran into the following hung regionserver:"regionserver7003.logRoller" daemon prio=10 tid=0x00007fd98028f800 nid=0x61af in Object.wait() [0x00007fd987bfa000] java.lang.Thread.State: WAITING (on object monitor) at java.lang.Object.wait(Native Method) at java.lang.Object.wait(Object.java:485) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3606) - locked &lt;0x00000000f8656788&gt; (a java.util.LinkedList) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3595) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3687) - locked &lt;0x00000000f8656458&gt; (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream) at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3626) at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61) at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86) at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966) - locked &lt;0x00000000f8655998&gt; (a org.apache.hadoop.io.SequenceFile$Writer) at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:214) at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:791) at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:578) - locked &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94) at java.lang.Thread.run(Thread.java:662)Other threads are like this (here's a sample):"regionserver7003.logSyncer" daemon prio=10 tid=0x00007fd98025e000 nid=0x61ae waiting for monitor entry [0x00007fd987cfb000] java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1074) - waiting to lock &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1195) at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1057) at java.lang.Thread.run(Thread.java:662)...."IPC Server handler 0 on 7003" daemon prio=10 tid=0x00007fd98049b800 nid=0x61b8 waiting for monitor entry [0x00007fd9872f1000] java.lang.Thread.State: BLOCKED (on object monitor) at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:1007) - waiting to lock &lt;0x00000000c443deb0&gt; (a java.lang.Object) at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1798) at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1668) at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2980) at sun.reflect.GeneratedMethodAccessor636.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364) at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1325)Looks like HDFS-1529? (Todd?)</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-9 01:00:00" id="5000" opendate="2011-12-9 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Speed up simultaneous reads of a block when block caching is turned off</summary>
      <description>With block caching, when one client starts reading a block and another one comes around asking for the same block, the second client waits for the first one to finish reading and returns the block from cache. This is achieved by locking on the block offset using IdLock, a "sparse lock" primitive allowing to lock on arbitrary long numbers. However, in case there is no block caching, there is no reason to wait for other clients that are reading the same block. One challenge optimizing this that we don't necessary have accurate information about whether other HFile API clients interested in the block would cache it.Setting priority as minor, as it is very unusual to turn off block caching.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2008-3-14 01:00:00" id="515" opendate="2008-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>At least double default timeouts between regionserver and master</summary>
      <description>501 added logging of how long we sleep at the end of the HRegionServer main run method before we send heartback back to the master. Last night during upload I saw this:2008-03-13 00:03:50,884 WARN org.apache.hadoop.hbase.util.Sleeper: We slept ten times longer than scheduled: 3000Above log has since been improved but its saying that we slept &gt; 30 seconds, the default timeout on master/regionserver communications (When the lease expires, master starts giving regions to someone else and when this regionserver reports in, its told to close all its regions).Server was under load from other processes but still...upload rate was not that rabid.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-23 01:00:00" id="5264" opendate="2012-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 0.92.0 upgrade guide</summary>
      <description>Add an upgrade guide for going from 0.90 to 0.92.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.images.favicon.ico</file>
      <file type="M">src.main.resources.hbase-webapps.static.favicon.ico</file>
      <file type="M">src.docbkx.upgrading.xml</file>
      <file type="M">src.docbkx.performance.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-4-30 01:00:00" id="5300" opendate="2012-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Research website symolic links for renaming of "HBase book" to "HBase Reference Guide"</summary>
      <description>Creating this task per a conversation I had with stack last week.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.metrics.xml</file>
      <file type="M">src.site.xdoc.acid-semantics.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-31 01:00:00" id="5304" opendate="2012-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pluggable split key policy</summary>
      <description>We need a way to specify custom policies to determine split keys.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ConstantSizeRegionSplitPolicy.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-18 01:00:00" id="5430" opendate="2012-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix licenses in 0.92.1 -- RAT plugin won&amp;#39;t pass</summary>
      <description>Use the -Drelease profile to see we are missing 30 or so license. Fix.</description>
      <version>None</version>
      <fixedVersion>0.92.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-4-30 01:00:00" id="554" opendate="2008-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>filters generate StackOverflowException</summary>
      <description>Below is from list.You're doing nothing wrong.The filters as written recurse until they find a match. If long stretches between matching rows, then you will get a StackOverflowError. Filters need to be changed. Thanks for pointing this out. Can you do without them for the moment until we get a chance to fix it?St.AckDavid Alves wrote:&gt; Hi St.Ack and all&gt; &gt; The error always occurs when trying to see if there are more rows to&gt; process.&gt; Yes I'm using a filter(RegExpRowFilter) to select only the rows (any&gt; row key) that match a specific value in one of the columns.&gt; Then I obtain the scanner just test the hasNext method, close the&gt; scanner and return.&gt; Am I doing something wrong?&gt; Still StackOverflowError is not supposed to happen right?&gt;&gt; Regards&gt; David Alves&gt; On Thu, 2008-03-27 at 12:36 -0700, stack wrote:&gt;&gt; You are using a filter? If so, tell us more about it.&gt;&gt; St.Ack&gt;&gt;&gt;&gt; David Alves wrote:&gt;&gt;&gt; Hi guys &gt;&gt;&gt;&gt;&gt;&gt; I 'm using HBase to keep data that is later indexed.&gt;&gt;&gt; The data is indexed in chunks so the cycle is get XXXX records index&gt;&gt;&gt; them check for more records etc...&gt;&gt;&gt; When I tryed the candidate-2 instead of the old 0.16.0 (which I&gt;&gt;&gt; switched to do to the regionservers becoming unresponsive) I got the&gt;&gt;&gt; error in the end of this email well into an indexing job.&gt;&gt;&gt; So you have any idea why? Am I doing something wrong?&gt;&gt;&gt;&gt;&gt;&gt; David Alves&gt;&gt;&gt;&gt;&gt;&gt; java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException:&gt;&gt;&gt; java.io.IOException: java.lang.StackOverflowError&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at java.io.DataInputStream.readLong(DataInputStream.java:399)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.readChunk(DFSClient.java:735)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.read(DFSClient.java:658)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.readBuffer(DFSClient.java:1130)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.read(DFSClient.java:1166)&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at org.apache.hadoop.io.DataOutputBuffer&gt;&gt;&gt; $Buffer.write(DataOutputBuffer.java:56)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1829)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1729)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1775)&gt;&gt;&gt; at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:461)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $StoreFileScanner.getNext(HStore.java:2350)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.hbase.HAbstractScanner.next(HAbstractScanner.java:256)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $HStoreScanner.next(HStore.java:2561)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1807)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; ...</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-10 01:00:00" id="5560" opendate="2012-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid RegionServer GC caused by timed-out calls</summary>
      <description>The HBaseRpcServer queues up rpc responses if the socket connection to the client is not yet ready to receive data. Calls are queued here until a 15 minute timeout occurs. I am able to generate a full GC when I artificially make a client read rpc-responses very slowly. This jira is to make this 15 minute time configurable.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-21 01:00:00" id="5610" opendate="2012-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GA to hbase.apache.org</summary>
      <description>Lets add the bit of script necessary tracking hbase.apache.org in google analytics. I was going to get it going first then open it to the PMC for viewing.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-5-8 01:00:00" id="569" opendate="2008-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DemoClient.php</summary>
      <description>Adding DemoClient.php implementation</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.README.txt</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-4-11 01:00:00" id="574" opendate="2008-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not load hadoop native libs</summary>
      <description>After moving out from hadoop/contrib, the standalone release does not include hadoop native libs in hbase/lib/native while it still includes hadoop-core.jar. I think they should be included as well to improve speed for compression and decompression.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>