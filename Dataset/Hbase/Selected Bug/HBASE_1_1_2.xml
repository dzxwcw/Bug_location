<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  <bug fixdate="2015-7-15 01:00:00" id="14086" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove unused bundled dependencies</summary>
      <description>We have some files with compatible non-ASL licenses that don't appear to be used, so remove them.</description>
      <version>None</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.resources.css.freebsd.docbook.css</file>
      <file type="M">src.main.asciidoc.asciidoctor.css</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-15 01:00:00" id="14087" opendate="2015-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ensure correct ASF policy compliant headers on source/docs</summary>
      <description>we have a couple of files that are missing their headers. we have one file using old-style ASF copyrights</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-native-client.src.rpc.CMakeLists.txt</file>
      <file type="M">src.main.xslt.configuration.to.asciidoc.chapter.xsl</file>
      <file type="M">src.main.site.xdoc.sponsors.xml</file>
      <file type="M">src.main.site.xdoc.resources.xml</file>
      <file type="M">src.main.site.xdoc.replication.xml</file>
      <file type="M">src.main.site.xdoc.pseudo-distributed.xml</file>
      <file type="M">src.main.site.xdoc.old.news.xml</file>
      <file type="M">src.main.site.xdoc.metrics.xml</file>
      <file type="M">src.main.site.xdoc.index.xml</file>
      <file type="M">src.main.site.xdoc.export.control.xml</file>
      <file type="M">src.main.site.xdoc.cygwin.xml</file>
      <file type="M">src.main.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.site.xdoc.acid-semantics.xml</file>
      <file type="M">src.main.site.asciidoc.sponsors.adoc</file>
      <file type="M">src.main.site.asciidoc.resources.adoc</file>
      <file type="M">src.main.site.asciidoc.replication.adoc</file>
      <file type="M">src.main.site.asciidoc.pseudo-distributed.adoc</file>
      <file type="M">src.main.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.site.asciidoc.metrics.adoc</file>
      <file type="M">src.main.site.asciidoc.index.adoc</file>
      <file type="M">src.main.site.asciidoc.export.control.adoc</file>
      <file type="M">src.main.site.asciidoc.cygwin.adoc</file>
      <file type="M">src.main.site.asciidoc.bulk-loads.adoc</file>
      <file type="M">src.main.site.asciidoc.acid-semantics.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift.HttpAuthenticationException.java</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.enable.table.replication.rb</file>
      <file type="M">hbase-shell.src.main.ruby.shell.commands.disable.table.replication.rb</file>
      <file type="M">hbase-server.src.test.resources.org.apache.hadoop.hbase.PerformanceEvaluation.Counter.properties</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.io.hfile.TestPrefetch.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestNullComparator.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestFuzzyRowAndColumnRangeFilter.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.filter.TestBitComparator.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.ProtoUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationLoad.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.JarFinder.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthChecker.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.HealthCheckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.RegionObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.EndpointObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionServerObserver.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.BaseRegionObserver.java</file>
      <file type="M">hbase-rest.src.test.java.org.apache.hadoop.hbase.rest.model.TestModelBase.java</file>
      <file type="M">hbase-native-client.src.sync.CMakeLists.txt</file>
      <file type="M">bin.considerAsDead.sh</file>
      <file type="M">bin.graceful.stop.sh</file>
      <file type="M">bin.hbase</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase-daemon.sh</file>
      <file type="M">bin.hbase-daemons.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.master-backup.sh</file>
      <file type="M">bin.regionservers.sh</file>
      <file type="M">bin.rolling-restart.sh</file>
      <file type="M">bin.start-hbase.sh</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.zookeepers.sh</file>
      <file type="M">conf.hadoop-metrics2-hbase.properties</file>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">conf.log4j.properties</file>
      <file type="M">dev-support.hbase.docker.README.md</file>
      <file type="M">dev-support.hbase.jdiff.acrossSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.afterSingularityTemplate.xml</file>
      <file type="M">dev-support.hbase.jdiff.template.xml</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.sh</file>
      <file type="M">dev-support.jdiffHBasePublicAPI.common.sh</file>
      <file type="M">dev-support.jenkinsEnv.sh</file>
      <file type="M">dev-support.publish.hbase.website.sh</file>
      <file type="M">dev-support.rebase.all.git.branches.sh</file>
      <file type="M">dev-support.smart-apply-patch.sh</file>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-util.sh</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.Coprocessor.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.CoprocessorEnvironment.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.DroppedSnapshotException.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.TableExistsException.java</file>
      <file type="M">hbase-client.src.main.resources.META-INF.services.org.apache.hadoop.security.token.TokenIdentifier</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZKUtil.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.io.LimitInputStream.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.AbstractByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimpleMutableByteRange.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.SimplePositionedMutableByteRange.java</file>
      <file type="M">hbase-examples.src.main.cpp.DemoClient.cpp</file>
      <file type="M">hbase-examples.src.main.cpp.Makefile</file>
      <file type="M">hbase-examples.src.main.perl.DemoClient.pl</file>
      <file type="M">hbase-examples.src.main.php.DemoClient.php</file>
      <file type="M">hbase-native-client.CMakeLists.txt</file>
      <file type="M">hbase-native-client.cmake.modules.FindGTest.cmake</file>
      <file type="M">hbase-native-client.cmake.modules.FindLibEv.cmake</file>
      <file type="M">hbase-native-client.README.md</file>
      <file type="M">hbase-native-client.src.async.CMakeLists.txt</file>
      <file type="M">hbase-native-client.src.core.CMakeLists.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14249" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded jar modules create spurious source and test jars with incorrect LICENSE/NOTICE info</summary>
      <description>the shaded jar modules don't need to create a source or test jar (because the jars contain nothing other than META-INF)currently we create the test jars are missing LICENSE source jars have LICENSE/NOTICE files that claim all the bundled works in the normal jar.hbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar/hbase-shaded-server-1.1.2-sources.jar//META-INFhbase-shaded-server-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-server-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-server-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar/hbase-shaded-client-1.1.2-sources.jar//META-INFhbase-shaded-client-1.1.2-sources.jar//META-INF/LICENSEhbase-shaded-client-1.1.2-sources.jar//META-INF/MANIFEST.MFhbase-shaded-client-1.1.2-sources.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar/hbase-shaded-client-1.1.2-tests.jar//META-INFhbase-shaded-client-1.1.2-tests.jar//META-INF/NOTICEhbase-1.1.2-rc0 busbey$ find hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar/hbase-shaded-server-1.1.2-tests.jar//META-INFhbase-shaded-server-1.1.2-tests.jar//META-INF/NOTICE</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14250" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>branch-1.1 hbase-server test-jar has incorrect LICENSE</summary>
      <description>test-jar LICENSE file for hbase-server claims jquery and the orca logo are present in the jar, when they are not.</description>
      <version>1.2.0,1.1.2,1.3.0,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="14251" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc jars use LICENSE/NOTICE from primary artifact</summary>
      <description>Our generated javadoc jars have the same LICENSE/NOTICE files as our primary artifacts but do not include a copy of hte full source.the following modules end up with incorrect artifacts: hbase-server hbase-common (maybe? depends on the are-apis-copyrightable court case) hbase-thrift</description>
      <version>1.2.0,1.1.2,0.98.15,1.0.3,2.0.0</version>
      <fixedVersion>0.98.14,1.0.2,1.2.0,1.1.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-30 01:00:00" id="14338" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>License notification misspells &amp;#39;Asciidoctor&amp;#39;</summary>
      <description>our License file contains 'asciidoctor' but with three "i"This project bundles a derivative of portions of the 'Asciiidoctor' projectunder the terms of the MIT license.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0</version>
      <fixedVersion>1.2.0,1.3.0,1.0.3,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-30 01:00:00" id="14340" opendate="2015-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add second bulk load option to Spark Bulk Load to send puts as the value</summary>
      <description>The initial bulk load option for Spark bulk load sends values over one by one through the shuffle. This is the similar to how the original MR bulk load worked.How ever the MR bulk loader have more then one bulk load option. There is a second option that allows for all the Column Families, Qualifiers, and Values or a row to be combined in the map side.This only works if the row is not super wide.But if the row is not super wide this method of sending values through the shuffle will reduce the data and work the shuffle has to deal with.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.BulkLoadSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseRDDFunctions.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.BulkLoadPartitioner.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="14348" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update download mirror link</summary>
      <description>Where we refer to www.apache.org/dyn/closer.cgi, we need to refer towww.apache.org/dyn/closer.lua instead .</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.old.news.xml</file>
      <file type="M">src.site.xdoc.index.xml</file>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.site.asciidoc.old.news.adoc</file>
      <file type="M">src.main.asciidoc..chapters.getting.started.adoc</file>
      <file type="M">README.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-1 01:00:00" id="14349" opendate="2015-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>pre-commit zombie finder is overly broad</summary>
      <description>Zombie detector is flagging processes from builds that aren't ours.ex from HBASE-14337:-1 core zombie tests. There are 4 zombie test(s): at org.apache.reef.io.network.DeprecatedNetworkConnectionServiceTest.testMultithreadedSharedConnMessagingNetworkConnServiceRate(DeprecatedNetworkConnectionServiceTest.java:343)</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-19 01:00:00" id="1437" opendate="2009-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>broken links in hbase.org</summary>
      <description>See http://hadoop.apache.org/hbase/docs/r0.19.2/. See along the LHS. E.g. metrics is at top level but here its relative and not found.</description>
      <version>None</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-2 01:00:00" id="14544" opendate="2015-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow HConnectionImpl to not refresh the dns on errors</summary>
      <description>Some clusters will have static ip addresses and forced dns lookup can cause extra instability. Allow users to tun that feature off, if wanted.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.ConnectionImplementation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2015-11-6 01:00:00" id="14778" opendate="2015-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make block cache hit percentages not integer in the metrics system</summary>
      <description>Once you're close to the 90%+ it's hard to see a difference because getting a full percent change is rare.</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperStub.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-hadoop-compat.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-10 01:00:00" id="14795" opendate="2015-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the spark-hbase scan operations</summary>
      <description>This is a sub-jira of HBASE-14789. This jira is to focus on the replacement of TableInputFormat for a more custom scan implementation that will make the following use case more effective.Use case:In the case you have multiple scan ranges on a single table with in a single query. TableInputFormat will scan the the outer range of the scan start and end range where this implementation can be more pointed.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.HBaseContext.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-11 01:00:00" id="14796" opendate="2015-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance the Gets in the connector</summary>
      <description>Current the Spark-Module Spark SQL implementation gets records from HBase from the driver if there is something like the following found in the SQL.rowkey = 123The reason for this original was normal sql will not have many equal operations in a single where clause.Zhan, had brought up too points that have value.1. The SQL may be generated and may have many many equal statements in it so moving the work to an executor protects the driver from load2. In the correct implementation the drive is connecting to HBase and exceptions may cause trouble with the Spark application and not just with the a single task execution</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseResources.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-13 01:00:00" id="14804" opendate="2015-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell&amp;#39;s create table command ignores &amp;#39;NORMALIZATION_ENABLED&amp;#39; attribute</summary>
      <description>I am trying to create a new table and set the NORMALIZATION_ENABLED as true, but seems like the argument NORMALIZATION_ENABLED is being ignored. And the attribute NORMALIZATION_ENABLED is not displayed on doing a desc command on that tablehbase(main):020:0&gt; create 'test-table-4', 'cf', {NORMALIZATION_ENABLED =&gt; 'true'}An argument ignored (unknown or overridden): NORMALIZATION_ENABLED0 row(s) in 4.2670 seconds=&gt; Hbase::Table - test-table-4hbase(main):021:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4 COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0430 secondsHowever, on doing an alter command on that table we can set the NORMALIZATION_ENABLED attribute for that tablehbase(main):022:0&gt; alter 'test-table-4', {NORMALIZATION_ENABLED =&gt; 'true'}Unknown argument ignored: NORMALIZATION_ENABLEDUpdating all regions with the new schema...1/1 regions updated.Done.0 row(s) in 2.3640 secondshbase(main):023:0&gt; desc 'test-table-4'Table test-table-4 is ENABLED test-table-4, {TABLE_ATTRIBUTES =&gt; {NORMALIZATION_ENABLED =&gt; 'true'} COLUMN FAMILIES DESCRIPTION {NAME =&gt; 'cf', BLOOMFILTER =&gt; 'ROW', VERSIONS =&gt; '1', IN_MEMORY =&gt; 'false', KEEP_DELETED_CELLS =&gt; 'FALSE', DATA_BLOCK_ENCODING =&gt; 'NONE', TTL =&gt; 'FOREVER', COMPRESSION =&gt; 'NONE', MIN_VERSIONS =&gt; '0', BLOCKCACHE =&gt; 'true', BLOCKSIZE =&gt; '65536', REPLICATION_SCOPE =&gt; '0'} 1 row(s) in 0.0190 secondsI think it would be better to have a single step process to enable normalization while creating the table itself, rather than a two step process to alter the table later on to enable normalization</description>
      <version>1.2.0,1.1.2</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-13 01:00:00" id="14805" opendate="2015-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>status should show the master in shell</summary>
      <description>status 'simple' or 'detailed' only shows the regionservers and regions, but not the active master. Actually, there is no way to know about the active masters from the shell it seems.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,0.98.17,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-19 01:00:00" id="14849" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to set block cache to false on SparkSQL executions</summary>
      <description>I was working at a client with a ported down version of the Spark module for HBase and realized we didn't add an option to turn of block cache for the scans. At the client I just disabled all caching with Spark SQL, this is an easy but very impactful fix.The fix for this patch will make this configurable</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,connector-1.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.test.scala.org.apache.hadoop.hbase.spark.DefaultSourceSuite.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.SerializableConfiguration.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.HBaseTableScanRDD.scala</file>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.datasources.Bound.scala</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-4 01:00:00" id="14928" opendate="2015-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start row should be set for query through HBase REST gateway involving globbing option</summary>
      <description>As Ben Sutton reported in the thread, Slow response on HBase REST api using globbing option, query through the Rest API with a globbing option i.e. http://&lt;HBase_Rest&gt;:&lt;HBase_Rest_Port&gt;/table/key&amp;#42; executes extremely slowly.Jerry He pointed out that PrefixFilter is used for query involving globbing option.This issue is to fix this bug by setting start row for such queries.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,0.98.17,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.src.main.java.org.apache.hadoop.hbase.rest.TableResource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-7 01:00:00" id="14939" opendate="2015-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document bulk loaded hfile replication</summary>
      <description>After HBASE-13153 is committed we need to add that information under the Cluster Replication section in HBase book.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-21 01:00:00" id="15021" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa doing false positives</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16930/consoleText says: +1 core tests. The patch passed unit tests in ....but here is what happened:...Results :Tests in error: org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testBasic(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testBasic:105 � NullPointer Run 2: TestRSStatusServlet.testBasic:105 � NullPointer Run 3: TestRSStatusServlet.testBasic:105 � NullPointerorg.apache.hadoop.hbase.regionserver.TestRSStatusServlet.testWithRegions(org.apache.hadoop.hbase.regionserver.TestRSStatusServlet) Run 1: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 2: TestRSStatusServlet.testWithRegions:119 � NullPointer Run 3: TestRSStatusServlet.testWithRegions:119 � NullPointerTests run: 1033, Failures: 0, Errors: 2, Skipped: 21...[INFO] Apache HBase - Server ............................. FAILURE [17:54.559s]...Why we reporting pass when it failed?</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-1-21 01:00:00" id="15145" opendate="2016-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBCK and Replication should authenticate to zookepeer using server principal</summary>
      <description>In secure clusters, we protect znodes with the server principal in zk. However, if a user wants to add a replication peer or run HBCK, then she will get Auth exception. This was not a problem due to an earlier bug. For replication, the long term fix is HBASE-11392. However, we should still have a way to launch zkcli with the server principals for manual inspection / manipulation. HBCK should always assume the server principals. Thanks Koelli for reporting this.</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,1.1.4,0.98.18,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-26 01:00:00" id="15172" opendate="2016-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support setting storage policy in bulkload</summary>
      <description>When using tiered HFile storage, we should be able to generating hfile with correct storage type during bulkload. This JIRA is targeting at making it possible.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-26 01:00:00" id="15174" opendate="2016-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Client Public API should not have PB objects in 2.0</summary>
      <description>Some more cleanup for the parent jira. We have leaked some PB structs in Admin (and possible other places). We should clean up these API before 2.0.Examples include: AdminProtos.GetRegionInfoResponse.CompactionState getCompactionState(final TableName tableName) throws IOException; .... void snapshot(final String snapshotName, final TableName tableName, HBaseProtos.SnapshotDescription.Type type) throws IOException, SnapshotCreationException, IllegalArgumentException; .... MasterProtos.SnapshotResponse takeSnapshotAsync(HBaseProtos.SnapshotDescription snapshot) throws IOException, SnapshotCreationException;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.util.Triple.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.TestInterfaceAudienceAnnotations.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.ServerLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RegionLoad.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RegionLoadStats.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-3 01:00:00" id="15958" opendate="2016-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement ClaimQueues on top of HBase</summary>
      <description>Building on HBase-15883. Now implementing the claim queues procedure within an HBase table. Peer tracking will still be performed by ZooKeeper though. Revision at https://reviews.apache.org/r/48232/</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateZKImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateHBaseImpl.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.TestReplicationStateBasic.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.replication.regionserver.TestReplicationSourceManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesZKImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesHBaseImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueuesArguments.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.replication.ReplicationQueues.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-12 01:00:00" id="16214" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add in UI description of Replication Table</summary>
      <description>Add a description for the Replication Table in the HBase UI</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-12 01:00:00" id="16215" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up references for EOM release lines</summary>
      <description>I've seen us state a few places that 1.0.z is EOM. We should clean up remaining references remove 1.0.z artifact from dist.apache remove it from the ref guide (java prereqs, hadoop prereqs, RM list) remove unreleased 1.0.z versions from JIRA archive released 1.0.z versions in JIRAedit: expanded to include 0.94.y first EOM DISCUSS, second EOM DISCUSS 0.98.y EOM DISCUSS</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-3 01:00:00" id="16750" opendate="2016-10-3 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>hbase compilation failed on power system</summary>
      <description>Hi,hbase compilation failed on IBM power system ppc64le architecture with below error:Hbase Failure:[INFO] ------------------------------------------------------------------------[INFO] BUILD FAILURE[INFO] ------------------------------------------------------------------------[INFO] Total time: 04:33 min[INFO] Finished at: 2016-09-30T08:58:47-04:00[INFO] Final Memory: 215M/843M[INFO] ------------------------------------------------------------------------[ERROR] Failed to execute goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc (output-pdf) on project hbase: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc (output-pdf) on project hbase: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288) at org.apache.maven.cli.MavenCli.main(MavenCli.java:199) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.PluginExecutionException: Execution output-pdf of goal org.asciidoctor:asciidoctor-maven-plugin:1.5.2.1:process-asciidoc failed: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:145) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207) ... 20 moreCaused by: org.jruby.exceptions.RaiseException: (NotImplementedError) fstat unimplemented unsupported or native support failed to load at org.jruby.RubyFile.size(org/jruby/RubyFile.java:1108) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/document_state.rb:69) at RUBY.each(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/object_store.rb:70) at org.jruby.RubyArray.each(org/jruby/RubyArray.java:1613) at RUBY.each(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/object_store.rb:69) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/pdf-core-0.2.5/lib/pdf/core/document_state.rb:68) at RUBY.render_body(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document/internals.rb:141) at RUBY.render(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:359) at RUBY.render_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:376) at org.jruby.RubyIO.open(org/jruby/RubyIO.java:1181) at RUBY.render_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/prawn-1.2.1/lib/prawn/document.rb:376) at RUBY.write(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj-pdf/1.5.0-alpha.6/asciidoctorj-pdf-1.5.0-alpha.6.jar!/gems/asciidoctor-pdf-1.5.0.alpha.6/lib/asciidoctor-pdf/converter.rb:1235) at RUBY.write(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor/document.rb:1051) at RUBY.convert(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor.rb:1504) at RUBY.convert_file(/grid/0/jenkins/.m2/repository/org/asciidoctor/asciidoctorj/1.5.2/asciidoctorj-1.5.2.jar!/gems/asciidoctor-1.5.2/lib/asciidoctor.rb:1562) at RUBY.convertFile(&lt;script&gt;:68) at org.jruby.gen.InterfaceImpl537412764.convertFile(org/jruby/gen/InterfaceImpl537412764.gen:13)[ERROR][ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionExceptionAs per this recommendation if we update asciidoctor version it fixes the problemhttps://github.com/asciidoctor/asciidoctorj/issues/402Please find the patch attached.</description>
      <version>1.1.2</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-3 01:00:00" id="16751" opendate="2016-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tuning information to HBase Book</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.schema.design.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-12 01:00:00" id="16818" opendate="2016-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid multiple copies of binary data during the conversion from Result to Row</summary>
      <description>In the buildRow() of HBaseRelation, CellUtil.cloneValue will already create a copy of the data. If the data type is BinaryType, another copy is being made within Utils.hbaseFieldToScalaType in Utils.scala. Generally, binary data can be fairly large, so copying may be an expensive operation.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-spark.src.main.scala.org.apache.hadoop.hbase.spark.DefaultSource.scala</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="17802" opendate="2017-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add note that minor versions can add methods to Interfaces</summary>
      <description>Clarify that adding methods to Interfaces in minor releases is allowed and that we'll always try to do it in a backward compatible way.Here is discussion from the list:http://search-hadoop.com/m/HBase/YGbbQfpjp1kozD7?subj=About+adding+methods+to+an+interface+which+is+part+of+our+public+API+in+minor+release</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-11 01:00:00" id="18577" opendate="2017-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded client includes several non-relocated third party dependencies</summary>
      <description>we have some unexpected unrelocated third party dependencies in our shaded artifacts.</description>
      <version>1.2.0,1.1.2,1.3.0,2.0.0-alpha-1</version>
      <fixedVersion>1.4.0,1.3.2,2.0.0-alpha-3,1.1.13,2.0.0,1.2.7</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-server.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-6 01:00:00" id="18765" opendate="2017-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The value of balancerRan is true even though no plans are executed</summary>
      <description>//We balance per group instead of per table List&lt;RegionPlan&gt; plans = new ArrayList&lt;&gt;(); for(Map.Entry&lt;TableName, Map&lt;ServerName, List&lt;HRegionInfo&gt;&gt;&gt; tableMap: getRSGroupAssignmentsByTable(groupName).entrySet()) { LOG.info("Creating partial plan for table " + tableMap.getKey() + ": " + tableMap.getValue()); List&lt;RegionPlan&gt; partialPlans = balancer.balanceCluster(tableMap.getValue()); LOG.info("Partial plan for table " + tableMap.getKey() + ": " + partialPlans); if (partialPlans != null) { plans.addAll(partialPlans); } } long startTime = System.currentTimeMillis(); balancerRan = plans != null;The plans is never null.</description>
      <version>None</version>
      <fixedVersion>2.0.0-alpha-3,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rsgroup.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupAdminServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-9 01:00:00" id="18970" opendate="2017-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The version of jruby we use now can&amp;#39;t get interactive input from prompt</summary>
      <description>case 1: press enterhbase(main):002:0&gt; disable_all 'chia(.*)'chia_1 Disable the above 1 tables (y/n)?y^Mcase 2: press ctrl-jhbase(main):001:0&gt; disable_all 'chia(.*)'chia_1 Disable the above 1 tables (y/n)?y^J1 tables successfully disabledTook 5.0059 seconds</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-1,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.main.ruby.irb.hirb.rb</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-26 01:00:00" id="20949" opendate="2018-7-26 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Split/Merge table can be executed concurrently with DisableTableProcedure</summary>
      <description>The top flaky tests on the dashboard are all because of this.TestRestoreSnapshotFromClientTestSimpleRegionNormalizerOnClusterTheoretically this should not happen, need to dig more.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.MasterProcedureScheduler.java</file>
      <file type="M">hbase-procedure.src.main.java.org.apache.hadoop.hbase.procedure2.LockAndQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-6 01:00:00" id="21017" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit the expected states for open/close</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStateStore.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestEnableTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionStates.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.RegionRemoteProcedureBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.OpenRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.CloseRegionProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>