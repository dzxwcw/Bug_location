<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  <bug fixdate="2009-1-18 01:00:00" id="2057" opendate="2009-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cluster won&amp;#39;t stop</summary>
      <description>It seems that clusters on trunk have some trouble stopping. Even manually deleting the shutdown file in ZK doesn't always help. Investigate.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.ZKMasterAddressWatcher.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-5-30 01:00:00" id="20660" opendate="2018-5-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reopen regions using ReopenTableRegionsProcedure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.replication.TransitPeerSyncReplicationStateProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-11 01:00:00" id="2109" opendate="2010-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>status &amp;#39;simple&amp;#39; should show total requests per second, also the requests/sec is wrong as is</summary>
      <description>status 'simple' doesnt give us aggregate load, leaving the user to add up numbers by hand. Futhermore, the per-server requests numbers are off, too high by a factor of 3 - they are using the default toString() which assumes a 1 second report rate, when the shipping default is 3 seconds.</description>
      <version>0.20.3,0.90.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.HBase.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-21 01:00:00" id="21091" opendate="2018-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Hadoop compatibility table</summary>
      <description>https://lists.apache.org/thread.html/7016d322a07e96dccdb071041c37238e43d3df4f93e9515d52ccfafc@%3Cdev.hbase.apache.org%3E covers some discussion around our Hadoop Version Compatibility table. A "leading" suggestion to make this more clear is to use a green/yellow/red (traffic-signal) style marking, instead of using specifics words/phrases (as they're often dependent on the interpretation of the reader).</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.configuration.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-9 01:00:00" id="21460" opendate="2018-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct Document Configurable Bucket Sizes in bucketCache</summary>
      <description>we use the bucket cache(offheap),found the doc was error,the property bucket sizes shoul be "hbase.bucketcache.bucket.sizes"  instead of "hfile.block.cache.sizes"CacheConfig.java /** A comma-delimited array of values for use as bucket sizes. */ public static final String BUCKET_CACHE_BUCKETS_KEY = "hbase.bucketcache.bucket.sizes";the doc was:  HBASE-10641 introduced the ability to configure multiple sizes for the buckets of the BucketCache, in HBase 0.98 and newer. To configurable multiple bucket sizes, configure the new property hfile.block.cache.sizes (instead of hfile.block.cache.size) to a comma-separated list of block sizes, ordered from smallest to largest, with no spaces. The goal is to optimize the bucket sizes based on your data access patterns. The following example configures buckets of size 4096 and 8192. &lt;property&gt; &lt;name&gt;hfile.block.cache.sizes&lt;/name&gt; &lt;value&gt;4096,8192&lt;/value&gt; &lt;/property&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-22 01:00:00" id="2156" opendate="2010-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-2037 broke Scan</summary>
      <description>Paul Ambrose wrote to the mailing list about some tests he has that doesn't pass on 0.20.3RC1-2. Looking into the issue it appears that this modification: public Scan addFamily(byte [] family) { familyMap.remove(family);- familyMap.put(family, null);+ familyMap.put(family, EMPTY_NAVIGABLE_SET); return this; }Makes it that when you use addColumn after that you put qualifiers into EMPTY_NAVIGABLE_SET which is static hence shared among all scanners after that like META scanners when calling tableExists.This was introduced by HBASE-2037.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.3,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestFromClientSide.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-13 01:00:00" id="21711" opendate="2019-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to git.apache.org/hbase.git</summary>
      <description>With the GitBox migration not only git-wip-us was removed but also git.apache.org/hbase.git is not available anymore. (INFRA-17640)We need to remove all references to this url.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.1.3,2.0.5,1.3.4,1.2.11</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.resources.doap.Hbase.rdf</file>
      <file type="M">src.main.asciidoc..chapters.zookeeper.adoc</file>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.hbase.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-14 01:00:00" id="21715" opendate="2019-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not throw UnsupportedOperationException in ProcedureFuture.get</summary>
      <description>This is really a bad practice, no one would expected that a Future does not support get, and this can not be detected at compile time. Even though we do not want user to wait for ever, we could set a long timeout, for example, 10 minutes,instead of throwing UnsuportedOperationException. I've already been hurt many times...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.1.3,2.0.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-15 01:00:00" id="21910" opendate="2019-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The nonce implementation is wrong for AsyncTable</summary>
      <description>We recreate the nonce every time when retry, which is not correct.And it is strange that we even do not have a UT for this...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableNoncedRetry.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncTableImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-5 01:00:00" id="21991" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix MetaMetrics issues - [Race condition, Faulty remove logic], few improvements</summary>
      <description>Here is a list of the issues related to the MetaMetrics implementation:Bugs: &amp;#91;_Lossy counting for top-k_&amp;#93; Faulty remove logic of non-eligible meters: Under certain conditions, we might end up storing/exposing all the meters rather than top-k-ish MetaMetrics can throw NPE resulting in aborting of the RS because of a Race Condition.Improvements: With high number of regions in the cluster, exposure of metrics for each region blows up the JMX from ~140 Kbs to 100+ Mbs depending on the number of regions. It's better to use lossy counting to maintain top-k for region metrics as well. As the lossy meters do not represent actual counts, I think, it'll be better to rename the meters to include "lossy" in the name. It would be more informative while monitoring the metrics and there would be less confusion regarding actual counts to lossy counts.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestLossyCounting.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMetaTableMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.LossyCounting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.coprocessor.MetaTableMetrics.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-5 01:00:00" id="21992" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add release manager for 2.2 in ref guide</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-5 01:00:00" id="21996" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set locale for javadoc</summary>
      <description>Headers in generated javadoc could have different language based on the user's locale.For example the published 2.1 javadoc's headers are in Chinese while 2.0 is in English.2.0: https://hbase.apache.org/2.0/apidocs/org/apache/hadoop/hbase/client/HTable.html2.1: https://hbase.apache.org/2.1/apidocs/org/apache/hadoop/hbase/client/HTable.html</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-6 01:00:00" id="21999" opendate="2019-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DEBUG] Exit if git returns empty revision!</summary>
      <description>Let me commit a bit of debug on branch-2.0. Can't figure out how we are getting empty git revision string. Have the build fail if empty....</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.1.4</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-common.src.saveVersion.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-24 01:00:00" id="22100" opendate="2019-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>False positive for error prone warnings in pre commit job</summary>
      <description>https://builds.apache.org/job/PreCommit-HBASE-Build/16516/artifact/patchprocess/branch-compile-javac-hbase-client.txt[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,42] [UnusedVariable] The parameter 'error' is never read.https://builds.apache.org/job/PreCommit-HBASE-Build/16516/artifact/patchprocess/patch-compile-javac-hbase-client.txt[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,42] [UnusedVariable] The parameter 'error' is never read.[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.And the output is 1 new and 1 fixed, the new one is[WARNING] /testptch/hbase/hbase-client/src/main/java/org/apache/hadoop/hbase/client/AsyncRpcRetryingCaller.java:[125,69] [UnusedVariable] The parameter 'updateCachedLocation' is never read.I think here we should report nothing, as it is just an order change...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-25 01:00:00" id="22101" opendate="2019-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AsyncAdmin.isTableAvailable should not throw TableNotFoundException</summary>
      <description>Should return false instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-27 01:00:00" id="22115" opendate="2019-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase RPC aspires to grow an infinite tree of trace scopes; some other places are also unsafe</summary>
      <description>All of those are ClientServices.Multi in this case.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.CallRunner.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-29 01:00:00" id="22131" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete the patches in hbase-protocol-shaded module</summary>
      <description>As now we will apply the patch in the hbase-thirdparty repo.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.0.6,2.1.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-17239.patch</file>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-17087.patch</file>
      <file type="M">hbase-protocol-shaded.src.main.patches.HBASE-15789.V2.patch</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-29 01:00:00" id="22490" opendate="2019-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nightly client integration test fails with hadoop-3</summary>
      <description>2019-05-28 15:27:10,107 WARN common.Storage: Storage directory /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 does not exist2019-05-28 15:27:10,109 WARN namenode.FSNamesystem: Encountered exception loading fsimageorg.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /home/jenkins/jenkins-slave/workspace/HBase_Nightly_master/output-integration/hadoop-3/target/test/data/dfs/name-0-1 is in an inconsistent state: storage directory does not exist or is not accessible. at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:376) at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1074) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:706) at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:628) at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:690) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:919) at org.apache.hadoop.hdfs.server.namenode.NameNode.&lt;init&gt;(NameNode.java:892) at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1622) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1305) at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1074) at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:949) at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:881) at org.apache.hadoop.hdfs.MiniDFSCluster.&lt;init&gt;(MiniDFSCluster.java:514) at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:473) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.start(MiniHadoopClusterManager.java:160) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.run(MiniHadoopClusterManager.java:132) at org.apache.hadoop.mapreduce.MiniHadoopClusterManager.main(MiniHadoopClusterManager.java:320) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71) at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144) at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:139) at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:147) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:244) at org.apache.hadoop.util.RunJar.main(RunJar.java:158)</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase.nightly.pseudo-distributed-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-10 01:00:00" id="22560" opendate="2019-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Jetty 9.3.latest and Jackson 2.9.latest</summary>
      <description>Worked through some static analysis scans and these two popped up as a result.Upgrades are: Jetty 9.3.25-&gt;9.3.27 Jackson 2.9.2-&gt;2.9.9Not expecting any pain with these, but we'll find out what QA thinks!</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-13 01:00:00" id="22580" opendate="2019-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a table attribute to make user scan snapshot feature configurable for table</summary>
      <description>If a cluster enable user scan snapshot feature, it will work for all tables. Since this feature will make some operations such as grant, revoke, snapshot... slower and some tables don't use scan snaphot to make scan faster. So add a table attribute to make it configurable at table level, in general, the feature is disabled by default, and if someone use feature, must enable the attribute of the specific table firstly.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestSnapshotScannerHDFSAclController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclHelper.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.SnapshotScannerHDFSAclController.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.PermissionStorage.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-20 01:00:00" id="22610" opendate="2019-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[BucketCache] Rename "hbase.offheapcache.minblocksize"</summary>
      <description>/** * The target block size used by blockcache instances. Defaults to * {@link HConstants#DEFAULT_BLOCKSIZE}. * TODO: this config point is completely wrong, as it's used to determine the * target block size of BlockCache instances. Rename. */ public static final String BLOCKCACHE_BLOCKSIZE_KEY = "hbase.offheapcache.minblocksize";</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCacheFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-22 01:00:00" id="22720" opendate="2019-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect link for hbase.unittests</summary>
      <description>Link for hbase.unittests link is incorrect in Categories and execution time chapter.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-23 01:00:00" id="22722" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson databind dependencies to 2.9.9.1</summary>
      <description>Due tohttps://nvd.nist.gov/vuln/detail/CVE-2019-12814https://nvd.nist.gov/vuln/detail/CVE-2019-12384</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.0.6,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-23 01:00:00" id="22724" opendate="2019-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a emoji on the vote table for pre commit result on github</summary>
      <description>zghaobac said that the current vote table on github is not good enough, as the colors are almost the same, it is not easy to find out which line is broken.Since github can not change the color of the text, he suggested that we add a column at the left most with some emojis to better notify the developpers.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,1.3.6,1.4.11,2.1.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-14 01:00:00" id="23022" opendate="2019-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>download page should use HTTPS for links to archive.a.o</summary>
      <description>Noticed this while working on some automation. We're https everywhere else in this page, just not this link.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-1-28 01:00:00" id="23751" opendate="2020-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move core to hbase-thirdparty 3.2.0</summary>
      <description>Move core to use just-released 3.2.0 hbase-thirdparty. The change has already been through build courtesy of https://github.com/apache/hbase/pull/1086</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-9 01:00:00" id="23818" opendate="2020-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup the remaining RSGroupInfo.getTables call in the code base</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.java.org.apache.hadoop.hbase.client.TestRSGroupShell.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsOfflineMode.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBase.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupMajorCompactionTTL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestMigrateRSGroupInfo.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestEnableRSGroups.java</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.master.rsgroup.jsp</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupMajorCompactionTTL.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rsgroup.RSGroupInfoManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterRpcServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RSGroupListTmpl.jamon</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.rsgroup.IntegrationTestRSGroup.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.RSGroupTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-26 01:00:00" id="2382" opendate="2010-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t rely on fs.getDefaultReplication() to roll HLogs</summary>
      <description>As I was commenting in HBASE-2234, using fs.getDefaultReplication() to roll HLogs if they lose replicas isn't reliable since that value is client-side and unless HBase is configured with it or has Hadoop's configurations on its classpath, it will do the wrong thing.Dhruba added:Can we use &lt;hlogpath&gt;.getFiletatus().getReplication() instead of fs.getDefaltReplication()? This will will ensure that we look at the repl factor of the precise file we are interested in, rather than what the system-wide default value is.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.javadoc.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-1 01:00:00" id="2399" opendate="2010-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Forced splits only act on the first family in a table</summary>
      <description>While working on a patch for HBASE-2375, I came across a few bugs in the existing code related to splits.If a user triggers a manual split, it flips a forceSplit boolean to true and then triggers a compaction (this is very similar to my current implementation for HBASE-2375). However, the forceSplit boolean is flipped back to false at the beginning of Store.compact(). So the force split only acts on the first family in the table. If that Store is not splittable for some reason (it is empty or has only one row), then the entire region will not be split, regardless of what is in other families.Even if there is data in the first family, the midKey is determined based solely on that family. If it has two rows and the next family has 1M rows, we pick the split key based on the two rows.</description>
      <version>0.20.3</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-14 01:00:00" id="23992" opendate="2020-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestAdminShell and TestQuotasShell mistakenly broken by parent commit</summary>
      <description>Minor fix...</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shell.src.test.ruby.hbase.quotas.test.rb</file>
      <file type="M">hbase-shell.src.test.ruby.hbase.admin.test.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-11 01:00:00" id="24170" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hadoop-2.0 profile</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-zookeeper.pom.xml</file>
      <file type="M">hbase-thrift.pom.xml</file>
      <file type="M">hbase-testing-util.pom.xml</file>
      <file type="M">hbase-shell.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client-byo-hadoop.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-rest.pom.xml</file>
      <file type="M">hbase-replication.pom.xml</file>
      <file type="M">hbase-procedure.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
      <file type="M">hbase-it.pom.xml</file>
      <file type="M">hbase-http.pom.xml</file>
      <file type="M">hbase-external-blockcache.pom.xml</file>
      <file type="M">hbase-examples.pom.xml</file>
      <file type="M">hbase-endpoint.pom.xml</file>
      <file type="M">hbase-common.pom.xml</file>
      <file type="M">hbase-client.pom.xml</file>
      <file type="M">hbase-backup.pom.xml</file>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-11 01:00:00" id="24171" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2.1.10 from download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-11 01:00:00" id="24172" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove 2.1 from the release managers section in ref guide</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.community.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-24 01:00:00" id="24423" opendate="2020-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to get lock in canSplit because hasReferences will get lock too</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-4-20 01:00:00" id="2470" opendate="2010-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Scan.setTimeRange() support in Shell</summary>
      <description>The shell does not support scans by time range.This is trivial, simply add two more optional values MINTIMESTAMP and MAXTIMESTAMP and if both are set call Scan.setTimeRange(minStamp, maxStamp).</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.ruby.hbase.table.test.rb</file>
      <file type="M">src.main.ruby.hbase.table.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-4-28 01:00:00" id="2496" opendate="2010-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less ArrayList churn on the scan path</summary>
      <description>Doing some profiling when testing the scanning speed of 0.20.4, I saw that we are spending a lot of time instantiating ArrayLists when scanning and that we could sometime set the right size of the arrays. I don't expect big improvements for short scans, but people like us who are scanning in batches of 10k could get some nice speedups.</description>
      <version>0.20.3</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ExplicitColumnTracker.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.ColumnCount.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-4-28 01:00:00" id="2499" opendate="2010-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Race condition when disabling a table leaves regions in transition</summary>
      <description>A lot of people reported that weren't able to add/delete a column because only some of the regions got the modification. I personally thought it was due to the CME bug in the Master, but I'm able to easily reproduce on 0.20.4 on a 1800 regions table.Since 0.20.3, we now call disableTable after every retry to make sure we don't miss any region. This creates a race where while we scan .META. in TableOperation, a region could be reported as closed after we scanned the row. We end up processing it like if it was assigned and we put it back into regionsInTransition. We need to either query .META. before processing each region or make some more check to see if the region was closed.This kills the RC in my book.In the mean time, anyone getting this can restart their HBase and it will pick up the change.</description>
      <version>0.20.3</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ChangeTableState.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-7-25 01:00:00" id="2792" opendate="2010-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a better way to chain log cleaners</summary>
      <description>From Stack's review of HBASE-2223:Why this implementation have to know about other implementations? Can't we do a chain of decision classes? Any class can say no? As soon as any decision class says no, we exit the chain.... So in this case, first on the chain would be the ttl decision... then would be this one... and third would be the snapshotting decision. You don't have to do the chain as part of this patch but please open an issue to implement.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestOldLogsCleaner.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.master.ReplicationLogCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.OldLogsCleaner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LogCleanerDelegate.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-26 01:00:00" id="2793" opendate="2010-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to extract a specified list of versions of a column in a single roundtrip</summary>
      <description>In one of the use cases we were looking at, each row contains a single column, but with several versions (e.g., each version representing an event in a log), and we want to be able to extract specific set of versions from the row in a single round-trip.Currently, on a Get, one can retrieve a specific version of a column using setTimeStamp(ts) or a range of versions using setTimeRange(min, max). But not a set of specified versions. It would be useful to add this ability.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.Filter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-5-16 01:00:00" id="3117" opendate="2010-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to 0.5 version</summary>
      <description>Thrift 0.5 has been released already and we want to upgrade to at least 0.3 but 0.5 has a lot of improvements so that would be the best.Unfortunately the Java lib has changed so that we'll have to regenerate the current Thrift interface and fix the implementation (byte[] -&gt; ByteBuffer).They also have problems getting Thrift into a Maven repository so we'll need to do our current workaround again unfortunately and upload it to a repository. That would be Ryan's I think?I'll upload an updated thrift jar and a patch for the old Thrift code.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>