<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  
  <bug fixdate="2008-3-30 01:00:00" id="281" opendate="2008-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shell should allow deletions in .META. and -ROOT- tables</summary>
      <description>For administrative and debugging purposes, it would be nice to be able to delete rows from .META. via the shell. The alternative is writing custom java code to do such operations, which is just ridiculous. The reality of HBase's maturity is that from time to time we're going to have to reach into the .META. and ROOT tables to fix things, so I think the shell should be where that happens.Currently, attempting to delete from either table gives a "non-existant table" error.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-2-10 01:00:00" id="434" opendate="2008-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestTableIndex failed in HBasePatch build #14</summary>
      <description>TestTableIndex failed in HBase-Patch build #14. See http://hudson.zones.apache.org/hudson/job/HBase-Patch/14/testReport/junit.framework.AssertionFailedError at org.apache.hadoop.hbase.MultiRegionTable.makeMultiRegionTable(MultiRegionTable.java:137) at org.apache.hadoop.hbase.mapred.TestTableIndex.setUp(TestTableIndex.java:125)</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-8 01:00:00" id="4350" opendate="2011-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix a Bloom filter bug introduced by HFile v2 and TestMultiColumnScanner that caught it</summary>
      <description>Nicolas pointed out to me that the new unit test TestMultiColumnScanner that I wrote for the multi-column scanner Bloom filter optimization (which we will soon release) did not pass on the open-source trunk, and it bisected down to the HFile v2 commit. I debugged the unit test and found that there was a serious bug in HFile v2 Bloom filter lookup not caught by any of the existing unit tests: Bloom filters were used for "non-Get" Scans, which did not have minimum/maximum row set correctly, and some scan results were not returned.This diff is the unit test that helped catch the problem and a one-line fix for the bug.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.ScanDeleteTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-4-10 01:00:00" id="436" opendate="2008-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>website</summary>
      <description>Make the hbase website. Base it on hadoop forrest templates.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docs.src.documentation.content.xdocs.tabs.xml</file>
      <file type="M">build.xml</file>
      <file type="M">src.docs.src.documentation.skinconf.xml</file>
      <file type="M">docs.linkmap.html</file>
      <file type="M">docs.index.html</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-2-12 01:00:00" id="440" opendate="2008-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add optional log roll interval so that log files are garbage collected</summary>
      <description>While optional cache flushes will increase the sequence id in the region server's log file, if the region server is basically idle, the log will not get rolled and consequently old log files will not get garbage collected.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HRegionServer.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-21 01:00:00" id="4450" opendate="2011-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test for number of blocks read: to serve as baseline for expected blocks read and for catching regressions</summary>
      <description>Add a simple test for number of blocks read. The tests intent is to serve as baseline for expected blocks read and for catching regressions. As optimizations for HBase-4433 or Hbase-4434 are committed, the test would need to be updated to adjust the counts for expected blocks read in various cases.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-22 01:00:00" id="4461" opendate="2011-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose getRowOrBefore via Thrift</summary>
      <description>In order for fat Thrift-based clients to locate region locations they need to utilize the getRowOrBefore method.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-2-23 01:00:00" id="462" opendate="2008-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update migration tool</summary>
      <description>HBASE-2 is really an incompatible change as it changes the format of region server log file names.Update Migration tool so that it ensures there are no unrecovered region server log files.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.TestScannerAPI.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.TestBloomFilters.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.StaticTestEnvironment.java</file>
      <file type="M">src.test.org.apache.hadoop.hbase.AbstractMergeTestBase.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Migrate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.IdentityTableReduce.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-19 01:00:00" id="4620" opendate="2011-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>I broke the build when I submitted HBASE-3581 (Send length of the rpc response)</summary>
      <description>Thanks to Ted, Ram and Gao for figuring my messup.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.ResponseFlag.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2008-3-29 01:00:00" id="477" opendate="2008-2-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for an HBASE_CLASSPATH</summary>
      <description>We have mention of HBASE_CLASSPATH in hbase-env.sh but its not actually read anywhere. Make it work like HADOOP_CLASSPATH. See classpath discussion on this page, http://wiki.apache.org/hadoop/Hbase/Jython, for a use case.</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-8-1 01:00:00" id="4920" opendate="2011-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>We need a mascot, a totem</summary>
      <description>We need a totem for our t-shirt that is yet to be printed. O'Reilly owns the Clyesdale. We need something else.We could have a fluffy little duck that quacks 'hbase!' when you squeeze it and we could order boxes of them from some off-shore sweatshop that subcontracts to a contractor who employs child labor only.....Or we could have an Orca (Big!, Fast!, Killer!, and in a poem that Marcy from Salesforce showed me, that was a bit too spiritual for me to be seen quoting here, it had the Orca as the 'Guardian of the Cosmic Memory': i.e. in translation, bigdata).</description>
      <version>None</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.site.site.xml</file>
      <file type="M">src.main.site.site.vm</file>
      <file type="M">NOTICE.txt</file>
      <file type="M">src.main.site.resources.css.site.css</file>
      <file type="M">src.main.docbkx.book.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.jumping-orca.rotated.12percent.png</file>
      <file type="M">hbase-server.src.main.resources.hbase-webapps.static.hbase.logo.small.png</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-9 01:00:00" id="5000" opendate="2011-12-9 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Speed up simultaneous reads of a block when block caching is turned off</summary>
      <description>With block caching, when one client starts reading a block and another one comes around asking for the same block, the second client waits for the first one to finish reading and returns the block from cache. This is achieved by locking on the block offset using IdLock, a "sparse lock" primitive allowing to lock on arbitrary long numbers. However, in case there is no block caching, there is no reason to wait for other clients that are reading the same block. One challenge optimizing this that we don't necessary have accurate information about whether other HFile API clients interested in the block would cache it.Setting priority as minor, as it is very unusual to turn off block caching.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2008-3-14 01:00:00" id="515" opendate="2008-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>At least double default timeouts between regionserver and master</summary>
      <description>501 added logging of how long we sleep at the end of the HRegionServer main run method before we send heartback back to the master. Last night during upload I saw this:2008-03-13 00:03:50,884 WARN org.apache.hadoop.hbase.util.Sleeper: We slept ten times longer than scheduled: 3000Above log has since been improved but its saying that we slept &gt; 30 seconds, the default timeout on master/regionserver communications (When the lease expires, master starts giving regions to someone else and when this regionserver reports in, its told to close all its regions).Server was under load from other processes but still...upload rate was not that rabid.</description>
      <version>0.1.0,0.2.0</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-4-17 01:00:00" id="523" opendate="2008-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>package-level javadoc should have example client</summary>
      <description>Package-level javadoc should have example client or at least point at the FAQ that ties to package release.For example, the BatchUpdate example code appears in the FAQ sample code, but it is missing in the javadoc.It will be better to tie the example code snippet with the new methods that replaces the deprecated methods which will also be tied to release in the javadoc.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2012-2-10 01:00:00" id="5382" opendate="2012-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test that we always cache index and bloom blocks</summary>
      <description>This is a unit test that should have been part of HBASE-4683 but was not committed. The original test was reviewed as part of https://reviews.facebook.net/D807. Submitting unit test as a separate JIRA and patch, and extending the scope of the test to also handle the case when block cache is enabled for the column family. The new review is at https://reviews.facebook.net/D1695.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-10 01:00:00" id="5384" opendate="2012-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up heap used by hadoopqa</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
      <file type="M">dev-support.test-patch.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-4-23 01:00:00" id="541" opendate="2008-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop jars used in HBase/lib are not compatible with Hadoop-trunk</summary>
      <description>The hadoop jars included in the HBase tree under /lib are not compatible with hadoop-core trunk.Apparently there have been a couple of revisions to the Hadoop RPC protocol so an HBase built with the included jars will not run against a hadoop trunk cluster.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.hadoop-0.17.0-dev.2008-03-04.15-19-00-test.jar</file>
      <file type="M">lib.hadoop-0.17.0-dev.2008-03-04.15-19-00-core.jar</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-18 01:00:00" id="5430" opendate="2012-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix licenses in 0.92.1 -- RAT plugin won&amp;#39;t pass</summary>
      <description>Use the -Drelease profile to see we are missing 30 or so license. Fix.</description>
      <version>None</version>
      <fixedVersion>0.92.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2008-4-30 01:00:00" id="554" opendate="2008-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>filters generate StackOverflowException</summary>
      <description>Below is from list.You're doing nothing wrong.The filters as written recurse until they find a match. If long stretches between matching rows, then you will get a StackOverflowError. Filters need to be changed. Thanks for pointing this out. Can you do without them for the moment until we get a chance to fix it?St.AckDavid Alves wrote:&gt; Hi St.Ack and all&gt; &gt; The error always occurs when trying to see if there are more rows to&gt; process.&gt; Yes I'm using a filter(RegExpRowFilter) to select only the rows (any&gt; row key) that match a specific value in one of the columns.&gt; Then I obtain the scanner just test the hasNext method, close the&gt; scanner and return.&gt; Am I doing something wrong?&gt; Still StackOverflowError is not supposed to happen right?&gt;&gt; Regards&gt; David Alves&gt; On Thu, 2008-03-27 at 12:36 -0700, stack wrote:&gt;&gt; You are using a filter? If so, tell us more about it.&gt;&gt; St.Ack&gt;&gt;&gt;&gt; David Alves wrote:&gt;&gt;&gt; Hi guys &gt;&gt;&gt;&gt;&gt;&gt; I 'm using HBase to keep data that is later indexed.&gt;&gt;&gt; The data is indexed in chunks so the cycle is get XXXX records index&gt;&gt;&gt; them check for more records etc...&gt;&gt;&gt; When I tryed the candidate-2 instead of the old 0.16.0 (which I&gt;&gt;&gt; switched to do to the regionservers becoming unresponsive) I got the&gt;&gt;&gt; error in the end of this email well into an indexing job.&gt;&gt;&gt; So you have any idea why? Am I doing something wrong?&gt;&gt;&gt;&gt;&gt;&gt; David Alves&gt;&gt;&gt;&gt;&gt;&gt; java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException:&gt;&gt;&gt; java.io.IOException: java.lang.StackOverflowError&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at java.io.DataInputStream.readLong(DataInputStream.java:399)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.readChunk(DFSClient.java:735)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:234)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:157)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $BlockReader.read(DFSClient.java:658)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.readBuffer(DFSClient.java:1130)&gt;&gt;&gt; at org.apache.hadoop.dfs.DFSClient&gt;&gt;&gt; $DFSInputStream.read(DFSClient.java:1166)&gt;&gt;&gt; at java.io.DataInputStream.readFully(DataInputStream.java:178)&gt;&gt;&gt; at org.apache.hadoop.io.DataOutputBuffer&gt;&gt;&gt; $Buffer.write(DataOutputBuffer.java:56)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:90)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1829)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1729)&gt;&gt;&gt; at org.apache.hadoop.io.SequenceFile&gt;&gt;&gt; $Reader.next(SequenceFile.java:1775)&gt;&gt;&gt; at org.apache.hadoop.io.MapFile$Reader.next(MapFile.java:461)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $StoreFileScanner.getNext(HStore.java:2350)&gt;&gt;&gt; at&gt;&gt;&gt; org.apache.hadoop.hbase.HAbstractScanner.next(HAbstractScanner.java:256)&gt;&gt;&gt; at org.apache.hadoop.hbase.HStore&gt;&gt;&gt; $HStoreScanner.next(HStore.java:2561)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1807)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; at org.apache.hadoop.hbase.HRegion&gt;&gt;&gt; $HScanner.next(HRegion.java:1843)&gt;&gt;&gt; ...</description>
      <version>0.16.0,0.1.0,0.2.0</version>
      <fixedVersion>0.1.1,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-2 01:00:00" id="558" opendate="2008-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output hbase+hadoop+jvm version as well as java opts, ulimit, into master/regionserver log on startup</summary>
      <description/>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-27 01:00:00" id="5641" opendate="2012-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>decayingSampleTick1 prevents HBase from shutting down.</summary>
      <description>I think this is the problem. It creates a non-daemon thread. private static final ScheduledExecutorService TICK_SERVICE = Executors.newScheduledThreadPool(1, Threads.getNamedThreadFactory("decayingSampleTick"));</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.histogram.ExponentiallyDecayingSample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-8 01:00:00" id="569" opendate="2008-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DemoClient.php</summary>
      <description>Adding DemoClient.php implementation</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.examples.thrift.README.txt</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-4-11 01:00:00" id="574" opendate="2008-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase does not load hadoop native libs</summary>
      <description>After moving out from hadoop/contrib, the standalone release does not include hadoop native libs in hbase/lib/native while it still includes hadoop-core.jar. I think they should be included as well to improve speed for compression and decompression.</description>
      <version>0.1.0,0.1.1,0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-6-6 01:00:00" id="615" opendate="2008-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region balancer oscillates during cluster startup</summary>
      <description>When starting a cluster with four region servers and a large table (49 regions) (+root +meta) = 51 total regions, the region balancer oscillates for a very long time and does not seem to reach a steady state.Additionally, for whatever reason, it seems reluctant to assign regions to the first of four region servers, which may be the root cause. In my test, the first server had 10 regions assigned, the second and fourth had 13 regions assigned, and the master would continually assign and deassign 2 regions to the third server, which oscillated between 13 and 15 regions. If it assigned the two fluctuating regions to the first server, it would achieve the best balance possible: 12, 13, 13, 13.After 20 minutes, it had not stopped oscillating. An application trying to work against this cluster would run very slowly as it would be continually re-finding the two regions in flux.When the table was being created, regions were nicely balanced. On restart, however, it just would not settle down.Perhaps the balancer should set a target number of regions for each server which when the server achieved +/- 1 regions, the rebalancer would not try to change unless the number of regions changed.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.HServerLoad.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-6-1 01:00:00" id="6150" opendate="2012-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove empty files causing rat check fail</summary>
      <description>Set of empty files found by Jesse.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.UniformSample.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.Snapshot.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.Sample.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.metrics.histogram.ExponentiallyDecayingSample.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-7 01:00:00" id="619" opendate="2008-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix &amp;#39;logs&amp;#39; link in UI</summary>
      <description>Clicking on the 'local logs' link in UI gives 404</description>
      <version>0.1.2,0.2.0</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.InfoServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-7-26 01:00:00" id="6272" opendate="2012-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In-memory region state is inconsistent</summary>
      <description>AssignmentManger stores region state related information in several places: regionsInTransition, regions (region info to server name map), and servers (server name to region info set map). However the access to these places is not coordinated properly. It leads to inconsistent in-memory region state information. Sometimes, some region could even be offline, and not in transition.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestRegionRebalancing.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestDrainingServer.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestSplitTransactionOnCluster.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestOpenedRegionHandler.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMXBean.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterFailover.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestAssignmentManager.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.Mocking.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.mapreduce.TestHFileOutputFormat.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.coprocessor.TestMasterObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHCM.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsckRepair.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.UnknownRegionException.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.OnlineRegions.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.protobuf.ProtobufUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.NotifiableConcurrentSkipListMap.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MXBeanImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterServices.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.MasterDumpServlet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.DeleteTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.CreateTableHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.handler.ClosedRegionHandler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.BulkReOpen.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ClusterStatus.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2012-10-3 01:00:00" id="6940" opendate="2012-10-3 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Enable GC logging by default</summary>
      <description>I think we should enable gc by default. Its pretty frictionless apparently and could help in the case where folks are getting off the ground.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-5-13 01:00:00" id="6990" opendate="2012-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pretty print TTL</summary>
      <description>I've seen a lot of users getting confused by the TTL configuration and I think that if we just pretty printed it it would solve most of the issues. For example, let's say a user wanted to set a TTL of 90 days. That would be 7776000. But let's say that it was typo'd to 77760000 instead, it gives you 900 days!So when we print the TTL we could do something like "x days, x hours, x minutes, x seconds (real_ttl_value)". This would also help people when they use ms instead of seconds as they would see really big values in there.</description>
      <version>None</version>
      <fixedVersion>0.99.0,0.96.3,0.98.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-6-25 01:00:00" id="704" opendate="2008-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update new shell docs and commands on help menu</summary>
      <description>From the help screen on the new shell[root@s2 hbase]# hbase shellHBase Shell; enter 'help&lt;RETURN&gt;' for list of supported commands.Version: 0.2.0-dev, r670701, Wed Jun 25 02:27:19 CDT 2008hbase(main):001:0&gt; create 't1' {NAME =&gt; 'f1', VERSIONS =&gt; 5}SyntaxError: (hbase):2: , unexpected tLCURLYThe help menu gives the above example on creating table in hbase but it does not work!If we release this for the new shell examples need to be more clear. I have not been able to create a table using the new shell yet,Also might be worth adding the old shell back in and remove it after we release 2.0 or at lease until we work out the bugs in new client. If it would not require much updating to keep it current with the new api.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hirb.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-12-16 01:00:00" id="7171" opendate="2012-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Initial web UI for region/memstore/storefiles details</summary>
      <description>Click on a region in UI and get a listing of hfiles in HDFS and summary of memstore content; click on an HFile and see its content</description>
      <version>None</version>
      <fixedVersion>1.2.0,1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.RegionListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-7-30 01:00:00" id="718" opendate="2008-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase shell help info</summary>
      <description>these lines below are in the alter table help section should not be using the words "create a table" should be something like "To alter a table to add columns 'xx', 'yy', 'zz' using all defaults" keep the confusion out of it. To create a table with an 'f1', 'f2', and 'f3' using all defaults: hbase&gt; alter 't1', {NAME =&gt; 'f1'}, {NAME =&gt; 'f2'}, {NAME =&gt; 'f3'}</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-18 01:00:00" id="7181" opendate="2012-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge documentation improvement</summary>
      <description>There is one small issue on the documentation regarding the merge. The class name is not correct. Also, it might be usefull to give an example of the region format.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-11-19 01:00:00" id="7190" opendate="2012-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to hbck to check only meta and assignment</summary>
      <description>Currently, hbck loads region info from HDFS for each run. It may take some time if there are many regions.We need an option to not check HDFS, i.e. just checking meta and assignment.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-7-9 01:00:00" id="735" opendate="2008-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase shell doesn&amp;#39;t trap CTRL-C signal</summary>
      <description>From withing the hbase shell, when there's a IO problem, the hbase client code tries to recover automatically but sometimes we know what's going on and all we want is to cancel the operation by pressing CTRL-C but the shell doesn't catch it and we need to either wait for the operation to timeout or close the terminal and open another one.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-13 01:00:00" id="7350" opendate="2012-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakey tests make CI unreliable</summary>
      <description>Most PreCommit and PostCommit builds are failing these days. Keeping an eye on usual suspects reveals them to be mostly longer-running tests. Either the tests need to me made more rigorous or the erroneous Jenkins configuration needs addressed.The usual suspects: org.apache.hadoop.hbase.client.TestMultiParallel org.apache.hadoop.hbase.TestDrainingServer org.apache.hadoop.hbase.regionserver.TestSplitTransaction org.apache.hadoop.hbase.client.TestMultiParallel.testFlushCommitsNoAbort org.apache.hadoop.hbase.replication.TestReplication org.apache.hadoop.hbase.util.TestHBaseFsck</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2008-7-15 01:00:00" id="747" opendate="2008-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a simple way to do batch updates of many rows</summary>
      <description>Add a simple to do batch updates of many rows as described in HBASE-48.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-1-3 01:00:00" id="7484" opendate="2013-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Restore with schema changes</summary>
      <description>The restore code in the offline branch doesn't handle the schema change.I think that I've lost it during the various rebase, the Handler restore the schema, but the restoreRegion() method in the helper handle just the "same schema" case.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestRestoreSnapshotFromClient.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.snapshot.RestoreSnapshotHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-7-18 01:00:00" id="754" opendate="2008-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The JRuby shell documentation is wrong in "get" and "put"</summary>
      <description>In the shell documentation we can read: hbase&gt; get 't1', 'r1', {TIMESTAMP =&gt; ts1, VERSIONS =&gt; 4}when in fact there are no facility for this. It will work only because it uses getRow(row, ts).Alsohbase&gt; put 't1', 'r1', 'c1', ts1does not work because the 'value' is missing.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-8-22 01:00:00" id="762" opendate="2008-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deleteFamily takes timestamp, should only take row and family. Javadoc describes both cases but only implements the timestamp case.</summary>
      <description>The three version of deleteFamily in client.HTable (Text, String, byte[]) have varying descriptions about whether they take timestamps or not.public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family, long timestamp) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family, long timestamp) throws IOException Delete all cells for a row with matching column family with timestamps less than or equal to timestamp. These will become:public void deleteFamily(org.apache.hadoop.io.Text row, org.apache.hadoop.io.Text family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(String row, String family) throws IOException Delete all cells for a row with matching column family at all timestamps. public void deleteFamily(byte[] row, byte[] family) throws IOException Delete all cells for a row with matching column family at all timestamps.Per Jean-Daniel's comment, deleteAll should then not permit families. I'm unsure whether this is currently allowed or not, but the documentation must be updated either way.Will post patch after more thorough testing.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-2-21 01:00:00" id="7641" opendate="2013-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port HBASE-6669 &amp;#39;Add BigDecimalColumnInterpreter for doing aggregations using AggregationClient&amp;#39; to trunk</summary>
      <description>ColumnInterpreter implementation in trunk is different from that in 0.94This issue ports BigDecimalColumnInterpreter to trunk</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.hbase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-7-5 01:00:00" id="7770" opendate="2013-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>minor integration test framework fixes</summary>
      <description>made FileSystem on HBaseTestingUtil.createMulti() not expect mini cluster added check if server is not running before deciding to restore a server</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.DistributedHBaseCluster.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-8-31 01:00:00" id="789" opendate="2008-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add clover coverage report targets</summary>
      <description>Clover has an open source license for projects under ASF. We can use that to generate coverage report.</description>
      <version>0.2.0</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-3-22 01:00:00" id="7902" opendate="2013-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deletes may be removed during minor compaction, in non-standard compaction schemes [rename enums]</summary>
      <description>Deletes are only removed during major compaction now. However, in presence of file ordering, deletes can be removed during minor compaction too, as long as there's no file that is not being compacted that is older than the files that are.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestCoprocessorScanPolicy.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.security.access.TestAccessController.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.NoOpScanPolicyObserver.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.HFileReadWriteTest.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanType.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.ScanQueryMatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactor.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.compactions.DefaultCompactionPolicy.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.coprocessor.example.ZooKeeperScanPolicyObserver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2013-2-25 01:00:00" id="7933" opendate="2013-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in TableLockManager</summary>
      <description>We are getting NPE in TableLockManager sometimes in tests.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.lock.ZKInterProcessLockBase.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-3-8 01:00:00" id="8050" opendate="2013-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Small fix to book/hbase.tests.html</summary>
      <description>Writing Tests section appears to have a formatting mistake. "Categories and execution time" looks like it was intended to be a sibling heading to "General Rules" and "Sleeps in tests".</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-3-8 01:00:00" id="8051" opendate="2013-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>0.95 build failing on site goal: &amp;#39;failed to get report for org.apache.maven.plugins:maven-project-info-reports-plugin: Could not find goal &amp;#39;dependency-info&amp;#39;&amp;#39;</summary>
      <description>I cannot reproduce locally using same mvn. Let me try upgrading our report plugin. Apparently 'dependency-info' is a new target since 2.5 and our version is 2.4 going by http://maven.apache.org/plugins/maven-project-info-reports-plugin/ (I can't find an explicity invocation of 'dependency-info')</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-8-8 01:00:00" id="806" opendate="2008-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change HbaseMapWritable and RowResult to implement SortedMap instead of Map</summary>
      <description>HbaseMapWritable and RowResult currently implement Map. However, it would be trivial (and highly useful) for them to implement SortedMap since HbaseMapWritable already uses a TreeMap for the map.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-8-10 01:00:00" id="811" opendate="2008-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTD is not fully copyable</summary>
      <description>Part of my HBASE-62 patch was not applied.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.UnmodifyableHTableDescriptor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-8-10 01:00:00" id="813" opendate="2008-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a row counter in the new shell</summary>
      <description>Asked for in the thread "Any equivalence of HQL "select count..." in 0.2 shell?". Add a row counter in the new shell.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hirb.rb</file>
      <file type="M">bin.HBase.rb</file>
      <file type="M">bin.Formatter.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-3-21 01:00:00" id="8162" opendate="2013-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix import of hbase-default.xml into refguide; broke</summary>
      <description>We are importing html version of the transformed hbase-default.xml; our styl'ing of the xml has broken.</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-8-11 01:00:00" id="819" opendate="2008-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove DOS-style ^M carriage returns from all code where found</summary>
      <description>There are a few files that contain DOS-style carriage returns. This is leading to issues when applying patches.The presence of these may also be causing a snowball effect as some IDEs/editors may see one and attempt to apply that LF/CR format to all lines or files.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.client.TestBatchUpdate.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.HRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-3-23 01:00:00" id="8191" opendate="2013-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Documentation is not giving the right class name for offline merges.</summary>
      <description>Existing:$ bin/hbase org.apache.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;Should be:$ bin/hbase org.apache.hadoop.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-8-14 01:00:00" id="832" opendate="2008-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem with row keys beginnig with characters &lt; than &amp;#39;,&amp;#39; and the region location cache</summary>
      <description>We currently have a problem the way we design .META. row keys. When user table row keys begin with characters lesser than ',' like a '$', any operation will fail when: A client has a certain set of regions in cache One region with the faulty row key splits The client receives a request for a row in the split regionThe reason is that it will first get a NSRE then it will try to locate a region using the passed row key. For example: Row in META: entities,,1216750777411Row passed: entities,$-94f9386f-e235-4cbd-aacc-37210a870991,99999999999999The passed row is lesser then the row in .META.</description>
      <version>0.2.0</version>
      <fixedVersion>0.2.1,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-4-10 01:00:00" id="8321" opendate="2013-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log split worker should heartbeat to avoid timeout when the hlog is under recovery</summary>
      <description>Currently, hlog splitter could spend quite sometime to split a log in case any HDFS issue and recoverLease/retry opening is needed. If distributed log split manager times out the log worker, other log worker to take over will run into the same issue.Ideally, we should not need a timeout monitor. Since we have a timeout monitor for DSL now, the worker should heartbeat to avoid wrong/unneeded timeouts.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.RegionSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSMapRUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.FSHDFSUtils.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogFactory.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-5-30 01:00:00" id="8470" opendate="2013-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data file used by TestReference should be excluded from Apache Rat check</summary>
      <description>The following was discovered by Matteo:hbase-server/src/test/data/a6a6562b777440fd9c34885428f5cb61.21e75333ada3d5bafb34bb918f29576c is used by TestReference to verify the format compatibility.We should exclude this file from Apache Rat check.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2008-9-3 01:00:00" id="865" opendate="2008-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc warnings</summary>
      <description>There are javadoc warnings in both the 0.2 branch and in trunk. They must be fixed before 0.2.2 or 0.18.0 are released.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.2.2,0.18.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HLog.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.ipc.TransactionalRegionInterface.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.HStoreKey.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.transactional.TransactionalTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-10-29 01:00:00" id="8652" opendate="2013-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Number of compacting KVs is not reset at the end of compaction</summary>
      <description>Looking at master:60010/master-status#compactStas , I noticed that 'Num. Compacting KVs' column stays unchanged at non-zero value(s).In DefaultCompactor#compact(), we have this at the beginning: this.progress = new CompactionProgress(fd.maxKeyCount);But progress.totalCompactingKVs is not reset at the end of compact().</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.master.RegionServerListTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-7-28 01:00:00" id="8826" opendate="2013-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-8695 is covered in Thrift 2</summary>
      <description>HBASE-8695 is about using the config file, make sure Thrift 2 is doing the same.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-7-28 01:00:00" id="8832" opendate="2013-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure HBASE-4658 is supported by Thrift 2</summary>
      <description>HBASE-4658 adds support for "attributes" for certain operations. Make sure Thrift 2 supports them where ever available in the native API.</description>
      <version>None</version>
      <fixedVersion>0.98.0,0.95.2,0.94.10</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-server.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-5-18 01:00:00" id="889" opendate="2008-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The current Thrift API does not allow a new scanner to be created without supplying a column list unlike the other APIs.</summary>
      <description>The current Thrift API does not allow a new scanner to be created without supplying a column list, unlike the REST api. I posted this on the HBase-Users mailing list. Others concurred that it appears to have been an oversight in the Thrift API. Its quite significant as there is no easy work around, unless you already know which the column families names then list them all when you open the scanner.</description>
      <version>0.2.0,0.2.1</version>
      <fixedVersion>0.20.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>