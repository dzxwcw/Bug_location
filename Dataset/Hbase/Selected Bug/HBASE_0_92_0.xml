<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  <bug fixdate="2010-10-6 01:00:00" id="3085" opendate="2010-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaResource broken on TRUNK up on HUDSON</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.EnableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.DisableTableHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-19 01:00:00" id="3133" opendate="2010-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only log compaction requests when a request is actually added to the queue</summary>
      <description>We always log compaction requests, even if a compaction has already been requested for the specified region.Propose only logging the big compaction request log line if a compaction request is actually added to the queue.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-7-20 01:00:00" id="3135" opendate="2010-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make our MR jobs implement Tool and use ToolRunner so can do -D trickery, etc.</summary>
      <description>Our TIF can take a bunch of config. If our MR jobs &amp;#8211; rowcounter, export, import, etc. &amp;#8211; all implemented Tool/ToolRunner, then we'd pick up the Tool cmdline parse of -D that sets config. Small change. Lots of utility.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.WALPlayer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.RowCounter.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Import.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.Export.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CopyTable.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.CellCounter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-7-16 01:00:00" id="3240" opendate="2010-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation of importtsv and bulk loads</summary>
      <description>Right now our bulk load features are a little confusing. We have loadtable.rb for new tables and completebulkload for existing tables. The docs only talk about the incremental case, and there are basically no docs for the ruby script. We should conslidate these things and make the documentation a little more clear on the full story.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.bulk-loads.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.ImportTsv.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-2-5 01:00:00" id="3419" opendate="2011-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>If re-transition to OPENING during log replay fails, server aborts. Instead, should just cancel region open.</summary>
      <description>The Progressable used on region open to tickle the ZK OPENING node to prevent the master from timing out a region open operation will currently abort the RegionServer if this fails for some reason. However it could be "normal" for an RS to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open.We had a cluster trip over some other issue (for some reason, the tickle was not happening in &lt; 30 seconds, so master was timing out every time). Because of the abort on BadVersion, this eventually led to every single RS aborting itself eventually taking down the cluster.</description>
      <version>0.90.0,0.92.0</version>
      <fixedVersion>0.90.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-10-14 01:00:00" id="3444" opendate="2011-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to prove Bytes.toBytesBinary and Bytes.toStringBinary() is reversible</summary>
      <description>Bytes.toStringBinary() doesn't escape \.Otherwise the transformation isn't reversiblebyte[] a = {'\', 'x' , '0', '0'}Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2011-3-14 01:00:00" id="3631" opendate="2011-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLONE - HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell</summary>
      <description>HBase 2984 breaks ability to specify BLOOMFILTER &amp; COMPRESSION via shell0.90 was fixed but in trunk there is still bug</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-5-30 01:00:00" id="3835" opendate="2011-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch web pages to Jamon template engine instead of JSP</summary>
      <description>Jamon (http://jamon.org) is a template engine that I think is preferable to JSP. You can read an interview with some comparisons vs JSP here: http://www.artima.com/lejava/articles/jamon.htmlIn particular, I think it will give us the following advantages: Since we'll have a servlet in front of each template, it will encourage us to write less inline Java code and do more code in the servlets. Makes proper unit testing easier since you can trivially render a template and pass in mock arguments without having to start a whole HTTP stack Static typing of template arguments makes it easier to know at compile-time if you've made a mistake.Thoughts? I converted the Master UI yesterday and only took a couple hours.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.regionserver.regionserver.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.regionserver.index.html</file>
      <file type="M">src.main.resources.hbase-webapps.master.master.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.master.index.html</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-5-1 01:00:00" id="3839" opendate="2011-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose in-progress tasks on web UIs</summary>
      <description>HBASE-3836 adds a TaskMonitor class which collects info about what's going on inside processes. This ticket is to expose the task monitor info on the web UIs.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.static.hbase.css</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-5-2 01:00:00" id="3844" opendate="2011-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Book.xml (removing link to defunct wiki) and Performance.xml (adding client tip)</summary>
      <description>Book.xml - in the FAQ it had a link to a "Frequently Seen Errors" wiki page. This page is labeled as defunct and doesn't even have anything useful on it anyway. Removed the link to that page.Performance.xml - added tip in Performance under client about attribute selection. This is one of those "obvious but not so obvious" topics, if you only need 3 attributes don't select the entire column family.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.performance.xml</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-11 01:00:00" id="3877" opendate="2011-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Determine Proper Defaults for Compaction ThreadPools</summary>
      <description>With the introduction of HBASE-1476, we now have multithreaded compactions + 2 different ThreadPools for large and small compactions. However, this is disabled by default until we can determine a proper default throttle point. Opening this JIRA to log all discussion on how to select a good default for this case.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.CompactSplitThread.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-6-26 01:00:00" id="3923" opendate="2011-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBASE-1502 Broke Shell&amp;#39;s status &amp;#39;simple&amp;#39; and &amp;#39;detailed&amp;#39;</summary>
      <description>This is due to the JRuby code using the now removed HServerInfo. Also getServers() is now getServersSize() etc.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-1-2 01:00:00" id="3949" opendate="2011-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "Master" link to RegionServer pages</summary>
      <description>Use the ZK info where the master is to add a UI link on the top of each RegionServer page. Currently you cannot navigate directly to the Master UI once you are on a RS page.Not sure if the info port is exposed OTTOMH, but we could either use the RS local config setting for that or add it to ZK to enable lookup.</description>
      <version>0.90.3,0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestRSStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-9 01:00:00" id="3970" opendate="2011-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address HMaster crash/failure half way through meta migration</summary>
      <description>When HMaster tries to migrate (after HBASE-451 goes live) the old HRI (with HTD) to new HRI (with out HTD) and if the Master or the migration process crashes/fails midway, it will leave the .META. in a corrupt state and may not allow successful cluster startup.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.HBaseTestingUtility.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestMetaMigration.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-9 01:00:00" id="3973" opendate="2011-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase IRB shell: Don&amp;#39;t pretty-print the output when stdout isn&amp;#39;t a TTY</summary>
      <description>In the HBase shell, when the output isn't a TTY, the shell assumes the "terminal" to be 100 characters wide. The way the shell wraps things around makes it very hard to script the output of the shell (e.g. redirect the output to a file and then work on that file, or pipe the output to another command).When stdout isn't a TTY, the shell shouldn't try to wrap things around.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-13 01:00:00" id="3983" opendate="2011-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>list command in shell seems broken</summary>
      <description>hbase(main):007:0&gt; listERROR: wrong number of arguments (1 for 2)</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.shell.formatter.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-6-27 01:00:00" id="4036" opendate="2011-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implementing a MultipleColumnPrefixFilter</summary>
      <description>Implementing a MultipleColumnPrefixFilter so that a user can now specify multiple column prefixes. If the qualifier matches any of the prefixes - it will be accepted</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-5-4 01:00:00" id="405" opendate="2008-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TIF and TOF use log4j directly rather than apache commons-logging</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.TableOutputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-7 01:00:00" id="4078" opendate="2011-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Silent Data Offlining During HDFS Flakiness</summary>
      <description>See HBASE-1436 . The bug fix for this JIRA is a temporary workaround for improperly moving partially-written files from TMP into the region directory when a FS error occurs. Unfortunately, the fix is to ignore all IO exceptions, which masks off-lining due to FS flakiness. We need to permanently fix the problem that created HBASE-1436 &amp; then at least have the option to not open a region during times of flakey FS.</description>
      <version>0.89.20100924,0.90.3,0.92.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestCompaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-7 01:00:00" id="4079" opendate="2011-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HTableUtil - helper class for loading data</summary>
      <description>A pattern that we use at Explorys is to chunk up Puts, and then bucket Puts by RegionServer. This reduces the number of RPC calls per writeBuffer flush, because the flushes will typically be going to one region with this approach.I didn't think adding such utility methods to HTable was the right approach, so I created an HTableUtil (in the .client package) that contained such functionality.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-7-22 01:00:00" id="4126" opendate="2011-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make timeoutmonitor timeout after 30 minutes instead of 3</summary>
      <description>See J-D comment here https://issues.apache.org/jira/browse/HBASE-4064?focusedCommentId=13069098&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13069098 where he thinks we should just turn off timeout monitor because it only ever wrecks havoc. Lets make it 30 minutes for 0.90.4.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-22 01:00:00" id="4127" opendate="2011-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBaseAdmin : Don&amp;#39;t modify table&amp;#39;s name away</summary>
      <description>One of the developers was using the default constructor for HTableDescriptor, which is sadly a bad constructor that should never be used. It made the tablename empty in META &amp; caused an ERROR cycle as region onlining kept failing. We should have never let this happen. Don't do table modifications if the HTableDescriptor name doesn't match the table name passed in.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.ModifyTableHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-4 01:00:00" id="4163" opendate="2011-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Split Strategy for YCSB Benchmark</summary>
      <description>Talked with Lars about how we can make it easier for users to run the YCSB benchmarks against HBase &amp; get realistic results. Currently, HBase is optimized for the random/uniform read/write case, which is the YCSB load. The initial reason why we perform bad when users test against us is because they do not presplit regions &amp; have the split ratio really low. We need a one-line way for a user to create a table that is pre-split to 200 regions (or some decent number) by default &amp; disable splitting. Realistically, this is how a uniform load cluster should scale, so it's not a hack. This will also give us a good use case to point to for how users should pre-split regions.</description>
      <version>0.90.3,0.92.0</version>
      <fixedVersion>0.99.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-5 01:00:00" id="4168" opendate="2011-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A client continues to try and connect to a powered down regionserver</summary>
      <description>Experiment-1Started a dev cluster - META is on the same regionserver as my key-value. I kill the regionserver process but donot power down the machine.The META is able to migrate to a new regionserver and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-2Started a dev cluster - META is on a different regionserver as my key-value. I kill the regionserver process but donot power down the machine.The META remains where it is and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-3Started a dev cluster - META is on a different regionserver as my key-value. I power down the machine hosting this regionserver.The META remains where it is and the regions are also able to reopen elsewhere.The client is able to talk to the META and find the new kv location and get it.Experiment-4 (This is the problematic one)Started a dev cluster - META is on the same regionserver as my key-value. I power down the machine hosting this regionserver.The META is able to migrate to a new regionserver - however - it takes a really long time (~30 minutes)The regions on that regionserver DONOT reopen (I waited for 1 hour)The client is able to find the new location of the META, however, the META keeps redirecting the client to powered downregionserver as the location of the key-value it is trying to get. Thus the client's get is unsuccessful.</description>
      <version>None</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-6 01:00:00" id="4171" opendate="2011-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell broken in trunk</summary>
      <description>The shell displays for any command entered:ERROR: undefined method `getZooKeeper' for #&lt;Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0x1c904f75&gt;</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-6 01:00:00" id="4174" opendate="2011-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Developer.xml - adding a sub-section for setting jira priorities</summary>
      <description>Porting a wiki page to HBase book on this subject.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-1-18 01:00:00" id="4224" opendate="2011-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a flush by regionserver rather than by table option</summary>
      <description>This evening needed to clean out logs on the cluster. logs are by regionserver. to let go of logs, we need to have all edits emptied from memory. only flush is by table or region. We need to be able to flush the regionserver. Need to add this.</description>
      <version>None</version>
      <fixedVersion>2.0.0-beta-2,2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.Admin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-7-7 01:00:00" id="424" opendate="2008-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should be able to enable/disable .META. table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.client.HBaseAdmin.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.master.TableOperation.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-4-23 01:00:00" id="4243" opendate="2011-8-23 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>HADOOP_HOME should be auto-detected</summary>
      <description>Now that HBASE-3465 has been integrated, perhaps we should try to auto-detect the HADOOP_HOME setting if it is not given explicitly. Something along the lines of:# check for hadoop in the path141 HADOOP_IN_PATH=`which hadoop 2&gt;/dev/null`142 if [ -f ${HADOOP_IN_PATH} ]; then143 HADOOP_DIR=`dirname "$HADOOP_IN_PATH"`/..144 fi145 # HADOOP_HOME env variable overrides hadoop in the path146 HADOOP_HOME=${HADOOP_HOME:-$HADOOP_DIR}147 if [ "$HADOOP_HOME" == "" ]; then148 echo "Cannot find hadoop installation: \$HADOOP_HOME must be set or hadoop must be in the path";149 exit 4;150 fiThoughts?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-8-29 01:00:00" id="4269" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests and restore semantics to TableInputFormat/TableRecordReader</summary>
      <description>HBASE-4196 Modified the semantics of failures in TableImportFormat/TableRecordReader, and had no tests cases. This patch restores semantics to rethrow when a DoNotRetryIOException is triggered and adds test cases.</description>
      <version>0.90.5,0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapred.TableRecordReaderImpl.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableRecordReaderImpl.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-29 01:00:00" id="4270" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IOE ignored during flush-on-close causes dataloss</summary>
      <description>If the RS experiences an exception during the flush of a region while closing it, it currently catches the exception, logs a warning, and keeps going. If the exception was a DroppedSnapshotException, this means that it will silently drop any data that was in memstore when the region was closed.Instead, the RS should do a hard abort so that its logs will be replayed.</description>
      <version>0.90.4,0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.handler.TestOpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-29 01:00:00" id="4272" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hbck feature to only inspect and try to repair META and ROOT</summary>
      <description>In cases that META is stuck unassigned, hbck currently crashes. It should be able to handle mis-deployed META and ROOT.</description>
      <version>0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-8-30 01:00:00" id="4291" opendate="2011-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve display of regions in transition in UI to be more readable</summary>
      <description>Currently the web UI shows which regions are in transition, including the timestamp of when they entered that state. It would be better to show this timestamp as a human-readable date and relative time ("15 seconds ago") in this context.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.AssignmentManagerStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-9-30 01:00:00" id="4302" opendate="2011-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Only run Snappy compression tests if Snappy is available</summary>
      <description>The presence of the Snappy CODEC it does not mean that the Snappy JNI-bindings and native library are available.Because of this it is not possible to assert SNAPPY compression</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestCompressionTest.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-2 01:00:00" id="4327" opendate="2011-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile HBase against hadoop 0.22</summary>
      <description>Pom contains a profile for hadoop-0.20 and one for hadoop-0.23, but not one for hadoop-0.22.When overriding hadoop.version to 0.22, then the (compile-time) dependency on hadoop-annotations cannot be met.That exists on 0.23 and 0.24/trunk, but not on 0.22.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-9-7 01:00:00" id="4339" opendate="2011-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve eclipse documentation and project file generation</summary>
      <description>Import via m2eclipse ask a few build path fixes. This should be documented in the hbase book.mvn eclipse:eclipse is helped with the build-helper-maven-plugin plugin where additional folder (target/...) are listed. The listed jamon folder is wrong.(Putting these 2 concerns on same jira as they are more or less related, avoiding jira proliferation).</description>
      <version>0.92.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-9-13 01:00:00" id="4381" opendate="2011-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor split decisions into a split policy class</summary>
      <description>This is a semantics-preserving refactor that moves the code that decides when and where to split into a new split policy class.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStore.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-9-16 01:00:00" id="4419" opendate="2011-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Resolve build warning messages</summary>
      <description>This item is created to clean up the build log.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-19 01:00:00" id="4445" opendate="2011-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not passing --config when checking if distributed mode or not</summary>
      <description>If config other than under hbase and we set distributed mode, we were not passing the config to our little property value setter</description>
      <version>0.90.4,0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.start-hbase.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-20 01:00:00" id="4447" opendate="2011-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow hbase.version to be passed in as command-line argument</summary>
      <description>Currently the build always produces the jars and tarball according to the version baked into the POM.When we modify this to allow the version to be passed in as a command-line argument, it can still default to the same behavior, yet give the flexibility for an internal build to tag on own version.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2011-11-12 01:00:00" id="4583" opendate="2011-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate RWCC with Append and Increment operations</summary>
      <description>Currently Increment and Append operations do not work with RWCC and hence a client could see the results of multiple such operation mixed in the same Get/Scan.The semantics might be a bit more interesting here as upsert adds and removes to and from the memstore.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestAtomicOperation.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MemStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-13 01:00:00" id="4588" opendate="2011-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The floating point arithmetic to validate memory allocation configurations need to be done as integers</summary>
      <description>The floating point arithmetic to validate memory allocation configurations need to be done as integers.On our cluster, we had block cache = 0.6 and memstore = 0.2. It was saying this was &gt; 0.8 when it is actually equal.Minor bug but annoying nonetheless.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HBaseConfiguration.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-17 01:00:00" id="4603" opendate="2011-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Uneeded sleep time for tests in hbase.master.ServerManager#waitForRegionServers</summary>
      <description>This functions waits for at least 2 times hbase.master.wait.on.regionservers.interval, defaulted at 3 seconds, i.e. 6 seconds for every mini hbase cluster starts.In the context of a mini cluster, it's not useful, as the regions servers are created locally.Changing this to a lower value such as 100ms gives 5.8 second per HBase cluser start. It should lower the build time on the apache server by more than 8%.Beeing more aggressive (removing all the wait time) could be possible as well. To be studied later.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.resources.hbase-site.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-20 01:00:00" id="4642" opendate="2011-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Apache License Header</summary>
      <description>executing mvn apache-rat:check fails with &amp;#91;ERROR&amp;#93; Failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: Too many unapproved licenses: 84 -&gt; &amp;#91;Help 1&amp;#93;org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.6:check (default-cli) on project hbase: Too many unapproved licenses: 84there are about 70 + files which are missing the Apache License Headers and rest of them should be added to the exclude list.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.set.meta.memstore.size.rb</file>
      <file type="M">bin.set.meta.block.caching.rb</file>
      <file type="M">bin.local-regionservers.sh</file>
      <file type="M">bin.local-master-backup.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-21 01:00:00" id="4645" opendate="2011-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Edits Log recovery losing data across column families</summary>
      <description>There is a data loss happening (for some of the column families) when we do the replay logs.The bug seems to be from the fact that during replay-logs we only choose to replaythe logs from the maximumSequenceID across ALL the stores. This is wrong. If acolumn family is ahead of others (because the crash happened before all the columnfamilies were flushed), then we lose data for the column families that have not yetcaught up.The correct logic for replay should begin the replay from the minimum across themaximum in each store.</description>
      <version>0.89.20100924,0.92.0</version>
      <fixedVersion>0.89-fb,0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-10-28 01:00:00" id="4691" opendate="2011-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove more unnecessary byte[] copies from KeyValues</summary>
      <description>Just looking through the code I found some more spots where we unnecessarily copy byte[] rather than just passing offset and length around.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.coprocessor.ColumnAggregationEndpoint.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.replication.regionserver.ReplicationSink.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.DependentColumnFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-28 01:00:00" id="4694" opendate="2011-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some cleanup of log messages in RS and M</summary>
      <description>I did a little edit of logging. We do way too much but am not going to do a big overhaul just yet. Here's a few small changes saving a few lines, some redundancy, and making others look like surrounding log lines.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.Store.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.MemStoreFlusher.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.LogRoller.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileWriterV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.DefaultLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-1 01:00:00" id="4716" opendate="2011-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve locking for single column family bulk load</summary>
      <description>HBASE-4552 changed the locking behavior for single column family bulk load, namely we don't need to take write lock.A read lock would suffice in this scenario.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-2 01:00:00" id="4734" opendate="2011-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[bulk load] Warn if bulk load directory contained no files</summary>
      <description>Bulk load exits if no files are found in the specified directory. This can happen if a directory has been bulk loaded already (bulk load renames/moves files). It would be good to provide some sort of warning when this happens.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.LoadIncrementalHFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-15 01:00:00" id="4791" opendate="2011-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Secure Zookeeper JAAS configuration to be programmatically set (rather than only by reading JAAS configuration file)</summary>
      <description>In the currently proposed fix for HBASE-2418, there must be a JAAS file specified in System.setProperty("java.security.auth.login.config"). However, it might be preferable to construct a JAAS configuration programmatically, as is done with secure Hadoop (see https://github.com/apache/hadoop-common/blob/a48eceb62c9b5c1a5d71ee2945d9eea2ed62527b/src/java/org/apache/hadoop/security/UserGroupInformation.java#L175).This would have the benefit of avoiding a usage of a system property setting, and allow instead an HBase-local configuration setting.</description>
      <version>None</version>
      <fixedVersion>0.94.4,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.zookeeper.HQuorumPeer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMasterCommandLine.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-17 01:00:00" id="4808" opendate="2011-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to Ensure Expired Deletes Don&amp;#39;t Override Puts</summary>
      <description>We originally thought we had a bug where expired delete markers would early-out valid puts. It ended up being a false alarm, but we added a unit test to ensure that this behavior is correctly maintained.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-5-29 01:00:00" id="481" opendate="2008-2-29 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Use smaller dataset in TestTableIndex and TestTableMapReduce</summary>
      <description>TestTableIndex and TestTableMapReduce rely on MakeMultiRegionTable, which in turn calls addContent in HBaseTestCase. This method produces 17k rows in the table, and something like 10 regions. That's a lot more than we actually need to prove the functionality of TTI and TTMR. Can we reduce the number of rows that we use by shortening the run of addContent? Is there any risk in changing addContent globally to produce less data?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.hbase-site.xml</file>
      <file type="M">conf.hbase-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-17 01:00:00" id="4815" opendate="2011-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable online altering by default, create a config for it</summary>
      <description>There's a whole class of bugs that we've been revealing from trying out online altering in conjunction with other operations like splitting. HBASE-4729, HBASE-4794, and HBASE-4814 are examples.It's not so much that the online altering code is buggy, but that it wasn't tested in an environment that permits splitting.I think we should mark online altering as experimental in 0.92 and add a config to enable it (so it would be disabled by default, requiring people to enable for altering table schema).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.client.TestAdmin.java</file>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.TableEventHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-18 01:00:00" id="4819" opendate="2011-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestShell broke in trunk; typo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-18 01:00:00" id="4820" opendate="2011-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Distributed log splitting coding enhancement to make it easier to understand, no semantics change</summary>
      <description>In reviewing distributed log splitting feature, we found some cosmetic issues. They make the code hard to understand.It will be great to fix them. For this issue, there should be no semantic change.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestSplitLogManager.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestDistributedLogSplitting.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-22 01:00:00" id="4851" opendate="2011-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop maven dependency needs to be an optional one</summary>
      <description>Given that HBase 0.92/0.94 is likely to be used with at least 3 different versions of Hadoop (0.20, 0.22 and 0.23) it seems appropriate to make hadoop maven dependencies into optional ones (IOW, the build of HBase will see NO changes in behavior, but any component that has HBase as a dependency will be in control of what version of Hadoop gets used).</description>
      <version>0.92.0,0.92.1,0.94.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-23 01:00:00" id="4854" opendate="2011-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>it seems that CLASSPATH elements coming from Hadoop change HBase behaviour</summary>
      <description>It looks like HBASE-3465 introduced a slight change in behavior. The ordering of classpath elements makes Hadoop ones go before the HBase ones, which leads to log4j properties picked up from the wrong place, etc. It seems that the easies way to fix that would be to revert the ordering of classpath.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-23 01:00:00" id="4856" opendate="2011-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper to 3.4.0 release</summary>
      <description>Zookeeper 3.4.0 has been released.We should upgade.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-23 01:00:00" id="4859" opendate="2011-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correctly PreWarm HBCK ThreadPool</summary>
      <description>See description at HBASE-3553. We had a patch ready for this in HBASE-3620 but never applied it publicly. Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-24 01:00:00" id="4861" opendate="2011-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some misspells and extraneous characters in logs; set some to TRACE</summary>
      <description>Some small clean up in logs.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.handler.SplitRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaEditor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-12-2 01:00:00" id="4937" opendate="2011-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Error in Quick Start Shell Exercises</summary>
      <description>The shell exercises in the Quick Start (http://hbase.apache.org/book/quickstart.html) startshbase(main):003:0&gt; create 'test', 'cf'0 row(s) in 1.2200 secondshbase(main):003:0&gt; list 'table'test1 row(s) in 0.0550 secondsIt looks like the second command is wrong. Running it, the actual output ishbase(main):001:0&gt; create 'test', 'cf'0 row(s) in 0.3630 secondshbase(main):002:0&gt; list 'table'TABLE 0 row(s) in 0.0100 secondsThe argument to list should be 'test', not 'table', and the output in the example is missing the TABLE line.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.getting.started.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-5-9 01:00:00" id="4990" opendate="2011-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document secure HBase setup</summary>
      <description/>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-12-15 01:00:00" id="5040" opendate="2011-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HBase builds fail</summary>
      <description>I saw the following in HBase-0.92-security build #39:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure[ERROR] &lt;https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&gt;:[590,4] method does not override or implement a method from a supertype[ERROR] -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure&lt;https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java&gt;:[590,4] method does not override or implement a method from a supertypeThe above was probably introduced by HBASE-5006</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-16 01:00:00" id="5052" opendate="2011-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The path where a dynamically loaded coprocessor jar is copied on the local file system depends on the region name (and implicitly, the start key)</summary>
      <description>When loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. The name is "cleaned", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-18 01:00:00" id="5062" opendate="2011-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing logons if security is enabled</summary>
      <description>Somehow the attached changes are missing from the security integration.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Strings.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.rest.Main.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-19 01:00:00" id="5068" opendate="2011-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RC1 can not build its hadoop-0.23 profile</summary>
      <description>The hadoop .23 version needs to be bumped to 0.23.1-SNAPSHOT</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-12-22 01:00:00" id="5087" opendate="2011-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Up the 0.92RC zk to 3.4.1RC0</summary>
      <description>ZK just found bad bug in 3.4.1 zookeeper-1333. They put up a fix and new rc, 3.4.1(Andrew, you saw Todds query asking if it'd possible to hold to zk 3.3.4 and just have 3.4.1 for secure installs?)</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-1-31 01:00:00" id="5111" opendate="2011-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper to 3.4.2 release</summary>
      <description>Zookeeper 3.4.2 has just been released.We should upgrade to this release.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-3 01:00:00" id="5119" opendate="2012-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set the TimeoutMonitor&amp;#39;s timeout back down</summary>
      <description>The TimeoutMonitor used to be extremely racy and caused more troubles than it fixed, but most of this has been fixed I believe in the context of 0.92 so I think we should set it down back to a useful level. Currently it's 30 minutes, what should the new value be?I think 5 minutes should be good, will do some testing.</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-4-13 01:00:00" id="512" opendate="2008-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add configuration for global aggregate memcache size</summary>
      <description>Currently, we have a configuration parameter for the size a Memcache must reach before it is flushed. This leads to pretty even sized mapfiles when flushes run, which is nice. However, as noted in the parent issue, we can often get to a point where we run out of memory because too much data is hanging around in Memcaches.I think that we should add a new configuration parameter that governs the total amount of memory that the region server should spend on Memcaches. This would have to be some number less than the heap size - we'll have to discover the proper values through experimentation. Then, when a put comes in, if the global aggregate size of all the Memcaches for all the stores is at the threshold, then we should block the current and any subsequent put operations from completing until forced flushes cause the memory usage to go back down to a safe level. The existing strategy for triggering flushes will still be in play, just augmented with this blocking behavior.This approach has the advantage of helping us avoid OOME situations by warning us well in advance of overflow. Additionally, it becomes something of a performance tuning knob, allowing you to allocate more memory to improve write performance. This is superior to the previously suggested PhantomReference approach because that would possibly causes us to bump into further OOMEs while we're trying to flush to avoid them.</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.org.apache.hadoop.hbase.MiniHBaseCluster.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.regionserver.Flusher.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.LocalHBaseCluster.java</file>
      <file type="M">conf.hbase-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-1-10 01:00:00" id="5167" opendate="2012-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>We shouldn&amp;#39;t be injecting &amp;#39;Killing [daemon]&amp;#39; into logs, when we aren&amp;#39;t doing that.</summary>
      <description>HBASE-4209 changed the behavior of the scripts such that we do not kill the daemons away anymore. We should have also changed the message shown in the logs.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase-daemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-10 01:00:00" id="5175" opendate="2012-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add DoubleColumnInterpreter</summary>
      <description>DoubleColumnInterpreter was requested by Royston Sellman.</description>
      <version>None</version>
      <fixedVersion>0.98.1,0.99.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-protocol.src.main.protobuf.HBase.proto</file>
      <file type="M">hbase-protocol.src.main.java.org.apache.hadoop.hbase.protobuf.generated.HBaseProtos.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-2-13 01:00:00" id="5195" opendate="2012-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Coprocessors] preGet hook does not allow overriding or wrapping filter on incoming Get</summary>
      <description>Without the ability to wrap the internal Scan on the Get, we can't override (or protect, in the case of access control) Gets as we can Scans. The result is inconsistent behavior.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2012-1-23 01:00:00" id="5259" opendate="2012-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Normalize the RegionLocation in TableInputFormat by the reverse DNS lookup.</summary>
      <description>Assuming the HBase and MapReduce running in the same cluster, the TableInputFormat is to override the split function which divides all the regions from one particular table into a series of mapper tasks. So each mapper task can process a region or one part of a region. Ideally, the mapper task should run on the same machine on which the region server hosts the corresponding region. That's the motivation that the TableInputFormat sets the RegionLocation so that the MapReduce framework can respect the node locality. The code simply set the host name of the region server as the HRegionLocation. However, the host name of the region server may have different format with the host name of the task tracker (Mapper task). The task tracker always gets its hostname by the reverse DNS lookup. And the DNS service may return different host name format. For example, the host name of the region server is correctly set as a.b.c.d while the reverse DNS lookup may return a.b.c.d. (With an additional doc in the end).So the solution is to set the RegionLocation by the reverse DNS lookup as well. No matter what host name format the DNS system is using, the TableInputFormat has the responsibility to keep the consistent host name format with the MapReduce framework.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-24 01:00:00" id="5266" opendate="2012-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation for ColumnRangeFilter</summary>
      <description>There are only a few lines of documentation for ColumnRangeFilter.Given the usefulness of this filter for efficient intra-row scanning (see HBASE-5229 and HBASE-4256), we should make this filter more prominent in the documentation.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-1-25 01:00:00" id="5278" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell script refers to removed "migrate" functionality</summary>
      <description>$ hbase migrateException in thread "main" java.lang.NoClassDefFoundError: org/apache/hadoop/hbase/util/MigrateCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.hbase.util.Migrateat java.net.URLClassLoader$1.run(URLClassLoader.java:202)at java.security.AccessController.doPrivileged(Native Method)at java.net.URLClassLoader.findClass(URLClassLoader.java:190)at java.lang.ClassLoader.loadClass(ClassLoader.java:306)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)at java.lang.ClassLoader.loadClass(ClassLoader.java:247)Could not find the main class: org.apache.hadoop.hbase.util.Migrate. Program will exit.The 'hbase' shell script has docs referring to a 'migrate' command which no longer exists.</description>
      <version>0.90.5,0.92.0,0.94.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-2-27 01:00:00" id="5294" opendate="2012-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure javadoc is included in tarball bundle when we release</summary>
      <description>0.92.0 doesn't have javadoc in the tarball. Fix.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2012-2-9 01:00:00" id="5364" opendate="2012-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix source files missing licenses in 0.92 and trunk</summary>
      <description>running 'mvn rat:check' shows that a few files have snuck in that do not have proper apache licenses. Ideally we should fix these before we cut another release/release candidate.This is a blocker for 0.94, and probably should be for the other branches as well.</description>
      <version>0.92.0,0.94.0</version>
      <fixedVersion>0.90.6,0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.TestHTableDescriptor.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestImportExport.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.constraint.RuntimeFailConstraint.java</file>
      <file type="M">src.packages.deb.conf-pseudo.control.control</file>
      <file type="M">src.main.python.hbase.merge.conf.py</file>
      <file type="M">dev-support.findHangingTest.sh</file>
      <file type="M">bin.hbase-jruby</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-17 01:00:00" id="5421" opendate="2012-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use hadoop-client/hadoop-minicluster artifacts for Hadoop 0.23 build</summary>
      <description>Hadoop recently added hadoop-client and hadoop-minicluster artifacts for Hadoop 0.23+ that don't export all the internal dependencies (HADOOP-8009).Let's use them instead of manually specifying transitive dependency exclusion lists (which is error prone and annoying).</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-2-17 01:00:00" id="5427" opendate="2012-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade our zk to 3.4.3</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-2-20 01:00:00" id="5436" opendate="2012-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Right-size the map when reading attributes.</summary>
      <description/>
      <version>0.92.0</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.OperationWithAttributes.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2012-3-2 01:00:00" id="5508" opendate="2012-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to allow test output to show on the terminal</summary>
      <description>Sometimes it is useful to directly see the test results on the terminal.We can add a property to achieve that.mvn test -Dtest.output.tofile=false</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-2 01:00:00" id="5511" opendate="2012-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>More doc on maven release process</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.developer.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-5 01:00:00" id="5519" opendate="2012-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect warning in splitlogmanager</summary>
      <description>because of recently added behavior - where the splitlogmanager timeout thread get's data from zk node just to check that the zk node is there ... we might have multiple watches firing without the task znode expiring.remove the poor warning message. (internally, there was an assert that failed in Mikhail's tests)</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-5 01:00:00" id="5522" opendate="2012-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hbase 0.92 test artifacts are missing from Maven central</summary>
      <description>Could someone with enough karma, please, publish the test artifacts for 0.92.0?</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-5 01:00:00" id="5524" opendate="2012-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a couple of more filters to our rat exclusion set</summary>
      <description>Build up on jenkins is failing because I just enabled the rat/license check as part of our build. We're failing because CP is writing test data into top-level at ./test.</description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-6 01:00:00" id="5529" opendate="2012-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR test failures becuase MALLOC_ARENA_MAX is not set</summary>
      <description>When running unit tests on CentOS 6 I get a bunch of unit test failures in mapreduce-related tests due to:2012-03-03 00:14:18,776 WARN &amp;#91;Container Monitor&amp;#93;monitor.ContainersMonitorImpl$MonitoringThread(436): Container&amp;#91;pid=21446,containerID=container_1330762435849_0002_01_000001&amp;#93; isrunning beyond virtual memory limits. Current usage: 223.1mb of 2.0gbphysical memory used; 6.9gb of 4.2gb virtual memory used. Killingcontainer.Note: this also came up in the mapreduce project. See: https://issues.apache.org/jira/browse/MAPREDUCE-3933Patch coming shortly</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-9 01:00:00" id="5552" opendate="2012-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up our jmx view; its a bit of a mess</summary>
      <description>Fix before we release 0.92.1</description>
      <version>None</version>
      <fixedVersion>0.92.1,0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.metrics.HBaseInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseRPCStatistics.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-9 01:00:00" id="5555" opendate="2012-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a pointer to a dns verification utility in hbase book/dns</summary>
      <description>DNS should work correctly in a Hbase cluster. I have a simple DNS checker utility, that verifies DNS on all machines of the cluster. https://github.com/sujee/hadoop-dns-checkeradd a pointer to the tool in hbase book : http://hbase.apache.org/book.html#dns</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-4-3 01:00:00" id="559" opendate="2008-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MR example job to count table rows</summary>
      <description>The Lars' import is a little messy; he's not sure how many records were imported. Running a select takes a couple of hours. He happens to have an idle MR cluster standing by. An example MR job that just did a count of records would be generally useful. Could even output row keys so you'd have a list of what made it in. Later, if this tool becomes popular with derivatives and similiars, we can bundle a jar of MR jobs to run against your tables that can answer common queries and that are amenable to subclassing/modification.</description>
      <version>None</version>
      <fixedVersion>0.1.2,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.mapred.package-info.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-16 01:00:00" id="5591" opendate="2012-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ThiftServerRunner.HBaseHandler.toBytes() is identical to Bytes.getBytes()</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-4-20 01:00:00" id="5604" opendate="2012-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>M/R tool to replay WAL files</summary>
      <description>Just an idea I had. Might be useful for restore of a backup using the HLogs.This could an M/R (with a mapper per HLog file).The tool would get a timerange and a (set of) table(s). We'd pick the right HLogs based on time before the M/R job is started and then have a mapper per HLog file.The mapper would then go through the HLog, filter all WALEdits that didn't fit into the time range or are not any of the tables and then uses HFileOutputFormat to generate HFiles.Would need to indicate the splits we want, probably from a live table.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestWALPlayer.java</file>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-5-23 01:00:00" id="600" opendate="2008-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filters have excessive DEBUG logging</summary>
      <description>Downgrade most to trace</description>
      <version>None</version>
      <fixedVersion>0.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.WhileMatchRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.StopRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RowFilterSet.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.RegExpRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.PageRowFilter.java</file>
      <file type="M">src.java.org.apache.hadoop.hbase.filter.InclusiveStopRowFilter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-7-25 01:00:00" id="6265" opendate="2012-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling getTimestamp() on a KV in cp.prePut() causes KV not to be flushed</summary>
      <description>There is an issue when you call getTimestamp() on any KV handed into a Coprocessor's prePut(). It initializes the internal "timestampCache" variable. When you then pass it to the normal processing, the region server sets the time to the server time in case you have left it unset from the client side (updateLatestStamp() call). The TimeRangeTracker then calls getTimestamp() later on to see if it has to include the KV, but instead of getting the proper time it sees the cached timestamp from the prePut() call.</description>
      <version>0.92.0,0.94.0,0.95.2</version>
      <fixedVersion>0.94.1,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.TestKeyValue.java</file>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-7-6 01:00:00" id="6341" opendate="2012-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Publicly expose HConnectionKey</summary>
      <description>HBASE-3777 introduced the concept of a HConnectionKey to quickly identify and compare Configuration instances which lack equals/hashCode values.We currently have use cases where being able to key Configuration objects in a similar way would be helpful. An example of this would be maintain a cache of HTablePool instances based on the HConnectionKey instead of the Configuration instance.I propose that HConnectionKey be made publicly available instead of its current package scope. Or another possibility would be to move it from being a static inner class to being part of the API.</description>
      <version>0.92.0</version>
      <fixedVersion>0.92.2,0.94.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-29 01:00:00" id="6471" opendate="2012-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression caused by HBASE-4054</summary>
      <description>The patch in HBASE-4054 switches the PooledHTable to extend HTable as opposed to implement HTableInterface.Since HTable does not have an empty constructor, the patch added a call to the super() constructor, which though does trigger the ZooKeeper and META scan, causing a considerable delay. With multiple threads using the pool in parallel, the first thread is holding up all the subsequent ones, in effect it negates the whole reason we have a HTable pool.We should complete HBASE-5728, or alternatively add a protected, empty constructor the HTable. I am +1 for the former.</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestHTablePool.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.rest.RegionsResource.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-9-4 01:00:00" id="6716" opendate="2012-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoopqa is hosed</summary>
      <description>See this thread on list: http://search-hadoop.com/m/PtDLC19vEd62/%2522Looks+like+HadoopQA+is+hosed%2522&amp;subj=Looks+like+HadoopQA+is+hosed+Lots of the hadoopqa builds are failing complaining about missing dir.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-21 01:00:00" id="6856" opendate="2012-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document the LeaseException thrown in scanner next</summary>
      <description>In some situations clients that fetch data from a RS get a LeaseException instead of the usual ScannerTimeoutException/UnknownScannerException.This particular case should be documented in the HBase guide.Some key points the source of exception is: org.apache.hadoop.hbase.regionserver.Leases.removeLease(Leases.java:230) it happens in the context of a slow/freezing RS#next it can be prevented by having hbase.rpc.timeout &gt; hbase.regionserver.lease.periodHarsh J investigated the issue and has some conclusions, seehttp://mail-archives.apache.org/mod_mbox/hbase-user/201209.mbox/%3CCAOcnVr3R-LqtKhFsk8Bhrm-YW2i9O6J6Fhjz2h7q6_sxvwd2yw%40mail.gmail.com%3E</description>
      <version>0.92.0</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-11-29 01:00:00" id="7060" opendate="2012-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Region load balancing by table does not handle the case where a table&amp;#39;s region count is lower than the number of the RS in the cluster</summary>
      <description>When the table's region count is less than the count of region servers, the region balance algorithm will not move the region. For example, the cluster has 100 RS, the table has 50 regions sitting on one RS, they will not be moved to any of the other 99 RS.This is because the algorithm did not calculate the under-loaded RS correctly. This is how the algorithm works with the above example:avg-regions-per-RS=0.5min-RS-per-RS=0max-RS-per-RS=1when they calculate the under loaded RS, the code is as below. Since regionCount=0, which is always &gt;=min, so it will always skip, therefore, no underloaded RS are found.Map&lt;ServerName, Integer&gt; underloadedServers = new HashMap&lt;ServerName, Integer&gt;();for (Map.Entry&lt;ServerAndLoad, List&lt;HRegionInfo&gt;&gt; server:serversByLoad.entrySet()) {int regionCount = server.getKey().getLoad();if (regionCount &gt;= min) { break; }underloadedServers.put(server.getKey().getServerName(), min - regionCount);}Later the function returns since underloaded RS size is 0if (serverUnerloaded ==0) return regionsToReturn;</description>
      <version>0.92.0</version>
      <fixedVersion>0.94.3,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.balancer.DefaultLoadBalancer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-7-8 01:00:00" id="7129" opendate="2012-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need documentation for REST atomic operations (HBASE-4720)</summary>
      <description>HBASE-4720 added checkAndPut/checkAndDelete capability to the REST interface, but the REST documentation (in the package summary) needs to be updated so people know that this feature exists and how to use it.http://wiki.apache.org/hadoop/Hbase/Stargatehttp://hbase.apache.org/book/rest.html</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.external.apis.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-12-28 01:00:00" id="7452" opendate="2012-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change ForeignException#receive(String, FE) to only be #receive(FE)</summary>
      <description>This was suggested but not completely finished before HBASE-7206 got committed. This finishes the job.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.errorhandling.TestTimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.errorhandling.TestForeignExceptionDispatcher.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.TimeoutExceptionInjector.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.ForeignExceptionListener.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.errorhandling.ForeignExceptionDispatcher.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>