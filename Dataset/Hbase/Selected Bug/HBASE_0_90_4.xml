<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  
  <bug fixdate="2011-10-14 01:00:00" id="3444" opendate="2011-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test to prove Bytes.toBytesBinary and Bytes.toStringBinary() is reversible</summary>
      <description>Bytes.toStringBinary() doesn't escape \.Otherwise the transformation isn't reversiblebyte[] a = {'\', 'x' , '0', '0'}Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.util.TestBytes.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-7-22 01:00:00" id="4126" opendate="2011-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make timeoutmonitor timeout after 30 minutes instead of 3</summary>
      <description>See J-D comment here https://issues.apache.org/jira/browse/HBASE-4064?focusedCommentId=13069098&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13069098 where he thinks we should just turn off timeout monitor because it only ever wrecks havoc. Lets make it 30 minutes for 0.90.4.</description>
      <version>None</version>
      <fixedVersion>0.90.4</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2011-8-19 01:00:00" id="4227" opendate="2011-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify the webUI so that default values of column families are not shown</summary>
      <description>With the introduction of online schema changes, it will become more advantageous to put configuration knobs at the column family level vs global configuration settings. This will create a nasty web UI experience for showing table properties unless we default to showing the custom values instead of all values. It's on the table if we want to modify the shell's 'describe' method as well. scan '.META.' should definitely return the full properties however.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-21 01:00:00" id="4236" opendate="2011-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t lock the stream while serializing the response</summary>
      <description>It is not necessary to hold the lock on the stream while the response is being serialized. This unnecessarily prevents serializing responses in parallel.</description>
      <version>0.90.4</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseClient.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-23 01:00:00" id="4246" opendate="2011-8-23 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Cluster with too many regions cannot withstand some master failover scenarios</summary>
      <description>We ran into the following sequence of events: master startup failed after only ROOT had been assigned (for another reason) restarted the master without restarting other servers. Since there was at least one region assigned, it went through the failover code path master scanned META and inserted every region into /hbase/unassigned in ZK. then, it called "listChildren" on the /hbase/unassigned znode, and crashed with "Packet len6080218 is out of range!" since the IPC response was larger than the default maximum.</description>
      <version>0.90.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-2-7 01:00:00" id="425" opendate="2008-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix doc. so it accomodates new hbase untethered context</summary>
      <description>Backport fixes to 0.1 branch.</description>
      <version>None</version>
      <fixedVersion>0.1.0,0.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.java.overview.html</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-25 01:00:00" id="4256" opendate="2011-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intra-row scanning (part deux)</summary>
      <description>Dave Revell was asking on IRC today if there's a way to scan ranges of qualifiers within a row. That is, to be able to specify a start qualifier and an end qualifier so that the Get or Scan seeks directly to the first qualifier and stops at some point which can be predeterminate by a qualifier or simply a batch configuration (already exists).This is particularly useful for large rows with time-based qualifiers.Dave also mentioned that another popular database has such a feature that they call "column slices".</description>
      <version>0.90.4</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-3-29 01:00:00" id="4284" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>document permissions that need to be set on importtsv output before completebulkload</summary>
      <description>I am using HBase 0.94 from CDH3u1.After running importtsv using the -Dimporttsv.bulk.output=&lt;output dir&gt; option, I find that completebulkload fails due to hbase not having write permissions on the contents of the output dir that importtsv wrote. I have to manually set write permissions on these contents before I can run completebulkload successfully.Ideally, I should not have to do that (set the permissions manually). Given that I do, this should at least be documented as a limitation of the importtsv utility.</description>
      <version>0.90.4,0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.ops.mgt.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-3-29 01:00:00" id="4285" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>partitions file created in user&amp;#39;s home directory by importtsv</summary>
      <description>I am using HBase 0.94 from CDH3u1.After running importtsv, I find that a temporary partitions_* file is written to my user home directory in HDFS. This file should really be deleted automatically when it is no longer needed.</description>
      <version>0.90.4,0.98.0,0.95.0</version>
      <fixedVersion>0.98.0,0.95.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.mapreduce.HFileOutputFormat.java</file>
      <file type="M">hbase-it.src.test.java.org.apache.hadoop.hbase.mapreduce.IntegrationTestImportTsv.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-29 01:00:00" id="4287" opendate="2011-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>If region opening fails, try to transition region back to "offline" in ZK</summary>
      <description>In the case that region-opening fails, we currently just close the region again, but don't do anything to the node in ZK. Instead, we should attempt to transition it from the OPENING state back to an OFFLINE state, or perhaps a new FAILED_OPEN state. Otherwise, we have to wait for the full timeoutMonitor period to elapse, which can be quite a long time.</description>
      <version>0.90.4</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.handler.TestOpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.AssignmentManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.ExecutorService.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.EventHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-9-1 01:00:00" id="4321" opendate="2011-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more comprehensive region split calculator</summary>
      <description>Hbck currently scans through meta one entry at a time, only keeping a reference to the previous meta entry. This is insufficient for capturing all the possible problems in meta and needs something more to properly identify holes, overlaps, duplicate start keys, and otherwise invalid meta entries.Ideally, this calculator could also be used online interrogating an existing meta (HBASE-4058), and also used to generate a completely new meta offline just from regioninfo and in hdfs (HBASE-3505).</description>
      <version>0.90.4</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-7 01:00:00" id="4342" opendate="2011-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Thrift to 0.7.0</summary>
      <description>The new version of Thrift is 0.7.0 and it has features and bug fixes that could be useful to include in the next release of HBase.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TScan.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRowResult.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TRegionInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.TCell.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Mutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.IOError.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.IllegalArgument.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.ColumnDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.BatchMutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.AlreadyExists.java</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-9 01:00:00" id="4359" opendate="2011-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show dead RegionServer names in the HMaster info page</summary>
      <description>Unlike other components of the cluster, like NameNode and JobTracker pages, the HMaster's info page does not show any data on dead region servers. While an RS is stateless being a good reason not to count dead nodes, I think having a list of dead nodes helps in cases where an administrator would want to find out which nodes are missing out on RS action (hey, everyone likes consistently spiking graphs! ).Following HBASE-3580, I think it makes sense to have a list of already maintained dead nodes show up in the info UI.</description>
      <version>0.90.4</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.master.TestMasterStatusServlet.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterStatusServlet.java</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-13 01:00:00" id="4378" opendate="2011-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[hbck] Does not complain about regions with startkey==endkey.</summary>
      <description>hbck doesn't seem to complain or have an error condition if there is a region where startkey==endkey.</description>
      <version>0.90.4</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.util.TestHBaseFsck.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.HBaseFsck.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-19 01:00:00" id="4432" opendate="2011-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable/Disable off heap cache with config</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-19 01:00:00" id="4437" opendate="2011-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update hadoop in 0.92 (0.20.205?)</summary>
      <description>We ship with branch-0.20-append a few versions back from the tip. If 205 comes out and hbase works on it, we should ship 0.92 with it (while also ensuring it work w/ 0.22 and 0.23 branches).</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-webapps.master.zk.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.master.tablesDetailed.jsp</file>
      <file type="M">src.main.resources.hbase-webapps.master.table.jsp</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.regionserver.RSStatusTmpl.jamon</file>
      <file type="M">src.main.jamon.org.apache.hbase.tmpl.master.MasterStatusTmpl.jamon</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-19 01:00:00" id="4445" opendate="2011-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not passing --config when checking if distributed mode or not</summary>
      <description>If config other than under hbase and we set distributed mode, we were not passing the config to our little property value setter</description>
      <version>0.90.4,0.92.0</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.stop-hbase.sh</file>
      <file type="M">bin.start-hbase.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-20 01:00:00" id="4449" opendate="2011-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LoadIncrementalHFiles should be able to handle CFs with blooms</summary>
      <description>When LoadIncrementalHFiles loads a store file that crosses region boundaries, it will split the file at the boundary to create two store files. If the store file is for a column family that has a bloom filter, then a "java.lang.ArithmeticException: / by zero" will be raised because ByteBloomFilter() is called with maxKeys of 0.The included patch assumes that the number of keys in each split child will be equal to the number of keys in the parent's bloom filter (instead of 0). This is an overestimate, but it's safe and easy.</description>
      <version>0.90.4</version>
      <fixedVersion>0.90.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.mapreduce.TestLoadIncrementalHFiles.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2008-2-14 01:00:00" id="448" opendate="2008-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing name mark</summary>
      <description>Added missing name mark</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-24 01:00:00" id="4480" opendate="2011-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Testing script to simplify local testing</summary>
      <description>As mentioned by http://search-hadoop.com/m/r2Ab624ES3e and http://search-hadoop.com/m/cZjDH1ykGIA it would be nice if we could have a script that would handle more of the finer points of running/checking our test suite.This script should:(1) Allow people to determine which tests are hanging/taking a long time to run(2) Allow rerunning of particular tests to make sure it wasn't an artifact of running the whole suite that caused the failure(3) Allow people to specify to run just unit tests or also integration tests (essentially wrapping calls to 'maven test' and 'maven verify').This script should just be a convenience script - running tests directly from maven should not be impacted.</description>
      <version>0.90.4</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2011-10-24 01:00:00" id="4656" opendate="2011-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Note how dfs.support.append has to be enabled in 0.20.205.0 clusters</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.docbkx.configuration.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-25 01:00:00" id="4658" opendate="2011-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Put attributes are not exposed via the ThriftServer</summary>
      <description>The Put api also takes in a bunch of arbitrary attributes that an application can use to associate metadata with each put operation. This is not exposed via Thrift.</description>
      <version>None</version>
      <fixedVersion>0.94.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.resources.org.apache.hadoop.hbase.thrift.Hbase.thrift</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftServerRunner.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.generated.Hbase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegionThriftServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-10-25 01:00:00" id="4670" opendate="2011-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix javadoc warnings</summary>
      <description>We have hundreds of javadoc warnings emitted on every build.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RegionServerTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Objects.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.FSUtils.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.CompoundBloomFilterBase.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.Bytes.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.ByteBloomFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilterFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.util.BloomFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.TableDescriptors.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ServerName.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogKey.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.StoreFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitTransaction.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.SplitLogWorker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.CatalogTracker.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaMigrationRemovingHTD.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.catalog.MetaReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Append.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.AggregationClient.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.coprocessor.LongColumnInterpreter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnection.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HConnectionManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTableInterface.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTablePool.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.Mutation.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.RetriesExhaustedWithDetailsException.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.AggregateProtocol.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.ColumnInterpreter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.MasterObserver.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.coprocessor.package-info.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.executor.RegionTransitionData.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.BitComparator.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.ParseFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.filter.RowFilter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HColumnDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerAddress.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HTableDescriptor.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.HalfStoreFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.AbstractHFileReader.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.BlockCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFile.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlock.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileBlockIndex.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV1.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.HFileReaderV2.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.InlineBlockWriter.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.io.hfile.slab.SlabCache.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.Delayable.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.KeyValue.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.mapreduce.hadoopbackport.InputSampler.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.HMaster.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.LoadBalancerFactory.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.MasterFileSystem.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.master.SplitLogManager.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.MonitoredTask.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.monitoring.ThreadMonitoring.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.KeyValueScanner.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-11-1 01:00:00" id="4715" opendate="2011-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove stale broke .rb scripts from bin dir</summary>
      <description>Lets clean up bin dir removing scripts that have gone stale and don't work any more.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.set.meta.memstore.size.rb</file>
      <file type="M">bin.rename.table.rb</file>
      <file type="M">bin.loadtable.rb</file>
      <file type="M">bin.check.meta.rb</file>
      <file type="M">bin.add.table.rb</file>
      <file type="M">bin.set.meta.block.caching.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-11-11 01:00:00" id="4775" opendate="2011-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove -ea from all but tests; enable it if you need it testing</summary>
      <description>Follows on from discussion on the tail of HBASE-2753</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-1 01:00:00" id="4922" opendate="2011-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[packaging] Assembly tars up hbase in a subdir; i.e. after untar hbase-0.92.0 has a subdir named 0.92.0</summary>
      <description>Reported by Roman.</description>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-1 01:00:00" id="4923" opendate="2011-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[packaging] Assembly should make only executables executable (docs should not be executable!)</summary>
      <description>Reported by Roman.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.assembly.all.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-7 01:00:00" id="4970" opendate="2011-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a parameter so that keepAliveTime of Htable thread pool can be changed</summary>
      <description>In my cluster, I changed keepAliveTime from 60 s to 3600 s. Increasing RES is slowed down.Why increasing keepAliveTime of HBase thread pool is slowing down our problem occurance &amp;#91;RES value increase&amp;#93;?You can go through the source of sun.nio.ch.Util. Every thread hold 3 softreference of direct buffer(mustangsrc) for reusage. The code names the 3 softreferences buffercache. If the buffer was all occupied or none was suitable in size, and new request comes, new direct buffer is allocated. After the service, the bigger one replaces the smaller one in buffercache. The replaced buffer is released.So I think we can add a parameter to change keepAliveTime of Htable thread pool.</description>
      <version>0.90.4</version>
      <fixedVersion>0.90.6,0.92.1,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.java.org.apache.hadoop.hbase.client.HTable.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-9 01:00:00" id="4995" opendate="2011-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase zk maxClientCnxns to give us some head room</summary>
      <description>It's pretty easy to run out of zk connections on a single host if it's running a master, region server, and a TT with a few slots. Just to make it easier for our users, we should set it to something like 100 by default.</description>
      <version>0.90.4</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.resources.hbase-default.xml</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.HConstants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-9 01:00:00" id="4996" opendate="2011-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoopqa not running long category tests, just short and medium</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.test-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-15 01:00:00" id="5044" opendate="2011-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clarify solution for problem described on http://hbase.apache.org/book/trouble.mapreduce.html</summary>
      <description>Add some documentation regarding how to fix the problem described on :http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/package-summary.html#classpathShould be some text like: You should run your mapreduce job with your HADOOP_CLASSPATH set to include the HBase jar and HBase's configured classpath. For example (substitute your own hbase jar location for is hbase-0.90.0-SNAPSHOT.jar):HADOOP_CLASSPATH=${HBASE_HOME}/target/hbase-0.90.0-SNAPSHOT.jar:`${HBASE_HOME}/bin/hbase classpath` ${HADOOP_HOME}/bin/hadoop jar ${HBASE_HOME}/target/hbase-0.90.0-SNAPSHOT.jar rowcounter usertable</description>
      <version>None</version>
      <fixedVersion>0.98.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.docbkx.troubleshooting.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-1-31 01:00:00" id="5111" opendate="2011-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade zookeeper to 3.4.2 release</summary>
      <description>Zookeeper 3.4.2 has just been released.We should upgrade to this release.</description>
      <version>None</version>
      <fixedVersion>0.92.0,0.94.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-1-4 01:00:00" id="5125" opendate="2012-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop to 1.0.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.92.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-5 01:00:00" id="5127" opendate="2012-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ref manual] Better block cache documentation</summary>
      <description>I've been playing a lot with 0.92's block caching and I wrote down some documentation that I think will be useful to others.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-6 01:00:00" id="5138" opendate="2012-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ref manual] Add a discussion on the number of regions</summary>
      <description>ntelford on IRC made the good point that we say people shouldn't have too many regions, but we don't say why. His problem currently is:09:21 &lt; ntelford&gt; problem is, if you're running MR jobs on a subset of that data, you need the regions to be as small as possible otherwise tasks don't get allocated in parallel much09:22 &lt; ntelford&gt; so we've found we have to strike a balance between keeping them small for MR and keeping them large for HBase to behave well09:22 &lt; ntelford&gt; we erred on the side of smaller regions because our MR issues were more immediate - we couldn't find any documentation or anecdotal evidence as to why HBase doesn't like lots of regionsThe three main issues I can think of when having too many regions are: mslab requires 2mb per memstore (that's 2mb per family per region). 1000 regions that have 2 families each is 3.9GB of heap used, and it's not even storing data yet. NB: the 2MB value is configurable. if you fill all the regions at somewhat the same rate, the global memory usage makes it that it forces tiny flushes when you have too many regions which in turn generates compactions. Rewriting the same data tens of times is the last thing you want. An example is filling 1000 regions (with one family) equally and let's consider a lower bound for global memstore usage of 5GB (the region server would have a big heap). Once it reaches 5GB it will force flush the biggest region, at that point they should almost all have about 5MB of data so it would flush that amount. 5MB inserted later, it would flush another region that will now have a bit over 5MB of data, and so on. the new master is allergic to tons of regions, and will take a lot of time assigning them and moving them around in batches. The reason is that it's heavy on ZK usage, and it's not very async at the moment (could really be improved).Another issue is the effect of the number of regions on mapreduce jobs. Keeping 5 regions per RS would be too low for a job, whereas 1000 will generate too many maps. This comes back to ntelford's problem of needing to scan portions of tables. To solve his problem, we discussed using a custom input format that generates many splits per region.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.configuration.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-4-21 01:00:00" id="5245" opendate="2012-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase shell should use alternate jruby if JRUBY_HOME is set, should pass along JRUBY_OPTS</summary>
      <description>Invoking hbase shell, the hbase runner launches the jruby jar directly, and so behaves differently than the traditional jruby runner. Specifically, it does not respect the JRUBY_OPTS environment variable (among other things, I cannot launch the shell to use ruby-1.9 mode) does not respect the JRUBY_HOME environment variable (placing things in an inconsistent state if my classpath holds the system jruby).This patch allows you to use an alternative jruby and to specify options to the jruby jar. When the command is 'shell', adds $JRUBY_OPTS to the CLASS When the command is 'shell' and $JRUBY_HOME is set, adds "$JRUBY_HOME/lib/jruby.jar" to the classpath, and sets -Djruby.home and -Djruby.job config variables.</description>
      <version>0.90.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-3-2 01:00:00" id="5508" opendate="2012-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to allow test output to show on the terminal</summary>
      <description>Sometimes it is useful to directly see the test results on the terminal.We can add a property to achieve that.mvn test -Dtest.output.tofile=false</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-14 01:00:00" id="5581" opendate="2012-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Creating a table with invalid syntax does not give an error message when it fails</summary>
      <description>Creating a table with invalid syntax does not give an error message when it fails. In this case, it doesn't actually create the CF requested, but doesn't give any indication to the user that it failed.create 'test', {NAME =&gt; 'test', VERSIONS =&gt; 1, BLOCKCACHE =&gt; true, NUMREGIONS =&gt; 20, SPLITALGO =&gt; "HexStringSplit", COMPRESSION =&gt; 'LZO', BLOOMFILTER =&gt; 'ROW'}0 row(s) in 3.0930 secondshbase(main):002:0&gt; describe 'test'DESCRIPTION ENABLED {NAME =&gt; 'test', FAMILIES =&gt; []} true 1 row(s) in 0.1430 secondsPutting {NUMREGIONS =&gt; 20, SPLITALGO =&gt; "HexStringSplit"} into a separate stanza works fine, so the feature is fine. create 'test', {NAME =&gt; 'test', VERSIONS =&gt; 1, BLOCKCACHE =&gt; true, COMPRESSION =&gt; 'LZO', BLOOMFILTER =&gt; 'ROW'}, {NUMREGIONS =&gt; 20, SPLITALGO =&gt; "HexStringSplit"}0 row(s) in 2.7860 secondshbase(main):002:0&gt; describe 'test'DESCRIPTION ENABLED {NAME =&gt; 'test', FAMILIES =&gt; [{NAME =&gt; 'test', DATA_BLOCK_ENCODING =&gt; 'NONE', true BLOOMFILTER =&gt; 'ROW', REPLICATION_SCOPE =&gt; '0', COMPRESSION =&gt; 'LZO', VERSIONS =&gt; '1', TTL =&gt; '2147483647', BLOCKSIZE =&gt; '65536', BLOOMFILTER_ERRORRATE =&gt; ' 0.01', ENCODE_ON_DISK =&gt; 'true', IN_MEMORY =&gt; 'false', BLOCKCACHE =&gt; 'true'}]} We should throw an error if we can't create the CF so it's clear to the user.</description>
      <version>None</version>
      <fixedVersion>0.94.0,0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.ruby.hbase.admin.rb</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-31 01:00:00" id="5691" opendate="2012-3-31 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Importtsv stops the webservice from which it is evoked</summary>
      <description>I was trying to run importtsv from a servlet. Everytime after the completion of job, the tomcat server was shutdown.</description>
      <version>0.90.4</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.docbkx.troubleshooting.xml</file>
      <file type="M">src.docbkx.developer.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-4-6 01:00:00" id="5744" opendate="2012-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift server metrics should be long instead of int</summary>
      <description>As we measure our Thrift call latencies in nanoseconds, we need to make latencies long instead of int everywhere.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.thrift.TestThriftServer.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.ThriftMetrics.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.thrift.HbaseHandlerMetricsProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-5-27 01:00:00" id="6113" opendate="2012-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[eclipse] Fix eclipse import of hbase-assembly null pointer</summary>
      <description>occasionally, eclipse will throw a null pointer when attempting to import all the modules via m2eclipse.</description>
      <version>None</version>
      <fixedVersion>0.95.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-10-6 01:00:00" id="6728" opendate="2012-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[89-fb] prevent OOM possibility due to per connection responseQueue being unbounded</summary>
      <description>The per connection responseQueue is an unbounded queue. The request handler threads today try to send the response in line, but if things start to backup, the response is sent via a per connection responder thread. This intermediate queue, because it has no bounds, can be another source of OOMs.&amp;#91;Have not looked at this issue in trunk. So it may or may not be applicable there.&amp;#93;</description>
      <version>None</version>
      <fixedVersion>0.94.3</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.util.TestSizeBasedThrottler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.util.SizeBasedThrottler.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.metrics.RegionServerDynamicMetrics.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegionServer.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.ipc.HBaseServer.java</file>
    </fixedFiles>
  </bug>
</bugrepository>