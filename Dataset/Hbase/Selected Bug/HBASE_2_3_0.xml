<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HBASE">
  
  <bug fixdate="2010-3-29 01:00:00" id="2174" opendate="2010-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stop from resolving HRegionServer addresses to names using DNS on every heartbeat</summary>
      <description>Over the time many parts of the code have evolved in different ways and one issue is that addresses are handled differently in different parts of the code. We need to set a standard and correct any inconsistencies.</description>
      <version>None</version>
      <fixedVersion>0.20.4,0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLog.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ServerManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessServerShutdown.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.HServerInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-18 01:00:00" id="21741" opendate="2019-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a note in "HFile Tool" section regarding &amp;#39;seqid=0&amp;#39;</summary>
      <description>In few parts of the HFile, where the seqid is irrelevant such as: firstKey=Optional&amp;#91;row0/cf:column/1547846312435/Put/seqid=0&amp;#93; lastKey=Optional&amp;#91;row9/cf:column/1547846312490/Put/seqid=0&amp;#93;Let's make a note on the doc in the 'HFile Tool' section, that seqid=0 in such cases means seqid is irrelevant here because it's a 'KeyOnlyKeyValue'.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-13 01:00:00" id="21889" opendate="2019-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use thrift 0.12.0 when build thrift by compile-thrift profile</summary>
      <description>Build command.mvn compile -Pcompile-thrift</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.0.5,2.3.0,2.1.4</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-25 01:00:00" id="22314" opendate="2019-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shaded byo-hadoop client should list needed hadoop modules as provided scope to avoid inclusion of unnecessary transitive depednencies</summary>
      <description>attempting to build against current hadoop trunk for HBASE-22087 shows that hte byo-hadoop client is trying to package transitive dependencies from the hadoop dependencies that we expressly say we don't need to bring with us.it's because we don't list those modules as provided, so all of their transitives are also in compile scope. The shading module does simple filtering when excluding things in a given scope, it doesn't e.g. make sure to also exclude the transitive dependencies of things it keeps out.since we don't want to list all the transitive dependencies of hadoop in our shading exclusion, we should list the needed hadoop modules as provided.</description>
      <version>2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,2.3.0,2.1.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-shaded.hbase-shaded-client-byo-hadoop.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-30 01:00:00" id="22345" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REST Server must have specific version of javax.annotations available at runtime</summary>
      <description>When compiled and run with JDK8, Rest server throws NoClassDefFoundError: javax/annotation/Priority Need to add in the correct dependency version or upgrade the appropriate rest component.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client.pom.xml</file>
      <file type="M">hbase-protocol-shaded.pom.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.hadoop-two-compat.xml</file>
      <file type="M">hbase-assembly.src.main.assembly.client.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-8 01:00:00" id="22384" opendate="2019-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Formatting issues in administration section of book</summary>
      <description>The administration section in the book (64.3.2. Administration) has some formatting issues. Due to that issues the list count is not accurate, as well as the indentation of some code snippets.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.2.0,1.4.10,2.3.0,2.0.6,2.1.5,1.3.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.security.adoc</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-10 01:00:00" id="22563" opendate="2019-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce retained jobs for Jenkins pipelines</summary>
      <description>Our jobs are taking up lots of space. Try to help out infra quickly by reducing the number of old builds we keep.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.2.0,2.3.0,2.0.6,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.Jenkinsfile</file>
      <file type="M">dev-support.flaky-tests.run-flaky-tests.Jenkinsfile</file>
      <file type="M">dev-support.flaky-tests.flaky-reporting.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-13 01:00:00" id="22689" opendate="2019-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Line break for fix version in documentation</summary>
      <description>The section describing the policy for the fix version in JIRA is missing line breaks.</description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-12 01:00:00" id="22838" opendate="2019-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>assembly:single failure: user id or group id &amp;#39;xxxxx&amp;#39; is too big</summary>
      <description> tarball build with assembly:single command fails with user id(mac) or group id(ubuntu) too big error:$ mvn clean install package assembly:single -DskipTests............[ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single (default-cli) on project hbase-assembly: Execution default-cli of goal org.apache.maven.plugins:maven-assembly-plugin:3.0.0:single failed: user id 'xxxxxxxx' is too big ( &gt; 2097151 ). -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException[ERROR][ERROR] After correcting the problems, you can resume the build with the command[ERROR]   mvn &lt;goals&gt; -rf :hbase-assemblyTo avoid this error and to get better features for tarball build, we should upgrade tarLongFileMode from gnu to posix: MPOM-132This works for assembly plugin &gt;= 2.5.0: MASSEMBLY-728 </description>
      <version>3.0.0-alpha-1,1.5.0,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.0.6,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-15 01:00:00" id="22863" opendate="2019-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid Jackson versions and dependencies with known CVEs</summary>
      <description>Partly forwardport from branch-1 Jira: HBASE-22728Even though master and branch-2 have moved away from Jackson1 some time back, HBase is still pulling in some vulnerable jackson dependencies (e.g. jackson-mapper-asl:1.9.13) from Hadoop: [INFO] --- maven-dependency-plugin:3.1.1:tree (default-cli) @ hbase-mapreduce ---[INFO] org.apache.hbase:hbase-mapreduce:jar:3.0.0-SNAPSHOT[INFO] +- org.apache.hbase:hbase-server:jar:3.0.0-SNAPSHOT:compile[INFO] | \- org.apache.hbase:hbase-http:jar:3.0.0-SNAPSHOT:compile[INFO] | \- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile[INFO] +- org.apache.hadoop:hadoop-mapreduce-client-jobclient:test-jar:tests:2.8.5:test[INFO] | \- org.apache.avro:avro:jar:1.7.7:compile[INFO] | \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile[INFO] \- org.apache.hadoop:hadoop-mapreduce-client-core:jar:2.8.5:compile[INFO] \- org.apache.hadoop:hadoop-yarn-common:jar:2.8.5:compile[INFO] +- org.codehaus.jackson:jackson-jaxrs:jar:1.9.13:compile[INFO] \- org.codehaus.jackson:jackson-xc:jar:1.9.13:compile[INFO] --- maven-dependency-plugin:3.1.1:tree (default-cli) @ hbase-shaded-testing-util ---[INFO] org.apache.hbase:hbase-shaded-testing-util:jar:3.0.0-SNAPSHOT[INFO] \- org.apache.hadoop:hadoop-common:test-jar:tests:2.8.5:compile[INFO] +- com.sun.jersey:jersey-json:jar:1.9:compile[INFO] | +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.3:compile[INFO] | \- org.codehaus.jackson:jackson-xc:jar:1.8.3:compile[INFO] +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile[INFO] \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile[INFO] org.apache.hbase:hbase-shaded-testing-util-tester:jar:3.0.0-SNAPSHOT[INFO] \- org.apache.hbase:hbase-shaded-testing-util:jar:3.0.0-SNAPSHOT:test[INFO] \- org.apache.hadoop:hadoop-common:test-jar:tests:2.8.5:test[INFO] +- com.sun.jersey:jersey-json:jar:1.9:test[INFO] | +- org.codehaus.jackson:jackson-jaxrs:jar:1.8.3:test[INFO] | \- org.codehaus.jackson:jackson-xc:jar:1.8.3:test[INFO] +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile[INFO] \- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compileJackson1 is not being used in HBase code anymore and hence, we should include it only at test scope if required by Hadoop but definitely exclude it from corresponding Hadoop dependencies. </description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-testing-util.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-testing-util-tester.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-mapreduce.pom.xml</file>
      <file type="M">hbase-shaded.hbase-shaded-client-byo-hadoop.pom.xml</file>
      <file type="M">hbase-server.pom.xml</file>
      <file type="M">hbase-mapreduce.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-24 01:00:00" id="22911" opendate="2019-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fewer concurrent github PR builds</summary>
      <description>we've been regularly getting 4-5 concurrent builds of PRs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11,2.0.7</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-24 01:00:00" id="22913" opendate="2019-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Hadoop label for nightly builds</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,2.2.1,2.1.6,1.3.6,1.4.11</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-30 01:00:00" id="22954" opendate="2019-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Whitelist net.java.dev.jna which got pulled in through Hadoop 3.3.0</summary>
      <description>YARN-9477 added a new dependency net.java.dev.jna, which resulted in license check failure in HBase because the checker thinks it's LGPL 2.1 licensed. But in fact, it is dual licensed. &lt;name&gt;Java Native Access&lt;/name&gt; &lt;description&gt;Java Native Access&lt;/description&gt; &lt;url&gt;https://github.com/java-native-access/jna&lt;/url&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;LGPL, version 2.1&lt;/name&gt; &lt;url&gt;http://www.gnu.org/licenses/licenses.html&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt; &lt;license&gt; &lt;name&gt;Apache License v2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt; &lt;/licenses&gt;We can(1) white list this dependency(2) or update the license checker to search for if any of the licenses is permitted</description>
      <version>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.1,2.1.7</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-17 01:00:00" id="23035" opendate="2019-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retain region to the last RegionServer make the failover slower</summary>
      <description>Now if one RS crashed, the regions will try to use the old location for the region deploy. But one RS only have 3 threads to open region by default. If a RS have hundreds of regions, the failover is very slower. Assign to same RS may have good locality if the Datanode is deploied on same host. But slower failover make the availability worse. And the locality is not big deal when deploy HBase on cloud.This was introduced by HBASE-18946.</description>
      <version>3.0.0-alpha-1,2.3.0,2.2.1,2.1.6</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestRegionReplicasWithRestartScenarios.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRetainAssignmentOnRestartSplitWithoutZk.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestRetainAssignmentOnRestart.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.TestMasterOperationsForRegionReplicas.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.master.procedure.TestSCPBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.procedure.ServerCrashProcedure.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.assignment.TransitRegionStateProcedure.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-18 01:00:00" id="23041" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should not show split parent regions in HBCK report&amp;#39;s unknown server part</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.HbckChore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.CatalogJanitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-18 01:00:00" id="23043" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestWALEntryStream times out</summary>
      <description>TestWALEntryStream#testDifferentCounts times out almost every time (90%+).On my machine the test runs in 9,5 minutes but on ASF infra it reaches the 720s timeout.</description>
      <version>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.7,2.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="23093" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid Optional Anti-Pattern where possible</summary>
      <description>Optional should be used as a return type only. It's a neat solution for handling data that might  not be present. We should avoid using Optional Anti-Patterns i.e. using it as a field or parameter type due to these reasons:1. Using Optional parameters causing conditional logic inside the methods is not productive.2. Packing an argument in an Optional is suboptimal for the compiler and does an unnecessary wrapping.3. Optional field is not serializable.</description>
      <version>3.0.0-alpha-1,2.3.0,1.6.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.2,2.1.8</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.regionserver.TestStoreScanner.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi3.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncTableAdminApi.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.client.TestAsyncRegionAdminApi2.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.replication.regionserver.RegionReplicaReplicationEndpoint.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.wal.FSWALEntry.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.StoreScanner.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.MetricsRegionServerWrapperImpl.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
      <file type="M">hbase-examples.src.main.java.org.apache.hadoop.hbase.client.example.HttpProxyExample.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.RawAsyncHBaseAdmin.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncTableRegionLocatorImpl.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncRegionLocatorHelper.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncNonMetaRegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.client.AsyncMetaRegionLocator.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.AsyncMetaTableAccessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="23094" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong log message in simpleRegionNormaliser while checking if merge is enabled.</summary>
      <description>In the following log message :LOG.debug("Unable to determine whether split is enabled", e);it should be "Unable to determine whether merge is enabled" while checking if merge is enabled. It can lead to confusion while debugging through logs.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,1.5.0,2.3.0,1.3.6,1.4.11,2.2.2,2.1.8</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.master.normalizer.SimpleRegionNormalizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-1 01:00:00" id="23106" opendate="2019-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WAL tools doc cleanup; talk of WAL Reader/Verifier; link WALPlayer</summary>
      <description>We had a WALPlayer that loads edits up into hbase cluster but what I wanted yesterday was a WAL verifier so I could find the bad WAL messing me up.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.ops.mgt.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="23156" opendate="2019-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>start-hbase.sh failed with ClassNotFoundException when build with hadoop3</summary>
      <description>Exception in thread "main" java.lang.NoClassDefFoundError: com/ctc/wstx/io/InputBootstrapperException in thread "main" java.lang.NoClassDefFoundError: com/ctc/wstx/io/InputBootstrapper at org.apache.hadoop.hbase.util.HBaseConfTool.main(HBaseConfTool.java:39)Caused by: java.lang.ClassNotFoundException: com.ctc.wstx.io.InputBootstrapper at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 1 moreException in thread "main" java.lang.NoClassDefFoundError: com/ctc/wstx/io/InputBootstrapper at org.apache.hadoop.hbase.zookeeper.ZKServerTool.main(ZKServerTool.java:63)Caused by: java.lang.ClassNotFoundException: com.ctc.wstx.io.InputBootstrapper at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 1 more</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-15 01:00:00" id="2328" opendate="2010-3-15 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Make important configurations more obvious to new users</summary>
      <description>Over the last 2 weeks, I encountered many situations where people didn't set file descriptors and xcievers higher and that was causing a ton of problems that are hard to debug if you're not used to them. To improve that we should: Refuse to start HBase if ulimit -n returns some small number smaller than 2048, or at least print out in big red blinking letters that the current configuration is bad and then link to a simple troubleshooting entry on the wiki. Write a clearer Getting Started document where we don't give as much explanations but add more stuff like "this is what your hbase-site.xml/hdfs-site/xml should look like now" and give a complete file example. At this point we don't even give a number for xcievers and we expect new users to come up with one.Any other low hanging fruit others can think of?</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.javadoc.overview.html</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-13 01:00:00" id="23283" opendate="2019-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide clear and consistent logging about the period of enabled chores</summary>
      <description>Similar to HBASE-23038, we should always log info about our enabled chores. Right now wether or not we get some information is up to particular Chore constructors and by and large we don't get any log messages when things can get started, even if the period is something impossibly long (e.g. 3000 days).When we go to schedule the chore here: if (chore.getPeriod() &lt;= 0) { LOG.info("The period is {} seconds, {} is disabled", chore.getPeriod(), chore.getName()); return false; }we should add an else clause that says it's enabled. It looks like we could then just call chore.toString to get the proper details about the chore and its period.</description>
      <version>3.0.0-alpha-1,2.3.0,1.7.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,2.1.8,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.main.java.org.apache.hadoop.hbase.ChoreService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-11-23 01:00:00" id="23334" opendate="2019-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The table-lock node of zk is not needed since HBASE-16786</summary>
      <description>The table-lock znode still be created when init,and it may cause confusion.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.3</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-zookeeper.src.main.java.org.apache.hadoop.hbase.zookeeper.ZKWatcher.java</file>
      <file type="M">hbase-client.src.test.java.org.apache.hadoop.hbase.zookeeper.TestZNodePaths.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.zookeeper.ZNodePaths.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-1-8 01:00:00" id="23664" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade JUnit to 4.13</summary>
      <description>New JUnit released a week ago. Let's give it a spin.https://github.com/junit-team/junit4/blob/master/doc/ReleaseNotes4.13.md</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-10 01:00:00" id="23675" opendate="2020-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move to Apache parent POM version 22</summary>
      <description>Apache parent POM version 22 was released on 2020/01/09.</description>
      <version>3.0.0-alpha-1,2.3.0,1.6.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-build-configuration.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-14 01:00:00" id="23688" opendate="2020-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update docs for setting up IntelliJ as a development environment</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-15 01:00:00" id="23689" opendate="2020-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bookmark for github PR to jira redirection</summary>
      <description>Following is a simple js snippet that redirects from any HBase PR to its corresponding jira. Without this, one has to copy the jira ID from the PR, construct a jira URL manually and paste it in the browser URL bar. Saves a bunch of clicks.javascript:location.href='https://issues.apache.org/jira/browse/'document.getElementsByClassName("js-issue-title")[0].innerHTML.match(/HBASE-\d/)[0];Particularly helpful for reviewers who'd like to read the jira contents often when reviewing a PR.For chrome: Right Click on the bookmarks bar Click on Add page. Fill in the following details: Name: HBase jira redirect (or any other that you prefer) URL: – snippet from above-- Click SaveNow you should see "HBase jira redirect" (or any other name you gave) bookmark on the bar. Go to any Github PR, click on this button and it redirects to the corresponding jira.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-15 01:00:00" id="23690" opendate="2020-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkstyle plugin complains about our checkstyle.xml format; doc how to resolve mismatched version</summary>
      <description>Trying to add the checkstyle.xml to the intellij checkstyle plugin after reading HBASE-23688, it complains with the following when it reads in the config file:com.puppycrawl.tools.checkstyle.api.CheckstyleException: cannot initialize module TreeWalker - TreeWalker is not allowed as a parent of LineLength Please review 'Parent Module' section for this Check in web documentation if Check is standard. at com.puppycrawl.tools.checkstyle.Checker.setupChild(Checker.java:473) at com.puppycrawl.tools.checkstyle.api.AutomaticBean.configure(AutomaticBean.java:198) at org.infernus.idea.checkstyle.service.cmd.OpCreateChecker.execute(OpCreateChecker.java:61) at org.infernus.idea.checkstyle.service.cmd.OpCreateChecker.execute(OpCreateChecker.java:26) at org.infernus.idea.checkstyle.service.CheckstyleActionsImpl.executeCommand(CheckstyleActionsImpl.java:130) at org.infernus.idea.checkstyle.service.CheckstyleActionsImpl.createChecker(CheckstyleActionsImpl.java:60) at org.infernus.idea.checkstyle.service.CheckstyleActionsImpl.createChecker(CheckstyleActionsImpl.java:51) at org.infernus.idea.checkstyle.checker.CheckerFactoryWorker.run(CheckerFactoryWorker.java:46)Caused by: com.puppycrawl.tools.checkstyle.api.CheckstyleException: TreeWalker is not allowed as a parent of LineLength Please review 'Parent Module' section for this Check in web documentation if Check is standard. at com.puppycrawl.tools.checkstyle.TreeWalker.setupChild(TreeWalker.java:147) at com.puppycrawl.tools.checkstyle.api.AutomaticBean.configure(AutomaticBean.java:198) at com.puppycrawl.tools.checkstyle.Checker.setupChild(Checker.java:468) ... 7 more</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-15 01:00:00" id="23691" opendate="2020-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.2.3 to download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-15 01:00:00" id="23697" opendate="2020-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document new RegionProcedureStore operation and migration</summary>
      <description>Add a few notes to the refguide on the new RegionProcedureStore, how it works (A 'Region' but buried in the Master with dedicated flushing/compacting threads and archivers for WAL and hfile), how it differs from WALPS, and note it auto-migrates and there should be new issue moving on to the new store.Mention the configuration. Mention it is on WALFS even though it is a 'Region', etc.</description>
      <version>2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-19 01:00:00" id="23709" opendate="2020-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unwrap the real user to properly dispatch proxy-user auth&amp;#39;n</summary>
      <description>Currently TestSecureRESTServer fails consistently on branch-2 and should be fixed.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.provider.BuiltInProviderSelector.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-27 01:00:00" id="23741" opendate="2020-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data loss when WAL split to HFile enabled</summary>
      <description>Very simple steps as below,1. Create table with 1 region2. Insert 1 record 3. Flush the table 4. Scan table and observe timestamp of the inserted row5. Insert same row key with same timestamp as previously inserted but with different value6. Kill -9 RS where table region is online7. Start RSScan the table and check the result, latest cell must be returned.Thanks sreenivasulureddy for finding this issue.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.wal.TestWALSplitToHFile.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.WALSplitUtil.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.wal.BoundedRecoveredHFilesOutputSink.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HStore.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.regionserver.HRegion.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-28 01:00:00" id="23755" opendate="2020-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[OpenTracing] Declare HTrace is unusable in the user doc</summary>
      <description>The trace doesn't work at all in HBase 2.0 and above after HBASE-18601 (the trace doesn't get picked up at the server side). We should make a note in the user doc stating it is unusable deprecated in HBase 2.x because HTrace is in Attic. removed from HBase 3.0 and above.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.upgrading.adoc</file>
      <file type="M">src.main.asciidoc..chapters.tracing.adoc</file>
      <file type="M">src.main.asciidoc.book.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-4 01:00:00" id="23793" opendate="2020-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase maven heap allocation to 4G in Yetus personality</summary>
      <description>I saw this over on https://builds.apache.org/view/H-L/view/HBase/job/HBase%20Nightly/job/branch-2/2447/console. Looks like we need to bump the memory allocation for maven. I wonder if this is the underlying cause of HBASE-22470. 6:38:47 ============================================================================16:38:47 ============================================================================16:38:47 Finished build.16:38:47 ============================================================================16:38:47 ============================================================================16:38:47 16:38:47 Post stage[Pipeline] stash16:38:48 Warning: overwriting stash 'hadoop2-result'16:38:48 Stashed 1 file(s)[Pipeline] junit16:38:48 Recording test results16:38:54 Remote call on H2 failedError when executing always post condition:java.io.IOException: Remote call on H2 failed at hudson.remoting.Channel.call(Channel.java:963) at hudson.FilePath.act(FilePath.java:1072) at hudson.FilePath.act(FilePath.java:1061) at hudson.tasks.junit.JUnitParser.parseResult(JUnitParser.java:114) at hudson.tasks.junit.JUnitResultArchiver.parse(JUnitResultArchiver.java:137) at hudson.tasks.junit.JUnitResultArchiver.parseAndAttach(JUnitResultArchiver.java:167) at hudson.tasks.junit.pipeline.JUnitResultsStepExecution.run(JUnitResultsStepExecution.java:52) at hudson.tasks.junit.pipeline.JUnitResultsStepExecution.run(JUnitResultsStepExecution.java:25) at org.jenkinsci.plugins.workflow.steps.SynchronousNonBlockingStepExecution.lambda$start$0(SynchronousNonBlockingStepExecution.java:47) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.OutOfMemoryError: Java heap space at com.sun.org.apache.xerces.internal.util.XMLStringBuffer.append(XMLStringBuffer.java:208) at com.sun.org.apache.xerces.internal.impl.XMLEntityScanner.scanData(XMLEntityScanner.java:1515) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanCDATASection(XMLDocumentFragmentScannerImpl.java:1654) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3014) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:602) at com.sun.org.apache.xerces.internal.impl.XMLNSDocumentScannerImpl.next(XMLNSDocumentScannerImpl.java:112) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:505) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:842) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:771) at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141) at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213) at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:643) at org.dom4j.io.SAXReader.read(SAXReader.java:465) at org.dom4j.io.SAXReader.read(SAXReader.java:343) at hudson.tasks.junit.SuiteResult.parse(SuiteResult.java:178) at hudson.tasks.junit.TestResult.parse(TestResult.java:348) at hudson.tasks.junit.TestResult.parsePossiblyEmpty(TestResult.java:281) at hudson.tasks.junit.TestResult.parse(TestResult.java:206) at hudson.tasks.junit.TestResult.parse(TestResult.java:178) at hudson.tasks.junit.TestResult.&lt;init&gt;(TestResult.java:143) at hudson.tasks.junit.JUnitParser$ParseResultCallable.invoke(JUnitParser.java:146) at hudson.tasks.junit.JUnitParser$ParseResultCallable.invoke(JUnitParser.java:118) at hudson.FilePath$FileCallableWrapper.call(FilePath.java:3052) at hudson.remoting.UserRequest.perform(UserRequest.java:212) at hudson.remoting.UserRequest.perform(UserRequest.java:54) at hudson.remoting.Request$2.run(Request.java:369) at hudson.remoting.InterceptingExecutorService$1.call(InterceptingExecutorService.java:72) ... 4 more[Pipeline] }[Pipeline] // withEnv[Pipeline] }[Pipeline] // node[Pipeline] }[Pipeline] // stage[Pipeline] }16:38:54 Failed in branch yetus jdk8 hadoop2 checks</description>
      <version>2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.6.0,1.3.7,2.1.9,1.4.13,2.2.4</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-5 01:00:00" id="23800" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add documentation about the CECPs changes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.protobuf.adoc</file>
      <file type="M">src.main.asciidoc..chapters.cp.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-5 01:00:00" id="23803" opendate="2020-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[DOC] Fix the maths on the section explaining call queue tuning options</summary>
      <description>Section "121.11. Tuning callQueue Options" from the RefGuide has some mismatches when explaining the percentages applied by different values set to hbase.ipc.server.callqueue.read.ratio and hbase.ipc.server.callqueue.scan.ratio, respectively, such as:A value of .3 uses 30% of the queues for reading and 60% for writing. Given a value of 10 for hbase.ipc.server.num.callqueue, 3 queues would be used for reads and 7 for writes.Should be:A value of .3 uses 30% of the queues for reading and 70% for writing. Given a value of 10 for hbase.ipc.server.num.callqueue, 3 queues would be used for reads and 7 for writes.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.performance.adoc</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-16 01:00:00" id="23855" opendate="2020-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change bytes size to human readable size for Server Metrics of RegionServer Web UI</summary>
      <description>I found that the “BytesBufferAllocator Status” in RegionServer Web UI still using "Bytes" as a fixture unit. I think we should use "MB" or "GB" when the size is too large  The Web UI after improvement:</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-17 01:00:00" id="23857" opendate="2020-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.9 to download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-17 01:00:00" id="23859" opendate="2020-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify "Block locality" of RegionServer Web UI to human readable percentage</summary>
      <description>The unit of "Block locality" in Web UI just like picture 1I think we should change it to percentage unit like picture 2 </description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.4,2.1.10</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.main.jamon.org.apache.hadoop.hbase.tmpl.regionserver.ServerMetricsTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2010-6-1 01:00:00" id="2400" opendate="2010-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>new connector for Avro RPC access to HBase cluster</summary>
      <description>Build a new connector contrib architecturally equivalent to the Thrift connector, but using Avro serialization and associated transport and RPC server work. Support AAA (audit, authentication, authorization).</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="24002" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shadedjars check does not propagate --hadoop-profile</summary>
      <description>After HBASE-23829, we see the shadedjars check fail on JDK11 stages. From the logTue Mar 17 00:14:24 UTC 2020cd /home/jenkins/jenkins-slave/workspace/Base-PreCommit-GitHub-PR_PR-1296/yetus-jdk11-hadoop3-check/src/opt/maven/bin/mvn --batch-mode -Dmaven.repo.local=/home/jenkins/jenkins-slave/workspace/Base-PreCommit-GitHub-PR_PR-1296/yetus-m2/hbase-branch-2-patch-1 clean verify -fae --batch-mode -pl hbase-shaded/hbase-shaded-check-invariants -am -Dtest=NoUnitTests -DHBasePatchProcess -Prelease -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true -Dspotbugs.skip=true...[INFO] --- maven-enforcer-plugin:3.0.0-M2:enforce (hadoop3-profile-required) @ hbase ---[INFO] Adding ignore: module-info[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireProperty failed with message:HBase with JDK11 requires Hadoop3. Activate the profile with `-Dhadoop.profile=3.0`.</description>
      <version>3.0.0-alpha-1,2.3.0,2.4.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hbase-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="24004" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include hadoop version in Nightly report name</summary>
      <description>A minor thing I missed in HBASE-23876. Have this report name match the other "JDKX, HadoopY" report names.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="24007" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Get `-PrunLargeTests` passing on JDK11</summary>
      <description>Build on HBASE-23829 and HBASE-24006, now looking at large tests.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-18 01:00:00" id="24016" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change nightly poll from cron @daily to pollSCM @daily; i.e. run nightly if a change ONLY</summary>
      <description>Change build on branch-1.3, 1.4, 2.1, and feature branches HBASE-23162-branch-1 and HBASE-22114-branch-1 to be pollSCM @daily &amp;#8211; i.e. poll once a day and if change run nightly &amp;#8211; rather than build every night regardless.See https://lists.apache.org/thread.html/r5dca2cacc123f2e5719c622add6853ac62b56b2a77885fe0b2eb53c3%40%3Cdev.hbase.apache.org%3E for dev list discussion on downing our nightly load.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.3.7,1.7.0,2.1.10,1.4.14,2.2.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-3 01:00:00" id="2406" opendate="2010-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define semantics of cell timestamps/versions</summary>
      <description>There is a lot of general confusion over the semantics of the cell timestamp. In particular, a couple questions that often come up: If multiple writes to a cell have the same timestamp, are all versions maintained or just the last? Is it OK to write cells in a non-increasing timestamp order?Let's discuss, figure out what semantics make sense, and then move towards (a) documentation, (b) unit tests that prove we have those semantics.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.site.xml</file>
      <file type="M">src.docbkx.sample.article.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">src.docbkx.book.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-26 01:00:00" id="24062" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add 2.1.10 to download page</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-27 01:00:00" id="24071" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JDK11] Remove `unit` filter from nightly and precommit jobs</summary>
      <description>Added in HBASE-23946, we can remove this filter once HBASE-24007 lands.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile.GitHub</file>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-31 01:00:00" id="24092" opendate="2020-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix links to build reports generated by nightly job</summary>
      <description>Links going back to JIRA look likeFor more information [see jdk8 (hadoop2) report|https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2574//JDK8_Nightly_Build_Report_(Hadoop2)/]But the actual URL to this report is https://builds.apache.org/job/HBase%20Nightly/job/branch-2/2574/JDK8_20Nightly_20Build_20Report_20_28Hadoop2_29/</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.Jenkinsfile</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-3 01:00:00" id="24113" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade the maven we use from 3.5.4 to 3.6.3 in nightlies</summary>
      <description>I want to up parallelism of nightlies and hopefully improve stability. Lets use latest maven, go from 3.5.4 to 3.6.3.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">dev-support.hbase.docker.Dockerfile</file>
      <file type="M">dev-support.docker.Dockerfile</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-4-6 01:00:00" id="24122" opendate="2020-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change machine ulimit-l to ulimit-a so dumps full ulimit rather than just &amp;#39;max locked memory&amp;#39;</summary>
      <description>Dump out full ulimit list under the machine dir job output rather than one-liner. More utility.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.gather.machine.environment.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-8 01:00:00" id="24143" opendate="2020-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[JDK11] Switch default garbage collector from CMS</summary>
      <description>When running HBase tools on the cli, one of the warnings generated isOpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.Java9+ use G1GC as the default collector. Maybe we simply omit GC configurations and use the default settings? Or someone has some suggested settings we can ship out of the box?</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hbase-env.sh</file>
      <file type="M">bin.hbase-config.sh</file>
      <file type="M">bin.hbase</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-14 01:00:00" id="24185" opendate="2020-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Junit tests do not behave well with System.exit or Runtime.halt or JVM exits in general.</summary>
      <description>This ends up exiting the JVM and confusing / erroring out the test runner that manages that JVM as well as cutting off test output files.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-common.src.test.java.org.apache.hadoop.hbase.HBaseClassTestRule.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-5-24 01:00:00" id="24258" opendate="2020-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hadoop3.3] Update license for org.ow2.asm:*</summary>
      <description>Hadoop 3.3 brings a few Jetty dependencies which transitively brings in org.ow2.asm:asm-analysis, org.ow2.asm:asm-commons, org.ow2.asm:asm-tree.When testing with the latest Jetty (9.4.26.v20200117) I found its org.ow2.asm:* updated from 7.1 to 7.2, which changed the declared license from "BSD" to "BSD-3-Clause License" (The actual license text did not change). The HBase's license checker doesn't accept it.File the jira to update it to "BSD 3-Clause License" so that HBase can build.[INFO] | | | +- org.eclipse.jetty.websocket:javax-websocket-server-impl:jar:9.4.26.v20200117:test[INFO] | | | | +- org.eclipse.jetty:jetty-annotations:jar:9.4.26.v20200117:test[INFO] | | | | | +- org.eclipse.jetty:jetty-plus:jar:9.4.26.v20200117:test[INFO] | | | | | | \- org.eclipse.jetty:jetty-jndi:jar:9.4.26.v20200117:test[INFO] | | | | | \- org.ow2.asm:asm-commons:jar:7.2:test[INFO] | | | | | +- org.ow2.asm:asm-tree:jar:7.2:test[INFO] | | | | | \- org.ow2.asm:asm-analysis:jar:7.2:testThis product includes asm-analysis licensed under the BSD-3-Clause.ERROR: Please check ^^^^^^^^^^^^ this License for acceptability here:https://www.apache.org/legal/resolvedIf it is okay, then update the list named 'non_aggregate_fine' in the LICENSE.vm file.If it isn't okay, then revert the change that added the dependency.More info on the dependency:&lt;groupId&gt;org.ow2.asm&lt;/groupId&gt;&lt;artifactId&gt;asm-analysis&lt;/artifactId&gt;&lt;version&gt;7.2&lt;/version&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.2.5</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-resource-bundle.src.main.resources.supplemental-models.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-24 01:00:00" id="24261" opendate="2020-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redo all of our github notification integrations on new ASF infra feature</summary>
      <description>The new ASF Infra feature for customizing how project gets notifications from github appears to have silently thrown away all the integration we already had set up.I don't know that full set of things we need. We presumably need to do this for all of our repos. make sure all notifications on PRs is going to issues@ make sure we get links on JIRA for related PRs make sure we do not get updates on JIRA for every PR comment</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.asf.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-26 01:00:00" id="24263" opendate="2020-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestDelegationToken is broken</summary>
      <description>After reverting HBASE-23881, it is fine.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.NettyHBaseSaslRpcClientHandler.java</file>
      <file type="M">hbase-client.src.main.java.org.apache.hadoop.hbase.security.AbstractHBaseSaslRpcClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-26 01:00:00" id="24264" opendate="2020-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable TestNettyIPC.testHedgedAsyncEcho</summary>
      <description>It is flaky and cause the flaky job time out.Disable it for now, and re-enable it in the sub task.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.ipc.AbstractTestIPC.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-27 01:00:00" id="24266" opendate="2020-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document of Hbase on Aarch64</summary>
      <description>Provide documentation on how to run hbase on aarch64 architecture.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.developer.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-1 01:00:00" id="24296" opendate="2020-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>install yetus as a part of building the rm docker image.</summary>
      <description>right now we have to download yetus on each release run. we should be able to point at a local install instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.release-util.sh</file>
      <file type="M">dev-support.create-release.release-build.sh</file>
      <file type="M">dev-support.create-release.hbase-rm.Dockerfile</file>
      <file type="M">dev-support.create-release.do-release.sh</file>
      <file type="M">dev-support.create-release.do-release-docker.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-1 01:00:00" id="24297" opendate="2020-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>release scripts should be able to use a custom git repo</summary>
      <description>doing a full new clone is expensive, especially for the main repo. we should be able to optionally point at an existing clone.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.release-util.sh</file>
      <file type="M">dev-support.create-release.release-build.sh</file>
      <file type="M">dev-support.create-release.do-release-docker.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-1 01:00:00" id="24303" opendate="2020-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Undo core of parent TestSecureRESTServer change; use fix over in HBASE-24280 instead</summary>
      <description>Parent issue puts in a place that makes TestSecureRESTServer pass up on jenkins by shoving into the dependency list the jersey1 ServletContainer. Root issue was change in how we specified profiles in nightlies; both hadoop3 and hadoop2 were mistakenly active (HBASE-24280). This issue is about undoing the dependency insertion after fix for HBASE-24280 goes in.Don't want to revert the parent. It has cleanups that should stay.</description>
      <version>3.0.0-alpha-1,2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-rest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-8 01:00:00" id="24343" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Document how to configure the http request log</summary>
      <description/>
      <version>2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-8 01:00:00" id="24345" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[ACL] renameRSGroup should require Admin level permission</summary>
      <description>Currently renameRSgroup can be called by anyone without permission</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,1.7.0,2.2.5</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsWithACL.java</file>
      <file type="M">hbase-server.src.test.java.org.apache.hadoop.hbase.rsgroup.TestRSGroupsBase.java</file>
      <file type="M">hbase-server.src.main.java.org.apache.hadoop.hbase.security.access.AccessController.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-4-14 01:00:00" id="2448" opendate="2010-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scanner threads are interrupted without acquiring lock properly</summary>
      <description>There are a few places where scanner threads are interrupted with .interrupt() instead of .interruptIfAlive(). This means that if they're in the midst of the checkFileSystem operation, it'll end up catching the interruption there, determine that the filesystem is down, and shut down the whole server. Other nasties can also result.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.util.Sleeper.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.RegionManager.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ProcessRegionOpen.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.ModifyTableMeta.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.master.BaseScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.Chore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-4-15 01:00:00" id="2453" opendate="2010-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revisit compaction policies after HBASE-2248 commit</summary>
      <description>HBASE-2248 turned Gets into Scans server-side. It also removed the invariant that deletes in a file only apply to other files and not itself (no longer processes MemStore deletes when the delete happens). This has implications for our minor compaction policy.We are currently processing deletes during minor compactions in a way that makes it so we do the actual deleting as we compact, but we retain the delete records themselves. This makes it so we retain the invariant of deletes only applying to other files.Since this is now gone post HBASE-2248, we should revisit our compaction policies.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">core.src.test.java.org.apache.hadoop.hbase.regionserver.TestMinorCompactingStoreScanner.java</file>
      <file type="M">core.src.main.java.org.apache.hadoop.hbase.regionserver.MinorCompactingStoreScanner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-10 01:00:00" id="24535" opendate="2020-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tweak the master registry docs for branch-2</summary>
      <description>As Nick pointed out in https://github.com/apache/hbase/pull/1880, we need to re-word the content a bit so that it makes sense to branch-2 users. Specifically the following What version has the feature shipped A slight modification to the config keys after hedging moved from rpc layer to the registry code (HBASE-24265)The latter applies to the master branch too.</description>
      <version>2.3.0</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.main.asciidoc..chapters.architecture.adoc</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-12 01:00:00" id="24547" opendate="2020-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift support for HBASE-23941</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-thrift.src.test.java.org.apache.hadoop.hbase.thrift2.TestThriftHBaseServiceHandler.java</file>
      <file type="M">hbase-thrift.src.main.resources.org.apache.hadoop.hbase.thrift2.hbase.thrift</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.ThriftUtilities.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTimeRange.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TThriftServerType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TTableDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TServerName.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TScan.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TRowMutations.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TResult.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TReadType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TPut.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TOnlineLogRecord.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TNamespaceDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TMutation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TLogType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TLogQueryFilter.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TKeepDeletedCells.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIOError.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TIllegalArgument.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionLocation.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THRegionInfo.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.THBaseService.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TGet.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDurability.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDeleteType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDelete.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TDataBlockEncoding.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TConsistency.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompressionAlgorithm.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCompareOperator.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnValue.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnIncrement.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumnFamilyDescriptor.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TColumn.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TCellVisibility.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TBloomFilterType.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAuthorization.java</file>
      <file type="M">hbase-thrift.src.main.java.org.apache.hadoop.hbase.thrift2.generated.TAppend.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-15 01:00:00" id="24567" opendate="2020-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create release should url-encode all characters when building git uri</summary>
      <description>The release tool doesn't url encode all characters provided for ASF_USERNAME, ASF_PASSWORD.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.create-release.release-util.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-21 01:00:00" id="24604" opendate="2020-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the stable-1 notice on our download page</summary>
      <description>We have already removed it from our dist release directory.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.site.xdoc.downloads.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-24 01:00:00" id="24630" opendate="2020-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Purge dev javadoc from client bin tarball</summary>
      <description>For 2.0, the decision was made to exclude the bulky "developer" api docs from the binary artifacts, via HBASE-20149. This change needs applied to the client tarball as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0-alpha-1,2.3.0,2.1.10,2.2.6</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-assembly.src.main.assembly.client-components.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-20 01:00:00" id="2471" opendate="2010-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Splitting logs, we&amp;#39;ll make an output file though the region no longer exists</summary>
      <description>The "human unit tester" (Kannan) last night wondered what happens splitting logs and we come across an edit whose region has since been removed. Taking a look, it looks like we'll create the output file and write the edits for the no-longer-extant region anyways. This will leave litter in the filesystem &amp;#8211; region split files that will never be used nor removed. This issue is about verifying that indeed this is whats happening (We do SequenceFile.createWriter with the overwrite flag set to true which tracing seems to mean create all intermediary directories &amp;#8211; to be verified) and if it indeed is happening, fixing split so unless the region dir exists, don't write out edits.. just drop them.</description>
      <version>None</version>
      <fixedVersion>0.90.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestWALReplay.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.java</file>
      <file type="M">src.test.java.org.apache.hadoop.hbase.regionserver.wal.TestHLog.java</file>
      <file type="M">src.main.java.org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
</bugrepository>