<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-3-26 01:00:00" id="10104" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Generate consistent splits and locations for the same split across jobs</summary>
      <description>Locations for splits are currently randomized. Also, the order of splits is random - depending on how threads end up generating the splits.Add an option to sort the splits, and generate repeatable locations - assuming all other factors are the same.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-19 01:00:00" id="13316" opendate="2016-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Calcite 1.10</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.HiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTopNQueryRecordReader.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveDefaultCostModel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveRelMdCost.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidIntervalUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidQueryType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidRules.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.DruidTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.druid.HiveDruidConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRexUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveDateGranularity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterProjectTSTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePreFilteringRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdUniqueKeys.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBOMaxNumToCNF.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-2 01:00:00" id="13666" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Provide the log url for a task attempt to display on the UI</summary>
      <description>The log url needs to be provided for task attempts, to display on the Tez UI associated with a query.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-2 01:00:00" id="13669" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: io.enabled config is ignored on the server side</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-25 01:00:00" id="1367" opendate="2010-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cluster by multiple columns does not work if parenthesis is present</summary>
      <description>The following query:select ... from src cluster by (key, value)throws a compile error:whereas the queryselect ... from src cluster by key, valueworks fine</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-6-2 01:00:00" id="13927" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding missing header to Java files</summary>
      <description/>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-6 01:00:00" id="13954" opendate="2016-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet logs should go to STDERR</summary>
      <description>Parquet uses java util logging. When java logging is not configured using default logging.properties file, parquet's default fallback handler writes to STDOUT at INFO level. Hive writes all logging to STDERR and writes only the query output to STDOUT. Writing logs to STDOUT may cause issues when comparing query results. If we provide default logging.properties for parquet then we can configure it to write to file or stderr.</description>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.0,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-9 01:00:00" id="13982" opendate="2016-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extensions to RS dedup: execute with different column order and sorting direction if possible</summary>
      <description>Pointed out by gopalv.RS dedup should kick in for these cases, avoiding an additional shuffle stage.select state, city, sum(sales) from tablegroup by state, cityorder by state, citylimit 10;select state, city, sum(sales) from tablegroup by city, stateorder by state, citylimit 10;select state, city, sum(sales) from tablegroup by city, stateorder by state desc, citylimit 10;</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.LimitPushdownOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientpositive.correlationoptimizer13.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptfgroupbyjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join3.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-9 01:00:00" id="13986" opendate="2016-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: kill Tez AM on token errors from plugin</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-14 01:00:00" id="14013" opendate="2016-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table doesn&amp;#39;t show unicode properly</summary>
      <description>Describe table output will show comments incorrectly rather than the unicode itself.hive&gt; desc formatted t1;# Detailed Table Information Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\"} comment \u8868\u4E2D\u6587\u6D4B\u8BD5 numFiles 0</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-15 01:00:00" id="14018" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make IN clause row selectivity estimation customizable</summary>
      <description>After HIVE-13287 went in, we calculate IN clause estimates natively (instead of just dividing incoming number of rows by 2). However, as the distribution of values of the columns is considered uniform, we might end up heavily underestimating/overestimating the resulting number of rows.This issue is to add a factor that multiplies the IN clause estimation so we can alleviate this problem. The solution is not very elegant, but it is the best we can do until we have histograms to improve our estimate.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-15 01:00:00" id="14024" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>setAllColumns is called incorrectly after some changes</summary>
      <description>h/t gopalv</description>
      <version>None</version>
      <fixedVersion>2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-15 01:00:00" id="14027" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL values produced by left outer join do not behave as NULL</summary>
      <description>Consider the following setup:create table tbl (n bigint, t string); insert into tbl values (1, 'one'); insert into tbl values(2, 'two');select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a left outer join (select * from tbl where 1 = 2) b on a.n = b.n;1 one false trueThe query should return true for isnull(b.n).I've tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case.</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-18 01:00:00" id="14055" opendate="2016-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>directSql - getting the number of partitions is broken</summary>
      <description>Noticed while looking at something else. If the filter cannot be pushed down it just returns 0</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-18 01:00:00" id="14056" opendate="2016-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for few tests</summary>
      <description>Click to add description</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.globallimit.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-20 01:00:00" id="14060" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: Remove bogus "localhost" from Hive splits</summary>
      <description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".This is worse than having no location information at all - on large clusters yarn waits upto 200&amp;#91;1&amp;#93; seconds for heartbeat from "localhost" before allocating a container.To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.&amp;#91;1&amp;#93; - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-21 01:00:00" id="14069" opendate="2016-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update curator version to 2.12.0</summary>
      <description>curator-2.10.0 has several bug fixes over current version (2.6.0), updating would help improve stability.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="14078" opendate="2016-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP input split should get task attempt number from conf if available</summary>
      <description>Currently the attempt number is hard-coded to 0. If the split is being fetched as part of a hadoop job we can get the task attempt ID from the conf if it has been set, and use the attempt number from that.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-22 01:00:00" id="14079" opendate="2016-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove file, method and line number from pattern layout</summary>
      <description>Using %F%M and %L in pattern layouts need location information which is expensive to get and is disabled by default. We should remove them from the default layouts. This will avoid creating empty brackets like belowlockmgr.DbTxnManager (:())</description>
      <version>2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.hive-exec-log4j2.properties</file>
      <file type="M">llap-server.src.test.resources.log4j2.properties</file>
      <file type="M">llap-server.src.test.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
      <file type="M">llap-server.src.main.resources.llap-cli-log4j2.properties</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.config.hive.hive-log4j2.properties</file>
      <file type="M">data.conf.spark.log4j2.properties</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">common.src.test.resources.hive-log4j2-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j2-test.properties</file>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-7-28 01:00:00" id="14118" opendate="2016-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the alter partition exception more meaningful</summary>
      <description>Right now when the alter partitions fails, "alter is not possible" is shown in the log while the real exception/failure is hidden.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-28 01:00:00" id="14119" opendate="2016-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external recordreader not returning non-ascii string properly</summary>
      <description>Strings with non-ascii chars showing up with "\�\�\� "</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-7-1 01:00:00" id="14147" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive PPD might remove predicates when they are defined as a simple expr e.g. WHERE &amp;#39;a&amp;#39;</summary>
      <description>Click to add description</description>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="14148" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add branch-2.1 branch to pre-commit tests</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-submit-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-2 01:00:00" id="14155" opendate="2016-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Custom UDF Vectorization annotations are ignored</summary>
      <description>@VectorizedExpressions(value = { VectorStringRot13.class })in a custom UDF Is ignored because the check for annotations happens after custom UDF detection.The custom UDF codepath is on the fail-over track of annotation lookups, so the detection during validation of SEL is sufficient, instead of during expression creation.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-6 01:00:00" id="14177" opendate="2016-7-6 00:00:00" resolution="Not A Bug">
    <buginformation>
      <summary>AddPartitionEvent contains the table location, but not the partition location</summary>
      <description>AddPartitionEvent contains the table location, but not the partition location</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-8 01:00:00" id="14197" opendate="2016-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP service driver precondition failure should include the values</summary>
      <description>LLAP service driver's precondition failure message are like belowWorking memory + cache has to be smaller than the container sizingIt will be better to include the actual values for the sizes in the precondition failure message.NO PRECOMMIT TESTS</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-7-9 01:00:00" id="14200" opendate="2016-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: disable auto-reducer parallelism when reducer-count * min.partition.factor &lt; 1.0</summary>
      <description>The min/max factors offer no real improvement when the fractions are meaningless, for example when 0.25 * 2 is applied as the min.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-9 01:00:00" id="14202" opendate="2016-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tez version used to 0.8.4</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-6-22 01:00:00" id="1421" opendate="2010-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>problem with sequence and rcfiles are mixed for null partitions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-19 01:00:00" id="14278" opendate="2016-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate TestHadoop23SAuthBridge.java from Unit3 to Unit4</summary>
      <description>Migrate TestHadoop23SAuthBridge.java from unit3 to unit4</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.thrift.TestHadoopAuthBridge23.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-20 01:00:00" id="14299" opendate="2016-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log serialized plan size</summary>
      <description>It will be good to log the size of the serialized plan. This can help identifying cases where large objects are accidentally serialized.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-21 01:00:00" id="14302" opendate="2016-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez: Optimized Hashtable can support DECIMAL keys of same precision</summary>
      <description>Decimal support in the optimized hashtable was decided on the basis of the fact that Decimal(10,1) == Decimal(10, 2) when both contain "1.0" and "1.00".However, the joins now don't have any issues with decimal precision because they cast to common.create temporary table x (a decimal(10,2), b decimal(10,1)) stored as orc;insert into x values (1.0, 1.0); &gt; explain logical select count(1) from x, x x1 where x.a = x1.b;OK LOGICAL PLAN:$hdt$_0:$hdt$_0:x TableScan (TS_0) alias: x filterExpr: (a is not null and true) (type: boolean) Filter Operator (FIL_18) predicate: (a is not null and true) (type: boolean) Select Operator (SEL_2) expressions: a (type: decimal(10,2)) outputColumnNames: _col0 Reduce Output Operator (RS_6) key expressions: _col0 (type: decimal(11,2)) sort order: + Map-reduce partition columns: _col0 (type: decimal(11,2)) Join Operator (JOIN_8) condition map: Inner Join 0 to 1 keys: 0 _col0 (type: decimal(11,2)) 1 _col0 (type: decimal(11,2)) Group By Operator (GBY_11) aggregations: count(1) mode: hash outputColumnNames: _col0See cast up to Decimal(11, 2) in the plan, which normalizes both sides of the join to be able to compare HiveDecimal as-is.</description>
      <version>2.2.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-22 01:00:00" id="14311" opendate="2016-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need to schedule Heartbeat task if the query doesn&amp;#39;t require locks</summary>
      <description>Otherwise the Heartbeat task will just stay there and not be cleaned up, which may cause OOM eventually.</description>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2010-6-25 01:00:00" id="1435" opendate="2010-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgraded naming scheme causes JDO exceptions</summary>
      <description>We recently upgraded from Datanucleus 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. Because of this change, connecting to an existing database would throw exceptions such as:2010-06-24 17:59:09,854 ERROR exec.DDLTask (SessionState.java:printError(277)) - FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list' at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:325) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:2012) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:144) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:633) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:506) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:384) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:302) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-29 01:00:00" id="14378" opendate="2016-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data size may be estimated as 0 if no columns are being projected after an operator</summary>
      <description>in those cases we still emit rows.. but they may not have any columns within it. We shouldn't estimate 0 data size in such cases.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-29 01:00:00" id="14386" opendate="2016-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UGI clone shim also needs to clone credentials</summary>
      <description>Discovered while testing HADOOP-13081</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-8-2 01:00:00" id="14405" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have tests log to the console along with hive.log</summary>
      <description>When running tests from the IDE (not itests), logs end up going to hive.log - making it difficult to debug tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-4 01:00:00" id="14421" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FS.deleteOnExit holds references to _tmp_space.db files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-4 01:00:00" id="14422" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IF: when using LLAP IF from multiple threads in secure cluster, tokens can get mixed up</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-4 01:00:00" id="14432" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP signing unit test may be timing-dependent</summary>
      <description>Seems like it's possible for slow background thread to roll the key after we have signed with it.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-8-5 01:00:00" id="14447" opendate="2016-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set HIVE_TRANSACTIONAL_TABLE_SCAN to the correct job conf for FetchOperator</summary>
      <description/>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-6 01:00:00" id="14455" opendate="2016-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade httpclient, httpcore to match updated hadoop dependency</summary>
      <description>Hive was having a newer version of httpclient and httpcore since 1.2.0 (HIVE-9709), when compared to Hadoop 2.x versions, to be able to make use of newer apis in httpclient 4.4.There was security issue in the older version of httpclient and httpcore that hadoop was using, and as a result moved to httpclient 4.5.2 and httpcore 4.4.4 (HADOOP-12767).As hadoop was using the older version of these libraries and they often end up earlier in the classpath, we have had bunch of difficulties in different environments with class/method not found errors. Now, that hadoops dependencies in versions with security fix are newer and have the API that hive needs, we can be on the same version. For older versions of hadoop this version update doesn't matter as the difference is already there.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-7 01:00:00" id="14459" opendate="2016-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBeeLineDriver - migration and re-enable</summary>
      <description>this test have been left behind in HIVE-14444 because it had some compile issues.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.DisabledTestBeeLineDriver.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.AbstractHiveService.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-7 01:00:00" id="14460" opendate="2016-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AccumuloCliDriver migration to junit4</summary>
      <description>This test have been left behind in HIVE-14444</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloTestSetup.java</file>
      <file type="M">accumulo-handler.src.test.templates.TestAccumuloCliDriver.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-8 01:00:00" id="14479" opendate="2016-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some join tests for acid table</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-9 01:00:00" id="14487" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add REBUILD statement for materialized views</summary>
      <description>Support for rebuilding existing materialized views. The statement is the following:ALTER MATERIALIZED VIEW [db_name.]materialized_view_name REBUILD;</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BasicStatsWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.ConstraintsSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-9 01:00:00" id="14493" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioning support for materialized views</summary>
      <description>We should support defining a partitioning specification for materialized views and that the results of the materialized view evaluation are stored meeting the partitioning spec. The syntax should be extended as follows:CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] -- NEW! [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)] AS select_statement;</description>
      <version>2.2.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-9 01:00:00" id="14495" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SHOW MATERIALIZED VIEWS statement</summary>
      <description>In the spirit of SHOW TABLES, we should support the following statement:SHOW MATERIALIZED VIEWS [IN database_name] ['identifier_with_wildcards'];In contrast to SHOW TABLES, this command would only list the materialized views.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-11 01:00:00" id="14519" opendate="2016-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi insert query bug</summary>
      <description>When running multi-insert queries, when one of the query is not returning results, the other query is not returning the right result.For example:After following query, there is no value in /tmp/emp/dir3/000000_0From (select * from src) ainsert overwrite directory '/tmp/emp/dir1/'select key, valueinsert overwrite directory '/tmp/emp/dir2/'select 'header'where 1=2insert overwrite directory '/tmp/emp/dir3/'select key, value where key = 100;where clause in the second insert should not affect the third insert.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.NullScanOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-12 01:00:00" id="14527" opendate="2016-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema evolution tests are not running in TestCliDriver</summary>
      <description>HIVE-14376 broke something that makes schema evolution tests being excluded from TestCliDriver test suite.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-13 01:00:00" id="14534" opendate="2016-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>modify tables in tests in HIVE-14479 to use transactional_properties=default</summary>
      <description>only need to do this for 2.2</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="14554" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Download the spark-assembly file on itests only if the MD5 checksum file is different</summary>
      <description>The itests/thridparty directory is created by hive on spark when downloading the spark-assembly file. Hive ptest should delete this directory everytime it runs a new set of tests to avoid conflicts when a new spark tarball is submitted.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="14559" opendate="2016-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove setting hive.execution.engine in qfiles</summary>
      <description>Some qfiles are explicitly setting execution engine. If we run those tests on different Mini CliDriver's it could be very slow.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.tez.fsstat.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.join.partition.key.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.mr.pathalias.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.acid.non.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.skewjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constprog.dpp.q</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-18 01:00:00" id="14574" opendate="2016-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use consistent hashing for LLAP consistent splits to alleviate impact from cluster changes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-19 01:00:00" id="14579" opendate="2016-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for date extract</summary>
      <description>https://www.postgresql.org/docs/9.1/static/functions-datetime.html#FUNCTIONS-DATETIME-EXTRACT</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-12-10 01:00:00" id="146" opendate="2008-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix builds for non-default build directory</summary>
      <description>Some build paths are specified as "${hive.root}/build" instead of "${build.dir.hive}". Correct these, including "build.dir.hadoop" (it remains relative to "build.dir.hive" by default). This allows builds to work when a non-default "build.dir.hive" is specified.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-23 01:00:00" id="14608" opendate="2016-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: slow scheduling due to LlapTaskScheduler not removing nodes on kill</summary>
      <description>See comments; this can result in a slowdown esp. if some critical task gets unlucky. public void workerNodeRemoved(ServiceInstance serviceInstance) { // FIXME: disabling this for now// instanceToNodeMap.remove(serviceInstance.getWorkerIdentity());</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-23 01:00:00" id="14613" opendate="2016-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move schema evolution tests to MiniLlap and disable LLAP IO</summary>
      <description>Move the slow schema evolution tests from TestCliDriver to TestMiniLlapCliDriver and disable LLAP IO for these tests so that non-llap reader codepath is used.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.stats.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.complex.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.fetchwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.mapwork.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acidvec.mapwork.part.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-23 01:00:00" id="14614" opendate="2016-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite local directory fails with IllegalStateException</summary>
      <description>insert overwrite local directory .... select * from table; fails with "java.lang.IllegalStateException: Cannot create staging directory" when the path sent to the getTempDirForPath(Path path) is a local fs path.This is a regression caused by the fix for HIVE-14270</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-13 01:00:00" id="1463" opendate="2010-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive output file names are unnecessarily large</summary>
      <description>Hive's output files are named like this:attempt_201006221843_431854_r_000000_0out of all of this goop - only one character '0' would have sufficed. we should fix this. This would help environments with namenode memory constraints.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-25 01:00:00" id="14647" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo fixes in Beeline help</summary>
      <description>https://github.com/apache/hive/pull/99</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-26 01:00:00" id="14652" opendate="2016-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect results for not in on partition columns</summary>
      <description>create table foo (i int) partitioned by (s string);insert overwrite table foo partition(s='foo') select cint from alltypesorc limit 10;insert overwrite table foo partition(s='bar') select cint from alltypesorc limit 10;select * from foo where s not in ('bar');No results. IN ... works correctly</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-27 01:00:00" id="14658" opendate="2016-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF abs throws NPE when input arg type is string</summary>
      <description>I know this is not the right use case, but NPE is not exptected.0: jdbc:hive2://10.64.35.144:21066/&gt; select abs("foo");Error: Error while compiling statement: FAILED: NullPointerException null (state=42000,code=40000)</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-2 01:00:00" id="14702" opendate="2016-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: after a long period of inactivity, signing key may be removed from local store</summary>
      <description>Then, an attempt to get and use it would NPE</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SigningSecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-9-17 01:00:00" id="14783" opendate="2016-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketing column should be part of sorting for delete/update operation when spdo is on</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-23 01:00:00" id="14831" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing Druid dependencies at runtime</summary>
      <description>Excluded some packages when shading in the initial patch that should have been included.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-23 01:00:00" id="14835" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve ptest2 build time</summary>
      <description>NO PRECOMMIT TESTS2 things can be improved1) ptest2 always downloads jars for compiling its own directory which takes about 1m30s which should take only 5s with cache jars. The reason for that is maven.repo.local is pointing to a path under WORKSPACE which will be cleaned by jenkins for every run.2) For hive build we can make use of parallel build and quite the output of build which should shave off another 15-30s.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-24 01:00:00" id="14837" opendate="2016-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: standalone jar is missing hadoop core dependencies</summary>
      <description>2016/09/24 00:31:57 ERROR - jmeter.threads.JMeterThread: Test failed! java.lang.NoClassDefFoundError: org/apache/hadoop/conf/Configuration at org.apache.hive.jdbc.HiveConnection.createUnderlyingTransport(HiveConnection.java:418) at org.apache.hive.jdbc.HiveConnection.createBinaryTransport(HiveConnection.java:438) at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:225) at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:182) at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-12-25 01:00:00" id="1487" opendate="2010-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>parallelize test query runs</summary>
      <description>HIVE-1464 speeded up serial runs somewhat - but looks like it's still too slow. we should use parallel junit or some similar setup to run test queries in parallel. this should be really easy as well need to just use a separate warehouse/metadb and potentiall mapred system dir location.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.Report.py</file>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-3 01:00:00" id="14878" opendate="2016-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>integrate MM tables into ACID: add separate ACID type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-10 01:00:00" id="14921" opendate="2016-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move slow CliDriver tests to MiniLlap - part 2</summary>
      <description>Continuation to HIVE-14877</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-10 01:00:00" id="14922" opendate="2016-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add perf logging for post job completion steps</summary>
      <description>Mostly FS related operations.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-11 01:00:00" id="14929" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding JDBC test for query cancellation scenario</summary>
      <description>There is some functional testing for query cancellation using JDBC which is missing in unit tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-11 01:00:00" id="14932" opendate="2016-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle bucketing for MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-12 01:00:00" id="14933" opendate="2016-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>include argparse with LLAP scripts to support antique Python versions</summary>
      <description>The module is a standalone file, and it's under Python license that is compatible with Apache. In the long term we should probably just move LlapServiceDriver code entirely to Java, as right now it's a combination of part-py, part-java.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-12 01:00:00" id="14938" opendate="2016-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add deployed ptest properties file to repo, update to remove isolated tests</summary>
      <description>The intent is to checkin the original file, and then modify it to remove isolated tests (and move relevant ones to the skipBatching list), which normally lead to stragglers, and sub-optimal resource utilization.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-13 01:00:00" id="14942" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 UI: Canceled queries show up in "Open Queries"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-13 01:00:00" id="14948" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>properly handle special characters in identifiers</summary>
      <description>The treatment of quoted identifiers in HIVE-14943 is inconsistent. Need to clean this up and if possible only quote those identifiers that need to be quoted in the generated SQL statement</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-17 01:00:00" id="14992" opendate="2016-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate several common libraries in hive jdbc uber jar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-18 01:00:00" id="14996" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle load for MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-20 01:00:00" id="15025" opendate="2016-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure-Socket-Layer (SSL) support for HMS</summary>
      <description>HMS server should support SSL encryption. When the server is keberos enabled, the encryption can be enabled. But if keberos is not enabled, then there is no encryption between HS2 and HMS. Similar to HS2, we should support encryption in both cases.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TServerSocketKeepAlive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIServiceFeatures.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-24 01:00:00" id="15039" opendate="2016-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A better job monitor console output for HoS</summary>
      <description>When there're many stages, it's very difficult to read the console output of job progress of HoS. Attached screenshot is an example.We may learn from HoT as it does much better than HoS.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.InPlaceUpdates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-11-3 01:00:00" id="15119" opendate="2016-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support standard syntax for ROLLUP &amp; CUBE</summary>
      <description>Standard ROLLUP and CUBE syntax is GROUP BY ROLLUP (expression list)... and GROUP BY CUBE (expression list) respectively. Currently HIVE only allows GROUP BY &lt;expression list&gt; WITH ROLLUP/CUBE syntax. We would like HIVE to support standard ROLLUP/CUBE syntax.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.id1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.grouping.sets.q</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.pushdown2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.infer.bucket.sort.grouping.operators.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.rollup1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube.multi.gby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.annotate.stats.groupby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-4 01:00:00" id="15125" opendate="2016-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Parallelize slider package generator</summary>
      <description>The metastore init + download of functions takes approx 4 seconds.This is enough time to complete all the other operations in parallel.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-5 01:00:00" id="1513" opendate="2010-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive starter scripts should load admin/user supplied script for configurability</summary>
      <description>it's difficult to add environment variables to Hive starter scripts except by modifying the scripts directly. this is undesirable (since they are source code). Hive starter scripts should load a admin supplied shell script for configurability. This would be similar to what hadoop does with hadoop-env.sh</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.util.execHiveCmd.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-5 01:00:00" id="1514" opendate="2010-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Be able to modify a partition&amp;#39;s fileformat and file location information.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterPartitionProtectModeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-8 01:00:00" id="15160" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-15 01:00:00" id="15207" opendate="2016-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement a capability to detect incorrect sequence numbers</summary>
      <description>We have seen next sequence number is smaller than the max(id) for certain tables. Seems it's caused by thread-safe issue in HMS, but still not sure if it has been fully fixed. Try to detect such issue.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-15 01:00:00" id="15208" opendate="2016-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query string should be HTML encoded for Web UI</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-15 01:00:00" id="15212" opendate="2016-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge branch into master</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.mm.truncate.cols.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.truncate.cols.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ImportCommitTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AbstractBucketJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.fs.FSStatsPublisher.java</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.insertonly.acid.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientnegative.mm.bucket.convert.q</file>
      <file type="M">ql.src.test.results.clientnegative.mm.bucket.convert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-16 01:00:00" id="15220" opendate="2016-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat test driver not capturing end time of test accurately</summary>
      <description>Webhcat e2e testsuite prints message while ending test run:Ending test &lt;testcase&gt; at 1479264720Currently it is not capturing the end time correctly.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-17 01:00:00" id="15233" opendate="2016-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF UUID() should be non-deterministic</summary>
      <description>The UUID() function should be non-deterministic.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFUUID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUUID.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-11-25 01:00:00" id="15284" opendate="2016-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add junit test to test replication scenarios</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-28 01:00:00" id="15295" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HCatalog javadoc generation with Java 8</summary>
      <description>Realized while generating artifacts for Hive 2.1.1 release.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-29 01:00:00" id="15307" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive MERGE: "when matched then update" allows invalid column names.</summary>
      <description>create table target ( id int, val int)CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ("transactional"="true");create table source2 ( id int, val int);insert into source2 values (2, 25), (3, 35), (4, 45);merge into targetusing source2 sub on sub.id = target.idwhen matched then update set invalid = sub.val;</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="15327" opendate="2016-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Outerjoin might produce wrong result depending on joinEmitInterval value</summary>
      <description>If joinEmitInterval is smaller than the group size, outerjoins might produce records with NULL appended values multiple times (once per group).HIVE-4689 targeted the same problem. However, the fix does not seem to cover all cases (in particular, it will not apply to left outer joins with filter conditions on the left input). The solution in HIVE-4689 was to disable (override) joinEmitInterval value for those cases. This fix follows the same approach.To reproduce the problem:set hive.strict.checks.cartesian.product=false;set hive.join.emit.interval=1;CREATE TABLE test1 (key INT, value INT, col_1 STRING);INSERT INTO test1 VALUES (99, 0, 'Alice');INSERT INTO test1 VALUES (99, 2, 'Mat');INSERT INTO test1 VALUES (100, 1, 'Bob');INSERT INTO test1 VALUES (101, 2, 'Car');CREATE TABLE test2 (key INT, value INT, col_2 STRING);INSERT INTO test2 VALUES (102, 2, 'Del');INSERT INTO test2 VALUES (103, 2, 'Ema');INSERT INTO test2 VALUES (104, 3, 'Fli');-- Equi-condition and condition on one input (left outer join)SELECT *FROM test1 LEFT OUTER JOIN test2ON (test1.value=test2.value AND test1.key between 100 and 102)LIMIT 10;-- Condition on one input (left outer join)SELECT *FROM test1 LEFT OUTER JOIN test2ON (test1.key between 100 and 102)LIMIT 10;For the first query, current (incorrect) result is: 99 0 Alice NULL NULL NULL 100 1 Bob NULL NULL NULL 101 2 Car 103 2 Ema 99 2 Mat NULL NULL NULL 101 2 Car 102 2 Del 99 2 Mat NULL NULL NULLExpected (correct) result is: 99 0 Alice NULL NULL NULL 100 1 Bob NULL NULL NULL 101 2 Car 103 2 Ema 101 2 Car 102 2 Del 99 2 Mat NULL NULL NULLFor the second query, current (incorrect) result is: 101 2 Car 104 3 Fli 100 1 Bob 104 3 Fli 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL 101 2 Car 103 2 Ema 100 1 Bob 103 2 Ema 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL 101 2 Car 102 2 Del 100 1 Bob 102 2 DelExpected (correct) result is: 101 2 Car 104 3 Fli 101 2 Car 103 2 Ema 101 2 Car 102 2 Del 100 1 Bob 104 3 Fli 100 1 Bob 103 2 Ema 100 1 Bob 102 2 Del 99 2 Mat NULL NULL NULL 99 0 Alice NULL NULL NULL</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-13 01:00:00" id="1535" opendate="2010-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter partition should throw exception if the specified partition does not exist.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-5 01:00:00" id="15360" opendate="2016-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nested column pruning: add pruned column paths to explain output</summary>
      <description>We should add the pruned nested column paths to the explain output for easier tracing and debugging.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-5 01:00:00" id="15362" opendate="2016-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the missing fields for 2.2.0 upgrade scripts</summary>
      <description>The 2.2.0 upgrade scripts were cut on 05/25/16, while HIVE-13354 (which added some fields to upgrade scripts) was committed to master on 05/27/16, and there's no conflict. So we accidentally missed those fields for 2.2.0.cc ekoifman</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-2.2.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-6 01:00:00" id="15370" opendate="2016-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include Join residual filter expressions in user level EXPLAIN</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-7 01:00:00" id="15383" opendate="2016-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional info to &amp;#39;desc function extended&amp;#39; output</summary>
      <description>Add additional info to the output to 'desc function extended'. The resources would be helpful for the user to check which jars are referred.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.replicate.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.short.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.int.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.weekofyear.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.var.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.var.pop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.variance.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.upper.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unhex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ucase.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.utc.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.tinyint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.tan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sum.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.subtract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.stddev.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.stddev.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sqrt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.soundex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.smallint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sign.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sha2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sha1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rtrim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rlike.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.replace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.replace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.regexp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.rand.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.radians.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.quarter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.printf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.power.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pow.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.positive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.PI.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nullif.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.next.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.named.struct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.months.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.month.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.min.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.md5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.show.last.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.show.first.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.last.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.first.n.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.mask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ltrim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.logged.in.user.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.log.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ln.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.levenshtein.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lessthanorequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lessthan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lcase.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.last.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.java.method.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.int.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.initcap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greaterthanorequal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greaterthan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.utc.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.format.number.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.floor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.field.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.factorial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.exp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.equal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.E.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.double.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.divide.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.div.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.degrees.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.decode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.dayofmonth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.day.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.sub.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.date.add.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.datediff.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.current.user.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.crc32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.cos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.conv.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udf.row.sequence.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udf.percentile2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.func1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udaf.collect.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.max.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.min.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.udf.percentile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.collect.set.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.corr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.covar.pop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.covar.samp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.acos.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.add.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.add.months.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.aes.decrypt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.aes.encrypt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.array.contains.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.asin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.atan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.avg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bigint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftleft.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftright.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.shiftrightunsigned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bitwise.xor.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.boolean.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.cbrt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ceil.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ceiling.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.chr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.concat.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-8 01:00:00" id="15391" opendate="2016-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Location validation for table should ignore the values for view.</summary>
      <description>When use schematool to do location validation, we got error message for views, for example:n DB with Name: viewaNULL Location for TABLE with Name: viewaIn DB with Name: viewaNULL Location for TABLE with Name: viewbIn DB with Name: viewa</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-3-9 01:00:00" id="15407" opendate="2016-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add distcp to classpath by default, because hive depends on it.</summary>
      <description>when i run hive queries, i get errors as followjava.lang.NoClassDefFoundError: org/apache/hadoop/tools/DistCpOptions...I dig into code, and find that hive depends on distcp ,but distcp is not in classpath by default.I think if adding distcp to hadoop classpath by default in hadoop project, but hadoop committers will not do that. discussions in HADOOP-13865 . They propose that Resolving this problem on HIVESo i add distcp to classpath on HIVE</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-12 01:00:00" id="15417" opendate="2016-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Glitches using ACID&amp;#39;s row__id hidden column</summary>
      <description>This only works if you turn PPD off.drop table if exists hello_acid;create table hello_acid (key int, value int)partitioned by (load_date date)clustered by(key) into 3 bucketsstored as orc tblproperties ('transactional'='true');insert into hello_acid partition (load_date='2016-03-01') values (1, 1);insert into hello_acid partition (load_date='2016-03-02') values (2, 2);insert into hello_acid partition (load_date='2016-03-03') values (3, 3);hive&gt; set hive.optimize.ppd=true;hive&gt; select tid from (select row__id.transactionid as tid from hello_acid) sub where tid = 15;FAILED: SemanticException MetaException(message:cannot find field row__id from [0:load_date])hive&gt; set hive.optimize.ppd=false;hive&gt; select tid from (select row__id.transactionid as tid from hello_acid) sub where tid = 15;OKtid15Time taken: 0.075 seconds, Fetched: 1 row(s)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-12 01:00:00" id="15420" opendate="2016-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP UI: Relativize resources to allow proxied/secured views</summary>
      <description>If the UI is secured behind a gateway firewall instance, this allows for the UI to function with a base URL like http://&lt;gateway&gt;/proxy/NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.index.html</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-13 01:00:00" id="15426" opendate="2016-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix order guarantee of event executions for REPL LOAD</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-14 01:00:00" id="15428" opendate="2016-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS DPP doesn&amp;#39;t remove cyclic dependency</summary>
      <description>More details in HIVE-15357</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruningBySize.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-14 01:00:00" id="15430" opendate="2016-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change SchemaTool table validator to test based on the dbType</summary>
      <description>Currently the validator parses the "oracle" schema file to determine what tables are expected in the database. (mostly because of ease of parsing the schema file compared to other syntax). We have learnt from HIVE-15118, that not all schema files have the same amount of tables. For example, derby has an old table that is never used that other DBs do not contain).</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-16 01:00:00" id="15445" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Subquery failing with ClassCastException</summary>
      <description>To reproduce:CREATE TABLE table_7 (int_col INT);SELECT(t1.int_col) * (t1.int_col) AS int_colFROM (SELECTMIN(NULL) OVER () AS int_colFROM table_7) t1WHERE(False) NOT IN (SELECTFalse AS boolean_colFROM (SELECTMIN(NULL) OVER () AS int_colFROM table_7) tt1WHERE(t1.int_col) = (tt1.int_col));The problem seems to be in the method that tries to resolve the subquery column MIN(NULL). It checks the column inspector and ends up returning a constant expression instead of a column expression for min(null).</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-16 01:00:00" id="15446" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive fails in recursive debug</summary>
      <description>When running hive recursive debug mode, for example,./bin/hive --debug:port=10008,childSuspend=yIt fails with error msg:&amp;#8211;ERROR: Cannot load this JVM TI agent twice, check your java command line for duplicate jdwp options.Error occurred during initialization of VMagent library failed to init: jdwp&amp;#8211;It is because HADOOP_OPTS and HADOOP_CLIENT_OPTS both have jvm debug options when invoking HADOOP.sh for the child process. The HADOOP_CLIENT_OPTS is appended to HADOOP_OPTS in HADOOP.sh which leads to the duplicated debug options.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-12-16 01:00:00" id="15459" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix unit test failures on master</summary>
      <description>Golden file updates missed in HIVE-15397 and HIVE-15192</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.subquery.shared.alias.q</file>
      <file type="M">ql.src.test.queries.clientnegative.subquery.nested.subquery.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-20 01:00:00" id="15472" opendate="2016-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Standalone jar is missing ZK dependencies</summary>
      <description>Exception in thread "main" java.lang.NoClassDefFoundError: org/apache/curator/RetryPolicy at org.apache.hive.jdbc.Utils.configureConnParams(Utils.java:514) at org.apache.hive.jdbc.Utils.parseURL(Utils.java:434) at org.apache.hive.jdbc.HiveConnection.&lt;init&gt;(HiveConnection.java:132) at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:107) at java.sql.DriverManager.getConnection(DriverManager.java:664) at java.sql.DriverManager.getConnection(DriverManager.java:247) at JDBCExecutor.getConnection(JDBCExecutor.java:65) at JDBCExecutor.executeStatement(JDBCExecutor.java:104) at JDBCExecutor.executeSQLFile(JDBCExecutor.java:81) at JDBCExecutor.main(JDBCExecutor.java:183)Caused by: java.lang.ClassNotFoundException: org.apache.curator.RetryPolicy at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-20 01:00:00" id="15478" opendate="2016-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add file + checksum list for create table/partition during notification creation (whenever relevant)</summary>
      <description>Currently, file list is being generated during REPL DUMP which will result in inconsistent data getting captured. This ticket is used for event dumping. Bootstrap dump checksum will be in a different Jira.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-21 01:00:00" id="15484" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PerfCliDriver test failures in master</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-12-21 01:00:00" id="15488" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Native Vector MapJoin fails when trying to serialize BigTable rows that have (unreferenced) complex types</summary>
      <description>When creating VectorSerializeRow we need to exclude any complex types.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-21 01:00:00" id="15489" opendate="2016-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alternatively use table scan stats for HoS</summary>
      <description>For MapJoin in HoS, we should provide an option to only use stats in the TS rather than the populated stats in each of the join branch. This could be pretty conservative but more reliable.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SparkRemoveDynamicPruningBySize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-30 01:00:00" id="15525" opendate="2016-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hooking ChangeManager to "drop table", "drop partition"</summary>
      <description>When Hive "drop table"/"drop partition", we will move data files into cmroot in case the replication destination will need it.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.worker.TestWarehousePartitionHelper.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-2 01:00:00" id="15529" opendate="2017-1-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: TaskSchedulerService can get stuck when scheduling tasks as disabled node is not re-enabled in NodeEnablerCallable</summary>
      <description>Easier way to simulate the issue:1. Start hive cli with "--hiveconf hive.execution.mode=llap"2. Run a sql script file (e.g sql script containing tpc-ds queries)3. In the middle of the run, press "ctrl+C" which would interrupt the current job. This should not exit the hive cli yet.4. After sometime, launch the same SQL script in same cli. This would get stuck indefinitely (waiting for computing the splits).Even when cli is quit, AM runs forever until explicitly killed. Issue seems to be around LlapTaskSchedulerService::schedulePendingTasks dealing with the loop when it encounters DELAYED_RESOURCES on task scheduling.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-5 01:00:00" id="15543" opendate="2017-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t try to get memory/cores to decide parallelism when Spark dynamic allocation is enabled</summary>
      <description>Presently Hive tries to get numbers for memory and cores from the Spark application and use them to determine RS parallelism. However, this doesn't make sense when Spark dynamic allocation is enabled because the current numbers doesn't represent available computing resources, especially when SparkContext is initially launched.Thus, it makes send not to do that when dynamic allocation is enabled.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="15579" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support HADOOP_PROXY_USER for secure impersonation in hive metastore client</summary>
      <description>Hadoop clients support HADOOP_PROXY_USER for secure impersonation. It would be useful to have similar feature for hive metastore client.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-11 01:00:00" id="15580" opendate="2017-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate unbounded memory usage for orderBy and groupBy in Hive on Spark</summary>
      <description>Currently, orderBy (sortBy) and groupBy in Hive on Spark uses unbounded memory. For orderBy, Hive accumulates key groups using ArrayList (described in HIVE-15527). For groupBy, Hive currently uses Spark's groupByKey operator, which has a shortcoming of not being able to spill to disk within a key group. Thus, for large key group, memory usage is also unbounded.It's likely that this will impact performance. We will profile and optimize afterwards. We could also make this change configurable.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.top.level.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GroupByShuffler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-13 01:00:00" id="15612" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include Calcite dependency in Druid storage handler jar</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-13 01:00:00" id="15619" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column pruner should handle DruidQuery</summary>
      <description>Even when we cannot push any operator into Druid, we might be able to prune some of the columns that are read from the Druid sources.One solution would be to extend the ColumnPruner so it can push the needed columns into DruidQuery.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.basic2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-13 01:00:00" id="15623" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use customized version of netty for llap</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-13 01:00:00" id="15625" opendate="2017-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>escape1 test fails on Mac</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.files.escapetest.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-16 01:00:00" id="15642" opendate="2017-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicate Insert Overwrites, Dynamic Partition Inserts and Loads</summary>
      <description>1. Insert Overwrites to a new partition should not capture new files as part of insert event but instead use the subsequent add partition event to capture the files + checksums.2. Insert Overwrites to an existing partition should capture new files as part of the insert event. Similar behaviour for DP inserts and loads.This will need changes from HIVE-15478</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-17 01:00:00" id="15645" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez session pool may restart sessions in a wrong queue</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="15649" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO may NPE on all-column read</summary>
      <description>It seems like very few paths use READ_ALL_COLUMNS config, but some do. LLAP IO doesn't account for that.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-17 01:00:00" id="15650" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set perflogger to DEBUG level for llap daemons</summary>
      <description>During Hive2 dev, the PerfLogger was moved to DEBUG levels only making it impossible to debug timings from LLAP logs without manually editing log4j2.properties and redeploying LLAP.Enable PerfLogger by default on LLAP.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-1-19 01:00:00" id="15663" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more interval tests to HivePerfCliDriver</summary>
      <description>following HIVE-13557</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query98.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query82.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query80.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query72.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query12.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-19 01:00:00" id="15668" opendate="2017-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>change REPL DUMP syntax to use "LIMIT" instead of "BATCH" keyword</summary>
      <description>Currently, REPL DUMP syntax goes:REPL DUMP [&lt;dbname&gt;[.&lt;tblname&gt;]] [FROM &lt;eventid&gt; [BATCH &lt;batchSize&gt;]]The BATCH directive says that when doing an event dump, to not dump out more than batchSize number of events. However, there is a clearer keyword for the same effect, and that is LIMIT. Thus, rephrasing the syntax as follows makes it clearer:REPL DUMP [&lt;dbname&gt;[.&lt;tblname&gt;]] [FROM &lt;eventid&gt; [LIMIT &lt;maxEventLimit&gt;]]</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-20 01:00:00" id="15669" opendate="2017-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Improve aging in shortest job first scheduler</summary>
      <description>Under high concurrency, some jobs can gets starved for longer time when hive.llap.task.scheduler.locality.delay is set to -1 (infinitely wait for locality).</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-3-21 01:00:00" id="15691" opendate="2017-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink</summary>
      <description>Create StrictRegexWriter to work with RegexSerializer for Flume Hive Sink.It is similar to StrictJsonWriter available in hive.Dependency is there in flume to commit.FLUME-3036 : Create a RegexSerializer for Hive Sink.Patch is available for Flume, Please verify the below linkhttps://github.com/kalyanhadooptraining/flume/commit/1c651e81395404321f9964c8d9d2af6f4a2aaef9</description>
      <version>None</version>
      <fixedVersion>1.2.2,1.3.0,2.3.0,3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-2-27 01:00:00" id="15745" opendate="2017-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver. vector_varchar_simple,vector_char_simple</summary>
      <description>TestMiniLlapLocalCliDriver. vector_varchar_simple,vector_char_simple are failing occasionallyvector_varchar_simple failed in https://builds.apache.org/job/PreCommit-HIVE-Build/3204/testReport/vector_char_simple failed in https://builds.apache.org/job/PreCommit-HIVE-Build/3205/testReport/org.apache.hadoop.hive.cli/TestMiniLlapLocalCliDriver/testCliDriver_vector_char_simple_/</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-30 01:00:00" id="15751" opendate="2017-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to run findbugs for itest modules as well</summary>
      <description>Remove relative paths from the findbugs configuration, so it could be run for every module.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-1 01:00:00" id="15777" opendate="2017-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>propagate LLAP app ID to ATS and log it</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceRegistry.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-2-2 01:00:00" id="15790" opendate="2017-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused beeline golden files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.beelinepositive.avro.joins.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.script.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.null.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.short.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.long.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.int.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.xpath.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.weekofyear.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.var.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.var.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.variance.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.upper.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.unhex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ucase.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.trim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.to.date.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.tinyint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.tan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sum.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.subtract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.stddev.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.std.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sqrt.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.smallint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.sign.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rtrim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rpad.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.round.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rlike.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.replace.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.regexp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.reflect.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.rand.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.radians.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.power.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.pow.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.positive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.PI.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.or.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.notop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.notequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.not.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.month.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ltrim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lpad.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.logic.java.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.log.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ln.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lessthanorequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lessthan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.lcase.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.last.day.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.java.method.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.in.file.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.int.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.initcap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.index.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.in.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.greaterthanorequal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.greaterthan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.get.json.object.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.floor.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.find.in.set.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.field.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.exp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.equal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.E.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.divide.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.div.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.degrees.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.dayofmonth.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.day.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.date.sub.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.date.add.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.datediff.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.cos.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.conv.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.ws.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.insert2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.insert1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.concat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.compare.java.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ceiling.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ceil.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.boolean.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.xor.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.or.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.not.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitwise.and.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bitmap.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.bigint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.avg.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.atan.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.asin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.array.contains.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.add.months.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.add.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.acos.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.covar.samp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.covar.pop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.udaf.corr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.transform2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.touch.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.lazy.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.comparison.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.subq.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.publisher.error.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.empty.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats.aggregator.error.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sort.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.tablestatus.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.describe.func.quotes.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.show.columns.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.showparts.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.set.variable.sub.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.serde.regex.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.repair.hadoop23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.repair.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rename.partition.location.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.exclude.gby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.toleratecorruptions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.merge1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.lazydecompress.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.query.result.fileformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.progress.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.print.header.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.pushdown.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.special.char.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.serde.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partitions.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.partcols1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.parenthesis.star.by.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.overridden.confs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.order2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ops.comparison.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullinput2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullinput.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nomore.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.newline.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.nestedvirtual.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.misc.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mi.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.overwrite.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.loadpart1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.ints.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.literal.double.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lateral.view.ppd.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.keyword.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.empty.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.casesensitive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join.1to1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join40.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join39.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join38.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join37.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join36.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join35.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join34.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join33.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join32.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.join0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert.compressed.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.dfs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.cb.delim.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input49.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input45.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input44.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input43.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input42.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input41.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input40.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input39.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input38.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input37.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input36.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input35.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input34.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input33.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input32.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.input.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.hook.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.hook.context.cs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.having.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.neg.float.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby.bigdata.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.explode.null.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.enforce.order.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.udf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table.removes.partition.dirs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.multi.partitions.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.index.removes.partition.dirs.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.index.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.drop.function.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.driverhook.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.disable.file.format.check.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.desc.non.existent.tbl.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.table.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.delimiter.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.default.partition.name.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ct.case.insensitive.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.merge.compressed.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cp.mj.rc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.count.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.concatenate.inherit.table.location.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.combine1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketizedhiveinputformat.auto.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binary.constant.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.schema.literal.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.sanity.test.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.numbuckets.partitioned.table.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.partition.protect.mode.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.archive.excludeHadoop20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.authorization.3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.change.schema.q.out</file>
      <file type="M">ql.src.test.results.beelinepositive.avro.evolved.schemas.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-3 01:00:00" id="15796" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: poor reducer parallelism when operator stats are not accurate</summary>
      <description>In HoS we use currently use operator stats to determine reducer parallelism. However, it is often the case that operator stats are not accurate, especially if column stats are not available. This sometimes will generate extremely poor reducer parallelism, and cause HoS query to run forever. This JIRA tries to offer an alternative way to compute reducer parallelism, similar to how MR does. Here's the approach we are suggesting:1. when computing the parallelism for a MapWork, use stats associated with the TableScan operator;2. when computing the parallelism for a ReduceWork, use the maximum parallelism from all its parents.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-3 01:00:00" id="15798" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP run.sh should use stop --force</summary>
      <description>It's both faster, and avoids slider issues when the app survives across kerberization and cannot be stopped by regular stop, which assumes it should have some token or other because the cluster is now secure.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-3 01:00:00" id="15801" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some logging improvements in LlapTaskScheduler</summary>
      <description>Excessive logging in some places. Not enough otherwise.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="15802" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes to expected entries for dynamic bloomfilter runtime filtering</summary>
      <description>Estimate bloom filter size based on distinct values from column stats if available</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="15803" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>msck can hang when nested partitions are present</summary>
      <description>Steps to reproduce. CREATE TABLE `repairtable`( `col` string) PARTITIONED BY ( `p1` string, `p2` string)hive&gt; dfs -mkdir -p /apps/hive/warehouse/test.db/repairtable/p1=c/p2=a/p3=b;hive&gt; dfs -touchz /apps/hive/warehouse/test.db/repairtable/p1=c/p2=a/p3=b/datafile;hive&gt; set hive.mv.files.thread;hive.mv.files.thread=15hive&gt; set hive.mv.files.thread=1;hive&gt; MSCK TABLE repairtable;</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.msck.repair.batchsize.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.msck.repair.batchsize.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="15805" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some minor improvement on the validation tool</summary>
      <description>To correct some types and make the output neat. And also add the validation servers to the command line.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="15806" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid schema inference for Select queries might produce wrong type for metrics</summary>
      <description>We inferred float automatically, instead of emitting a metadata query to Druid and checking the type of the metric.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-3 01:00:00" id="15808" opendate="2017-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove semijoin reduction branch if it is on bigtable along with hash join</summary>
      <description>If there is a semijoin branch on the same operator pipeline which contains a hash join then it is by design on big table which is not optimal. The operator cycle detection logic may not find a cycle as there is no cycle at operator level. However, once Tez builds its task there can be a cycle at task level causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-4 01:00:00" id="15809" opendate="2017-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in the PostgreSQL database name for druid service</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-6 01:00:00" id="15829" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: disable memory tracking on the writer</summary>
      <description>See ORC-141 and HIVE-15672 for context</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-7 01:00:00" id="15839" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t force cardinality check if only WHEN NOT MATCHED is specified</summary>
      <description>should've been part of HIVE-14949if only WHEN NOT MATCHED is specified then the join is basically an anti-join and we are not retrieving any values from target side. So the cardinality check is just overhead (though presumably very minor since the filter above the join will filter everything and thus there is nothing to group)</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sqlmerge.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-23 01:00:00" id="1584" opendate="2010-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong log files in contrib client positive</summary>
      <description>TestContribCliDriver still gets some diffs</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-7 01:00:00" id="15841" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to ORC 1.3.3</summary>
      <description>Hive needs ORC-141 and ORC-135, so we should upgrade to ORC 1.3.3 once it releases.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcRawRecordMerger.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-7 01:00:00" id="15843" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable slider YARN resource normalization for LLAP</summary>
      <description>The normalization can lead to LLAP starting with invalid configuration with regard to cache size, jmx and container size. If the memory configuration is invalid, it should fail immediately.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-8 01:00:00" id="15846" opendate="2017-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Relocate more dependencies (e.g. org.apache.zookeeper) for JDBC uber jar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-9 01:00:00" id="15864" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix typo introduced in HIVE-14754</summary>
      <description>hs2_suceeded_queries needs another "c": hs2_succeeded_queries.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.operation.TestSQLOperationMetrics.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.common.MetricsConstant.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-10 01:00:00" id="15878" opendate="2017-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP text cache: bug in last merge</summary>
      <description>While rebasing the last patch, a bug was introduced.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-12 01:00:00" id="15886" opendate="2017-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Provide logs URL for in-progress and completed task attemtps</summary>
      <description>YARN provides a webservice to access logs with YARN-6011. This can be used to populate the in-progress and completed task attempts logs.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-13 01:00:00" id="15895" opendate="2017-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HDFS for stats collection temp dir on blob storage</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-14 01:00:00" id="15901" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: incorrect usage of gap cache</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-2-16 01:00:00" id="15934" opendate="2017-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Downgrade Maven surefire plugin from 2.19.1 to 2.18.1</summary>
      <description>Surefire 2.19.1 has some issue (https://issues.apache.org/jira/browse/SUREFIRE-1255) which caused debugging session to abort after a short period of time. Many IntelliJ users have seen this, although it looks fine for Eclipse users. Version 2.18.1 works fine.We'd better make the change to not impact the development for IntelliJ guys. We can upgrade again once the root cause is figured out.cc kgyrtkirk ashutoshc</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-16 01:00:00" id="15935" opendate="2017-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACL is not set in ATS data</summary>
      <description>When publishing ATS info, Hive does not set ACL, that make Hive ATS entries visible to all users. On the other hand, Tez ATS entires is using Tez DAG ACL which limit both view/modify ACL to end user only. We shall make them consistent. In the Jira, I am going to limit ACL to end user for both Tez ATS and Hive ATS, also provide config "hive.view.acls" and "hive.modify.acls" if user need to overridden.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-16 01:00:00" id="15951" opendate="2017-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make sure base persist directory is unique and deleted</summary>
      <description>In some cases the base persist directory will contain old data or shared between reducer in the same physical VM.That will lead to the failure of the job till that the directory is cleaned.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-17 01:00:00" id="15954" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some Tez INFO logs are too noisy</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-17 01:00:00" id="15956" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError when drop lots of partitions</summary>
      <description>Repro steps:1. Create partitioned table and add 10000 partitionscreate table test_partition(id int) partitioned by (dt int);alter table test_partition add partition(dt=1);alter table test_partition add partition(dt=3);alter table test_partition add partition(dt=4);...alter table test_partition add partition(dt=10000);2. Drop 9000 partitions:alter table test_partition drop partition(dt&lt;9000);Step 2 will fail with StackOverflowError:Exception in thread "pool-7-thread-161" java.lang.StackOverflowError at org.datanucleus.query.expression.ExpressionCompiler.isOperator(ExpressionCompiler.java:819) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:190) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179) at org.datanucleus.query.expression.ExpressionCompiler.compileOrAndExpression(ExpressionCompiler.java:192) at org.datanucleus.query.expression.ExpressionCompiler.compileExpression(ExpressionCompiler.java:179)Exception in thread "pool-7-thread-198" java.lang.StackOverflowError at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:83) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87) at org.datanucleus.query.expression.DyadicExpression.bind(DyadicExpression.java:87)</description>
      <version>1.3.0,2.2.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="15957" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Follow Hive&amp;#39;s rules for type inference instead of Calcite</summary>
      <description>Mostly those rules are same, but in case they diverge we should pick Hive's rule for type inference.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.interval.arithmetic.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="15958" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: IPC connections are not being reused for umbilical protocol</summary>
      <description>During concurrency testing, observed 1000s of ipc thread creations. Ideally, the connections to same hosts should be reused.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-17 01:00:00" id="15964" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-18 01:00:00" id="15972" opendate="2017-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime filtering not vectorizing for decimal/timestamp/char/varchar</summary>
      <description>Looks like versions of vectorized BetweenDynamicValue that use Java objects needs to be initialized with non-null values</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction2.q</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-20 01:00:00" id="15974" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support real, double precision and numeric data types</summary>
      <description>Support the standard names for these datatypes, which map to existing Hive datatypes.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="15975" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the MOD function</summary>
      <description>SQL defines the mod expression as a function allowing 2 numeric value expressions. Hive allows the infix notation using %. It would be good for Hive to support the standard approach as well. SQL standard reference T441</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.modulo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.modulo.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="15978" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support regr_* functions</summary>
      <description>Support the standard regr_* functions, regr_slope, regr_intercept, regr_r2, regr_sxx, regr_syy, regr_sxy, regr_avgx, regr_avgy, regr_count. SQL reference section 10.9</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="15979" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support character_length and octet_length</summary>
      <description>SQL defines standard ways to get number of characters and octets. SQL reference: section 6.28. Example:vagrant=# select character_length('欲速则不达'); character_length------------------ 5(1 row)vagrant=# select octet_length('欲速则不达'); octet_length-------------- 15(1 row)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLength.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-20 01:00:00" id="15981" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow empty grouping sets</summary>
      <description>group by () should be treated as equivalent to no group by clause. Currently it throws a parse error</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-21 01:00:00" id="15991" opendate="2017-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky Test: TestEncryptedHDFSCliDriver encryption_join_with_different_encryption_keys</summary>
      <description>I ran a git-bisect and seems HIVE-15703 started causing this failure. Not entirely sure why, but I updated the .out file and the diff is pretty straightforward, so I think its safe to just update it.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.with.different.encryption.keys.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-21 01:00:00" id="15993" opendate="2017-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive REPL STATUS is not returning last event ID</summary>
      <description>While running "REPL STATUS" on target to get last event ID for DB, it returns zero rows.0: jdbc:hive2://localhost:10001/repl&gt; REPL status repl;No rows affected (932.167 seconds)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-22 01:00:00" id="16002" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Correlated IN subquery with aggregate asserts in sq_count_check UDF</summary>
      <description>Reproducercreate table t(i int, j int);insert into t values(0,1), (0,2);create table tt(i int, j int);insert into tt values(0,3);select * from t where i IN (select count(i) from tt where tt.j = t.j);</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSQCountCheck.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-22 01:00:00" id="16013" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fragments without locality can stack up on nodes</summary>
      <description>When no locality information is provide, task requests can stack up on a node because of consistent no selection. When locality information is not provided we should fallback to random selection for better work distribution.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-23 01:00:00" id="16015" opendate="2017-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some Tez INFO logs are too noisy II</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.llap-daemon-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-23 01:00:00" id="16023" opendate="2017-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong estimation for number of rows generated by IN expression</summary>
      <description>Code seems to be wrong, as we are factoring the number of rows to create the multiplying factor, instead of NDV for given column(s) and NDV in IN clause.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-24 01:00:00" id="16033" opendate="2017-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use PrintGCDateStamps for gc logging</summary>
      <description>This print human readable timestamps instead of timestamp relative to jvm startup</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-25 01:00:00" id="16038" opendate="2017-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables: fix (or disable) inferring buckets</summary>
      <description>The following tests on minimr produce diffs if all tables are changed to MM:infer_bucket_sort_dyn_partinfer_bucket_sort_num_bucketsinfer_bucket_sort_mergeinfer_bucket_sort_reducers_power_twoSome of these disable strict checks for bucketing load, which wouldn't work by design; the rest should work. Either that, or we should disable this for MM tables - seems like an obscure feature.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-27 01:00:00" id="16047" opendate="2017-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shouldn&amp;#39;t try to get KeyProvider unless encryption is enabled</summary>
      <description>Found lots of following errors in HS2 log:hdfs.KeyProviderCache: Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!Similar to HDFS-7931</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-28 01:00:00" id="16053" opendate="2017-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove newRatio from llap JAVA_OPTS_BASE</summary>
      <description>The G1GC is supposed to be able to resize regions as required. Setting the newRatio or other parameters which size the new gen disables this capability.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-28 01:00:00" id="16058" opendate="2017-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable falling back to non-cbo for SemanticException for tests</summary>
      <description>Currently optimizer falls back to non-cbo path if cbo path throws an exception of type SemanticException. This might be eclipsing some genuine issues within cbo-path.We would like to turn off the fall back mechanism for tests to see if there are indeed genuine issues/bugs within cbo path.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.wrong.column.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.scalar.multi.columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.corr.grandparent.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.with.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.acid.overwrite.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.complex.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.approx.23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.position.alias.test.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.jdbc.handler.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.windowing.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.views.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.unionDistinct.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.semijoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.gby.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">contrib.src.test.results.clientnegative.case.with.row.sequence.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-1 01:00:00" id="16072" opendate="2017-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add some additional jvm metrics for hadoop-metrics2</summary>
      <description>It will be helpful for debugging to expose some metrics like buffer pool, file descriptors etc. that are not exposed via Hadoop's JvmMetrics. We already a /jmx endpoint that gives out these info but we don't know the timestamp of allocations, number file descriptors to correlated with the logs. This will better suited for graphing tools.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-1 01:00:00" id="16076" opendate="2017-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP packaging - include aux libs</summary>
      <description>The old auxlibs (or whatever) should be packaged by default, if present.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-2 01:00:00" id="16087" opendate="2017-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove multi append of log4j.configurationFile in hive script</summary>
      <description>hive script appends -Dlog4j.configurationFile twice.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-2 01:00:00" id="16097" opendate="2017-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>minor fixes to metrics and logs in LlapTaskScheduler</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-3 01:00:00" id="16101" opendate="2017-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QTest failure BeeLine escape_comments after HIVE-16045</summary>
      <description>HIVE-16045 committed immediately after HIVE-14459, and added two extra lines to the output which is written there with another thread. We should remove these lines before comparing the out file</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.util.QFileClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-3 01:00:00" id="16104" opendate="2017-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: preemption may be too aggressive if the pre-empted task doesn&amp;#39;t die immediately</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-6 01:00:00" id="16117" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SortProjectTransposeRule should check for monotonicity preserving CAST</summary>
      <description>Due to CALCITE-1618, we need to move to new Calcite release to fix it.Due to this, SortProjectTransposeRule ignores CAST in the Project operator.For instance:...HiveSortLimit(sort0=$4,sort1=$2,dir0=ASC-nulls-first,dir1=DESC-nulls-last,fetch=10) HiveProject(robot=$0,_o__c1=$2,m=$3,s=$4,(tok_function tok_int (tok_table_or_col robot))=CAST($0):INTEGER))...will be transformed into:...HiveProject(robot=$0,_o__c1=$2,m=$3,s=$4,(tok_function tok_int (tok_table_or_col robot))=CAST($0):INTEGER)) HiveSortLimit(sort0=$0,sort1=$2,dir0=ASC-nulls-first,dir1=DESC-nulls-last,fetch=10)...which is incorrect.The problem seems to be in the permutation method in RelOptUtil, which is called in L87. The method actually considers a CAST on a reference as a valid column permutation of the column referenced; probably it should not.permutation is only called by this rule and UnionPullUpConstantsRule, thus it seems it is safe to fix the semantics of the method.</description>
      <version>2.2.0</version>
      <fixedVersion>2.2.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortProjectTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-6 01:00:00" id="16122" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE Hive Druid split introduced by HIVE-15928</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.HiveDruidSplit.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="16123" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Let user pick the granularity of bucketing and max in row memory</summary>
      <description>Currently we index the data with granularity of none which puts lot of pressure on the indexer.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="16124" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop the segments data as soon it is pushed to HDFS</summary>
      <description>Drop the pushed segments from the indexer as soon as the HDFS push is done.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-8 01:00:00" id="16148" opendate="2017-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: schema_evol_text_vec_table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.StringToDouble.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-4 01:00:00" id="1615" opendate="2010-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web Interface JSP needs Refactoring for removed meta store methods</summary>
      <description>Some meta store methods being called from JSP have been removed. Really should prioritize compiling jsp into servlet code again.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.show.databases.jsp</file>
      <file type="M">hwi.web.show.database.jsp</file>
      <file type="M">hwi.web.session.result.jsp</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-9 01:00:00" id="16152" opendate="2017-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBeeLineDriver logging improvements</summary>
      <description>During the review of HIVE-16127 we agreed, that it would be great to have improved logging and error messages during the TestBeeLineDriver run.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFileBeeLineClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-10 01:00:00" id="16166" opendate="2017-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 may still waste up to 15% of memory on duplicate strings</summary>
      <description>A heap dump obtained from one of our users shows that 15% of memory is wasted on duplicate strings, despite the recent optimizations that I made. The problematic strings just come from different sources this time. See the excerpt from the jxray (www.jxray.com) analysis attached.Adding String.intern() calls in the appropriate places reduces the overhead of duplicate strings with this workload to ~6%. The remaining duplicates come mostly from JDK internal and MapReduce data structures, and thus are more difficult to fix.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringInternUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-10 01:00:00" id="16167" opendate="2017-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove transitive dependency on mysql connector jar</summary>
      <description>Brought in by druid storage handler transitively.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-10 01:00:00" id="16170" opendate="2017-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude relocation of org.apache.hadoop.security.* in the JDBC standalone jar</summary>
      <description>There has been a use case that core-site.xml file is used along with the JDBC jar, which sets "hadoop.security.group.mapping" using the class names such as "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback". This will cause CNF errors due to the renaming. So we need to exclude those security related classes in the relocation part.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-10 01:00:00" id="16176" opendate="2017-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SchemaTool should exit with non-zero exit code when one or more validator&amp;#39;s fail.</summary>
      <description>Currently schematool exits with a code of 0 when one or more schema tool validation fail. Ideally, it should return a non-zero exit code when any of the validators fail.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-13 01:00:00" id="16189" opendate="2017-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table column stats might be invalidated in a failed table rename</summary>
      <description>If the table rename does not succeed due to its failure in moving the data to the new renamed table folder, the changes in TAB_COL_STATS are not rolled back which leads to invalid column stats.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.encryption.move.tbl.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-13 01:00:00" id="16190" opendate="2017-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support expression in merge statement</summary>
      <description>Right now, we only support atomExpression, rather than expression in values in MergeStatement.</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMergeStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-14 01:00:00" id="16212" opendate="2017-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables: suspicious ORC HDFS counter changes</summary>
      <description>org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_llap_counters1&amp;#93; (batchId=136)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_llap_counters&amp;#93; (batchId=139)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_ppd_basic&amp;#93; (batchId=136)org.apache.hadoop.hive.cli.TestMiniLlapCliDriver.testCliDriver&amp;#91;orc_ppd_schema_evol_3a&amp;#93; (batchId=137)HDFS counters for operation counts go up (which I can repro locally).</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-9-8 01:00:00" id="1622" opendate="2010-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use CombineHiveInputFormat for the merge job if hive.merge.mapredfiles=true</summary>
      <description>Currently map-only merge (using CombineHiveInputFormat) is only enabled for merging files generated by mappers. It should be used for files generated at readers as well.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.merge3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-17 01:00:00" id="16242" opendate="2017-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Run BeeLine tests parallel</summary>
      <description>Provide the ability for BeeLine tests to run parallel against the MiniHS2 cluster</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.qfile.QFile.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-20 01:00:00" id="16254" opendate="2017-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>metadata for values temporary tables for INSERTs are getting replicated during bootstrap</summary>
      <description>create table a (age int);insert into table a values (34),(4);repl dump default;there is a temporary table created as values_tmptable_&amp;#91;nmber&amp;#93;, which is also present in the dumped information with only metadata, this should not be processed.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.EventUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="16256" opendate="2017-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: TestCliDriver.testCliDriver[comments]</summary>
      <description>Test has been failing for 6 consecutive runs. Most recent:https://builds.apache.org/job/PreCommit-HIVE-Build/4245/testReport/Diff:147a148&gt; COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}167a169&gt; COLUMN_STATS_ACCURATE {"BASIC_STATS":"true","COLUMN_STATS":{"key":"true","value":"true"}}</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.updateAccessTime.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-21 01:00:00" id="16266" opendate="2017-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable function metadata to be written during bootstrap</summary>
      <description/>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-3-21 01:00:00" id="16273" opendate="2017-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Make non-column key expressions work in MERGEPARTIAL mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-22 01:00:00" id="16281" opendate="2017-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade master branch to JDK8</summary>
      <description>This is to track the JDK 8 upgrade work for the master branch.Here are threads for the discussion:https://lists.apache.org/thread.html/83d8235bc9547cc94a0d689580f20db4b946876b6d0369e31ea12b51@1460158490@%3Cdev.hive.apache.org%3Ehttps://lists.apache.org/thread.html/dcd57844ceac7faf8975a00d5b8b1825ab5544d94734734aedc3840e@%3Cdev.hive.apache.org%3EJDK7 is end of public update and some newer version of dependent libraries like jetty require newer JDK. Seems it's reasonable to upgrade to JDK8 in 2.x.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.build.properties</file>
      <file type="M">.travis.yml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-22 01:00:00" id="16282" opendate="2017-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin: Disable slow-start for the bloom filter aggregate task</summary>
      <description>The slow-start of the bloom filter vertex is a scheduling problem which causes more pre-emption than is useful.When the bloom filters are arranged as followsMap 1(10 tasks)&gt;Reducer 2(1 task)&gt;Map 3(100 tasks)Map 3 and Map 1 are immediately active since Reducer 2 -&gt; Map 3 is a broadcast edge.Once 3 tasks in Map 1 finish, the engine kills one active task from Map 3 to make room for Reducer 2.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-27 01:00:00" id="16305" opendate="2017-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM</summary>
      <description>This is a followup for HIVE-16160. We see additional ClassLoaderResolverImpl leaks even with the patch.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="16307" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add IO memory usage report to LLAP UI</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-13 01:00:00" id="1633" opendate="2010-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineHiveInputFormat fails with "cannot find dir for emptyFile"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-30 01:00:00" id="16334" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query lock contains the query string, which can cause OOM on ZooKeeper</summary>
      <description>When there are big number of partitions in a query this will result in a huge number of locks on ZooKeeper. Since the query object contains the whole query string this might cause serious memory pressure on the ZooKeeper services.It would be good to have the possibility to truncate the query strings that are written into the locks</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-30 01:00:00" id="16335" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline user HS2 connection file should use /etc/hive/conf instead of /etc/conf/hive</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clientssays: BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/conf/hive in that order.shouldn't it be?BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/hive/conf in that order?Most distributions I've used have a /etc/hive/conf dir.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-31 01:00:00" id="16347" opendate="2017-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastoreChecker should skip listing partitions which are not valid when hive.msck.path.validation is set to skip or ignore</summary>
      <description>HIVE-16299 improves msck query so that sub-directories which do not follow the partition column order as defined when table is created should not be added. It needs to skip these partitions when hive.msck.path.validation is not set to "throw". Currently it goes ahead and adds them.This was unfortunately found late in the review of HIVE-16299 and the patch was commit before it could be fixed.</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-18 01:00:00" id="16468" opendate="2017-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BeeLineDriver should be able to run tests against an externally created cluster</summary>
      <description>It should be possible to run the query tests against an externally created cluster using the BeeLineDriver, and the query files and results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-19 01:00:00" id="16482" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Ser/Des need to use dimension output name</summary>
      <description>Druid Ser/Desr need to use dimension output name in order to function with Extraction function.Some part of the Ser/Desr code uses the method DimensionSpec.getDimension() although when extraction function are in game the name of the dimension will be defined by DimensionSpec.getOutputName()</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-19 01:00:00" id="16483" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS should populate split related configurations to HiveConf</summary>
      <description>There are several split related configurations, such as MAPREDMINSPLITSIZE, MAPREDMINSPLITSIZEPERNODE, MAPREDMINSPLITSIZEPERRACK, etc., that should be populated to HiveConf. Currently we only do this for MAPREDMINSPLITSIZE.All the others, if not set, will be using the default value, which is 1.Without these, Spark sometimes will not merge small files for file formats such as text.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-19 01:00:00" id="16485" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable outputName for RS operator in explain formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.formatted.oid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explain.formatted.oid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AnnotateReduceSinkOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Vertex.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Stage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-21 01:00:00" id="16504" opendate="2017-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addition of binary licenses broke rat check</summary>
      <description>The clean up of Hive's licenses (HIVE-15035) broke the rat check, as all the license files get reported as invalid licenses. The rat check needs to be modified to ignore those files.</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-21 01:00:00" id="16505" opendate="2017-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "unknown" boolean truth value</summary>
      <description>according to the standard, boolean truth value might be: TRUE|FALSE|UNKNOWN.similar queries to the following should be supported:select 1 where null is unknown;select 1 where (select cast(null as boolean) ) is unknown;"unknown" behaves similarily to null. (null=null) is null "All boolean values and SQL truth values are comparable and all are assignable to a site of type boolean. The value True is greater than the value False, and any comparison involving the null value or an Unknown truth value will return an Unknown result. The values True and False may be assigned to any site having a boolean data type; assignment of Unknown, or the null value, is subject to the nullability characteristic of the target." Truth table for the AND boolean operatorAND True False UnknownTrue True False UnknownFalse False False FalseUnknown Unknown False UnknownTruth table for the OR boolean operatorOR True False UnknownTrue True True TrueFalse True False UnknownUnknown True Unknown UnknownTruth table for the IS boolean operatorIS TRUE FALSE UNKNOWNTrue True False FalseFalse False True FalseUnknown False False True </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-22 01:00:00" id="16511" opendate="2017-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO looses inner casts on constants of complex type</summary>
      <description>type for map &lt;10, cast(null as int)&gt; becomes map &lt;int,string&gt;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-26 01:00:00" id="16542" opendate="2017-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make merge that targets acid 2.0 table fail-fast</summary>
      <description>Until HIVE-14947 is fixed, need to add a check so that acid 2.0 tables are not written to by Merge stmt that has both Insert and Update clauses</description>
      <version>2.2.0</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2WithSplitUpdate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-11 01:00:00" id="16647" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the validation output to make the output to stderr and stdout more consistent</summary>
      <description>Some output are printed to stderr or stdout inconsistently. Here are some of them. Update to make them more consistent. Version table validation When the version table is missing, the err msg goes to stderr When the version table is not valid, the err msg goes to stdout with a message like "Failed in schema version validation: &lt;err_msg&gt; Metastore/schema table validation When the version table contains the wrong version or there are no rows in the version table, err msg goes to stderr When there diffs between the schema and metastore tables, the err msg goes to stdout</description>
      <version>2.2.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-26 01:00:00" id="16764" opendate="2017-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support numeric as same as decimal</summary>
      <description>for example numeric(12,2) -&gt; decimal(12,2) This will make Numeric reserved keyword</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.perf.query69.q</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query98.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query97.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query96.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query95.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query94.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query93.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query92.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query91.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query90.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query89.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query88.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query87.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query86.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query85.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query84.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query83.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query82.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query81.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query80.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query79.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query76.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query75.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query73.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query72.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query71.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query70.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query7.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query22.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query24.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query25.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query26.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query27.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query28.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query29.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query30.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query31.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query32.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query33.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query34.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query36.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query37.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query39.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query40.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query42.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query43.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query46.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query48.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query50.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query51.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query52.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query54.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query55.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query56.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query58.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query60.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query64.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query65.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query66.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query67.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query68.q</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-2 01:00:00" id="17233" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set "mapred.input.dir.recursive" for HCatInputFormat-based jobs.</summary>
      <description>This has to do with HIVE-15575. TezCompiler seems to set mapred.input.dir.recursive to true. This is acceptable for Hive jobs, since this allows Hive to consume its peculiar UNION ALL output, where the output of each relation is stored in a separate sub-directory of the output-dir.For such output to be readable through HCatalog (via Pig/HCatLoader), mapred.input.dir.recursive should be set from HCatInputFormat as well. Otherwise, one gets zero records for that input.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-5-19 01:00:00" id="1731" opendate="2010-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-25 01:00:00" id="17391" opendate="2017-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction fails if there is an empty value in tblproperties</summary>
      <description>create table t1 (a int) tblproperties ('serialization.null.format'='');alter table t1 compact 'major';fails</description>
      <version>2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StringableMap.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-1 01:00:00" id="17429" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-22 01:00:00" id="1743" opendate="2010-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-7 01:00:00" id="17473" opendate="2017-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement workload management pools</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-7 01:00:00" id="17475" opendate="2017-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable mapjoin using hint</summary>
      <description>Using hint disable mapjoin for a given query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-9-25 01:00:00" id="17601" opendate="2017-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling in LlapServiceDriver</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-26 01:00:00" id="17607" opendate="2017-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove ColumnStatsDesc usage from columnstatsupdatetask</summary>
      <description>it's not entirely connected to this task...it should either has its own descriptor; or work sould take on the: tablename/coltype/colname payload</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColumnStatsUpdateWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-1 01:00:00" id="1761" opendate="2010-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support show locks for a particular table</summary>
      <description>Currently, only show locks is supported - it would be very useful to show locks for a particular table</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lock2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lock1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lock2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lock1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-26 01:00:00" id="17610" opendate="2017-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: an exception in exception handling can hide the original exception</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-2 01:00:00" id="17664" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description/>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-4 01:00:00" id="1767" opendate="2010-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge files does not work with dynamic partition</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-10 01:00:00" id="17756" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable subquery related Qtests for Hive on Spark</summary>
      <description>HIVE-15456 and HIVE-15192 using Calsite to decorrelate and plan subqueries. This JIRA is to indroduce subquery test and verify the subqueries plan for Hive on Spark</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-12 01:00:00" id="17792" opendate="2017-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Bucket Map Join when there are extra keys other than bucketed columns</summary>
      <description>Currently this wont go through Bucket Map Join(BMJ)CREATE TABLE tab_part (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;CREATE TABLE tab(key int, value string) PARTITIONED BY(ds STRING) STORED AS TEXTFILE;select a.key, a.value, b.valuefrom tab a join tab_part b on a.key = b.key and a.value = b.value;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-8 01:00:00" id="18251" opendate="2017-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-12-19 01:00:00" id="18306" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spark smb tests</summary>
      <description>seems to me that TestSparkCliDriver#testCliDriver[auto_sortmerge_join_10] and TestSparkCliDriver#testCliDriver[bucketsortoptimize_insert_7] is failing since HIVE-18208 is in.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-11 01:00:00" id="18443" opendate="2018-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure git gc finished in ptest prep phase before copying repo</summary>
      <description>In ptest's prep phase script first we checkout the latest Hive code from git, and then we make copy of its contents (along .git folder) for that will serve as Yetus' working directory.In some cases we can see errors such as+ cp -R . ../yetuscp: cannot stat ?./.git/gc.pid?: No such file or directorye.g. hereThis is caused by git running its gc feature in the background when our prep script has already started copying. In cases where gc finishes while cp is running, we'll get this error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-30 01:00:00" id="19083" opendate="2018-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-25 01:00:00" id="19306" opendate="2018-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow batch serializer</summary>
      <description>Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-3-28 01:00:00" id="22673" opendate="2019-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in contrib Package</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-8-15 01:00:00" id="8472" opendate="2014-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ALTER DATABASE SET LOCATION</summary>
      <description>Similarly to ALTER TABLE tablename SET LOCATION, it would be helpful if there was an equivalent for databases.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>