<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  <bug fixdate="2015-3-26 01:00:00" id="10099" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for Decimal</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-2 01:00:00" id="101" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add various files and directories to svn:ignore</summary>
      <description>When creating patches or committing code it's nice to know that certain directories and files will never be included.I suggest we add the following to the svn:ignore variable (for more information see: http://svnbook.red-bean.com/en/1.1/ch07s02.html):build.classpath.project.settings</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-23 01:00:00" id="1010" opendate="2009-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INFORMATION_SCHEMA in Hive</summary>
      <description>INFORMATION_SCHEMA is part of the SQL92 standard and would be useful to implement using our metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.jdbc.handler.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.InputEstimatorTestClass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">jdbc-handler.src.test.java.org.apache.hive.config.JdbcStorageConfigManagerTest.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcStorageHandler.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcSerDe.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcRecordReader.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputFormat.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.MySqlDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.JdbcRecordIterator.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.JdbcStorageConfigManager.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.DatabaseType.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-30 01:00:00" id="10140" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-1-28 01:00:00" id="1015" opendate="2009-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Java MapReduce wrapper for TRANSFORM/MAP/REDUCE scripts</summary>
      <description>Larry Ogrodnek has written a set of wrapper classes that make it possibleto write Hive TRANSFORM/MAP/REDUCE scripts in Java in a style thatmore closely resembles conventional Hadoop MR programs.A blog post describing this library can be found here: http://dev.bizo.com/2009/10/hive-map-reduce-in-java.htmlThe source code (with Apache license) is available here: http://github.com/ogrodnek/shmrjWe should add this to contrib.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-30 01:00:00" id="10150" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>delete from acidTbl where a in(select a from nonAcidOrcTbl) fails</summary>
      <description>this query raises an error "10297,FAILED: SemanticException &amp;#91;Error 10297&amp;#93;: Attempt to do update or delete on table nonAcidOrcTbl that does not use an AcidOutputFormat or is not bucketed"even though nonAcidOrcTbl is only being read, not written.select b from " + Table.ACIDTBL + " where a in (select b from " + Table.NONACIDORCTBL + ")runs fine.There doesn't seem to be any logical reason why we should rise the error here.Same for 'update' statement.</description>
      <version>1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-31 01:00:00" id="10167" opendate="2015-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 logs the server started only before the server is shut down</summary>
      <description>TThreadPoolServer#serve() blocks till the server is down. We should log before that.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-1 01:00:00" id="10177" opendate="2015-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for char &amp; varchar</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-8 01:00:00" id="10263" opendate="2015-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Aggregate checking input for bucketing should be conditional</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-10 01:00:00" id="10304" opendate="2015-4-10 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add deprecation message to HiveCLI</summary>
      <description>As Beeline is now the recommended command line tool to Hive, we should add a message to HiveCLI to indicate that it is deprecated and redirect them to Beeline. This is not suggesting to remove HiveCLI for now, but just a helpful direction for user to know the direction to focus attention in Beeline.</description>
      <version>1.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-13 01:00:00" id="10326" opendate="2015-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Invoke Hive&amp;#39;s Cumulative Cost</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-14 01:00:00" id="10328" opendate="2015-4-14 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Enable new return path for cbo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorConditionalExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnColumn.txt</file>
      <file type="M">itests.hive-jmh.src.main.java.org.apache.hive.benchmark.vectorization.VectorizationBench.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-23 01:00:00" id="10451" opendate="2015-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTF deserializer fails if values are not used in reducer</summary>
      <description>In this particular case no values are needed from reducer to complete processing.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-23 01:00:00" id="10455" opendate="2015-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Different data types at Reducer before JoinOp</summary>
      <description>The following error occured for cbo_subq_not_in.q java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error: Unable to deserialize reduce input key from x1x128x0x0x1 with properties {columns=reducesinkkey0, serialization.lib=org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe, serialization.sort.order=+, columns.types=double} at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462) at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)A more easier way to reproduce is set hive.cbo.enable=true;set hive.exec.check.crossproducts=false;set hive.stats.fetch.column.stats=true;set hive.auto.convert.join=false;select p_size, src.keyfrom part join srcon p_size=key;As you can see, p_size is integer while src.key is string. Both of them should be cast to double when they join. When return path is off, this will happen before Join, at RS. However, when return path is on, this will be considered as an expression in Join. Thus, when reducer is collecting different types of keys from different join branches, it throws exception.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortExchange.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-23 01:00:00" id="10468" opendate="2015-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create scripts to do metastore upgrade tests on jenkins for Oracle DB.</summary>
      <description>This JIRA is to isolate the work specific to Oracle DB in HIVE-10239. Because of absence of 64 bit debian packages for oracle-xe, the apt-get install fails on the AWS systems.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.dbs.oracle.prepare.sh</file>
      <file type="M">metastore.dbs.oracle.execute.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-29 01:00:00" id="10533" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join to MultiJoin support for outer joins</summary>
      <description>CBO return path: auto_join7.q can be used to reproduce the problem.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-29 01:00:00" id="10535" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Cleanup map join cache when a query completes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapObjectCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.StreamUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.CacheChunk.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedTreeReaderFactory.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="10542" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Full outer joins in tez produce incorrect results in certain cases</summary>
      <description>If there is no records for one of the tables in the full outer join, we do not read the other input and end up not producing rows which we should be.</description>
      <version>1.0.0,1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mergejoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-1 01:00:00" id="10568" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-1 01:00:00" id="10576" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add jar command does not work with Windows OS</summary>
      <description>Steps to reproduce this issue in Windows OS:hadoop.cmd fs -mkdir -p /tmp/testjarshadoop.cmd fs copyFromLocal &lt;hive-hcatalog-core*.jar&gt; /tmp/testjarsfrom hive cli:add jar hdfs:///tmp/testjars/hive-hcatalog-core-*.jar;add jar D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;hive&gt; add jar hdfs:///tmp/testjars/hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;converting to local hdfs:///tmp/testjars/hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarIllegal character in opaque part at index 2: C:\Users\hadoopqa\AppData\Local\Temp\cf0c70a4-f8e5-43ae-8c94-aa528f90887d_resources\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarQuery returned non-zero code: 1, cause: java.net.URISyntaxException: Illegal character in opaque part at index 2: C:\Users\hadoopqa\AppData\Local\Temp\cf0c70a4-f8e5-43ae-8c94-aa528f90887d_resources\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarhive&gt; add jar D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar;Illegal character in opaque part at index 2: D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jarQuery returned non-zero code: 1, cause: java.net.URISyntaxException: Illegal character in opaque part at index 2: D:\hdp\hive-1.2.0.2.3.0.0-1737\hcatalog\share\hcatalog\hive-hcatalog-core-1.2.0.2.3.0.0-1737.jar</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-7 01:00:00" id="10639" opendate="2015-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create SHA1 UDF</summary>
      <description>Calculates an SHA-1 160-bit checksum for the string and binary, as described in RFC 3174 (Secure Hash Algorithm). The value is returned as a string of 40 hex digits, or NULL if the argument was NULL.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-12 01:00:00" id="10682" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-6-14 01:00:00" id="10705" opendate="2015-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update tests for HIVE-9302 after removing binaries</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveTestUtils.java</file>
      <file type="M">beeline.src.test.resources.postgresql-9.3.jdbc3.jar</file>
      <file type="M">beeline.src.test.resources.DummyDriver-1.0-SNAPSHOT.jar</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-6-19 01:00:00" id="10748" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace StringBuffer with StringBuilder where possible</summary>
      <description>I found 40 places in Hive where "new StringBuffer(" is used."Where possible, it is recommended that StringBuilder be used in preference to StringBuffer as it will be faster under most implementations"https://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveVarchar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.tez.TezJsonParser.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-27 01:00:00" id="10835" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="10868" opendate="2015-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release note for 1.2.0 and 1.1.0</summary>
      <description>It's recently found that Hive's release notes don't contain all JIRAs fixed. This happened due to a lack of correct or missing fix version in a JIRA. A large chunk of such JIRAs are due to the fact that their fix versions didn't get updated when a merge from feature branch to trunk (master). This JIRA is to fix such JIRAs related to Hive on Spark work.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="10896" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-13 01:00:00" id="10999" opendate="2015-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark dependency to 1.4 [Spark Branch]</summary>
      <description>Spark 1.4.0 is release. Let's update the dependency version from 1.3.1 to 1.4.0.</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-9-26 01:00:00" id="11132" opendate="2015-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries using join and group by produce incorrect output when hive.auto.convert.join=false and hive.optimize.reducededuplication=true</summary>
      <description>Queries using join and group by produce multiple output rows with the same key when hive.auto.convert.join=false and hive.optimize.reducededuplication=true. This interaction between configuration parameters is unexpected and should be well documented at the very least and should likely be considered a bug.e.g. hive&gt; set hive.auto.convert.join = false;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 79XYZ 74XYZ 297XYZ 66hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 516</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-2-2 01:00:00" id="1121" opendate="2010-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombinedHiveInputFormat for hadoop 19</summary>
      <description>Creating the jira from the mail from Roberto Congiu &amp;#91;roberto.congiu@openx.org&amp;#93;</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.19.java.org.apache.hadoop.hive.shims.Hadoop19Shims.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-8 01:00:00" id="11213" opendate="2015-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: more out file changes compared to master</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-9 01:00:00" id="11214" opendate="2015-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into ACID table switches vectorization off</summary>
      <description>PROBLEM:vectorization is switched off automatically after run insert into ACID table.STEPS TO REPRODUCE:set hive.vectorized.execution.enabled=true;create table testv (id int, name string) clustered by (id) into 2 buckets stored as orc tblproperties("transactional"="true");insert into testv values(1,'a');set hive.vectorized.execution.enabled;false</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-14 01:00:00" id="11250" opendate="2015-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change in spark.executor.instances (and others) doesn&amp;#39;t take effect after RSC is launched for HS2 [Spark Brnach]</summary>
      <description>Hive CLI works as expected.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.LocalSparkJobStatus.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-14 01:00:00" id="11252" opendate="2015-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): DUMMY project in plan</summary>
      <description>When the return path is on, we might end up with a Project with DUMMY column in the plan; thus, we need to run the ProjectMergeRule after the column trimmer runs for the second time. To reproduce it, we can run interval_udf.q with the return path on.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-14 01:00:00" id="11254" opendate="2015-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Process result sets returned by a stored procedure</summary>
      <description>Stored procedure can return one or more result sets. A caller should be able to process them.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.queries.local.exception5.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception4.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception3.sql</file>
      <file type="M">hplsql.src.test.queries.local.exception2.sql</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Query.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-16 01:00:00" id="11282" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Inferring Hive type char/varchar of length zero which is not allowed</summary>
      <description>When RT is on, we try to infer the Hive type from the Calcite type for the value '’ e.g. in udf3.q, and we end up with char (length=0) as a result. The min length of char/varchar in Hive is 1, thus an Exception is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2008-12-4 01:00:00" id="114" opendate="2008-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop partition should not delete data for external tables</summary>
      <description>As a continuation of HIVE-86, dropping partitions in an external table shouldn't delete the underlying data</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-8 01:00:00" id="1140" opendate="2010-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect ambiguous column reference error message</summary>
      <description>Whenever there is an ambiguous column name reference, the error message does not reference the proper column.hive&gt; FROM (SELECT key, concat(value) AS key FROM src) a SELECT a.key;FAILED: Error in semantic analysis: line 1:25 Ambiguous Column Reference value</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-31 01:00:00" id="11424" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rule to transform OR clauses into IN clauses in CBO</summary>
      <description>We create a rule that will transform OR clauses into IN clauses (when possible).</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.semijoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-6 01:00:00" id="11484" opendate="2015-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-2-11 01:00:00" id="11526" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="11607" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export tables broken for data &gt; 32 MB</summary>
      <description>Broken for both hadoop-1 as well as hadoop-2 line</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.0.20S.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-23 01:00:00" id="11623" opendate="2015-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): fix the tableAlias for ReduceSink operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-31 01:00:00" id="11694" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hbase-metastore for hadoop-1</summary>
      <description>hbase-metastore doesn't compile for hadoop-1 and we don't have development plan to make it work with hadoop-1. Exclude hbase-metastore related file so hadoop-1 still compiles.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-9-14 01:00:00" id="11810" opendate="2015-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Exception is ignored if MiniLlap cluster fails to start</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-18 01:00:00" id="11892" opendate="2015-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDTF run in local fetch task does not return rows forwarded during GenericUDTF.close()</summary>
      <description>Using the example UDTF GenericUDTFCount2, which is part of hive-contrib:create temporary function udtfCount2 as 'org.apache.hadoop.hive.contrib.udtf.example.GenericUDTFCount2';set hive.fetch.task.conversion=minimal;-- Task created, correct output (2 rows)select udtfCount2() from src;set hive.fetch.task.conversion=more;-- Runs in local task, incorrect output (0 rows)select udtfCount2() from src;</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.dummy.source.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-1-4 01:00:00" id="119" opendate="2008-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add more informative error messages to grammar parsing</summary>
      <description>Some error messages give the user no context or help really to know what part of their stmt is wrong.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.create.tbl2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-3-24 01:00:00" id="1194" opendate="2010-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sorted merge join</summary>
      <description>If the input tables are sorted on the join key, and a mapjoin is being performed, it is useful to exploit the sorted properties of the table.This can lead to substantial cpu savings - this needs to work across bucketed map joins also.Since, sorted properties of a table are not enforced currently, a new parameter can be added to specify to use the sort-merge join.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-6 01:00:00" id="12042" opendate="2015-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update some out files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-7 01:00:00" id="12058" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive script to record errors when calling hbase fails</summary>
      <description>By default hive will try to find out which jars need to be added to the classpath in order to run MR jobs against an HBase cluster, however if hbase can't be found or if hbase mapredcp fails, the hive script will fail silently and ignore some of the jars to be included into the. That makes very difficult to analyze the real problem.Hive script should record the error not just simply redirect two hbase failures:HBASE_BIN=${HBASE_BIN:-"$(which hbase 2&gt;/dev/null)"}$HBASE_BIN mapredcp 2&gt;/dev/null</description>
      <version>0.14.0,1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-7 01:00:00" id="12059" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up reference to deprecated constants in AvroSerdeUtils</summary>
      <description>AvroSerdeUtils contains several deprecated String constants that are used by other Hive modules. Those should be cleaned up.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-7 01:00:00" id="12060" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: create separate variable for llap tests</summary>
      <description>No real reason to just reuse tez one; also needed to parallelize the tests</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-11-3 01:00:00" id="12327" opendate="2015-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e tests TestJob_1 and TestJob_2 fail</summary>
      <description>The tests are added in HIVE-7035. Both are negative tests and check if the http status code is 400. The original patch capture the exception containing specific message. However, in latter version of Hadoop, the message change so the exception is not contained.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-9 01:00:00" id="12372" opendate="2015-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve to support the multibyte character at lpad and rpad</summary>
      <description>The current lpad and rpad don't support the multibyte character at "str" and "pad".For example, we can see the following result.hive&gt; select name from sample1;OKtokyoＴＯＫＹＯhive&gt; select lpad(name, 20, '*') from sample1;OK***************tokyo*****ＴＯＫＹＯThis is improved as follows.hive&gt; select lpad(name, 20, '*') from sample1;***************tokyo***************ＴＯＫＹＯ</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBasePad.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-21 01:00:00" id="12721" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UUID built in function</summary>
      <description>A UUID function would be very useful for ETL jobs that need to generate surrogate keys.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-21 01:00:00" id="12723" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats_filemetadata test was added to the wrong driver</summary>
      <description>HBase metastore is only used in MiniTez, but the test somehow got added to MiniLlap</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-12 01:00:00" id="12850" opendate="2016-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes after changes made in TEZ-2669 and TEZ-3024</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.SourceStateTracker.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-13 01:00:00" id="12857" opendate="2016-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: modify the decider to allow using LLAP with whitelisted UDFs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-3-28 01:00:00" id="1286" opendate="2010-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove debug message from stdout in ColumnarSerDe</summary>
      <description>'Found class for org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe'should go to stderr where other informational messages are sent.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-14 01:00:00" id="12875" opendate="2016-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Verify sem.getInputs() and sem.getOutputs()</summary>
      <description>For every partition entity object present in sem.getInputs() and sem.getOutputs(), we must verify the appropriate Table in the list of Entities.</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.1.2,1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-18 01:00:00" id="12885" opendate="2016-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LDAP Authenticator improvements</summary>
      <description>Currently Hive's LDAP Atn provider assumes certain defaults to keep its configuration simple. 1) One of the assumptions is the presence of an attribute "distinguishedName". In certain non-standard LDAP implementations, this attribute may not be available. So instead of basing all ldap searches on this attribute, getNameInNamespace() returns the same value. So this API is to be used instead.2) It also assumes that the "user" value being passed in, will be able to bind to LDAP. However, certain LDAP implementations, by default, only allow the full DN to be used, just short user names are not permitted. We will need to be able to support short names too when hive configuration only has "BaseDN" specified (not userDNPatterns). So instead of hard-coding "uid" or "CN" as keys for the short usernames, it probably better to make this a configurable parameter.</description>
      <version>1.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-22 01:00:00" id="12907" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading - II</summary>
      <description>Remove unnecessary calls to metastore.</description>
      <version>0.14.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-22 01:00:00" id="12908" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading III</summary>
      <description>Remove unnecessary Namenode calls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-27 01:00:00" id="12941" opendate="2016-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unexpected result when using MIN() on struct with NULL in first field</summary>
      <description>Using MIN() on struct with NULL in first field of a row yields NULL as result.Example:select min(a) FROM (select 1 as a union all select 2 as a union all select cast(null as int) as a) tmp;OK_c01As expected. But if we wrap it in a struct:select min(a) FROM (select named_struct("field",1) as a union all select named_struct("field",2) as a union all select named_struct("field",cast(null as int)) as a) tmp;OK_c0NULLUsing MAX() works as expected for structs.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-22 01:00:00" id="13118" opendate="2016-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add some logging to LLAP token related paths</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-29 01:00:00" id="13183" opendate="2016-2-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>More logs in operation logs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskRunner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.StreamPrinter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-29 01:00:00" id="13184" opendate="2016-2-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: DAG credentials (e.g. HBase tokens) are not passed to the tasks in Tez plugin</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-5-29 01:00:00" id="1331" opendate="2010-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>select * does not work if different partitions contain different formats</summary>
      <description>Will try to come up with a concrete test - but looks like we are using the table's input format</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.wise.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-29 01:00:00" id="13373" opendate="2016-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use most specific type for numerical constants</summary>
      <description>tinyint &amp; shortint are currently inferred as ints, if they are without postfix.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.type.widening.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-3-29 01:00:00" id="13379" opendate="2016-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-12851 args do not work (slider-keytab-dir, etc.)</summary>
      <description>I've no idea how they ever worked. But I'm pretty sure they did... go figure.</description>
      <version>None</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-30 01:00:00" id="13380" opendate="2016-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Decimal should have lower precedence than double in type hierachy</summary>
      <description>Currently its other way round. Also, decimal should be lower than float.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query32.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.table.cascade.q</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.partition.change.col.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-13 01:00:00" id="13502" opendate="2016-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesnt support session parameters in JDBC URL as documentation states.</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clients#HiveServer2Clients-ConnectionURLsdocuments that sessions variables like credentials etc are accepted as part of the URL. However, Beeline does not support such URLs today.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-13 01:00:00" id="13505" opendate="2016-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip running TestDummy where possibe during precommit builds</summary>
      <description>On the main Hive build - this does nothing. There are some tests named TestDummy under qtests - I'm not sure they do anything useful though.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-15 01:00:00" id="13525" opendate="2016-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS hangs when job is empty</summary>
      <description>Observed in local tests. This should be the cause of HIVE-13402.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-2 01:00:00" id="13666" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP Provide the log url for a task attempt to display on the UI</summary>
      <description>The log url needs to be provided for task attempts, to display on the Tez UI associated with a query.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-2 01:00:00" id="13667" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve performance for ServiceInstanceSet.getByHost</summary>
      <description>ServiceInstanceSet.getByHost is used for scheduling local tasks as well as constructing the log URL.It ends up traversing all hosts on each lookup. This should be avoided.cc prasanth_j</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-2 01:00:00" id="13669" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: io.enabled config is ignored on the server side</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-25 01:00:00" id="1367" opendate="2010-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cluster by multiple columns does not work if parenthesis is present</summary>
      <description>The following query:select ... from src cluster by (key, value)throws a compile error:whereas the queryselect ... from src cluster by key, valueworks fine</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-12 01:00:00" id="13749" opendate="2016-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in Hive Metastore</summary>
      <description>Looking a heap dump of 10GB, a large number of Configuration objects(&gt; 66k instances) are being retained. These objects along with its retained set is occupying about 95% of the heap space. This leads to HMS crashes every few days.I will attach an exported snapshot from the eclipse MAT.</description>
      <version>1.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-24 01:00:00" id="14091" opendate="2016-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>some errors are not propagated to LLAP external clients</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-12 01:00:00" id="14218" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: ACL validation fails if the user name is different from principal user name</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-12 01:00:00" id="14219" opendate="2016-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external client on secure cluster: Protocol interface org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol is not known</summary>
      <description>2016-07-07T23:10:35,249 INFO [TaskHeartbeatThread[]]: task.TezTaskRunner2 (:()) - TaskReporter reporter error which will cause the task to failorg.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): Protocol interface org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol is not known. at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1551) at org.apache.hadoop.ipc.Client.call(Client.java:1495) at org.apache.hadoop.ipc.Client.call(Client.java:1395) at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:241) at com.sun.proxy.$Proxy39.heartbeat(Unknown Source) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.heartbeat(LlapTaskReporter.java:280) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.call(LlapTaskReporter.java:202) at org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter$HeartbeatCallable.call(LlapTaskReporter.java:139) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-12-15 01:00:00" id="1466" opendate="2010-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add NULL DEFINED AS to ROW FORMAT specification</summary>
      <description>NULL values are passed to transformers as a literal backslash and a literal N. NULL values are saved when INSERT OVERWRITing LOCAL DIRECTORies as "NULL". This is inconsistent.The ROW FORMAT specification of tables should be able to specify the manner in which a null character is represented. ROW FORMAT NULL DEFINED AS '\N' or '\003' or whatever should apply to all instances of table export and saving.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-12 01:00:00" id="14737" opendate="2016-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem accessing /logs in a Kerberized Hive Server 2 Web UI</summary>
      <description>The /logs menu fails with error &amp;#91;1&amp;#93; when the cluster is Kerberized. Other menu items are working properly.&amp;#91;1&amp;#93; HTTP ERROR: 401Problem accessing /logs/. Reason: Unauthenticated users are not authorized to access this page.Powered by Jetty://</description>
      <version>1.1.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-26 01:00:00" id="15064" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix explain for MM tables - don&amp;#39;t output for non-MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="15322" opendate="2016-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skipping "hbase mapredcp" in hive script for certain services</summary>
      <description>"hbase mapredcp" is intended to append hbase classpath to hive. However, the command can take some time when the system is heavy loaded. In some extreme cases, we saw ~20s delay due to it. For certain commands, such as "schemaTool", hbase classpath is certainly useless, and we can safely skip invoking it.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="15323" opendate="2016-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow the user to turn off reduce-side SMB join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-1-20 01:00:00" id="15674" opendate="2017-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more setOp tests to HivePerfCliDriver</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query87.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query87.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-17 01:00:00" id="15964" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-2 01:00:00" id="16090" opendate="2017-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Addendum to HIVE-16014</summary>
      <description>HIVE-16014 changed the HiveMetastoreChecker to use METASTORE_FS_HANDLER_THREADS_COUNT for pool size. Some of the tests in TestHiveMetastoreChecker still use HIVE_MOVE_FILES_THREAD_COUNT which leads to incorrect test behavior.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-18 01:00:00" id="16471" opendate="2017-4-18 00:00:00" resolution="Resolved">
    <buginformation>
      <summary>Add metrics for "waiting compilation time"</summary>
      <description>When parallel compilation is off, there could be multiple queries waiting on the compilation lock. Currently we have metrics for the # of waiting queries, but it's good to also track the average/min/max waiting time, etc.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.log.PerfLogger.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-18 01:00:00" id="16473" opendate="2017-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive-on-Tez may fail to write to an HBase table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-19 01:00:00" id="16721" opendate="2017-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent behavior in dealing with Timestamp stats</summary>
      <description>HIVE-15003 added support for additional types for col stats. However, it treats timestamp as DateColumnStatsData whereas when we read the timestamp stats, we read as LongColumnStatsData (https://github.com/apache/hive/blob/master/metastore/src/java/org/apache/hadoop/hive/metastore/StatObjectConverter.java#L229). We should make it consistent with original hive behavior</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-5 01:00:00" id="1691" opendate="2010-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ANALYZE TABLE command should check columns in partition spec</summary>
      <description>ANALYZE TABEL PARTITION (col1, col2,...) should check whether col1, col2 etc are partition columns.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-8-5 01:00:00" id="17256" opendate="2017-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a notion of a guaranteed task to LLAP</summary>
      <description>Tasks are basically on two levels, guaranteed and speculative, with speculative being the default. As long as noone uses the new flag, the tasks behave the same.All the tasks that do have the flag also behave the same with regard to each other.The difference is that a guaranteed task is always higher priority, and preempts, a speculative task.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.FirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-9-5 01:00:00" id="17452" opendate="2017-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL function variable block is not initialized</summary>
      <description>Variable inside declaration block are not initialized:CREATE FUNCTION test1() RETURNS STRINGAS ret string DEFAULT 'Initial value';BEGIN print(ret); ret := 'VALUE IS SET'; print(ret);END;test1();Output:ret VALUE IS SETShould be:Initial value VALUE IS SET</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.create.function4.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.function3.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.test.queries.local.create.procedure4.sql</file>
      <file type="M">hplsql.src.test.queries.local.create.function5.sql</file>
      <file type="M">hplsql.src.test.queries.db.summary.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-23 01:00:00" id="17590" opendate="2017-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade hadoop to 2.8.1</summary>
      <description>seems like hadoop 2.8.0 has no source attachment:http://central.maven.org/maven2/org/apache/hadoop/hadoop-common/2.8.0/howeverhttp://central.maven.org/maven2/org/apache/hadoop/hadoop-common/2.8.1/has source.jar-s</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-27 01:00:00" id="18814" opendate="2018-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Add Partition For Acid tables</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/LanguageManual%2BDDL#LanguageManualDDL-AddPartitionsAdd Partition command creates a Partition metadata object and sets the location to the directory containing data files.In current master (Hive 3.0), Add partition on an acid table doesn't fail and at read time the data is decorated with row__id but the original transaction is 0.  I suspect in earlier Hive versions this will throw or return no data.Since this new partition didn't have data before, assigning txnid:0 isn't going to generate duplicate IDs but it could violate Snapshot Isolation in multi stmt txns. Suppose txnid:7 runs select * from T. Then txnid:8 adds a partition to T. Now if txnid:7 runs the same query again, it will see the data in the new partition.This can't be release like this since a delete on this data (added via Add partition) will use row_ids with txnid:0 so a later upgrade that sees un-compacted may generate row_ids with different txnid (assuming this is fixed by then) One option is follow Load Data approach and create a new delta_x_x/ and move/copy the data there. Another is to allocate a new writeid and save it in Partition metadata.  This could then be used to decorate data with ROW__IDs.  This avoids move/copy but retains data "outside" of the table tree which make it more likely that this data will be modified in some way which can really break things if done after and SQL update/delete on this data have happened.  It performs no validations on add (except for partition spec) so any file with any format can be added. It allows add to bucketed tables as well.Seems like a very dangerous command. Maybe a better option is to block it and advise using Load Data. Alternatively, make this do Add partition metadata op followed by Load Data.   </description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-27 01:00:00" id="18815" opendate="2018-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unused feature in HPL/SQL</summary>
      <description>Remove FTP feature in HPL/SQL.</description>
      <version>None</version>
      <fixedVersion>2.3.3,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.db.copy.from.ftp.out.txt</file>
      <file type="M">hplsql.src.test.queries.db.copy.from.ftp.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Ftp.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-8 01:00:00" id="19833" opendate="2018-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce LLAP IO min allocation to match ORC variable CB size</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-26 01:00:00" id="19993" opendate="2018-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Using a table alias which also appears as a column name is not possible</summary>
      <description>drop table if exists tableA;drop table if exists tableB;create table tableA (a integer,z integer);create table tableB (a integer,b integer,z integer);select a.z, b.b from tableB as b JOIN tableA as aon a.a=b.b;Error: Error while compiling statement: FAILED: SemanticException Column a Found in more than One Tables/Subqueries (state=42000,code=40000)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="19994" opendate="2018-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Impala "drop table" fails with Hive Metastore exception</summary>
      <description>"drop table" statement in Impala shell fails with the following exception:ImpalaRuntimeException: Error making 'dropTable' RPC to Hive Metastore: CAUSED BY: MetaException: One or more instances could not be deleted Metastore log file shows that "DELETE FROM `PARTITION_KEYS` WHERE `TBL_ID`=?" statement fails because of foreign key violation (full stacktrace will be added):Caused by: java.sql.BatchUpdateException: Cannot delete or update a parent row: a foreign key constraint fails ("hivemetastore_emtig3vtq7qp1tiooo07sb70ud"."COLUMNS_V2", CONSTRAINT "COLUMNS_V2_FK1" FOREIGN KEY ("CD_ID") REFERENCES "CDS" ("CD_ID")) The table is created and then dropped as a part of ETL process executed every hour. Most of the time it works fine, the issue is not reproducible at will.Table creation script is:CREATE TABLE IF NOT EXISTS price_advisor_ouput.t_switching_coef_source{{( }}...fields here...PRIMARY KEY (...PK field here...))PARTITION BY HASH(matrix_pcd) PARTITIONS 3STORED AS KUDU; Not sure how to approach diagnostics and fix, so any input will be really appreciated. Thanks in advance, Rodion Myronov</description>
      <version>1.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-5 01:00:00" id="20093" opendate="2018-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LlapOutputFomatService: Use ArrowBuf with Netty for Accounting</summary>
      <description>Combining Unpooled.wrappedBuffer with Arrow buffers can create corrupted buffers from buffer reuse race-condition.This change ensures Arrow memory to be accounted by the same BufferAllocator.RootAllocator will return an ArrowBuf which cooperates with Arrow memory arrow accounting after Netty release(1) the buffer.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.WritableByteChannelAdapter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-12-30 01:00:00" id="20987" opendate="2018-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split Druid Tests to avoid Timeouts</summary>
      <description>Currently Druid Tests fail with Timeout issue.I am plaining on splitting the test into 2 batches at least to avoid timeouts.I will tweak the test code to pick random Druid nodes ports like that minimize the collision issue that we saw before.  </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.SingleNodeKafkaCluster.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.MiniDruidCluster.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.ForkingDruidNode.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2009-9-23 01:00:00" id="853" opendate="2009-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a hint to select which tables to stream in a join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.rc.seq.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.uniquejoin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.union2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.load.wrong.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8530" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Preserve types of literals</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-10-15 01:00:00" id="877" opendate="2009-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>error with cast(null as short)</summary>
      <description>also cast(null as long)</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.negative.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-16 01:00:00" id="8892" opendate="2014-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use MEMORY_AND_DISK for RDD caching [Spark Branch]</summary>
      <description>In HIVE-8844, we made the persistent policy for RDD caching configurable. We should do something simpler and don't add additional configurations.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-16 01:00:00" id="9118" opendate="2014-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support auto-purge for tables, when dropping tables/partitions.</summary>
      <description>HIVE-7100 introduced a way to skip the trash directory, when deleting table-data, while dropping tables.In HIVE-9083/HIVE-9086, I extended this to work when partitions are dropped.Here, I propose a table-parameter ("auto.purge") to set up tables to skip-trash when table/partition data is deleted, without needing to say "PURGE" on the Hive CLI. Apropos, on dropTable() and dropPartition(), table data is deleted directly (and not moved to trash) if the following hold true: The table is MANAGED. The deleteData parameter to the HMSC.drop*() methods is true. Either PURGE is explicitly specified on the command-line (or rather, "ifPurge" is set in the environment context, OR TBLPROPERTIES contains "auto.purge"="true"</description>
      <version>1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-28 01:00:00" id="9499" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.limit.query.max.table.partition makes queries fail on non-partitioned tables</summary>
      <description>If you use hive.limit.query.max.table.partition to limit the amount of partitions that can be queried it makes queries on non-partitioned tables fail.Example:CREATE TABLE tmp(test INT);SELECT COUNT(*) FROM TMP; -- works fineSET hive.limit.query.max.table.partition=20;SELECT COUNT(*) FROM TMP; -- generates NPE (FAILED: NullPointerException null)SET hive.limit.query.max.table.partition=-1;SELECT COUNT(*) FROM TMP; -- works fine again</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-2 01:00:00" id="95" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve cli error messages by lowering backtracking to 1</summary>
      <description>Stop antlr from backtracking so much should (and does) improve error messages since antlr will report the error closer to where it happened.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-29 01:00:00" id="9513" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-31 01:00:00" id="9527" opendate="2015-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include dot files in tarball</summary>
      <description>Ideally the source tarball exactly matches the svn tag. On item that is missing is the dot files.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-1 01:00:00" id="9538" opendate="2015-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude thirdparty directory from tarballs</summary>
      <description/>
      <version>spark-branch,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-2 01:00:00" id="9549" opendate="2015-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include missing directories in source tarball</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-2 01:00:00" id="9552" opendate="2015-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge trunk to Spark branch 2/2/2015 [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ql.rewrite.gbtoidx.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-2 01:00:00" id="9554" opendate="2015-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename 0.15 upgrade scripts to 1.1</summary>
      <description/>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.14.0-to-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.14.0-to-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.14.0-to-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.14.0-to-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.14.0-to-0.15.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.15.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-2-8 01:00:00" id="9610" opendate="2015-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Continuation of HIVE-9438 - The standalone-jdbc jar missing some classes</summary>
      <description>We've not had success only including specific shim classes as part of the standalone jdbc jar. Since all shim classes shouldn't be too large we'll include them all.</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-9 01:00:00" id="9621" opendate="2015-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 http mode - embedded jetty should use SynchronousQueue</summary>
      <description>Noticed an unreproducible bug (customer reported), where HiveServer2 in http mode would accept the incoming tcp connection but would not allocate a new thread to process the request. Switching from LinkedBlockingQueue to SynchronousQueue fixes the issue and also simplifies the threading model.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-10 01:00:00" id="9646" opendate="2015-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesn&amp;#39;t show Spark job progress info [Spark Branch]</summary>
      <description>Beeline can show MR job progress info, but can't show that of Spark job. CLI doesn't have this problem.</description>
      <version>spark-branch,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-11 01:00:00" id="9652" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez in place updates should detect redirection of STDERR</summary>
      <description>Tez in place updates detects STDOUT redirection and logs using old logging method. Similarly it should detect STDERR redirection as well. This will make sure following will log using old methodhive -e '&lt;some_query&gt;' 2&gt; err.log</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-16 01:00:00" id="9701" opendate="2015-2-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JMH module does not compile under hadoop-1 profile</summary>
      <description/>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-17 01:00:00" id="9708" opendate="2015-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove testlibs directory</summary>
      <description>The testlibs directory is left over from the old ant build. We can delete it as it's downloaded by maven now:https://github.com/apache/hive/blob/trunk/pom.xml#L610</description>
      <version>1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testlibs.ant-contrib.LICENSE.txt</file>
      <file type="M">testlibs.ant-contrib-1.0b3.jar</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-19 01:00:00" id="9727" opendate="2015-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GroupingID translation from Calcite</summary>
      <description>The translation from Calcite back to Hive might produce wrong results while interacting with other Calcite optimization rules.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-19 01:00:00" id="9728" opendate="2015-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add heap mode to allocator (for q files, YARN w/o direct buffer accounting support)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.llap.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.Allocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-23 01:00:00" id="9750" opendate="2015-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid log locks in operators</summary>
      <description>Basically wrap all LOG.xx calls in isLogXXXEnabled to avoid unnecessary locks on these calls.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-2 01:00:00" id="9829" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: fix unit tests</summary>
      <description>Unit tests are broken.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-2 01:00:00" id="9830" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map join could dump a small table multiple times [Spark Branch]</summary>
      <description>We found auto_sortmerge_join_8 is flaky is flaky for Spark. Sometimes, the output could be wrong.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-2 01:00:00" id="9831" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should use ConcurrentHashMap in ThreadFactory</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-3 01:00:00" id="9836" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: fails when virtual columns are present in the join conditions (for e.g. partition columns)</summary>
      <description>explainselect a.key, a.value, b.valuefrom tab a join tab_part b on a.key = b.key and a.ds = b.ds;fails.</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-3 01:00:00" id="9839" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks OperationHandle on async queries which fail at compile phase</summary>
      <description>Using beeline to connect to HiveServer2.And type the following:drop table if exists table_not_exists;select * from table_not_exists;There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .</description>
      <version>0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
</bugrepository>