<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2011-7-12 01:00:00" id="2498" opendate="2011-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
      <description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-20 01:00:00" id="2519" opendate="2011-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-1-22 01:00:00" id="2735" opendate="2012-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-25 01:00:00" id="2748" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-30 01:00:00" id="2762" opendate="2012-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter Table Partition Concatenate Fails On Certain Characters</summary>
      <description>Alter table partition concatenate creates a Java URI object for the location of a partition. If the partition name contains certain characters, such as } or space ' ', the object constructor fails, causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-2-9 01:00:00" id="2792" opendate="2012-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
      <description/>
      <version>0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.substr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ba.table.udfs.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-12-9 01:00:00" id="2794" opendate="2012-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregations without grouping should return NULL when applied to partitioning column of a partitionless table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2012-4-19 01:00:00" id="2965" opendate="2012-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2612</summary>
      <description>In 4/19 contrib meeting it was decided to revert HIVE-2612.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.scripts.upgrade.derby.009-HIVE-2612.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.9.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.8.0-to-0.9.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.009-HIVE-2612.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.9.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.8.0-to-0.9.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.009-HIVE-2612.postgres.sql</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RegionStorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.constants.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MRegionStorageDescriptor.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.results.clientpositive.create.union.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-7-24 01:00:00" id="305" opendate="2009-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Hadoop streaming&amp;#39;s counters/status reporters to Hive Transforms</summary>
      <description>https://issues.apache.org/jira/browse/HADOOP-1328" Introduced a way for a streaming process to update global counters and status using stderr stream to emit information. Use "reporter:counter:&lt;group&gt;,&lt;counter&gt;,&lt;amount&gt; " to update a counter. Use "reporter:status:&lt;message&gt;" to update status. "</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-5-26 01:00:00" id="3058" opendate="2012-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.transform.escape.input breaks tab delimited data</summary>
      <description>With the hive.transform.escape.input set, all tabs going into a script, including those used to separate columns are escaped.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.newline.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.newline.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-5-28 01:00:00" id="3059" opendate="2012-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-2703</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-25 01:00:00" id="306" opendate="2009-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "INSERT [INTO] destination"</summary>
      <description>Currently hive only supports "INSERT OVERWRITE destination". We should support "INSERT &amp;#91;INTO&amp;#93; destination".</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-5-30 01:00:00" id="3063" opendate="2012-5-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop partition for non-string columns is failing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-9-31 01:00:00" id="3068" opendate="2012-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to export table metadata as JSON on table drop</summary>
      <description>When a table is dropped, the contents go to the users trash but the metadata is lost. It would be super neat to be able to save the metadata as well so that tables could be trivially re-instantiated via thrift.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-6-1 01:00:00" id="3079" opendate="2012-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2989</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.tablelink.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.table.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.tablelink.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure5.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableIdentifier.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-2989.mysql.sql</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-9-22 01:00:00" id="3173" opendate="2012-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement getTypeInfo database metadata method</summary>
      <description>The JDBC driver does not implement the database metadata method getTypeInfo. Hence, an application cannot dynamically determine the available type information and associated properties.</description>
      <version>0.8.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-12-23 01:00:00" id="3181" opendate="2012-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDatabaseMajor/Minor version does not return values</summary>
      <description>This is really a sub-issue of HIVE-3174 (which is a lot of properties) but given that the driver will return databaseProductVersion it makes no sense to not have implemented these as well.</description>
      <version>0.8.1</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-3 01:00:00" id="3333" opendate="2012-8-3 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Specified SerDe does not get used when executing a query over JSON data</summary>
      <description>I found a JSON SerDe that I wanted to try out, and I ran into some issues attempting to use it. The script I was executing looks like this:ADD JAR /home/natty/hive-test-case/hive-json-serde-0.2.jar;CREATE TABLE bar ( id INT, integers ARRAY&lt;INT&gt;, datum STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.JsonSerde';LOAD DATA LOCAL INPATH '/home/natty/sample_data/json.sample' OVERWRITE INTO TABLE bar;SELECT * FROM bar;The data I loaded in looks like this:{ "id": 1, "integers": [ 1, 2, 3 ], "datum": "hello" },When the "SELECT * FROM bar" query executes, it returns with a failure:hive&gt; ADD JAR /home/natty/hive-test-case/hive-json-serde-0.2.jar;Added /home/natty/hive-test-case/hive-json-serde-0.2.jar to class pathAdded resource: /home/natty/hive-test-case/hive-json-serde-0.2.jarhive&gt; SELECT * FROM bar;OKFailed with exception java.io.IOException:java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object;Time taken: 2.335 secondsNow, this alone doesn't bother me. What bothers me is that, if I look at the log file, I see the following exception:2012-08-03 13:12:11,407 ERROR CliDriver (SessionState.java:printError(380)) - Failed with exception java.io.IOException:java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object;java.io.IOException: java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object; at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:173) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1383) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:266) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:208)Caused by: java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object; at org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getList(StandardListObjectInspector.java:98) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:287) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:213) at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:59) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:365) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:163) ... 11 moreNote that this exception indicates that Hive is executing code for the DelimitedJSONSerDe, rather than the one that I specified (JsonSerde from the jar file). Seems incorrect.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-13 01:00:00" id="3375" opendate="2012-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketed map join should check that the number of files match the number of buckets</summary>
      <description>Currently, we get NPE if that is not the case</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-14 01:00:00" id="3379" opendate="2012-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats are not being collected correctly for analyze table with dynamic partitions</summary>
      <description>analyze table T partition (ds, hr);does not collect stats correctly</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.stats8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats10.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-23 01:00:00" id="3405" opendate="2012-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-14 01:00:00" id="3465" opendate="2012-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert into statement overwrites if target table is prefixed with database name</summary>
      <description>insert into statement works as expected when the target table is not prefixed with database name. It does append correctly.However, if I specify a database name prefixing the target table, the statement just overwrites instead of appends:insert into table my_database.target select * from source;</description>
      <version>0.8.1</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.insert1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-24 01:00:00" id="3505" opendate="2012-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>log4j template has logging threshold that hides all audit logs</summary>
      <description>With the "template" for log4j configuration provided in the tarball, audit logging is hidden (it's logged as "INFO"). By making the log threshold a parameter, this information remains hidden when using the CLI (which is desired) but can be overridden when starting services to enable audit-logging.(This is primarily so that Hive is more functional out-of-the-box as installed by Apache Bigtop).</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-9-25 01:00:00" id="3507" opendate="2012-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some of the tests are not deterministic</summary>
      <description>skewjoinopt* tests are not deterministic</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt1.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-4-7 01:00:00" id="3682" opendate="2012-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>when output hive table to file,users should could have a separator of their own choice</summary>
      <description>By default,when output hive table to file ,columns of the Hive table are separated by ^A character (that is \001).But indeed users should have the right to set a seperator of their own choice.Usage Example:create table for_test (key string, value string);load data local inpath './in1.txt' into table for_testselect * from for_test;UT-01：default separator is \001 line separator is \ninsert overwrite local directory './test-01' select * from src ;create table array_table (a array&lt;string&gt;, b array&lt;string&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ',';load data local inpath "../hive/examples/files/arraytest.txt" overwrite into table table2;CREATE TABLE map_table (foo STRING , bar MAP&lt;STRING, STRING&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'STORED AS TEXTFILE;UT-02：defined field separator as ':'insert overwrite local directory './test-02' row format delimited FIELDS TERMINATED BY ':' select * from src ;UT-03: line separator DO NOT ALLOWED to define as other separator insert overwrite local directory './test-03' row format delimited FIELDS TERMINATED BY ':' select * from src ;UT-04: define map separators insert overwrite local directory './test-04' row format delimited FIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'select * from src;</description>
      <version>0.8.1</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-3-9 01:00:00" id="3874" opendate="2013-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a new Optimized Row Columnar file format for Hive</summary>
      <description>There are several limitations of the current RC File format that I'd like to address by creating a new format: each column value is stored as a binary blob, which means: the entire column value must be read, decompressed, and deserialized the file format can't use smarter type-specific compression push down filters can't be evaluated the start of each row group needs to be found by scanning user metadata can only be added to the file when the file is created the file doesn't store the number of rows per a file or row group there is no mechanism for seeking to a particular row number, which is required for external indexes. there is no mechanism for storing light weight indexes within the file to enable push-down filters to skip entire row groups. the type of the rows aren't stored in the file</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-5-16 01:00:00" id="3907" opendate="2013-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should support adding multiple resources at once</summary>
      <description>Currently hive adds resources in one by one manner. And for JAR resources, one classloader is created for each jar file, which seemed not good idea.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-2-24 01:00:00" id="3937" opendate="2013-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Profiler</summary>
      <description>Adding a Hive Profiler implementation which tracks inclusive wall times and call counts of the operators</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-6 01:00:00" id="473" opendate="2009-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up after tests</summary>
      <description>The test suite creates a lot of temporary files that aren't cleaned up. For example plan xml files, mapred/local and mapred/system files.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
</bugrepository>