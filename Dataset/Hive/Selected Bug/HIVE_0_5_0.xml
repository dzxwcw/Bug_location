<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2010-1-11 01:00:00" id="1039" opendate="2010-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>multi-insert doesn&amp;#39;t work for local directories</summary>
      <description>As wd pointed out in hive-user, the following query only load data to the first local directory. Multi-insert to tables works fine. hive&gt; from test &gt; INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/0' select *where a = 1 &gt; INSERT OVERWRITE LOCAL DIRECTORY '/home/stefdong/tmp/1' select *where a = 3;</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-12 01:00:00" id="1045" opendate="2010-1-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>(bigint % int) should return bigint instead of double</summary>
      <description>This expression should return bigint instead of double.CREATE TABLE test (a BIGINT);EXPLAIN SELECT a % 3 FROM test;There must be something wrong in FunctionRegistry.getMethodInternal</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDFMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ComparisonOpMethodResolver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-16 01:00:00" id="1056" opendate="2010-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Predicate push down does not work with UDTF&amp;#39;s</summary>
      <description>Predicate push down does not work with UDTF's in lateral viewshive&gt; SELECT * FROM src LATERAL VIEW explode(array(1,2,3)) myTable AS k WHERE k=1;FAILED: Unknown exception: nullhive&gt;</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen-php.queryplan.types.php</file>
      <file type="M">ql.src.gen-javabean.org.apache.hadoop.hive.ql.plan.api.OperatorType.java</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-4 01:00:00" id="10603" opendate="2015-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase default permgen space for HS2 on windows</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.hiveserver2.cmd</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-23 01:00:00" id="1086" opendate="2010-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add "-Doffline=true" option to ant</summary>
      <description>Currently I am seeing ivy retrieve for 4 times, each time for 4 of the hadoop versions.It takes a long time.ivy-retrieve-hadoop-source:[ivy:retrieve] :: Ivy 2.0.0-rc2 - 20081028224207 :: http://ant.apache.org/ivy/ :::: loading settings :: file = /hive/trunk/VENDOR.hive/trunk/ivy/ivysettings.xml[ivy:retrieve] :: resolving dependencies :: org.apache.hadoop.hive#shims;working@zshao.com[ivy:retrieve] confs: [default][ivy:retrieve] found hadoop#core;0.17.2.1 in hadoop-source[ivy:retrieve] found hadoop#core;0.18.3 in hadoop-source...We should fix this problem. Also it will help if we can add an option "offline" like what hadoop has.</description>
      <version>0.5.0,0.6.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-28 01:00:00" id="1116" opendate="2010-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bug with alter table rename when table has property EXTERNAL=FALSE</summary>
      <description>if the location is not an external location - this would be safer.the problem right now is that it's tricky to use the drop and rename way of writing new data into a table. consider:Initialization block:drop table a_tmpcreate table a_tmp like a;Loading block:load data &lt;newdata&gt; into a_tmp;drop table a;alter table a_tmp rename to a;this looks safe. but it's not. if one runs this multiple times - then data is lost (since 'a' is pointing to 'a_tmp''s location after any iteration. and dropping table 'a' blows away loaded data in the next iteration). if the location is being managed by Hive - then 'rename' should switch location as well.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter3.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-17 01:00:00" id="11290" opendate="2015-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cursor attributes %ISOPEN, %FOUND, %NOTFOUND and SYS_REFCURSOR variable</summary>
      <description>Cursor attributes allow you to get information about the current cursor state. Cursor variable can be used to pass result sets between routines.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Query.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-17 01:00:00" id="11291" opendate="2015-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid allocation storm while doing rule matching on operator/expression trees</summary>
      <description>RuleRegExMatch repeatedly allocates string while trying to find a matching pattern. This results in huge GC churn for large trees.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PrunerUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.RuleExactMatch.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-28 01:00:00" id="11390" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Fix table alias propagation for windowing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-2-22 01:00:00" id="1188" opendate="2010-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE when running TestJdbcDriver/TestHiveServer</summary>
      <description>% ant test -Dtestcase=TestJdbcDriverBUILD FAILED/Users/carl/Projects/hive/hd11/hive/build.xml:154: The following error occurred while executing this line:/Users/carl/Projects/hive/hd11/hive/build.xml:93: The following error occurred while executing this line:/Users/carl/Projects/hive/hd11/hive/contrib/build.xml:77: java.lang.NullPointerException at java.util.Arrays$ArrayList.&lt;init&gt;(Arrays.java:3357) at java.util.Arrays.asList(Arrays.java:3343) at org.apache.hadoop.hive.ant.QTestGenTask.execute(QTestGenTask.java:248) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360) at org.apache.tools.ant.helper.SingleCheckExecutor.executeTargets(SingleCheckExecutor.java:38) at org.apache.tools.ant.Project.executeTargets(Project.java:1212) at org.apache.tools.ant.taskdefs.Ant.execute(Ant.java:441) at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:302) at org.apache.tools.ant.taskdefs.SubAnt.execute(SubAnt.java:221) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:398) at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291) at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106) at org.apache.tools.ant.Task.perform(Task.java:348) at org.apache.tools.ant.Target.execute(Target.java:390) at org.apache.tools.ant.Target.performTasks(Target.java:411) at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1360) at org.apache.tools.ant.Project.executeTarget(Project.java:1329) at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41) at org.apache.tools.ant.Project.executeTargets(Project.java:1212) at org.apache.tools.ant.Main.runBuild(Main.java:801) at org.apache.tools.ant.Main.startAnt(Main.java:218) at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280) at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)TestHiveServer throws the same error.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-18 01:00:00" id="1255" opendate="2010-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add mathematical UDFs PI, E, degrees, radians, tan, sign, and atan</summary>
      <description>Add support for PI, E, degrees, radians, tan, sign and atan</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-23 01:00:00" id="1271" opendate="2010-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case sensitiveness of type information specified when using custom reducer causes type mismatch</summary>
      <description>Type information specified while using a custom reduce script is converted to lower case, and causes type mismatch during query semantic analysis . The following REDUCE query where field name = "userId" failed.hive&gt; CREATE TABLE SS (&gt; a INT,&gt; b INT,&gt; vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt; );OKhive&gt; FROM (select * from srcTable DISTRIBUTE BY id SORT BY id) s&gt; INSERT OVERWRITE TABLE SS&gt; REDUCE *&gt; USING 'myreduce.py'&gt; AS&gt; (a INT,&gt; b INT,&gt; vals ARRAY&lt;STRUCT&lt;userId:INT, y:STRING&gt;&gt;&gt; )&gt; ;FAILED: Error in semantic analysis: line 2:27 Cannot insert intotarget table because column number/types are different SS: Cannotconvert column 2 from array&lt;struct&lt;userId:int,y:string&gt;&gt; toarray&lt;struct&lt;userid:int,y:string&gt;&gt;.The same query worked fine after changing "userId" to "userid".</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.MapTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.ListTypeInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-18 01:00:00" id="12710" opendate="2015-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add better logging for Tez session creation thread failures</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-3-24 01:00:00" id="1281" opendate="2010-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucketing column names in create table should be case-insensitive</summary>
      <description>This create table fails because 'userId' != 'userid'CREATE TABLE tmp_pyang_bucket3 (userId INT) CLUSTERED BY (userid) INTO 32 BUCKETS;</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-4-9 01:00:00" id="1296" opendate="2010-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CLI set and set -v commands should dump properties in alphabetical order</summary>
      <description>This makes it easier to find properties by name when dumping them all at once.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-29 01:00:00" id="12960" opendate="2016-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate Column Stats Extrapolation and UniformDistribution to HBaseStore</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.hbase.TestHBaseAggregateStatsCacheWithBitVector.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.ColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BooleanColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.BinaryColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.StatsCache.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-5-3 01:00:00" id="1335" opendate="2010-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataNucleus should use connection pooling</summary>
      <description>Currently each Data Nucleus operation disconnects and reconnects to the MetaStore over jdbc. Queries fail to even explain properly in cases where a table has many partitions. This is fixed by enabling one parameter and including several jars.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-13 01:00:00" id="13501" opendate="2016-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invoke failure hooks if query fails on exception</summary>
      <description>When a query fails on some exception, failure hooks are not called currently. It's better to invoke such hooks so that we know the query is failed.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-21 01:00:00" id="13583" opendate="2016-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>E061-14: Search Conditions</summary>
      <description>This is a part of the SQL:2011 Analytics Complete Umbrella JIRA HIVE-13554. Support for various forms of search conditions are mandatory in the SQL standard. For example, "&lt;predicate&gt; is not true;" Hive should support those forms mandated by the standard.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestMergeStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-21 01:00:00" id="1360" opendate="2010-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow UDFs to access constant parameter values at compile time</summary>
      <description>UDFs should be able to access constant parameter values at compile time.</description>
      <version>0.5.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeConstantEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-24 01:00:00" id="1364" opendate="2010-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the maximum length of various metastore fields, and remove TYPE_NAME from COLUMNS primary key</summary>
      <description>The value component of a SERDEPROPERTIES key/value pair is currently limitedto a maximum length of 767 characters. I believe that the motivation for limiting the length to 767 characters is that this value is the maximum allowed length of an index ina MySQL database running on the InnoDB engine: http://bugs.mysql.com/bug.php?id=13315 The Metastore OR mapping currently limits many fields (including SERDEPROPERTIES.PARAM_VALUE) to a maximum length of 767 characters despite the fact that these fields are not indexed. The maximum length of a VARCHAR value in MySQL 5.0.3 and later is 65,535. We can expect many users to hit the 767 character limit on SERDEPROPERTIES.PARAM_VALUE when using the hbase.columns.mapping serdeproperty to map a table that has many columns.I propose increasing the maximum allowed length of SERDEPROPERTIES.PARAM_VALUE to 8192.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-20 01:00:00" id="13800" opendate="2016-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable auth enabled by default on LLAP UI for secure clusters</summary>
      <description>There's no sensitive information that I'm aware of. (The logs would be the most sensitive).Similar to the HS2 UI, the LLAP UI can be default unprotected even on secure clusters.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-2 01:00:00" id="13921" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix constprog_partitioner for HoS</summary>
      <description>index_bitmap3 and constprog_partitioner have been failing. Let's fix them here.EDIT: index_bitmap3 will be fixed via HIVE-13997.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-11 01:00:00" id="1401" opendate="2010-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Web Interface can ony browse default</summary>
      <description/>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.show.database.jsp</file>
      <file type="M">hwi.web.set.processor.jsp</file>
      <file type="M">hwi.web.session.result.jsp</file>
      <file type="M">hwi.web.index.jsp</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-17 01:00:00" id="14041" opendate="2016-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>llap scripts add hadoop and other libraries from the machine local install to the daemon classpath</summary>
      <description>`hadoop classpath` ends up getting added to the classpath of llap daemons. This essentially means picking up the classpath from the local deploy.This isn't required since the slider package includes relevant libraries (shipped from the client)</description>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-7-15 01:00:00" id="1411" opendate="2010-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataNucleus throws NucleusException if core-3.1.1 JAR appears more than once on CLASSPATH</summary>
      <description>DataNucleus barfs when the core-3.1.1 JAR file appears more than once on the CLASSPATH:2010-03-06 12:33:25,565 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter nalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:258) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:879) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:103) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:379) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:285) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156) Caused by: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1186)at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:803) at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:698) at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:164) at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:181)at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:125) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:104) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:130)at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:146)at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:118) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.(HiveMetaStore.java:100) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.(HiveMetaStoreClient.java:74) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:783) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:794) at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:252) ... 12 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)at java.lang.reflect.Method.invoke(Method.java:597) at javax.jdo.JDOHelper$16.run(JDOHelper.java:1956) at java.security.AccessController.doPrivileged(Native Method) at javax.jdo.JDOHelper.invoke(JDOHelper.java:1951) at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1159)... 28 more Caused by: org.datanucleus.exceptions.NucleusException: Plugin (Bundle) "org.eclipse.jdt.core" is already registered. Ensure you do nt have multiple JAR versions of the same plugin in the classpath. The URL "file:/Users/hadop/hadoop-0.20.1+152/build/ivy/lib/Hadoo p/common/core-3.1.1.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/Users/hado p/hadoop-0.20.1+152/lib/core-3.1.1.jar." at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:437)at org.datanucleus.plugin.NonManagedPluginRegistry.registerBundle(NonManagedPluginRegistry.java:343)at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensions(NonManagedPluginRegistry.java:227)at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensionPoints(NonManagedPluginRegistry.java:159)at org.datanucleus.plugin.PluginManager.registerExtensionPoints(PluginManager.java:82) at org.datanucleus.OMFContext.(OMFContext.java:164) at org.datanucleus.OMFContext.(OMFContext.java:145) at org.datanucleus.ObjectManagerFactoryImpl.initialiseOMFContext(ObjectManagerFactoryImpl.java:143)at org.datanucleus.jdo.JDOPersistenceManagerFactory.initialiseProperties(JDOPersistenceManagerFactory.java:317)at org.datanucleus.jdo.JDOPersistenceManagerFactory.(JDOPersistenceManagerFactory.java:261)at org.datanucleus.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:174)... 36 more 2010-03-06 12:33:25,575 ERROR ql.Driver (SessionState.java:printError(279)) - FAILED: Execution Error, return code 1 from org.apach e.hadoop.hive.ql.exec.DDLTask 2010-03-06 12:42:30,457 ERROR exec.DDLTask (SessionState.java:printError(279)) - FAILED: Error in metadata: javax.jdo.JDOFatalInter nalException: Unexpected exception caught. NestedThrowables: java.lang.reflect.InvocationTargetException org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDOFatalInternalException: Unexpected exception caught. NestedThrowables:</description>
      <version>0.4.0,0.4.1,0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-30 01:00:00" id="14140" opendate="2016-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: package codec jars</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-12-18 01:00:00" id="1415" opendate="2010-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add CLI command for executing a SQL script</summary>
      <description>Suggestion in HIVE-1405 was "source", e.g.source somescript.sql;</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">docs.xdocs.language.manual.cli.xml</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-7 01:00:00" id="14182" opendate="2016-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert "HIVE-13084: Vectorization add support for PROJECTION Multi-AND/OR</summary>
      <description>To many issues with scratch column allocation.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-17 01:00:00" id="14262" opendate="2016-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inherit writetype from partition WriteEntity for table WriteEntity</summary>
      <description>For partitioned table operations, a Table WriteEntity is being added to the list to be authorized if there is a partition in the output list from semantic analyzer. However, it is being added with a default WriteType of DDL_NO_TASK.The new Table WriteEntity should be created with the WriteType of the partition WriteEntity.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-5 01:00:00" id="14444" opendate="2016-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade qtest execution framework to junit4 - migrate most of them</summary>
      <description>this is the second step..migrating all exiting qtestgen generated tests to junit4it might be possible that not all will get migrated in this ticket...I will leave out the problematic ones...</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestPerfCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestBeeLineDriver.vm</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-23 01:00:00" id="14830" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move a majority of the MiniLlapCliDriver tests to use an inline AM</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-3 01:00:00" id="15530" opendate="2017-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize the column stats update logic in table alteration</summary>
      <description>Currently when a table is altered, if any of below conditions is true, HMS would try to update column statistics for the table: database name is changed table name is changed old columns and new columns are not the sameAs a result, when a column is added to a table, Hive also tries to update column statistics, which is not necessary. We can loose the last condition by checking whether all existing columns are changed or not. If not, we don't have to update stats info.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-25 01:00:00" id="1594" opendate="2010-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo of hive.merge.size.smallfiles.avgsize prevents change of value</summary>
      <description>The setting is described as &lt;name&gt;hive.merge.size.smallfiles.avgsize&lt;/name&gt;, however common/src/java/org/apache/hadoop/hive/conf/HiveConf.java reads it as "hive.merge.smallfiles.avgsize" (note the missing '.size.') so the user's setting has no effect and the value is stuck at the default of 16MB.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-6 01:00:00" id="16130" opendate="2017-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove jackson classes from hive-jdbc standalone jar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-7 01:00:00" id="16131" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive building with Hadoop 3 - additional stuff broken recently</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-7 01:00:00" id="16132" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DataSize stats don&amp;#39;t seem correct in semijoin opt branch</summary>
      <description>For the following operator tree snippet, the second Select is the start of a semijoin optimization branch. Take a look at the Data size - it is the same as the data size for its parent Select, even though the second select has only a single bigint column in its projection (the parent has 2 columns). I would expect the size to be 533328 (16 bytes * 33333).Fixing this estimate may become important if we need to estimate the cost of generating the min/max/bloomfilter.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-7 01:00:00" id="16133" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Footer cache in Tez AM can take too much memory</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.LocalCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-21 01:00:00" id="1659" opendate="2010-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>parse_url_tuple: a UDTF version of parse_url</summary>
      <description>The UDF parse_url take s a URL, parse it and extract QUERY/PATH etc from it. However it can only extract an atomic value from the URL. If we want to extract multiple piece of information, we need to call the function many times. It is desirable to parse the URL once and extract all needed information and return a tuple in a UDTF.</description>
      <version>0.5.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-5 01:00:00" id="1693" opendate="2010-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the compile target depend on thrift.home</summary>
      <description>Per http://wiki.apache.org/hadoop/Hive/HiveODBC the ant compile targets require thrift.home be set. Rather than fail to compile fail with a message indicating it should be set.</description>
      <version>0.5.0</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-8 01:00:00" id="1973" opendate="2011-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting error when join on tables where name of table has uppercase letters</summary>
      <description>When execute a join query on tables containing Uppercase letters in the table names hit an exception Ex: create table a(b int); create table tabForJoin(b int,c int); select * from a join tabForJoin on(a.b=tabForJoin.b); Got an exception like this FAILED: Error in semantic analysis: Invalid Table Alias tabForJoinBut if i give without capital letters ,It is working</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-30 01:00:00" id="21670" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replacing mockito-all with mockito-core dependency</summary>
      <description>The mockito-all dependency contains an old version of Hamcrest core which can collide with other Hamcrest dependencies. Replacint it with mockito-core should be straightforward.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-11-24 01:00:00" id="2178" opendate="2011-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log related Check style Comments fixes</summary>
      <description>Fix Log related Check style Comments</description>
      <version>0.5.0,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.SimpleCharStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-10 01:00:00" id="21981" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When LlapDaemon capacity is set to 0 and the waitqueue is not empty then the queries are stuck</summary>
      <description>When an LlapDaemon executor capacity is set to 0 then the already queued tasks are not handled causing the queries to stuck</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-11 01:00:00" id="22630" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not retrieve Materialized View definitions for rebuild if query is test SQL</summary>
      <description>for the query like select 1, select current_timestamp, select current_datehive retrieve all the Materialized view from metastore, if the one of databases are too large then this call take lots of time, the situation becomes worse if there are too frequent if hive server receives frequent "select 1" query ( connection pool uses it to check if the connection is valid or not).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="22631" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid deep copying partition list in listPartitionsByExpr</summary>
      <description>This is an expensive call, I am not sure why deepCopy is required.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-12 01:00:00" id="22632" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve estimateRowSizeFromSchema</summary>
      <description>estimateRowSizeFromSchema un-necessarily iterate and do look-up. This could be avoided.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-26 01:00:00" id="2307" opendate="2011-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="23073" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-10-17 01:00:00" id="5578" opendate="2013-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcat script doesn&amp;#39;t include jars from HIVE_AUX_JARS_PATH</summary>
      <description>hcat script include jars from $HIVE_HOME/lib but not from HIVE_AUX_JARS_PATH.</description>
      <version>0.5.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.bin.hcat</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-10-18 01:00:00" id="5591" opendate="2013-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use TezGroupedSplits to combine splits based on headroom in Tez</summary>
      <description/>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-14 01:00:00" id="7732" opendate="2014-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:JoinOrder Algo update to use HiveRels</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveFilterRel.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-15 01:00:00" id="7740" opendate="2014-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>qfile and qfile_regex should override includeFiles</summary>
      <description>qfile and qfile_regex should override include files so they can be used by devs to run tests speculatively.</description>
      <version>None</version>
      <fixedVersion>spark-branch</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-16 01:00:00" id="7750" opendate="2014-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: PPD should Push Predicates on On clause if possible</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-16 01:00:00" id="7755" opendate="2014-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable avro* tests [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-16 01:00:00" id="7758" opendate="2014-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 separates test files with spaces while QTestGen uses commas [Spark Branch]</summary>
      <description>HIVE-7757 for spark branch</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-22 01:00:00" id="7860" opendate="2014-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Query on partitioned table which filter out all partitions fails</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="8010" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle nested types</summary>
      <description>need to handle ExprNodeFieldDesc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-8 01:00:00" id="8021" opendate="2014-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: support CTAS and insert ... select</summary>
      <description>Need to send only the select part to CBO for now</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.serde.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.colname.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-9 01:00:00" id="8410" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typo in DOAP - incorrect category URL</summary>
      <description>NO PRECOMMIT TESTSThe DOAP contains the following:&lt;category rdf:resource="http://www.apache.org/category/database" /&gt;However, the URL is incorrect; it must be&lt;category rdf:resource="http://projects.apache.org/category/database" /&gt;Please fix this</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">doap.Hive.rdf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-9 01:00:00" id="8411" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support partial partition spec for certain ALTER PARTITION statements</summary>
      <description>To help address concerns hagleitn had about having to update many partitions here</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.change.col.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.partition.change.col.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-9 01:00:00" id="8412" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make reduce side join work for all join queries [Spark Branch]</summary>
      <description>Regardless all these join related optimizations such as map join, bucket join, skewed join, etc, reduce side join is the fallback. That means, if a join query wasn't taken care of by any of the optimization, it should work with reduce side join (might in a less optimal fashion).It's found that this isn't case at the moment. For instance, auto_sortmerge_join_1.q failed to execute on Spark.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-11-10 01:00:00" id="923" opendate="2009-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should not use CombineHiveInputFormat by default because it breaks for hadoop 0.20 releases.</summary>
      <description>HADOOP-5759 is committed to Hadoop 0.20 on 19/Oct/09 but not released yet.We should not use CombineHiveInputFormat for hadoop 0.20 for now. Otherwise all users will encounter problems using Hive trunk against Hadoop 0.20.We can switch the default back when a new release from hadoop 0.20 comes out.</description>
      <version>0.5.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-11-18 01:00:00" id="940" opendate="2009-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>restrict creation of partitions with empty partition keys</summary>
      <description>create table pc (a int) partitioned by (b string, c string);alter table pc add partition (b="f", c='');above alter cmd fails but actually creates a partition with name 'b=f/c=' but describe partition on the same name fails. creation of such partitions should not be allowed.</description>
      <version>0.3.0,0.4.0,0.4.1,0.5.0,0.6.0</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-5 01:00:00" id="9592" opendate="2015-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix ArrayIndexOutOfBoundsException in date_add and date_sub initialize</summary>
      <description>hive&gt; select date_add('2015-01-01', map(1,1));FAILED: ArrayIndexOutOfBoundsException 2hive&gt; select date_sub('2015-01-01', map(1,1));FAILED: ArrayIndexOutOfBoundsException 2</description>
      <version>None</version>
      <fixedVersion>1.0.2,1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>