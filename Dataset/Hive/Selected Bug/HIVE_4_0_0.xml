<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-4-16 01:00:00" id="10364" opendate="2015-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HMS upgrade script test does not publish results when prepare.sh fails.</summary>
      <description>The HMS upgrade script must publish succeed or failure results to JIRA. This bug is not publishing any results on JIRA is the prepare.sh script fails.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-11 01:00:00" id="17303" opendate="2017-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missmatch between roaring bitmap library used by druid and the one coming from tez</summary>
      <description>Caused by: java.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: org.roaringbitmap.buffer.MutableRoaringBitmap.runOptimize()Z  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:165)  ... 25 moreCaused by: java.lang.NoSuchMethodError: org.roaringbitmap.buffer.MutableRoaringBitmap.runOptimize()Z  at org.apache.hive.druid.com.metamx.collections.bitmap.WrappedRoaringBitmap.toImmutableBitmap(WrappedRoaringBitmap.java:65)  at org.apache.hive.druid.com.metamx.collections.bitmap.RoaringBitmapFactory.makeImmutableBitmap(RoaringBitmapFactory.java:88)  at org.apache.hive.druid.io.druid.segment.StringDimensionMergerV9.writeIndexes(StringDimensionMergerV9.java:348)  at org.apache.hive.druid.io.druid.segment.IndexMergerV9.makeIndexFiles(IndexMergerV9.java:218)  at org.apache.hive.druid.io.druid.segment.IndexMerger.merge(IndexMerger.java:438)  at org.apache.hive.druid.io.druid.segment.IndexMerger.persist(IndexMerger.java:186)  at org.apache.hive.druid.io.druid.segment.IndexMerger.persist(IndexMerger.java:152)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl.persistHydrant(AppenderatorImpl.java:996)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl.access$200(AppenderatorImpl.java:93)  at org.apache.hive.druid.io.druid.segment.realtime.appenderator.AppenderatorImpl$2.doCall(AppenderatorImpl.java:385)  at org.apache.hive.druid.io.druid.common.guava.ThreadRenamingCallable.call(ThreadRenamingCallable.java:44)  ... 4 more]], Vertex did not succeed due to OWN_TASK_FAILURE, failedTasks:1 killedTasks:89, Vertex vertex_1502470020457_0005_12_05 [Reducer 2] killed/failed due to:OWN_TASK_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:0 (state=08S01,code=2) OptionsAttachments</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-25 01:00:00" id="18545" opendate="2018-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDF to parse complex types from json</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StrictJsonWriter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-1 01:00:00" id="19367" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load Data should fail for empty Parquet files.</summary>
      <description>Load data does not validate the input for Parquet tables. This results in query failures.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-29 01:00:00" id="19727" opendate="2018-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Signature matching of table aliases</summary>
      <description>there is a probable problem with alias matching: "t1 as a" is matched to "t2 as a"</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-2 01:00:00" id="19775" opendate="2018-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should use HS2 embedded mode in privileged auth mode</summary>
      <description>Follow up of HIVE-19389.Authorization checks don't make sense for embedded mode and since it is not used in that mode it leads to issues if authorization is enabled (eg, username not set).</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-3 01:00:00" id="19778" opendate="2018-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable a flaky test: TestCliDriver#input31</summary>
      <description>Noticed this one has been failing occasionally on precommit test runs.Running: diff -a /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/itests/qtest/target/qfile-results/clientpositive/input31.q.out /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/ql/src/test/results/clientpositive/input31.q.out128c128&lt; 496---&gt; 242</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-5 01:00:00" id="19796" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push Down TRUNC Fn to Druid Storage Handler</summary>
      <description>Push down Queries with TRUNC date function such as SELECT SUM((`ssb_druid_100`.`discounted_price` * `ssb_druid_100`.`net_revenue`)) AS `sum_calculation_4998925219892510720_ok`, CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE)</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-5 01:00:00" id="19799" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove jasper dependency</summary>
      <description>jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run). </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-9 01:00:00" id="1980" opendate="2011-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging using mapreduce rather than map-only job failed in case of dynamic partition inserts</summary>
      <description>In dynamic partition insert and if merge is set to true and hive.mergejob.maponly=false, the merge MapReduce job will fail.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-3-10 01:00:00" id="1983" opendate="2011-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j configuration files in Hive JARs</summary>
      <description>Splitting this off as a subtask so that it can be resolved independently of the hive-default.xml issue.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">conf.hive-log4j.properties</file>
      <file type="M">conf.hive-exec-log4j.properties</file>
      <file type="M">common.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="19851" opendate="2018-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade jQuery version</summary>
      <description>jQuery version seems to be very old. Update to latest stable version. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.static.js.jquery.min.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="19852" opendate="2018-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jackson to latest</summary>
      <description>Update jackson version to latest 2.9.5</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-12 01:00:00" id="19867" opendate="2018-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle concurrent INSERTS</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.TxnIdUtils.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MPartition.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-12 01:00:00" id="19868" opendate="2018-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for float aggregator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-12 01:00:00" id="19875" opendate="2018-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase LLAP IO queue size for perf</summary>
      <description>According to gopalv queue limit has perf impact, esp. during hashtable load for mapjoin where in the past IO used to queue up more data for processing to process.1) Overall the default limit could be adjusted higher.2) Depending on Decimal64 availability, the weight for decimal columns could be reduced.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-13 01:00:00" id="19881" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow metadata-only dump for database which are not source of replication</summary>
      <description>If the dump is meta data only then allow dump even if the db is not source of replication</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-14 01:00:00" id="19902" opendate="2018-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore micro-benchmarks</summary>
      <description>It would be very useful to have metastore benchmarks to be able to track perf issues.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-15 01:00:00" id="19903" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable temporary insert-only transactional table</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-15 01:00:00" id="19904" opendate="2018-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data rewrite into Tez job fails for ACID</summary>
      <description>Load data rewrite into IAS fails for ACID as there is some code which does not take into account the table name could be in upper case, specifically ValidTxnWriteIdList</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.load.data.using.job.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-21 01:00:00" id="19956" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include yarn registry classes to jdbc standalone jar</summary>
      <description>HS2 Active/Passive HA requires some yarn registry classes. Include it in JDBC standalone jar. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-21 01:00:00" id="19963" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>metadata_only_queries.q fails</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.StatsOptimizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-22 01:00:00" id="19972" opendate="2018-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup to HIVE-19928 : Fix the check for managed table</summary>
      <description>The check for managed table should use ENUM comparison rather than string comparison.The check in the patch will always return false, thus maintaining existing behavior.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-7-27 01:00:00" id="20016" opendate="2018-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate TestJdbcWithMiniHS2.testParallelCompilation3 random failure</summary>
      <description>org.apache.hive.jdbc.TestJdbcWithMiniHS2.testParallelCompilation3 failed with:java.lang.AssertionError: Concurrent Statement failed: org.apache.hive.service.cli.HiveSQLException: java.lang.AssertionError: Authorization plugins not initialized! at org.junit.Assert.fail(Assert.java:88) at org.apache.hive.jdbc.TestJdbcWithMiniHS2.finishTasks(TestJdbcWithMiniHS2.java:374) at org.apache.hive.jdbc.TestJdbcWithMiniHS2.testParallelCompilation3(TestJdbcWithMiniHS2.java:304)</description>
      <version>4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-27 01:00:00" id="20019" opendate="2018-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ban commons-logging and log4j</summary>
      <description>Still seeing several references to commons-logging. We should move all classes to slf4j instead. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.http.JdbcJarDownloadServlet.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.FileMetadataManager.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.FileMetadataHandler.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.valcoersion.JavaIOTmpdirVariableCoercion.java</file>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseInputFormatUtil.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapIoMemoryServlet.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapServerSecurityInfo.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.endpoint.LlapPluginSecurityInfo.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.keyseries.VectorKeySeriesSingleImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDecimalSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDenseRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorDoubleSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongAvg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongFirstValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorLongSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFEvaluatorRowNumber.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFGroupBatches.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkEmptyKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PartitionColumnsSeparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TablePropertyEnrichmentOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.BaseMaskUDF.java</file>
      <file type="M">ql.src.test.org.apache.hive.testutils.MiniZooKeeperCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-28 01:00:00" id="20028" opendate="2018-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore client cache config is used incorrectly</summary>
      <description>Metastore client cache config is not used correctly. Enabling the cache actually disables it and vice versa. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-28 01:00:00" id="20029" opendate="2018-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add parallel insert, analyze, iow tests</summary>
      <description>1) We need a few tests, esp. for parallel case, where we verify that stats are NOT used.Right now many code paths don't fail but return -1, null or whatever when something else is not present, so positive tests might pass because they skip the check, not because the check passes.2) Analyze table needs a test, esp analyze table after parallel insert, and also analyze table after an invalid transaction.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-7-9 01:00:00" id="20123" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-9 01:00:00" id="20127" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix some issues with LLAP Parquet cache</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapCacheAwareFs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-10 01:00:00" id="20130" opendate="2018-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better logging for information schema synchronizer</summary>
      <description>The logging of information schema synchronizer should be more useful.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.PrivilegeSynchonizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-12-12 01:00:00" id="20150" opendate="2018-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey pushdown</summary>
      <description>TopNKey operator is implemented in HIVE-17896, but it needs more work in pushdown implementation. So this issue covers TopNKey pushdown implementation with proper tests.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.text.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.identity.reuse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.conversion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">kudu-handler.src.test.results.positive.kudu.complex.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.TopNKeyProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.topnkey.q</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.q93.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-13 01:00:00" id="20164" opendate="2018-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Murmur Hash : Make sure CTAS and IAS use correct bucketing version</summary>
      <description>With the migration to Murmur hash, CTAS and IAS from old table version to new table version does not work as intended and data is hashed using old hash logic.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-8-17 01:00:00" id="20191" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit patch application doesn&amp;#39;t fail if patch is empty</summary>
      <description>I've created some backport tickets to branch-3 (e.g. HIVE-20181) and made the mistake of uploading the patch files with wrong filename (. instead of - between version and branch).These get applied on master, where they're already present, since git apply with -3 won't fail if patch is already there. Tests are run on master instead of failing.I think the patch application should fail if the patch is empty and branch selection logic should probably fail too if the patch name is malformed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-17 01:00:00" id="20195" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Split MetastoreUtils into common and server-specific parts</summary>
      <description>Parts of MetastoreUtils are used by clients and the server, parts are used by server only. We need to separate server-only parts in a separate class.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.HiveStrictManagedUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.ColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.BooleanColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.BinaryColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.AbstractTestAuthorizationApiAuthorizer.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-3-3 01:00:00" id="2025" opendate="2011-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStore broken by HIVE-2022</summary>
      <description>The patch for HIVE-2022 broke TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStorehttps://hudson.apache.org/hudson/job/Hive-trunk-h0.20/590/@Paul: Assigning this to you.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-9-29 01:00:00" id="20267" opendate="2018-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expanding WebUI to include form to dynamically config log levels</summary>
      <description>Expanding the possibility to change the log levels during runtime, the webUI can be extended to interact with the Log4j2ConfiguratorServlet, this way it can be directly used and users/admins don't need to execute curl commands from commandline.</description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.llap.html</file>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-8-2 01:00:00" id="20299" opendate="2018-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>potential race in LLAP signer unit test</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-3 01:00:00" id="20300" opendate="2018-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>VectorFileSinkArrowOperator</summary>
      <description>Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-3 01:00:00" id="20301" opendate="2018-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for materialized view rewriting tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.empty.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rebuild.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.time.window.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.rebuild.dummy.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.mv.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-9 01:00:00" id="20347" opendate="2018-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.optimize.sort.dynamic.partition should work with partitioned CTAS and MV</summary>
      <description/>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2018-8-13 01:00:00" id="20379" opendate="2018-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewriting with partitioned materialized views may reference wrong column</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.part.2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-5-9 01:00:00" id="2038" opendate="2011-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore listener</summary>
      <description>Provide to way to observe changes happening on Metastore</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-3-9 01:00:00" id="2039" opendate="2011-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove hadoop version check from hive cli shell script</summary>
      <description>looking at cli startup times - one thing i noticed is that the version check in execHiveCmd.sh consumes 0.5-1s of wall-clock time (depending on where hive is installed).AFAIK - hive doesn't support versions less than 20 right now - and this check is only to check if version is less than 20. So we should be able to safely take it out. please comment if that is not the case.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-15 01:00:00" id="20394" opendate="2018-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimized and cleaned up HBaseQTest runner</summary>
      <description>Set proper cluster destroy order Propagated proper HBaseTestContext Ported downstream fixes (CDH-63695) General clean up</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseNegativeCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreHBaseCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCoreBlobstoreCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.snapshot.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.cascade.dbdrop.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-28 01:00:00" id="20472" opendate="2018-8-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn test failing for metastore-tool module</summary>
      <description>Fails because there are no applicable tests. [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.20.1:test (default-test) on project hive-metastore-benchmarks: No tests were executed! (Set -DfailIfNoTests=false to ignore this error.) -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException[ERROR][ERROR] After correcting the problems, you can resume the build with the command[ERROR] mvn &lt;goals&gt; -rf :hive-metastore-benchmarks</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-9-30 01:00:00" id="20489" opendate="2018-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain plan of query hangs</summary>
      <description>Explain on a query that joins 47 views, in effect around 94 joins after view expansion seems to take forever. The case here tries to generate a plan using map join with conditional tasks.When the task graph is huge with many paths, there can be a performance issue during compilation. This is caused by recursive traversal of task graph in internTableDesc and deriveFinalExplainAttributes. The use of recursion is inefficient in a couple of ways. For large graphs the recursion was filling up the stack Instead of finding the map works, the traversal was walking all possible paths from root causing a huge performance problem.The fix is to replace the traversal from recursive to an iterative one, keeping track of the nodes already visited. The fix uses getMRTasks, getSparkTasks and getTezTasks to do iterative traversal. These calls were changed to using iterative calls through HIVE-17195. When pushing this patch to an older release, please make sure HIVE-17195 is also pushed to that release.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.util.DAGTraversal.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-31 01:00:00" id="20493" opendate="2018-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test getGroupNames for SessionStateUserAuthenticator</summary>
      <description>HIVE-20118 changed the behavior of SessionStateUserAuthenticator which had an NPE bug. This was fixed in HIVE-20389. We should have had unit-tests for these!Add unit tests for getGroupNames..</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.authorization.TestSessionUserName.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-4 01:00:00" id="20498" opendate="2018-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support date type for column stats autogather</summary>
      <description>set hive.stats.column.autogather=true;create table dx2(a int,b int,d date);explain insert into dx2 values(1,1,'2011-11-11');-- no compute_stats callsinsert into dx2 values(1,1,'2011-11-11');insert into dx2 values(1,1,'2001-11-11');explain analyze table dx2 compute statistics for columns;-- as expected; has compute_stats callsanalyze table dx2 compute statistics for columns;-- runs okdesc formatted dx2 d;-- looks good</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMergerTest.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">ql.src.test.results.clientpositive.test.teradatabinaryfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.table.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsAutoGatherContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-5 01:00:00" id="20505" opendate="2018-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade org.openjdk.jmh:jmh-core to 1.21</summary>
      <description>This ticket tracks the following CVE(s) that were found in the direct dependency org.openjdk.jmh:jmh-core:1.19: CVE-2009-1896, CVE-2009-2689, CVE-2009-3879, CVE-2009-0733, CVE-2009-2475, CVE-2009-3883, CVE-2009-2476, CVE-2009-3884, CVE-2013-0169, CVE-2012-5373, CVE-2009-3880, CVE-2009-3881, CVE-2009-3882, CVE-2009-0581, CVE-2009-2690, CVE-2012-2739, CVE-2009-0723, CVE-2009-3728 </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-6 01:00:00" id="20513" opendate="2018-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Improve Fast Vector MapJoin Bytes Hash Tables</summary>
      <description>Based on HIVE-20491 / HIVE-20503 discussions, improve Fast Vector MapJoin Bytes Hash Tables by only storing a one word slot entry.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.WriteBuffers.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-9-11 01:00:00" id="20537" opendate="2018-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-column joins estimates with uncorrelated columns different in CBO and Hive</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.correlated.join.keys.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-11-12 01:00:00" id="20545" opendate="2018-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to exclude potentially large parameters in HMS Notifications</summary>
      <description>Clients can add large-sized parameters in Table/Partition objects. So we need to enable adding regex patterns through HiveConf to match parameters to be filtered from table and partition objects before serialization in HMS notifications.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-13 01:00:00" id="20553" opendate="2018-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>more acid stats tests</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.acid.stats2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.stats2.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-14 01:00:00" id="20561" opendate="2018-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the position of the Kafka Consumer to track progress instead of Consumer Records offsets</summary>
      <description>Kafka Partitions with transactional messages (post 0.11) will include commit or abort markers which indicate the result of a transaction. The markers are not returned to applications, yet have an offset in the log. Therefore the end of Stream position can be the offset of a control message. This Patch change the way how we keep track of the consumer position by using consumer.position(topicP) as oppose to using the offset of the consumed messages.Also I have done some refactoring to help code readability hopefully.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaRecordIteratorTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordIterator.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaPullerRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-9-20 01:00:00" id="20612" opendate="2018-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create new join multi-key correlation flag for CBO</summary>
      <description>Currently we reuse the flag in Hive side. It would be good to have the flag separated for debugging purposes.</description>
      <version>4.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-9-23 01:00:00" id="20625" opendate="2018-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regex patterns not working in SHOW MATERIALIZED VIEWS &amp;#39;&lt;pattern&gt;&amp;#39;</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.materialized.views.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.show.materialized.views.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-9-26 01:00:00" id="20640" opendate="2018-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hive to use ORC 1.5.3</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2018-10-1 01:00:00" id="20662" opendate="2018-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable TestMiniLlapLocalCliDriver.testCliDriver[load_dyn_part3]</summary>
      <description>TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;load_dyn_part3&amp;#93; is more than flaky.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-10-5 01:00:00" id="20702" opendate="2018-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Account for overhead from datastructure aware estimations during mapjoin selection</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-6 01:00:00" id="20705" opendate="2018-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Native Vector MapJoin doesn&amp;#39;t support Complex Big Table values</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2018-10-16 01:00:00" id="20752" opendate="2018-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In case of LLAP start failure add info how to find YARN logs</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-3-24 01:00:00" id="2076" opendate="2011-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore upgrade scripts and default schemas for PostgreSQL</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-17 01:00:00" id="20763" opendate="2018-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add google cloud storage (gs) to the exim uri schema whitelist</summary>
      <description>import/export is enabled for s3a by default. Ideally this list should include other cloud storage options. This Jira adds Google Storage to the list.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-17 01:00:00" id="20765" opendate="2018-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fetch partitions for txn stats validation in get_aggr_stats with one call</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionProjectionEvaluator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-17 01:00:00" id="20767" opendate="2018-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple project between join operators may affect join reordering using constraints</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinProjectTransposeRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-18 01:00:00" id="20768" opendate="2018-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding Tumbling Window UDF</summary>
      <description>Goal is to provide a UDF that truncates a timestamp to a beginning of a tumbling window interval./** * Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. * Tumbling windows are inclusive start exclusive end. * By default the beginning instant of fist window is Epoch 0 Thu Jan 01 00:00:00 1970 UTC. * Optionally users may provide a different origin as a timestamp arg3. * * This an example of series of window with an interval of 5 seconds and origin Epoch 0 Thu Jan 01 00:00:00 1970 UTC: * * * interval 1 interval 2 interval 3 * Jan 01 00:00:00 Jan 01 00:00:05 Jan 01 00:00:10 * 0 -------------- 4 : 5 --------------- 9: 10 --------------- 14 * * This UDF rounds timestamp agr1 to the beginning of window interval where it belongs to. * */</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-24 01:00:00" id="20796" opendate="2018-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc URL can contain sensitive information that should not be logged</summary>
      <description>It is possible to put passwords in the jdbc connection url and some jdbc drivers will supposedly use that. (derby, mysql). This information is considered sensitive, and should be masked out, while logging the connection url.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-25 01:00:00" id="20806" opendate="2018-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ASF license for files added in HIVE-20679</summary>
      <description>HIVE-20679 added couple of new files Deserialzer/Serialzer that needs the ASF license header.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.gzip.Serializer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.gzip.DeSerializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-26 01:00:00" id="20820" opendate="2018-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV partition on clause position</summary>
      <description>It should obey the following syntax as per https://cwiki.apache.org/confluence/display/Hive/Materialized+views :CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db_name.]materialized_view_name [DISABLE REWRITE] [COMMENT materialized_view_comment] [PARTITIONED ON (col_name, ...)] [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] [LOCATION hdfs_path] [TBLPROPERTIES (property_name=property_value, ...)]AS&lt;query&gt;;Currently it is positioned just before TBLPROPERTIES.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-26 01:00:00" id="20821" opendate="2018-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite SUM0 into SUM + COALESCE combination</summary>
      <description>Since SUM0 is not vectorized, but SUM + COALESCE are.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateReduceFunctionsRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-26 01:00:00" id="20822" opendate="2018-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvements to push computation to JDBC from Calcite</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedwork.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.external.jdbc.table2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.package-info.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCSortPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCAggregationPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.HiveJdbcImplementor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.JdbcHiveTableScan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.jdbc.HiveJdbcConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveBetween.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-30 01:00:00" id="20835" opendate="2018-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Interaction between constraints and MV rewriting may create loop in Calcite planner</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query11.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectJoinTransposeRule.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-3 01:00:00" id="20862" opendate="2018-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryId no longer shows up in the logs</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnCommonUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-8 01:00:00" id="20893" opendate="2018-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BloomK Filter probing method is not thread safe</summary>
      <description>As far i can tell this is not an issue for Hive yet (most of the usage of probing seems to be done by one thread at a time) but it is an issue of other users like Druid as per the following issue.https://github.com/apache/incubator-druid/issues/6546The fix is proposed by the author of https://github.com/apache/incubator-druid/pull/6584 is to make couple of local fields as ThreadLocals.Idea looks good to me and doesn't have any perf drawbacks. </description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomKFilter.java</file>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestBloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-9 01:00:00" id="20898" opendate="2018-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>For time related functions arguments may not be casted to a non nullable type</summary>
      <description>create table t (a string); insert into t values (null),('1988-11-11');set hive.cbo.enable=true;select 'expected 1 (second)', count(1) from t where second(a) is null;this may only cause trouble if Calcite is exploiting the datatype nullability; it will be need after CALCITE-2469 (1.18.0)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-12 01:00:00" id="20904" opendate="2018-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus fails to resolve module dependencies due to usage of exec plugin in metastore-server</summary>
      <description>metastore-server uses exec-maven-plugin to generate metastore-site.xml.template with ConfTemplatePrinter.It expects some arguments. Because yetus also uses the exec-maven-plugin to determine the order of the modules to be built, but with zero params, the execution fails.https://github.com/apache/yetus/blob/6ebaa1119e611db14f219e289e33ab8ac5c254a7/precommit/src/main/shell/test-patch.d/maven.sh#L658Steps to reproduce the issue:mvn -q exec:exec -Dexec.executable=pwd -Dexec.args=''</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-15 01:00:00" id="20926" opendate="2018-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join reduction hint fails when bloom filter entries are high or when there are no stats</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-20 01:00:00" id="20949" opendate="2018-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve PKFK cardinality estimation in physical planning</summary>
      <description>Missing case for cartesian product and full outer joins.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-20 01:00:00" id="20951" opendate="2018-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set Xms to 50% always</summary>
      <description>The lack of GC pauses is killing LLAP containers whenever the significant amount of memory is consumed by the off-heap structures which aren't cleaned up automatically until the GC runs.There's a java.nio.DirectByteBuffer.Deallocator which runs when the Direct buffers are garbage collected, which actually does the cleanup of the underlying off-heap buffers.The lack of Garbage collection activity for several hours while responding to queries triggers a build-up of these off-heap structures which end up forcing YARN to kill the process instead.It is better to hit a GC pause occasionally rather than to lose a node every few hours.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-21 01:00:00" id="20953" opendate="2018-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove a function from function registry when it can not be added to the metastore when creating it.</summary>
      <description>The testcase is intended to test REPL LOAD with retry. The test creates a partitioned table and a function in the source database and loads those to the replica. The first attempt to load a dump is intended to fail while loading one of the partitions. Based on the order in which the objects get loaded, if the function is queued after the table, it will not be available in replica after the load failure. But if it's queued before the table, it will be available in replica even after the load failure. The test assumes the later case, which may not be true always.Hence fix the testcase to order the objects by a fixed ordering. By setting hive.in.repl.test.files.sorted to true, the objects are ordered by the directory names. This ordering is available with minimal changes for testing, hence we use it. With this ordering a function gets loaded before a table. So changed the test to not expect the function to be available after the failed load, but be available after the retry.While writing that testcase, I found that even if a function fails to load, it's visible through show functions and also is available to be called just as if the failure has not happened. Digging further it was found that when creating a function we add it to the registry and also to the metastore. If the later fails, we do not clean it up from the registry and thus it remains visible after failure. Fixed the same.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-22 01:00:00" id="20961" opendate="2018-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retire NVL implementation</summary>
      <description>Right now we have coalesce and nvl implemented separetly; it might be better to remove one of them as they are doing the same. Because Coalesce is in the standard - I think NVL have to go...and became an alias to Coalesce.Further benefit is: that optimizations which coalesce already recieves; will be done on NVL calls as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropWhen.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nvl.mismatch.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFNvl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-12-10 01:00:00" id="21023" opendate="2018-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test for replication to a target with hive.strict.managed.tables enabled</summary>
      <description>Tests added are timing out in ptest run. Need to skip these test cases from batching and run them separately.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-16 01:00:00" id="21048" opendate="2018-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove needless org.mortbay.jetty from hadoop exclusions</summary>
      <description>During HIVE-20638 i found that org.mortbay.jetty exclusions from e.g. hadoop don't take effect, as the actual groupId of jetty is org.eclipse.jetty for most of the current projects, please find attachment (example for hive commons project).https://en.wikipedia.org/wiki/Jetty_(web_server)#History</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-1-11 01:00:00" id="21116" opendate="2019-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HADOOP_CREDSTORE_PASSWORD is not populated under yarn.app.mapreduce.am.admin.user.env</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestHiveCredentialProviders.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-17 01:00:00" id="21132" opendate="2019-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join edge is not being removed despite max bloomfilter entries set to 1</summary>
      <description>Reproducer--! qt:dataset:lineitem--! qt:dataset:part--! qt:dataset:srcset hive.support.concurrency=true;set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;--set hive.compute.query.using.stats=false;set hive.mapred.mode=nonstrict;set hive.explain.user=false;set hive.optimize.ppd=true;set hive.ppd.remove.duplicatefilters=true;set hive.tez.dynamic.partition.pruning=true;set hive.tez.dynamic.semijoin.reduction=true;set hive.optimize.metadataonly=false;set hive.optimize.index.filter=true;set hive.stats.autogather=true;set hive.tez.bigtable.minsize.semijoin.reduction=1;set hive.tez.min.bloom.filter.entries=1;set hive.stats.fetch.column.stats=true;set hive.tez.bloom.filter.factor=1.0f;set hive.auto.convert.join=false;set hive.optimize.shared.work=false;create database tpch_test;use tpch_test;CREATE TABLE `customer`( `c_custkey` bigint, `c_name` string, `c_address` string, `c_nationkey` bigint, `c_phone` string, `c_acctbal` double, `c_mktsegment` string, `c_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543026723');CREATE TABLE `lineitem`( `l_orderkey` bigint, `l_partkey` bigint, `l_suppkey` bigint, `l_linenumber` int, `l_quantity` double, `l_extendedprice` double, `l_discount` double, `l_tax` double, `l_returnflag` string, `l_linestatus` string, `l_shipdate` string, `l_commitdate` string, `l_receiptdate` string, `l_shipinstruct` string, `l_shipmode` string, `l_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543027179');CREATE TABLE `orders`( `o_orderkey` bigint, `o_custkey` bigint, `o_orderstatus` string, `o_totalprice` double, `o_orderdate` string, `o_orderpriority` string, `o_clerk` string, `o_shippriority` int, `o_comment` string)ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'TBLPROPERTIES ( 'bucketing_version'='2', 'transactional'='true', 'transactional_properties'='default', 'transient_lastDdlTime'='1543026824');alter table customer update statistics set('numRows'='150000000','rawDataSize'='8633707142');alter table lineitem update statistics set('numRows'='5999989709','rawDataSize'='184245066955');alter table orders update statistics set('numRows'='1500000000','rawDataSize'='46741318253');create view q18_tmp_cached asselect l_orderkey, sum(l_quantity) as t_sum_quantityfrom lineitemwhere l_orderkey is not nullgroup by l_orderkey;-- Set bloom filter size to huge number so we get any possible semijoin reductionsset hive.tez.min.bloom.filter.entries=0;set hive.tez.max.bloom.filter.entries=1;create table q18_large_volume_customer_cached stored as orc tblproperties ('transactional'='true', 'transactional_properties'='default') asselect c_name, c_custkey, o_orderkey, o_orderdate, o_totalprice, sum(l_quantity)from customer, orders, q18_tmp_cached t, lineitem lwhere c_custkey = o_custkey and o_orderkey = t.l_orderkey and o_orderkey is not null and t.t_sum_quantity &gt; 300 and o_orderkey = l.l_orderkey and l.l_orderkey is not nullgroup by c_name, c_custkey, o_orderkey, o_orderdate, o_totalpriceorder by o_totalprice desc, o_orderdatelimit 100;drop database tpch_test cascade;To reproduce run the above as TestMiniLlapLocalCliDriver test</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-18 01:00:00" id="21134" opendate="2019-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Build Version as UDF</summary>
      <description>This Jira is to get the Hive Build Version as UDF.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-27 01:00:00" id="21173" opendate="2019-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache Thrift to 0.9.3-1</summary>
      <description>The project currently depends on libthrift-0.9.3, however thrift released 0.12.0 on 2019-JAN-04. This release includes a security fix for THRIFT-4506 (CVE-2018-1320). Updating thrift to the latest version will remove that vulnerability.Also note the Apache Thrift project does not publish "libfb303" any longer. fb303 is contributed code (in '/contrib') and it has not been maintained. Ps.: 0.9.3.1 also addresses the CVE, see THRIFT-4506</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2019-2-7 01:00:00" id="21227" opendate="2019-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-20776 causes view access regression</summary>
      <description>HIVE-20776 introduces a change that causes regression for view access.Before the change, a user with select access of a view can get all columns of a view with select access of a view that is derived from a partitioned table.With the change, that user cannot access that view.The reason is that When user accesses columns of a view, Hive needs to get the partitions of the table that the view is derived from. The user name is the user who issues the query to access the view. The change in HIVE-20776 checks if user has access to a table before getting its partitions. When user only has access of a view, not the access of a table itself, this change denies the user access of the view.The solution is when getting table partitions, do not filter on table at HMS client</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-8 01:00:00" id="21232" opendate="2019-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a cache-miss friendly split affinity provider</summary>
      <description>If one of the LLAP nodes have data-locality, preferring that over another does have advantages for the first query or a more general cache-miss.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-2-13 01:00:00" id="21261" opendate="2019-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL LOAD adds redundant COPY and MOVE tasks for external table events.</summary>
      <description>For external tables replication, the data gets copied as separate task based on data locations listed in _external_tables_info file in the dump. So, individual events such as ADD_PARTITION or INSERT on the external tables should avoid copying data. So, it is enough to create table/add partition DDL tasks. COPY and MOVE tasks should be skipped.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-14 01:00:00" id="21269" opendate="2019-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mandate -update and -delete as DistCp options to sync data files for external tables replication.</summary>
      <description>Currently, external tables replication, copies the data in directory level. So, if target directory exist, then DistCp should compare and update or skip data files in the directory instead of creating new directory inside pre-existing target directory.This can be achieved using -update.Also, -delete option is needed to delete the files missing in source directory but present in target.Hive should mandate these DistCp options even if user passes other options.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.test.org.apache.hadoop.hive.shims.TestHadoop23Shims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-17 01:00:00" id="21283" opendate="2019-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Synonym mid for substr, position for locate</summary>
      <description>Create new synonym for the existing function Mid for substrpostiion for locate </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substring.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLocate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-2-20 01:00:00" id="21298" opendate="2019-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move Hive Schema Tool classes to their own package to have cleaner structure</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestMetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.DbInstallBase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskValidate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskUpgrade.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskMoveTable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskMoveDatabase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInitOrUpgrade.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskCreateUser.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskCreateCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTaskAlterCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolTask.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreSchemaInfo.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.schematool.TestSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolCatalogOps.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.schematool.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.schematool.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-22 01:00:00" id="21306" opendate="2019-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade HttpComponents to the latest versions similar to what Hadoop has done.</summary>
      <description>The use of HTTPClient 4.5.2 breaks the use of SPNEGO over TLS.It mistakenly added HTTPS instead of HTTP to the principal when over SSL and thus breaks the authentication.This was upgraded recently in Hadoop and needs to be done for Hive as well.See: HADOOP-16076Where we upgraded from 4.5.2 and 4.4.4 to 4.5.6 and 4.4.10.&lt;!-- httpcomponents versions --&gt;&lt;httpclient.version&gt;4.5.2&lt;/httpclient.version&gt;&lt;httpcore.version&gt;4.4.4&lt;/httpcore.version&gt;+ &lt;httpclient.version&gt;4.5.6&lt;/httpclient.version&gt;+ &lt;httpcore.version&gt;4.4.10&lt;/httpcore.version&gt;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-22 01:00:00" id="21308" opendate="2019-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Negative forms of variables are not supported in HPL/SQL</summary>
      <description>In the following HPL/SQL programs:declare num = 1; print -num;The expected result should be '-1'，but it print '1' .</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.declare2.out.txt</file>
      <file type="M">hplsql.src.test.queries.local.declare2.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-25 01:00:00" id="21316" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comparision of varchar column and string literal should happen in varchar</summary>
      <description>this is most probably the root cause behind HIVE-21310 as well</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.varchar.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-25 01:00:00" id="21320" opendate="2019-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_fields() and get_tables_by_type() are not protected by HMS server access control</summary>
      <description>User without any privilege can call these functions and get all meta data back as if user has full access privilege.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-27 01:00:00" id="21329" opendate="2019-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom Tez runtime unordered output buffer size depending on operator pipeline</summary>
      <description>For instance, if we have a reduce sink operator with no keys followed by a Group By (merge partial), we can decrease the output buffer size since we will only produce a single row.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-28 01:00:00" id="21340" opendate="2019-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Prune non-key columns feeding into a SemiJoin</summary>
      <description>explain cbo with ss as (select count(1), ss_item_sk, ss_ticket_number from store_sales group by ss_item_sk, ss_ticket_number having count(1) &gt; 1) select count(1) from item where i_item_sk IN (select ss_item_sk from ss);Notice the HiveProject(ss_item_sk=&amp;#91;$0&amp;#93;, ss_ticket_number=&amp;#91;$1&amp;#93;, $f2=&amp;#91;$2&amp;#93;) Only ss_item_sk is relevant for the HiveSemiJoinCBO PLAN:HiveAggregate(group=[{}], agg#0=[count()]) HiveSemiJoin(condition=[=($0, $1)], joinType=[inner]) HiveProject(i_item_sk=[$0]) HiveFilter(condition=[IS NOT NULL($0)]) HiveTableScan(table=[[tpcds_copy_orc_partitioned_10000, item]], table:alias=[item]) HiveProject(ss_item_sk=[$0], ss_ticket_number=[$1], $f2=[$2]) HiveFilter(condition=[&gt;($2, 1)]) HiveAggregate(group=[{1, 8}], agg#0=[count()]) HiveFilter(condition=[IS NOT NULL($1)]) HiveTableScan(table=[[tpcds_copy_orc_partitioned_10000, store_sales]], table:alias=[store_sales])</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.semijoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSemiJoinRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-28 01:00:00" id="21362" opendate="2019-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an input format and serde to read from protobuf files.</summary>
      <description>Logs are being generated using the HiveProtoLoggingHook and tez ProtoHistoryLoggingService. These are sequence files written using ProtobufMessageWritable.Implement a SerDe and input format to be able to create tables using these files.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.metatool.package-info.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-1 01:00:00" id="21372" opendate="2019-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Apache Commons IO To Read Stream To String</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-5 01:00:00" id="21389" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive distribution miss javax.ws.rs-api.jar after HIVE-21247</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-5 01:00:00" id="21390" opendate="2019-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BI split strategy does not work for blob stores</summary>
      <description>BI split strategy cuts the split at block boundaries however there are no block boundaries in blob storage so we end up with 1 split for BI split strategy. </description>
      <version>3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.mutate.StreamingTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2019-10-14 01:00:00" id="21449" opendate="2019-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement &amp;#39;WITHIN GROUP&amp;#39; clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.disc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.cont.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.disc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.cont.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFPercentileDisc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFPercentileCont.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileDisc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileCont.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.WindowFunctionDescription.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-3-19 01:00:00" id="21473" opendate="2019-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bumping jackson version to 2.9.8</summary>
      <description>Bump jackson version to 2.9.8</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-19 01:00:00" id="21474" opendate="2019-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preparation for bumping guava version</summary>
      <description>Bump guava to 24.1.1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStats.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-5-3 01:00:00" id="2148" opendate="2011-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add interface classification in Hive.</summary>
      <description>Add mechanism for marking stability and intended audiences in Hive.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-25 01:00:00" id="21500" opendate="2019-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable conversion of managed table to external and vice versa at source via alter table.</summary>
      <description>Couple of scenarios for Hive2 to Hive3(strict managed tables enabled) replication where managed table is converted to external at source. Scenario-1: (ACID/MM table converted to external at target)1. Create non-ACID ORC format table.2. Insert some rows3. Replicate this create event which creates ACID table at target (due to migration rule). Each insert event adds transactional metadata in HMS corresponding to the current table.4. Convert table to external table using ALTER command at source.Scenario-2: (External table at target changes table location)1. Create non-ACID avro format table.2. Insert some rows3. Replicate this create event which creates external table at target (due to migration rule). The data path is chosen under default external warehouse directory.4. Convert table to external table using ALTER command at source.It is unable to convert an ACID table to external table at target. Also, it is hard to detect what would be the table type at target when perform this ALTER table operation at source.So, it is decided to disable conversion of managed table at source (Hive2) to EXTERNAL or vice-versa if the DB is enabled for replication and strict managed is disabled.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.HiveStrictManagedUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-2 01:00:00" id="21564" opendate="2019-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load data into a bucketed table is ignoring partitions specs and loads data into default partition.</summary>
      <description>When running below command to load data into bucketed tables it is not loading into specified partition instead loaded into default partition.LOAD DATA INPATH '/tmp/files/000000_0' OVERWRITE INTO TABLE call PARTITION(year_partition=2012, month=12);SELECT * FROM call WHERE year_partition=2012 AND month=12; --&gt; returns 0 rows.CREATE TABLE call( date_time_date date, ssn string, name string, location string) PARTITIONED BY ( year_partition int, month int) CLUSTERED BY ( date_time_date) SORTED BY ( date_time_date ASC) INTO 1 BUCKETS STORED AS ORC;If set hive.exec.dynamic.partition to false, it fails with below error.Error: Error while compiling statement: FAILED: SemanticException 1:18 Dynamic partition is disabled. Either enable it by setting hive.exec.dynamic.partition=true or specify partition column values. Error encountered near token 'month' (state=42000,code=40000)When we "set hive.strict.checks.bucketing=false;", the load works fine.This is a behaviour imposed by HIVE-15148 to avoid incorrectly named data files being loaded to the bucketed tables. In customer use case, if the files are named properly with bucket_id (00000_0, 00000_1 etc), then it is safe to set this flag to false.However, current behaviour of loading into default partitions when hive.strict.checks.bucketing=true and partitions specified, was a bug injected by HIVE-19311 where the given query is re-written into a insert query (to handle incorrect file names and Orc versions) but missed to incorporate the partitions specs to it.</description>
      <version>4.0.0</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-3 01:00:00" id="21571" opendate="2019-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW COMPACTIONS shows column names as its first output row</summary>
      <description>SHOW COMPACTIONS yields a resultset with nice column names, and then the first row of data is a repetition of those column names. This is somewhat confusing and hard to read.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.ShowCompactionsOperation.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-1-7 01:00:00" id="216" opendate="2009-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>generate ruby bindings for service</summary>
      <description>From: Josh Ferguson &lt;josh@besquared.net&gt;Reply-To: &lt;hive-user@hadoop.apache.org&gt;Date: Tue, 6 Jan 2009 21:03:50 -0800To: &lt;hive-user@hadoop.apache.org&gt;Subject: rb-gen loveCan we get some ruby loving in the default service/src dir?Josh</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-11 01:00:00" id="21602" opendate="2019-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dropping an external table created by migration case should delete the data directory.</summary>
      <description>For external table, if the table is dropped, the location is not removed. But If the source table is managed and at target the table is converted to external, then the table location should be removed if the table is dropped.Replication flow should set additional parameter "external.table.purge"="true" for migration to external table.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-12 01:00:00" id="21604" opendate="2019-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>preCommit job should not be triggered on non-patch attachments</summary>
      <description>latest example: HIVE-14469https://issues.apache.org/jira/browse/HIVE-14669?focusedCommentId=16815520&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16815520I think one should be able to upload any kind of attachments (e.g. screenshot) without triggering the precommit job2 possible ways:1. strict: enable only .patch (should work)(2. lenient: introduce blacklist, .png ...)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-4-26 01:00:00" id="21654" opendate="2019-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>External table location is not preserved at target when base dir is set as /.</summary>
      <description>External table location is not preserved same as source path when base directory is set as "/".Source path: /tmp/ext/src/db1/ext1Target path: /ext/src/db1/ext1 --&gt; It should be /tmp/ext/src/db1/ext1 itself.External table base dir supplied: /If the base dir input is changed to "/abc", then target path is set as "/abc/tmp/ext/src/db1/ext1 which is correct.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-27 01:00:00" id="21657" opendate="2019-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable flaky cbo_rp_limit.q in TestMiniLlapLocalCliDriver</summary>
      <description>Fails intermittently with diff:Client Execution succeeded but contained differences (error code = 1) after executing cbo_rp_limit.q 11c11&lt; 1 4 2---&gt; 1 4 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-3 01:00:00" id="21685" opendate="2019-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong simplification in query with multiple IN clauses</summary>
      <description>Simple test to reproduce:select * from table1 where name IN(‘g’,‘r’) AND name IN(‘a’,‘b’);</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePointLookupOptimizerRule.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-6 01:00:00" id="21696" opendate="2019-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include partition columns and column stats in explain cbo formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-7 01:00:00" id="21700" opendate="2019-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive incremental load going OOM while adding load task to the leaf nodes of the DAG.</summary>
      <description>While listing the child nodes to check for leaf node, we need to filter out tasks which are already added to the children list. If a task is added multiple time to the children list then it may cause the list to grow exponentially. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.util.DAGTraversal.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2019-5-22 01:00:00" id="21777" opendate="2019-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven jar goal is producing warning due to missing dependency</summary>
      <description>org.apache.directory.client.ldap:ldap-client-directory is a test scope dependecy. Hive is using version 0.1 but 0.1-SNAPSHOT is also there as transitive dependency (omitted for collision with 0.1 which is already there on top level) causing warning in the maven default lifecycle execution:&amp;#91;WARNING&amp;#93; The POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT is missing, no dependency information availableThe warning appears in the jar goal logs and it can easily be removed by excluding this transitive dependency. </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2019-6-11 01:00:00" id="21858" opendate="2019-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Default to store runtime statistics in the metastore</summary>
      <description>Right now the reuse scope of runtime statistics is limited to re-running the actual query</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-26 01:00:00" id="2186" opendate="2011-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic Partitioning Failing because of characters not supported globStatus</summary>
      <description>Some dynamic queries failed on the stage of loading partitions if dynamic partition columns contain special characters. We need to escape all of them.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-13 01:00:00" id="21868" opendate="2019-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorize CAST...FORMAT</summary>
      <description>Vectorize UDFs for CAST (&lt;TIMESTAMP/DATE&gt; AS STRING/CHAR/VARCHAR FORMAT &lt;STRING&gt;) and CAST (&lt;STRING/CHAR/VARCHAR&gt; AS TIMESTAMP/DATE FORMAT &lt;STRING&gt;).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cast.datetime.with.sql.2016.format.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cast.datetime.with.sql.2016.format.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCastFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDateToString.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-6-13 01:00:00" id="21869" opendate="2019-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up the Kafka storage handler readme and examples</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-19 01:00:00" id="21894" opendate="2019-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hadoop credential password storage for the Kafka Storage handler when security is SSL</summary>
      <description>The Kafka storage handler assumes that if the Hive service is configured with Kerberos then the destination Kafka cluster is also secured with the same Kerberos realm or trust of realms.  The security configuration of the Kafka client can be overwritten due to the additive operations of the Kafka client configs, but, the only way to specify SSL and the keystore/truststore user/pass is via plain text table properties. This ticket proposes adding Hadoop credential security to the Kafka storage handler in support of SSL secured Kafka clusters.  </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.kafka.kafka.storage.handler.q.out</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaUtilsTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaUtils.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaTableProperties.java</file>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-4 01:00:00" id="21957" opendate="2019-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create temporary table like should omit transactional properties</summary>
      <description>In case of create temporary table like queries, where the source table is transactional, the transactional properties should not be copied over to the new table. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-4 01:00:00" id="21958" opendate="2019-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The list of table expression in the inclusion and exclusion list should be separated by &amp;#39;|&amp;#39; instead of comma.</summary>
      <description>Java regex expression does not support comma. If user wants multiple expression to be present in the include or exclude list, then the expressions can be provided separated by pipe ('|') character. The policy will look something like db_name.'(t1*)|(t3)'.'t100'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.common.repl.ReplScope.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationWithTableMigrationEx.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-1-17 01:00:00" id="22007" opendate="2019-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not push unsupported types to specific JDBC sources from Calcite</summary>
      <description>We should not push a project expression if it uses a type that a specific dialect does not support, e.g., boolean in Oracle.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCProjectPushDownRule.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-29 01:00:00" id="22059" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-30 01:00:00" id="22063" opendate="2019-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Authorization in Hive based on object ownership - HMS code path</summary>
      <description>This takes care of adding the owner and ownertype in the HMS code path</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateTableEvent.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-31 01:00:00" id="22066" opendate="2019-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache parent POM to version 21</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.bootstrap-theme.min.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.images.hive.logo.jpeg</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-1 01:00:00" id="22075" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the max-reducers=1 regression from HIVE-14200</summary>
      <description>The condition does not kick in when minPartition=1, maxPartition=1, nReducers=1, maxReducers=1</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-16 01:00:00" id="22120" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix wrong results/ArrayOutOfBound exception in left outer map joins on specific boundary conditions</summary>
      <description>Vectorized version of left outer map join produces wrong results or encounters ArrayOutOfBound exception.The boundary conditions are: The complete batch of the big table should have the join key repeated for all the join columns. The complete batch of the big table should have not have a matched key value in the small table The repeated value should not be a null value Some rows should be filtered out as part of the on clause filter.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-16 01:00:00" id="22125" opendate="2019-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move to Kafka 2.3 Clients</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.TransactionalKafkaWriterTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaBrokerResource.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.HiveKafkaProducerTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.TransactionalKafkaWriter.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaStorageHandler.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.HiveKafkaProducer.java</file>
      <file type="M">kafka-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-22 01:00:00" id="22136" opendate="2019-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on tez.bucket.pruning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-26 01:00:00" id="22145" opendate="2019-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid optimizations for analyze compute statistics</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-27 01:00:00" id="22151" opendate="2019-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off hybrid grace hash join by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.join.noncbo.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-29 01:00:00" id="22158" opendate="2019-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation layer - Disallow non-ACID MANAGED tables.</summary>
      <description>In the recent commits, we have allowed non-ACID MANAGED tables to be created by clients that have some form of ACID WRITE capabilities. I think it would make sense to disallow this entirely. MANAGED tables should be ACID tables only.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationOnHDFSEncryptedZones.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-30 01:00:00" id="22164" opendate="2019-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Limit operator returns wrong number of results with offset</summary>
      <description>Vectorized Limit operator returns wrong number of results with offset</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorLimitOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-5 01:00:00" id="22170" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-7-14 01:00:00" id="2219" opendate="2011-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make "alter table drop partition" more efficient</summary>
      <description>The current function dropTable() that handles dropping multiple partitions is somewhat inefficient. For each partition you want to drop, it loops through each partition in the table to see if the partition exists. This is an O(mn) operation, where m is the number of partitions to drop, and n is the number of partitions in the table. The running time of this function can be improved, which is useful for tables with many partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-9-17 01:00:00" id="22211" opendate="2019-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change maven phase to generate test sources</summary>
      <description>Some protobuf files are generated in the wrong phase; so I get compile errors because they are not there for eclipse...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-18 01:00:00" id="22214" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain vectorization should disable user level explain</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-20 01:00:00" id="22227" opendate="2019-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez bucket pruning produces wrong result with shared work optimization</summary>
      <description>Reproducerset hive.tez.bucket.pruning=true;set hive.optimize.shared.work=true;CREATE TABLE srcbucket_mapjoin_n16(key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 2 BUCKETS STORED AS TEXTFILE;CREATE TABLE tab_part_n10 (key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 4 BUCKETS STORED AS ORCFILE;CREATE TABLE srcbucket_mapjoin_part_n17 (key int, value string) partitioned by (ds string) CLUSTERED BY (key) INTO 4 BUCKETS STORED AS TEXTFILE;load data local inpath '$HIVE_SRC/data/files/bmj/000000_0' INTO TABLE srcbucket_mapjoin_n16 partition(ds='2008-04-08');load data local inpath '.$HIVE_SRC/data/files/bmj1/000001_0' INTO TABLE srcbucket_mapjoin_n16 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000000_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000001_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');load data local inpath '$HIVE_SRC/data/files/bmj/000002_0' INTO TABLE srcbucket_mapjoin_part_n17 partition(ds='2008-04-08');set hive.optimize.bucketingsorting=false;insert overwrite table tab_part_n10 partition (ds='2008-04-08')select key,value from srcbucket_mapjoin_part_n17;CREATE TABLE tab_n9(key int, value string) PARTITIONED BY(ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS STORED AS ORCFILE;insert overwrite table tab_n9 partition (ds='2008-04-08')select key,value from srcbucket_mapjoin_n16;select * from(select * from tab_n9 where tab_n9.key = 0)ajoin(select * from tab_part_n10 where tab_part_n10.key = 98)b full outer join tab_part_n10 c on a.key = b.key and b.key = c.keyorder by 1,2,3,4,5,6,7,8,9;</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mergejoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-29 01:00:00" id="22267" opendate="2019-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support password based authentication in HMS</summary>
      <description>Similar to HS2, support password based authentication in HMS.Right now we provide LDAP and CONFIG based options. The later allows to set user and password in config and is used only for testing.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-10-9 01:00:00" id="22315" opendate="2019-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 column division with decimal64 scalar</summary>
      <description>Currently division operation is not supported for Decimal64 column. This Jira will take care of supporting decimal64 column division with a decimal64 scalar.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.col.scalar.division.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-13 01:00:00" id="22331" opendate="2019-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>unix_timestamp without argument returns timestamp in millisecond instead of second.</summary>
      <description>After HIVE-22170, select unix_timestamp(); is returning milliseconds, but expected output is in seconds.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-14 01:00:00" id="22332" opendate="2019-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should ensure valid schema evolution settings since ORC-540</summary>
      <description>For details please see: https://issues.apache.org/jira/browse/ORC-558</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-14 01:00:00" id="22339" opendate="2019-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default time for MVs refresh in registry</summary>
      <description>Default was set to 60secs in HIVE-21344. It seems it may be too aggressive; suggestion is to change default to 1500secs.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-15 01:00:00" id="22342" opendate="2019-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HMS Translation: HIVE-22189 too strict with location for EXTERNAL tables</summary>
      <description>HIVE-22189 restricts EXTERNAL tables being created to be restricted to the EXTERNAL_WAREHOUSE_DIR. This might be too strict as any other location should be allowed as long as the location is outside the MANAGED warehouse directory.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-22 01:00:00" id="22382" opendate="2019-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 column division with decimal64 Column</summary>
      <description>Support Decimal64 column division with decimal64 Column</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal64.div.decimal64scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.div.decimal64scalar.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.decimal64.div.decimal64scalar.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.Decimal64ColumnDivideDecimal64Scalar.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-1 01:00:00" id="22444" opendate="2019-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Project POM Files</summary>
      <description>Address warnings in the build process Use DependencyManagement in Root POM for ITest (see HIVE-22426) General POM cleanup</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-kudu.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-14 01:00:00" id="22498" opendate="2019-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema tool enhancements to merge catalogs</summary>
      <description>Schema tool currently supports relocation of database from one catalog to another, one at a time. While having to do this one at a time is painful, it also lacks support for converting them to external tables during migration, in lieu of the changes to the translation layer where a MANAGED table is strictly ACID-only table.Hence we also need to convert them to external tables during relocation.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskMergeCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-20 01:00:00" id="22515" opendate="2019-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support cast to decimal64 in Vectorization</summary>
      <description>Support cast to decimal64 in Vectorization</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-20 01:00:00" id="22518" opendate="2019-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLStdHiveAuthorizerFactoryForTest doesn&amp;#39;t work correctly for llap tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.schq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sysdb.schq.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-1 01:00:00" id="2252" opendate="2011-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Display a sample of partitions created when Fatal Error occurred due to too many partitioned created</summary>
      <description>In dynamic partition inserts, if a mapper created too many partitions, a fatal error is raised and the job got killed. Sometimes the error is caused by data error and it will be helpful for users to debug if we display a sample of dynamic partitions generated.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-12-27 01:00:00" id="22553" opendate="2019-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Expose locks and transactions in sys db</summary>
      <description>Create new sysdb tables/views to access lock and transaction data.This allows to provide admins with live data about ongoing locks and transacions. Due to this being in the sys db access to this information can be restricted to select privileged users.Information about locks and compactions can be joined and accessed at the same time.Compaction related transactions would also be visible.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-1-7 01:00:00" id="22596" opendate="2019-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RawStore used by Initiator is not thread-safe</summary>
      <description>RawStore used by Initiator is not thread-safe. To avoid synchronization, we can replace it with ThreadLocal variable.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MetaStoreCompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-11 01:00:00" id="22629" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AST Node Children can be quite expensive to build due to List resizing</summary>
      <description>As per the attached profile, The AST Node can be a major source of CPU and memory churn, due to the ArrayList resizing and copy.In my Opinion this can be amortized by providing the actual size.jcamachorodriguez / vgarg</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="22635" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-12 01:00:00" id="22637" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid cost based rules during generating expressions from AST</summary>
      <description>genExprNode uses default dispatcher which fire rules based on cost, computation of cost is expensive and is likely un-necessary.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-12 01:00:00" id="22638" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix insert statement issue with return path</summary>
      <description>Insert statements were not handled properly with return path. It was revealed during examining why TestUpgradeTool is not working with return path.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.opconventer.HiveTableScanVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.opconventer.HiveTableFunctionScanVisitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-8 01:00:00" id="22708" opendate="2020-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test fix for http transport</summary>
      <description/>
      <version>4.0.0</version>
      <fixedVersion>2.3.8,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-7-8 01:00:00" id="2275" opendate="2011-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2219 and apply correct patch to improve the efficiency of dropping multiple partitions</summary>
      <description>HIVE-2219 applied an incorrect patch that fails unit tests. This patch reverts those changes and adds the intended changes to improve the efficiency of dropping multiple partitions.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-21 01:00:00" id="22754" opendate="2020-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trim some extra HDFS find file name calls that can be deduced using current TXN watermark</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-27 01:00:00" id="22783" opendate="2020-1-27 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add test for HIVE-22366</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestNumMetastoreCallsObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestNumMetastoreCalls.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-28 01:00:00" id="22786" opendate="2020-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Agg with distinct can be optimised in HASH mode</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.nocolumnalign.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.distinct.gby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-29 01:00:00" id="22788" opendate="2020-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query cause NPE due to implicit cast on ROW__ID</summary>
      <description>ReproCREATE TABLE table_16 (timestamp_col_19 timestamp,timestamp_col_29 timestamp,int_col_27 int,int_col_39 int,boolean_col_18 boolean,varchar0045_col_23 varchar(45));CREATE TABLE table_7 (int_col_10 int,bigint_col_3 bigint);CREATE TABLE table_10 (boolean_col_8 boolean,boolean_col_16 boolean,timestamp_col_5 timestamp,timestamp_col_15 timestamp,timestamp_col_30 timestamp,decimal3825_col_26 decimal(38, 25),smallint_col_9 smallint,int_col_18 int);explain cbo SELECT DISTINCT COALESCE(a4.timestamp_col_15, IF(a4.boolean_col_16, a4.timestamp_col_30, a4.timestamp_col_5)) AS timestamp_colFROM table_7 a3RIGHT JOIN table_10 a4 WHERE (a3.bigint_col_3) &gt;= (a4.int_col_18)INTERSECT ALLSELECT COALESCE(LEAST( COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP)) ), GREATEST(COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP))) ) AS timestamp_colFROM table_16 a1 GROUP BY COALESCE(LEAST( COALESCE(a1.timestamp_col_19, CAST('2010-03-29 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2014-08-16 00:00:00' AS TIMESTAMP)) ), GREATEST( COALESCE(a1.timestamp_col_19, CAST('2013-07-01 00:00:00' AS TIMESTAMP)), COALESCE(a1.timestamp_col_29, CAST('2028-06-18 00:00:00' AS TIMESTAMP))) );</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-12 01:00:00" id="2279" opendate="2011-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement sort_array UDF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-30 01:00:00" id="22793" opendate="2020-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default settings in HMS Benchmarking tool</summary>
      <description>HMS Benchmarking tool has invalid default setting values. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.src.main.java.org.apache.hadoop.hive.metastore.tools.Constants.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.README.md</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-2-4 01:00:00" id="22827" opendate="2020-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Flatbuffer version</summary>
      <description>Hive currently uses Flatbuffer 1.2.0. Other Apache projects use a more up-to-date version, e.g. 1.6.0.1. Upgrade to that version.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2020-2-12 01:00:00" id="22881" opendate="2020-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revise non-recommended Calcite api calls</summary>
      <description>RexUtil.simplify* methods</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectOverIntersectRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinConstraintsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterSetOpTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveSubQRemoveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-1-13 01:00:00" id="229" opendate="2009-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug output enabled in HIVE-206</summary>
      <description>The log4j was changed in HIVE-206 to enable debug for the root logger. This makes the shell hard to use with a lot of logging output flying by.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-20 01:00:00" id="22914" opendate="2020-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive Connection ZK Interactions Easier to Troubleshoot</summary>
      <description>Add better logging and make errors more consistent and meaningful.Recently was trying to troubleshoot an issue where the ZK namespace of the client and the HS2 were different and it was way too difficult to diagnose.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-19 01:00:00" id="2292" opendate="2011-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comment clause should immediately follow identifier field in CREATE DATABASE statement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.database.location.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-21 01:00:00" id="22922" opendate="2020-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: ShuffleHandler may not find shuffle data if pod restarts in k8s</summary>
      <description>Executor logs shows "Invalid map id: TTP/1.1 500 Internal Server Error". This happens when executor pod restarts with same hostname and port, but missing shuffle data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-25 01:00:00" id="22927" opendate="2020-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP should filter tasks in HB, instead of killing all tasks on error attempts</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-4 01:00:00" id="22972" opendate="2020-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow table id to be set for table creation requests</summary>
      <description>Hive Metastore should accept requests for table creation where the id is set, ignoring it.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-4 01:00:00" id="22974" opendate="2020-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore&amp;#39;s table location check should be applied when location changed</summary>
      <description>In HIVE-22189 a check was introduced to make sure managed and external tables are located at the proper space. This condition cannot be satisfied during an upgrade.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-5 01:00:00" id="22982" opendate="2020-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopN Key efficiency check might disable filter too soon</summary>
      <description>The check is triggered after every n batches but there can be multiple filters, one for each partition. Some filters might have less data then the others.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestTopNKeyFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TopNKeyDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperGeneralComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-6 01:00:00" id="22987" opendate="2020-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassCastException in VectorCoalesce when DataTypePhysicalVariation is null</summary>
      <description>ClassCastException in VectorCoalesce when DataTypePhysicalVariation is null</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-4-9 01:00:00" id="23171" opendate="2020-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create Tool To Visualize Hive Parser Tree</summary>
      <description>For some of the work I would like to do on HIVE-23149, it would be nice to visualize the output of the statement parser.I have created a tool that spits out the parser tree in DOT file format. This allows it to be visualized using a plethora of tools.To use it, compile the hive-parser test JAR and run it.  The application takes a single command line argument of a String.  The String is the SQL statement to parse:HqlParser "SELECT 1"I have attached an example of the output that I generated for a SELECT 1 statement:  </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">parser.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-9 01:00:00" id="23173" opendate="2020-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>User login success/failed attempts should be logged</summary>
      <description>User login success &amp; failure attempts should be logged in server logs</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PlainSaslHelper.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpRequestInterceptorBase.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-13 01:00:00" id="23192" opendate="2020-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"default" database locationUri should be external warehouse root.</summary>
      <description>When creating the default database, the database locationUri should be set to external warehouse.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-8-28 01:00:00" id="2322" opendate="2011-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ColumnarSerDe to the list of native SerDes</summary>
      <description>We store metadata about ColumnarSerDes in the metastore, so it should be considered a native SerDe. Then, column information can be retrieved from the metastore instead of from deserialization.Currently, for non-native SerDes, column comments are only shown as "from deserializer". Adding ColumnarSerDe to the list of native SerDes will persist column comments. See HIVE-2171 for persisting the column comments of custom SerDes.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample.islocalmode.hook.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-21 01:00:00" id="23260" opendate="2020-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for unmodified_metadata capability</summary>
      <description>Currently, the translator removes bucketing info for tables for clients that do not possess the HIVEBUCKET2 capability. While this is desirable, some clients that have write access to these tables can turn around overwrite the metadata thus corrupting original bucketing info.So adding support for a capability for client that are capable of interpreting the original metadata would prevent such corruption.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDefaultTransformer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetastoreTransformer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2020-6-4 01:00:00" id="23365" opendate="2020-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Put RS deduplication optimization under cost based decision</summary>
      <description>Currently, RS deduplication is always executed whenever it is semantically correct. However, it could be beneficial to leave both RS operators in the plan, e.g., if the NDV of the second RS is very low. Thus, we would like this decision to be cost-based. We could use a simple heuristic that would work fine for most of the cases without introducing regressions for existing cases, e.g., if NDV for partition column is less than estimated parallelism in the second RS, do not execute deduplication.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplicationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-5 01:00:00" id="23368" opendate="2020-5-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV rebuild should produce the same view as the one configured at creation time</summary>
      <description>There might be some configrations which might affect the rel-tree of the materialized views.In case rewrites to use datasketches for count(distinct) is enabled; the view should store sketches instead of distinct values</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-6 01:00:00" id="23375" opendate="2020-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Track MJ HashTable Load time</summary>
      <description>Introduce TezCounter to track MJ HashTable Load time</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.PostExecTezSummaryPrinter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-3 01:00:00" id="2338" opendate="2011-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter table always throws an unhelpful error on failure</summary>
      <description>Every failure in an alter table function always return a MetaException. When altering tables and catching exceptions, we throw a MetaException in the "finally" part of a try-catch-finally block, which overrides any other exceptions thrown.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-8 01:00:00" id="23423" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check of disabling hash aggregation ignores grouping set</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-23356 fixed the issue with disabling hash aggregation on grouping set queries. Need a fix for VectorGroupbyOperator operator.</description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-8 01:00:00" id="23424" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Dependency on Log4J from hive-shims-common</summary>
      <description>The project uses SLF4J but not the log4j specific libraries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-3 01:00:00" id="2343" opendate="2011-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats not updated for non "load table desc" operations</summary>
      <description>Bug introduced in HIVE-306 so that stats are updated only for LoadTableDesc operations. For other operations (analyze table), null ptr is thrown and stats are not updated.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="23432" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Ranger Replication Metrics</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.ReplLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.IncrementalLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.log.BootstrapLoadLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.IncrementalDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.BootstrapDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.NoOpRangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpTask.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-11 01:00:00" id="23433" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Deny Policy on Target Database After Ranger Replication to avoid writes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.RangerRestClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.RangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ranger.NoOpRangerRestClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2020-5-14 01:00:00" id="23470" opendate="2020-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TestCliDriver tests to TestMiniLlapCliDriver if they are failing with TestMiniLlapLocalCliDriver</summary>
      <description>Some tests are failing with TestMiniLlapLocalCliDriver, but running fine with TestMiniLlapCliDriver, let's move them there.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.temp.table.partcols1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.custom.udf.configure.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.script.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.printf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.sum.list.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform1.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientpositive.input5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.regexp.extract.q</file>
      <file type="M">ql.src.test.queries.clientpositive.select.transform.hint.q</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.binary.data.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.macro.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.macro.duplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.test.dummy.operator.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.newline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partcols1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.with.semi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-28 01:00:00" id="23563" opendate="2020-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Early abort the build in case new commits are added to the PR</summary>
      <description>if the job is waiting for a long to acquire the lock for a long time; it would be favourable to do a quick check after lock acqusition wether there are new triggershttps://github.com/apache/hive/pull/948#discussion_r432093713</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-29 01:00:00" id="23582" opendate="2020-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make SplitLocationProvider impl pluggable</summary>
      <description>LLAP uses HostAffinitySplitLocationProvider implementation by default. For non zookeeper based environments, a different split location provider may be used. To facilitate that make the SplitLocationProvider implementation class a pluggable. </description>
      <version>4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-2 01:00:00" id="23590" opendate="2020-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Close stale PRs automatically</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">.asf.yaml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="23619" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new plugin to rerun queries when Tez AM is down due to lost node</summary>
      <description>If the TezAM was running a query and it gets killed because of external factors like node going node, HS2 should retry the query in different TezAM.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="23620" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explore moving to SpotBugs</summary>
      <description>We may want to eventually switch to SpotBugs&amp;#8211; the spiritual successor of FindBugs, carrying on from the point where it left off with support of its communitySpotBugs is in a reality a fork of FindBugs: https://mailman.cs.umd.edu/pipermail/findbugs-discuss/2016-November/004321.htmlmvn -P spotbugs test-compile com.github.spotbugs:spotbugs-maven-plugin:4.0.0:spotbugs</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.findbugs.findbugs-exclude.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.findbugs.findbugs-exclude.xml</file>
      <file type="M">standalone-metastore.findbugs.findbugs-exclude.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">Jenkinsfile</file>
      <file type="M">findbugs.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="23621" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce ASF headers on source files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">Jenkinsfile</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.split125.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.cli.control.splitsupport.split0.SplitSupportDummy.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryServiceProvider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.scheduled.QTestScheduledQueryCleaner.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.qoption.QTestReplaceHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.SplitSupport.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.MiniDruidLlapLocalCliDriver.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.Wikipedia.java</file>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.kafka.SingleNodeKafkaCluster.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestTransactionalValidationListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStoreUpdateUsingEvents.java</file>
      <file type="M">common.src.java.org.apache.hive.http.JMXJsonServlet.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-5 01:00:00" id="23625" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 Web UI displays query drill-down results in plain text, not html</summary>
      <description>Opening a drilldown link on the HS2 Web UI, you are directed to the following URL: /query_page?operationId=&lt;ID&gt;Since the path /query_page contains no file extensions, Jetty cannot determine the mimetype and therefore the Hive HttpServer returns response header Content-Type: text/plain;charset=utf-8, and the information does not render as html in the browser. This should be corrected to return text/html;charset=utf-8.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-5 01:00:00" id="23626" opendate="2020-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build failure is incorrectly reported as tests passed</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-7 01:00:00" id="23631" opendate="2020-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the test target instead of install</summary>
      <description>after a full install; there are some issues with:time mvn test -pl ql,itests/hive-unit -Dtest=TestJdbcGenericUDTFGetSplits[INFO] Running org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits[ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 15.317 s &lt;&lt;&lt; FAILURE! - in org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits[ERROR] org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits Time elapsed: 15.316 s &lt;&lt;&lt; ERROR!org.apache.hive.service.ServiceException: org.apache.hive.service.ServiceException: Unable to setup tez session pool at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:733) at org.apache.hive.jdbc.miniHS2.MiniHS2.start(MiniHS2.java:382) at org.apache.hive.jdbc.AbstractTestJdbcGenericUDTFGetSplits.beforeTest(AbstractTestJdbcGenericUDTFGetSplits.java:88) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:383) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:344) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:417)Caused by: org.apache.hive.service.ServiceException: Unable to setup tez session pool at org.apache.hive.service.server.HiveServer2.initAndStartTezSessionPoolManager(HiveServer2.java:832) at org.apache.hive.service.server.HiveServer2.startOrReconnectTezSessions(HiveServer2.java:807) at org.apache.hive.service.server.HiveServer2.start(HiveServer2.java:730) ... 20 moreCaused by: java.io.FileNotFoundException: /home/dev/hive/ql/target/classes (Is a directory) at java.io.FileInputStream.open0(Native Method) at java.io.FileInputStream.open(FileInputStream.java:195) at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138) at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.&lt;init&gt;(RawLocalFileSystem.java:110) at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:212) at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.&lt;init&gt;(ChecksumFileSystem.java:147) at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:347) at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:950) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.getSha(TezSessionState.java:863) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:809) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.openInternal(TezSessionState.java:288) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.openInternal(TezSessionPoolSession.java:124) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:240) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:231) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.startInitialSession(TezSessionPool.java:297) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.start(TezSessionPool.java:117) at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.startPool(TezSessionPoolManager.java:112) at org.apache.hive.service.server.HiveServer2.initAndStartTezSessionPoolManager(HiveServer2.java:829) ... 22 more[INFO] [INFO] Results:[INFO] [ERROR] Errors: [ERROR] TestJdbcGenericUDTFGetSplits&gt;AbstractTestJdbcGenericUDTFGetSplits.beforeTest:88 » Service[INFO] [ERROR] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-8-17 01:00:00" id="2384" opendate="2011-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>import of multiple partitions from a partitioned table with external location overwrites files</summary>
      <description>when we import multiple partitions from an exported partitioned table, if we import it with a specified external location, then the partitions end up overwriting.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-18 01:00:00" id="2393" opendate="2011-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix whitespace test diff accidentally introduced in HIVE-1360</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.named.struct.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-1-22 01:00:00" id="242" opendate="2009-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>line breaks are not treated as spaces if there are more than 1 line break in the whole query.</summary>
      <description>describeextendedtab_name;is translated as 'describeextended tab_name'</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-2 01:00:00" id="2426" opendate="2011-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test that views with joins work properly</summary>
      <description>With the testcasedrop table invites;drop table invites2;create table invites (foo int, bar string) partitioned by (ds string);create table invites2 (foo int, bar string) partitioned by (ds string);set hive.mapred.mode=strict;-- test join views: see HIVE-1989create view v as select invites.bar, invites2.foo, invites2.ds from invites join invites2 on invites.ds=invites2.ds;explain select * from v where ds='2011-09-01';drop view v;drop table invites;drop table invites2;We should not have the partition pruner complain about invites.ds not having a predicate because the predicate invites2.ds='2011-09-01' will be inferred with the ppd transitivity optimization</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-22 01:00:00" id="2602" opendate="2011-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add support for insert partition overwrite(...) if not exists</summary>
      <description>INSERT OVERWRITE TABLE X PARTITION (a=b, c=d) IF NOT EXISTS ...The partition should be created and written if and only if it's not there already.The support can be added for dynamic partitions in the future, but this jira is for adding this support for static partitions.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-24 01:00:00" id="2607" opendate="2011-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add caching to json_tuple</summary>
      <description>get_json_object uses a variety of caches to improve its performance. json_tuple could benefit from having a similar cache from JSON string to JSONObject.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-2-30 01:00:00" id="262" opendate="2009-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>outer join gets some duplicate rows in some scenarios</summary>
      <description>SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key &lt; 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key &lt; 20);returns duplicate rows for outer join</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-2 01:00:00" id="2622" opendate="2011-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive POMs reference the wrong Hadoop artifacts</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-9 01:00:00" id="2642" opendate="2011-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix Hive-2566 and make union optimization more aggressive</summary>
      <description>Hive-2566 did some optimizations to union, but cause some problems. And then got reverted. This is to get it back and fix the problems we saw, and also make union optimization more aggressive.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-12 01:00:00" id="2648" opendate="2011-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel tests fail if master directory is not present</summary>
      <description>Parallel tests should create directories as needed.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-12-26 01:00:00" id="2681" opendate="2011-12-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUCESS is misspelled</summary>
      <description>C'mon!</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.MapRedStats.java</file>
    </fixedFiles>
  </bug>
</bugrepository>