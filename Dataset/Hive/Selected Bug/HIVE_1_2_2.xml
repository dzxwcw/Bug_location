<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  <bug fixdate="2016-11-5 01:00:00" id="15136" opendate="2016-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: allow slider placement policy configuration during install</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-10 01:00:00" id="16413" opendate="2017-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table as select does not check ownership of the location</summary>
      <description>1. following statement failed: create table foo(id int) location 'hdfs:///tmp/foo';Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=userx, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/tmp/foo]] (state=42000,code=40000)2. but when use create table as select, it successed:0: jdbc:hive2://189.39.151.44:21066/&gt; create table foo location 'hdfs:///tmp/foo' as select * from xxx2;INFO : Number of reduce tasks is set to 0 since there's no reduce operatorINFO : number of splits:1INFO : Submitting tokens for job: job_1491449632882_0094INFO : Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:haclusterINFO : The url to track the job: https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Starting Job = job_1491449632882_0094, Tracking URL = https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Kill Command = /opt/hive-1.3.0/bin/..//../hadoop/bin/hadoop job -kill job_1491449632882_0094INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0INFO : 2017-04-10 09:44:49,185 Stage-1 map = 0%, reduce = 0%INFO : 2017-04-10 09:44:57,202 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.98 secINFO : MapReduce Total cumulative CPU time: 1 seconds 980 msecINFO : Ended Job = job_1491449632882_0094INFO : Stage-3 is selected by condition resolver.INFO : Stage-2 is filtered out by condition resolver.INFO : Stage-4 is filtered out by condition resolver.INFO : Moving data to directory hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001 from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10003INFO : Moving data to directory hdfs:/tmp/foo from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001No rows affected (26.969 seconds)3. and the table location is hdfs://hacluster/tmp/foo :0: jdbc:hive2://189.39.151.44:21066/&gt; desc formatted foo;+-------------------------------+-------------------------------------------------------+-----------------------+--+| col_name | data_type | comment |+-------------------------------+-------------------------------------------------------+-----------------------+--+| # col_name | data_type | comment || | NULL | NULL || id | int | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || Owner: | userx | NULL || CreateTime: | Mon Apr 10 09:44:59 CST 2017 | NULL || LastAccessTime: | UNKNOWN | NULL || Protect Mode: | None | NULL || Retention: | 0 | NULL || Location: | hdfs://hacluster/tmp/foo | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | COLUMN_STATS_ACCURATE | false || | numFiles | 1 || | numRows | -1 || | rawDataSize | -1 || | totalSize | 56 || | transient_lastDdlTime | 1491788699 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe | NULL || InputFormat: | org.apache.hadoop.hive.ql.io.RCFileInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.RCFileOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+-------------------------------------------------------+-----------------------+--+</description>
      <version>1.2.2,1.3.0,2.1.1</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16425" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16429" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</summary>
      <description>Should call invokeFailureHooks in handleInterruption to track failed query execution due to interrupted command.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-17 01:00:00" id="17107" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Yetus to 0.5.0</summary>
      <description>Yetus 0.5.0 is released, and it contains our fixes.We should upgrade and remove our extra patched files.CC: kgyrtkirk</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.yetus-wrapper.sh</file>
      <file type="M">dev-support.maven.YETUS-506.sh</file>
      <file type="M">dev-support.findbugs.YETUS-471.sh</file>
      <file type="M">dev-support.checkstyle.YETUS-484.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-25 01:00:00" id="17167" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-25 01:00:00" id="17168" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-5-19 01:00:00" id="1731" opendate="2010-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-3 01:00:00" id="17682" opendate="2017-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: IF stmt produces wrong results</summary>
      <description>A query using with a vectorized IF(condition, thenExpr, elseExpr) function can produce wrong results.</description>
      <version>1.2.2,2.3.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-6 01:00:00" id="17988" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace patch utility usage with git apply in ptest</summary>
      <description>It would be great to replace the standard diff util because git can do a 3-way merge - which in most cases successfull.This could reduce the ptest results which are erroring out because of build failure.error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:7003Falling back to three-way merge...Applied patch to 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java' cleanly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-7 01:00:00" id="17996" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ASF headers</summary>
      <description>Yetus check reports some ASF header related issues in Hive code. Let's fix them up.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.resources.log4j2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumEmptyIsZeroAggFunction.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-9 01:00:00" id="18919" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove separate keytab setting for ZK in LLAP</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-6 01:00:00" id="1892" opendate="2011-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>show functions also returns internal operators</summary>
      <description>show functions: returns bigint etc. in its outputs, which are not valid external functions</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-9 01:00:00" id="18920" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Initialize the Janino providers ahead of 1st query</summary>
      <description>Hive Calcite metadata providers are compiled when the 1st query comes in.If a second query arrives before the 1st one has built a metadata provider, it will also try to do the same thing, because the cache is not populated yet.With 1024 concurrent users, it takes 6 minutes for the 1st query to finish fighting all the other queries which are trying to load that cache.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-8-16 01:00:00" id="20402" opendate="2018-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ITest needs explicit dependency on hbase-common test-jar</summary>
      <description>itest currently relies on hbase-common test jar for HBaseMiniCluster, but pulls in this dependency transitively via a bug in hbase-procedure's own dependency tree. This was fixed in HBase 2.1, so Hive should prepare for that by explicitly declaring the dependency.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-26 01:00:00" id="22053" opendate="2019-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function name is not normalized when creating function</summary>
      <description>If a function is created with a name containing upper case characters, we get NoSuchObjectException when trying to get that function.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestFunctions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>