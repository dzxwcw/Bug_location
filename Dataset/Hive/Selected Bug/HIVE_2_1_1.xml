<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  
  <bug fixdate="2016-4-26 01:00:00" id="14348" opendate="2016-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for alter table exchange partition</summary>
      <description/>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exchgpartition2lel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exchange.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-29 01:00:00" id="14378" opendate="2016-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data size may be estimated as 0 if no columns are being projected after an operator</summary>
      <description>in those cases we still emit rows.. but they may not have any columns within it. We shouldn't estimate 0 data size in such cases.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partial.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-4 01:00:00" id="14421" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FS.deleteOnExit holds references to _tmp_space.db files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-4 01:00:00" id="14422" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IF: when using LLAP IF from multiple threads in secure cluster, tokens can get mixed up</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapProtocolClientImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-5 01:00:00" id="14447" opendate="2016-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set HIVE_TRANSACTIONAL_TABLE_SCAN to the correct job conf for FetchOperator</summary>
      <description/>
      <version>1.3.0,2.1.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-10-25 01:00:00" id="14639" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle SKEWED BY for MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-25 01:00:00" id="14641" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle writing to dynamic partitions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-25 01:00:00" id="14643" opendate="2016-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle ctas for the MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MoveWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-6 01:00:00" id="14706" opendate="2016-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage information not set properly</summary>
      <description>I am trying to fetch column level lineage after a CTAS query in a Post Execution hook in Hive. Below are the queries:create table t1(id int, name string);create table t2 as select * from t1;The lineage information is retrieved using the following sample piece of code:lInfo = hookContext.getLinfo()for(Map.Entry&lt;LineageInfo.DependencyKey, LineageInfo.Dependency&gt; e : lInfo.entrySet()) { System.out.println("Col Lineage Key : " + e.getKey()); System.out.println("Col Lineage Value: " + e.getValue());}The Dependency field(i.e Col Lineage Value) is coming in as null.</description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-30 01:00:00" id="14865" opendate="2016-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix comments after HIVE-14350</summary>
      <description>there are still some comments in the code that should've been updated in HIVE-14350</description>
      <version>2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-7 01:00:00" id="14913" opendate="2016-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add new unit tests</summary>
      <description>Moving bunch of tests from system test to hive unit tests to reduce testing overhead</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.acid.part.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.acid.non.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.acid.part.update.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lvj.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.acid.non.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.current.date.timestamp.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cte.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.table.stats.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.basic.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-7 01:00:00" id="14915" opendate="2016-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to skip log collection for successful tests</summary>
      <description>We generate multiple gigs of tests at the moment. An option to skip log collection for successful tests could be useful.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestHostExecutor.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.AbstractTestPhase.java</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.server.TestExecutor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-5 01:00:00" id="1514" opendate="2010-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Be able to modify a partition&amp;#39;s fileformat and file location information.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.mix.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.diff.part.input.formats.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterPartitionProtectModeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-15 01:00:00" id="15434" opendate="2016-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDF to allow interrogation of uniontype values</summary>
      <description>OverviewAs stated in the documention:UNIONTYPE support is incomplete The UNIONTYPE datatype was introduced in Hive 0.7.0 (HIVE-537), but full support for this type in Hive remains incomplete. Queries that reference UNIONTYPE fields in JOIN (HIVE-2508), WHERE, and GROUP BY clauses will fail, and Hive does not define syntax to extract the tag or value fields of a UNIONTYPE. This means that UNIONTYPEs are effectively look-at-only.It is essential to have a usable uniontype. Until full support is added to Hive users should at least have the ability to inspect and extract values for further comparison or transformation.ProposalI propose to add a GenericUDF that has 2 modes of operation. Consider the following schema and data that contains a union:Schema:struct&lt;field1:uniontype&lt;int,string&gt;&gt;Query:hive&gt; select field1 from thing;{0:0}{1:"one"}Explode to StructThis method will recursively convert all unions within the type to structs with fields named tag_n, n being the tag number. Only the tag_* field that matches the tag of the union will be populated with the value. In the case above the schema of field1 will be converted to:struct&lt;tag_0:int,tag_1:string&gt;hive&gt; select extract_union(field1) from thing;{"tag_0":0,"tag_1":null}{"tag_0":null,"tag_1":one}hive&gt; select extract_union(field1).tag_0 from thing;0nullExtract the specified tagThis method will simply extract the value of the specified tag. If the tag number matches then the value is returned, if it does not, then null is returned.hive&gt; select extract_union(field1, 0) from thing;0null</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-6 01:00:00" id="15550" opendate="2017-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix arglist logging in schematool</summary>
      <description>In DEBUG mode schemaTool prints the password to log file.This is also seen if the user includes --verbose option.</description>
      <version>2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">errata.txt</file>
      <file type="M">pom.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-2-30 01:00:00" id="15748" opendate="2017-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove cycles created due to semi join branch and map join Op on same operator pipeline</summary>
      <description>If a semi join branch and map join operator are on same operator pipeline, then there could be a cycle created. Where the other map feeding into the mapjoin operator is waiting for the semi join branch to finish causing a cycle.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-31 01:00:00" id="15765" opendate="2017-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bracketed comments</summary>
      <description>C-style comments are in the SQL spec as well as supported by all major DBs. The are useful for inline annotation of the SQL. We should have them too.Example:select/*+ MAPJOIN(a) */ /* mapjoin hint */a /* column */from foo join bar;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-8 01:00:00" id="15853" opendate="2017-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin removed in case of dynamically partitioned hash join</summary>
      <description>Incase of dynamically partitioned hash join, the semijoin branch is removed to avoid a cycle even though it does not create a cycle.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-9 01:00:00" id="15867" opendate="2017-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add blobstore tests for import/export</summary>
      <description>This patch covers ten separate tests testing import and export operations running against blobstore filesystems: Import addpartition blobstore -&gt; file file -&gt; blobstore blobstore -&gt; blobstore blobstore -&gt; hdfs import/export blobstore -&gt; file file -&gt; blobstore blobstore -&gt; blobstore (partitioned and non-partitioned) blobstore -&gt; HDFS (partitioned and non-partitioned)</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-14 01:00:00" id="15903" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute table stats when user computes column stats</summary>
      <description/>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partition.with.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.table.invalidate.column.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-14 01:00:00" id="15916" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add blobstore tests for CTAS</summary>
      <description>This patch covers 3 tests checking CTAS operations against blobstore filesystems. The tests check we can create a table with a CTAS statement from another table, for the source-target combinations blobtore-blobstore, blobstore-hdfs, hdfs-blobstore, and for two target tables, one in the same default database as the source, and another in a new database.</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.ctas.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-17 01:00:00" id="15964" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-1 01:00:00" id="16064" opendate="2017-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ALL set quantifier with aggregate functions</summary>
      <description>SQL:2011 allows &lt;set quantifier&gt; ALL with aggregate functions which is equivalent to aggregate function without ALL.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-1 01:00:00" id="16067" opendate="2017-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: send out container complete messages after a fragment completes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapContainerLauncher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-1 01:00:00" id="16068" opendate="2017-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BloomFilter expectedEntries not always using NDV when it&amp;#39;s available during runtime filtering</summary>
      <description>The current logic only uses NDV if it's the only ColumnStat available, but it looks like there can sometimes be other ColStats in the semijoin Select operator.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-2 01:00:00" id="16084" opendate="2017-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW COMPACTIONS should display CompactionID</summary>
      <description>together with HIVE-13353 it will let users search for specific job</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.showlocks.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowCompactionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-3 01:00:00" id="16106" opendate="2017-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Datanucleus 4.2.12</summary>
      <description>As described in HIVE-14698, the datanucleus-rdbms package that we have currently (4.1.7) has a bug which generates incorrect synatx for MS SQL Server. The bug has been fixed in later releases. HIVE-14698 was a workaround for hive, but since DN has the fix in its 4.2.x line, we should pick it from there. Link to DN releases: https://sourceforge.net/projects/datanucleus/files/datanucleus-accessplatform/. We'll be upgrading the relevant jars in hive to their corresponding versions present in datanucleus-accessplatform v4.2.12</description>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-9 01:00:00" id="16154" opendate="2017-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Determine when dynamic runtime filtering should be disabled</summary>
      <description>Currently dynamic min/max/bloom optimization is always enabled. However there are times where it may not be beneficial, such as if the semijoin has a PK-FK relation and there are no filters on the semijoin table. Try to devise a way to do a cost/benefit calculation to see if there is enough benefit to adding the runtime filter.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.semijoin.reduction2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RuntimeValuesInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-9 01:00:00" id="16155" opendate="2017-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>No need for ConditionalTask if no conditional map join is created</summary>
      <description/>
      <version>2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinTaskDispatcher.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-24 01:00:00" id="16293" opendate="2017-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column pruner should continue to work when SEL has more than 1 child</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.varchar.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.null.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-4-30 01:00:00" id="16335" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline user HS2 connection file should use /etc/hive/conf instead of /etc/conf/hive</summary>
      <description>https://cwiki.apache.org/confluence/display/Hive/HiveServer2+Clientssays: BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/conf/hive in that order.shouldn't it be?BeeLine looks for it in ${HIVE_CONF_DIR} location and /etc/hive/conf in that order?Most distributions I've used have a /etc/hive/conf dir.</description>
      <version>2.1.1,2.2.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.hs2connection.UserHS2ConnectionFileParser.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-10 01:00:00" id="16413" opendate="2017-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table as select does not check ownership of the location</summary>
      <description>1. following statement failed: create table foo(id int) location 'hdfs:///tmp/foo';Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=userx, type=USER] does not have following privileges for operation CREATETABLE [[OBJECT OWNERSHIP] on Object [type=DFS_URI, name=hdfs://hacluster/tmp/foo]] (state=42000,code=40000)2. but when use create table as select, it successed:0: jdbc:hive2://189.39.151.44:21066/&gt; create table foo location 'hdfs:///tmp/foo' as select * from xxx2;INFO : Number of reduce tasks is set to 0 since there's no reduce operatorINFO : number of splits:1INFO : Submitting tokens for job: job_1491449632882_0094INFO : Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:haclusterINFO : The url to track the job: https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Starting Job = job_1491449632882_0094, Tracking URL = https://189-39-151-44:26001/proxy/application_1491449632882_0094/INFO : Kill Command = /opt/hive-1.3.0/bin/..//../hadoop/bin/hadoop job -kill job_1491449632882_0094INFO : Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0INFO : 2017-04-10 09:44:49,185 Stage-1 map = 0%, reduce = 0%INFO : 2017-04-10 09:44:57,202 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 1.98 secINFO : MapReduce Total cumulative CPU time: 1 seconds 980 msecINFO : Ended Job = job_1491449632882_0094INFO : Stage-3 is selected by condition resolver.INFO : Stage-2 is filtered out by condition resolver.INFO : Stage-4 is filtered out by condition resolver.INFO : Moving data to directory hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001 from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10003INFO : Moving data to directory hdfs:/tmp/foo from hdfs://hacluster/user/hive/warehouse/.hive-staging_hive_2017-04-10_09-44-32_462_4902211653847168915-1/-ext-10001No rows affected (26.969 seconds)3. and the table location is hdfs://hacluster/tmp/foo :0: jdbc:hive2://189.39.151.44:21066/&gt; desc formatted foo;+-------------------------------+-------------------------------------------------------+-----------------------+--+| col_name | data_type | comment |+-------------------------------+-------------------------------------------------------+-----------------------+--+| # col_name | data_type | comment || | NULL | NULL || id | int | || | NULL | NULL || # Detailed Table Information | NULL | NULL || Database: | default | NULL || Owner: | userx | NULL || CreateTime: | Mon Apr 10 09:44:59 CST 2017 | NULL || LastAccessTime: | UNKNOWN | NULL || Protect Mode: | None | NULL || Retention: | 0 | NULL || Location: | hdfs://hacluster/tmp/foo | NULL || Table Type: | MANAGED_TABLE | NULL || Table Parameters: | NULL | NULL || | COLUMN_STATS_ACCURATE | false || | numFiles | 1 || | numRows | -1 || | rawDataSize | -1 || | totalSize | 56 || | transient_lastDdlTime | 1491788699 || | NULL | NULL || # Storage Information | NULL | NULL || SerDe Library: | org.apache.hadoop.hive.serde2.columnar.ColumnarSerDe | NULL || InputFormat: | org.apache.hadoop.hive.ql.io.RCFileInputFormat | NULL || OutputFormat: | org.apache.hadoop.hive.ql.io.RCFileOutputFormat | NULL || Compressed: | No | NULL || Num Buckets: | -1 | NULL || Bucket Columns: | [] | NULL || Sort Columns: | [] | NULL || Storage Desc Params: | NULL | NULL || | serialization.format | 1 |+-------------------------------+-------------------------------------------------------+-----------------------+--+</description>
      <version>1.2.2,1.3.0,2.1.1</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-11 01:00:00" id="16419" opendate="2017-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hadoop related classes for JDBC stabdalone jar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16425" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-19 01:00:00" id="16482" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Ser/Des need to use dimension output name</summary>
      <description>Druid Ser/Desr need to use dimension output name in order to function with Extraction function.Some part of the Ser/Desr code uses the method DimensionSpec.getDimension() although when extraction function are in game the name of the dimension will be defined by DimensionSpec.getOutputName()</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-19 01:00:00" id="16483" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS should populate split related configurations to HiveConf</summary>
      <description>There are several split related configurations, such as MAPREDMINSPLITSIZE, MAPREDMINSPLITSIZEPERNODE, MAPREDMINSPLITSIZEPERRACK, etc., that should be populated to HiveConf. Currently we only do this for MAPREDMINSPLITSIZE.All the others, if not set, will be using the default value, which is 1.Without these, Spark sometimes will not merge small files for file formats such as text.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-19 01:00:00" id="16485" opendate="2017-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable outputName for RS operator in explain formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.formatted.oid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.explain.formatted.oid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.AnnotateReduceSinkOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Vertex.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Stage.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParser.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="16554" opendate="2017-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Make HouseKeeperService threads daemon</summary>
      <description/>
      <version>2.0.1,2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.HouseKeeperServiceBase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-27 01:00:00" id="16556" opendate="2017-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify schematool scripts to initialize and create METASTORE_DB_PROPERTIES table</summary>
      <description>sub-task to modify schema tool and its related changes so that the new table is added to the schema when schematool initializes or upgrades the schema.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-28 01:00:00" id="16557" opendate="2017-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Specialize ReduceSink empty key case</summary>
      <description>Gopal pointed out that native Vectorization of ReduceSink is missing the empty key case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkUniformHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorReduceSinkDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-19 01:00:00" id="16712" opendate="2017-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StringBuffer v.s. StringBuilder</summary>
      <description>Where appropriate, replaced StringBuffer with StringBuilder to remove superfluous synchronization.</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseFilterPlanUtil.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.Schema.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapRowRecordReader.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.DummyHiveMetastoreAuthorizationProvider.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.GroupingValidator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.DagJsonParserUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-25 01:00:00" id="16759" opendate="2017-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add table type information to HMS log notifications</summary>
      <description>The DB notifications used by HiveMetaStore should include the table type for all notifications that include table events, such as create, drop and alter table.This would be useful for consumers to identify views vs tables.</description>
      <version>2.1.1</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONInsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.InsertMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.DropTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.DropPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.api.TestHCatClientNotification.java</file>
      <file type="M">hcatalog.server-extensions.src.test.java.org.apache.hive.hcatalog.listener.TestNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONMessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONInsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONDropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.InsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-1 01:00:00" id="16801" opendate="2017-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: throwExpandError should be an immediate fatal</summary>
      <description>Vectorized hashtable throwExpandError() should not be retried before failing query - it is a deterministic failure (&amp; immediate query failure).</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-6-9 01:00:00" id="16871" opendate="2017-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CachedStore.get_aggr_stats_for has side affect</summary>
      <description>Every get_aggr_stats_for accumulates the stats and propagated to the first partition stats object. It accumulates and gives wrong result in the follow up invocations.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-5 01:00:00" id="1691" opendate="2010-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ANALYZE TABLE command should check columns in partition spec</summary>
      <description>ANALYZE TABEL PARTITION (col1, col2,...) should check whether col1, col2 etc are partition columns.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.dyn.part1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-28 01:00:00" id="16985" opendate="2017-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: enable SMB join in elevator after the former is fixed</summary>
      <description>We currently skip the IO elevator when we encounter an SMB join (see HIVE-16761). However, it might work with elevator with the code commented out in HIVE-16761. Need to look again after HIVE-16965 is fixed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.llap.smb.q</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-7-30 01:00:00" id="17005" opendate="2017-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure REPL DUMP and REPL LOAD are authorized properly</summary>
      <description>Currently, we piggyback REPL DUMP and REPL LOAD on EXPORT and IMPORT auth privileges. However, work is on to not populate all the relevant objects in inputObjs and outputObjs, which then requires that REPL DUMP and REPL LOAD be authorized at a higher level, and simply require ADMIN_PRIV to run,</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-1 01:00:00" id="17008" opendate="2017-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix boolean flag switchup in DropTableEvent</summary>
      <description>When dropping a non-existent database, the HMS will still fire registered DROP_DATABASE event listeners. This results in an NPE when the listeners attempt to deref the null database parameter.</description>
      <version>None</version>
      <fixedVersion>2.3.2,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-12 01:00:00" id="171" opendate="2008-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>distribute by and sort by ignored in the case of select *</summary>
      <description>create table tab1(col1 string, col2 string)select * from tab1 distribute by col1 sort by col2is different from:select col1, col2 from tab1 distribute by col1 sort by col2</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-25 01:00:00" id="17167" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-25 01:00:00" id="17168" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-15 01:00:00" id="1719" opendate="2010-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move RegexSerDe out of hive-contrib and over to hive-serde</summary>
      <description>RegexSerDe is as much a part of the standard Hive distribution as the other SerDescurrently in hive-serde. I think we should move it over to the hive-serde module so thatusers don't have to go to the added effort of manually registering the contrib jar beforeusing it.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">contrib.src.test.queries.clientpositive.serde.regex.q</file>
      <file type="M">contrib.src.test.queries.clientnegative.serde.regex.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-19 01:00:00" id="1731" opendate="2010-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve miscellaneous error messages</summary>
      <description>This is a place for accumulating error message improvements so that we can update a bunch in batch.</description>
      <version>None</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.table1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.function1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column6.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column5.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column4.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column3.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.unknown.column1.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.nonkey.groupby.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.map.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.list.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.index.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.function.param2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.dot.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.create.table.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.insert.wrong.number.columns.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.garbage.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.duplicate.alias.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.ambiguous.table.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.when.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.size.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.locate.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.instr.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.in.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.if.not.bool.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.field.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.type.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.elt.wrong.args.len.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.case.type.wrong.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.array.contains.wrong1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.strict.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.smb.bucketmapjoin.q.out</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.bad.sample.clause.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbydistributeby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clustern4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.function.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.index.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exim.00.unsupported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.fileformat.void.input.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby2.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.map.skew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby3.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.groupby.key.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.input.part0.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.part.nospec.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.noof.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.nopart.load.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.notable.alias3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.orderbysortby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.regex.col.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.sample.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.udtf.alias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-21 01:00:00" id="17364" opendate="2017-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test to "alter view" replication</summary>
      <description>Adding a unit test to HIVE-17354 change.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-27 01:00:00" id="17619" opendate="2017-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude avatica-core.jar dependency from avatica shaded jar</summary>
      <description>avatica.jar is included in the project but this jar has a dependency on avatica-core.jar and it's pulled into the project as well. If avatica-core.jar is included in the classpath in front of avatica.jar, then hive could run into missing class which is shaded inside avatica.jar.</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-2 01:00:00" id="17664" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description/>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-3 01:00:00" id="17679" opendate="2017-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>http-generic-click-jacking for WebHcat server</summary>
      <description>The web UIs do not include the "X-Frame-Options" header to prevent the pages from being framed from another site.Reference:https://www.owasp.org/index.php/Clickjackinghttps://www.owasp.org/index.php/Clickjacking_Defense_Cheat_Sheethttps://developer.mozilla.org/en-US/docs/Web/HTTP/X-Frame-Options</description>
      <version>2.1.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-15 01:00:00" id="177" opendate="2008-12-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying custom input and output format in CREATE TABLE</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.createTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-5 01:00:00" id="17701" opendate="2017-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added restriction to historic queries on web UI</summary>
      <description>The HiveServer2 Web UI (HIVE-12550) shows recently completed queries. However, a user can see the queries run by other users as well, and that is a security/privacy concern.Only admin users should be allowed to see queries from other users (similar to behavior of display for configs, stack trace etc).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-5 01:00:00" id="17702" opendate="2017-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect isRepeating handling in decimal reader in ORC</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-10 01:00:00" id="17765" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>expose Hive keywords</summary>
      <description>This could be useful e.g. for BI tools (via ODBC/JDBC drivers) to decide on SQL capabilities of Hive</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.GetInfoType.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TGetInfoType.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-12-4 01:00:00" id="18207" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the test failure for TestCliDriver#vector_complex_join</summary>
      <description>The test result miss the info about bigTableKeyExpressions &amp; bigTableValueExpressions</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.join.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="18208" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Fix the unit tests to run SMB Joins.</summary>
      <description>Most of the SMB Join tests are actually not creating SMB Joins. Need them to test the intended join.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.quotedid.smb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.cache.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="18210" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create resource plan allows duplicates</summary>
      <description>Create resource plan allows duplicates. This was seen in a cluster:+----------+-----------+--------------------+| rp_name | status | query_parallelism |+----------+-----------+--------------------+| plan_2 | ACTIVE | 10 || plan_2 | DISABLED | NULL |+----------+-----------+--------------------+</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2017-12-19 01:00:00" id="18306" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix spark smb tests</summary>
      <description>seems to me that TestSparkCliDriver#testCliDriver[auto_sortmerge_join_10] and TestSparkCliDriver#testCliDriver[bucketsortoptimize_insert_7] is failing since HIVE-18208 is in.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-3-6 01:00:00" id="1833" opendate="2010-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task-cleanup task should be disabled</summary>
      <description>Currently when task fails, a cleanup attempt will be scheduled right after that.This is unnecessary and increase the latency. MapReduce will allow disabling this (see MAPREDUCE-2206).After that patch is committed, we should set the JobConf in HIVE to disable cleanup task.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-22 01:00:00" id="18331" opendate="2017-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renew the Kerberos ticket used by Druid Query runner</summary>
      <description>Druid Http Client has to renew the current user Kerberos ticket when it is close to expire.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.ResponseCookieHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.KerberosHttpClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-11 01:00:00" id="18443" opendate="2018-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure git gc finished in ptest prep phase before copying repo</summary>
      <description>In ptest's prep phase script first we checkout the latest Hive code from git, and then we make copy of its contents (along .git folder) for that will serve as Yetus' working directory.In some cases we can see errors such as+ cp -R . ../yetuscp: cannot stat ?./.git/gc.pid?: No such file or directorye.g. hereThis is caused by git running its gc feature in the background when our prep script has already started copying. In cases where gc finishes while cp is running, we'll get this error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-25 01:00:00" id="18536" opendate="2018-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>IOW + DP is broken for insert-only ACID</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-30 01:00:00" id="18586" opendate="2018-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Derby to 10.14.1.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreGetMetaConf.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitionPublish.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-30 01:00:00" id="18587" opendate="2018-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert DML event may attempt to calculate a checksum on directories</summary>
      <description>Looks like in union case, some code path may pass directories in newFiles. Probably legacy copyData/moveData; both seem to assume that these paths are files, but do not actually enforce it.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-30 01:00:00" id="19083" opendate="2018-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-8 01:00:00" id="19451" opendate="2018-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Query Execution fails with ClassNotFoundException org.antlr.v4.runtime.CharStream</summary>
      <description>Stack trace - ERROR : Status: FailedERROR : Vertex failed, vertexName=Map 1, vertexId=vertex_1524814504173_1344_45_00, diagnostics=[Task failed, taskId=task_1524814504173_1344_45_00_000029, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure ) : attempt_1524814504173_1344_45_00_000029_0:java.lang.RuntimeException: java.io.IOException: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at org.apache.hadoop.hive.llap.daemon.impl.StatsRecordingThreadPool$WrappedCallable.call(StatsRecordingThreadPool.java:110) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.io.IOException: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderCreationException(HiveIOExceptionHandlerChain.java:97) at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderCreationException(HiveIOExceptionHandlerUtil.java:57) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:438) at org.apache.tez.mapreduce.lib.MRReaderMapred.setupOldRecordReader(MRReaderMapred.java:157) at org.apache.tez.mapreduce.lib.MRReaderMapred.setSplit(MRReaderMapred.java:83) at org.apache.tez.mapreduce.input.MRInput.initFromEventInternal(MRInput.java:703) at org.apache.tez.mapreduce.input.MRInput.initFromEvent(MRInput.java:662) at org.apache.tez.mapreduce.input.MRInputLegacy.checkAndAwaitRecordReaderInitialization(MRInputLegacy.java:150) at org.apache.tez.mapreduce.input.MRInputLegacy.init(MRInputLegacy.java:114) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.getMRInput(MapRecordProcessor.java:525) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:171) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266) ... 15 moreCaused by: org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Cannot construct instance of `org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn`, problem: org/antlr/v4/runtime/CharStream at [Source: (String)"{"queryType":"scan","dataSource":{"type":"table","name":"tpcds_real_bin_partitioned_orc_1000.tpcds_denormalized_druid_table_7mcd"},"intervals":{"type":"segments","segments":[{"itvl":"1998-11-30T00:00:00.000Z/1998-12-01T00:00:00.000Z","ver":"2018-05-03T11:35:22.230Z","part":0}]},"virtualColumns":[{"type":"expression","name":"vc","expression":"\"__time\"","outputType":"LONG"}],"resultFormat":"compactedList","batchSize":20480,"limit":9223372036854775807,"filter":{"type":"bound","dimension":"i_brand"[truncated 241 chars]; line: 1, column: 376] (through reference chain: org.apache.hive.druid.io.druid.query.scan.ScanQuery["virtualColumns"]-&gt;java.util.ArrayList[0]) at org.apache.hive.druid.com.fasterxml.jackson.databind.exc.InvalidDefinitionException.from(InvalidDefinitionException.java:67) at org.apache.hive.druid.com.fasterxml.jackson.databind.DeserializationContext.instantiationException(DeserializationContext.java:1601) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.wrapAsJsonMappingException(StdValueInstantiator.java:484) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.rewrapCtorProblem(StdValueInstantiator.java:503) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:285) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.ValueInstantiator.createFromObjectWith(ValueInstantiator.java:229) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.impl.PropertyBasedCreator.build(PropertyBasedCreator.java:195) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:488) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:288) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:245) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.CollectionDeserializer.deserialize(CollectionDeserializer.java:27) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromArray(BeanDeserializerBase.java:1428) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:185) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.SettableBeanProperty.deserialize(SettableBeanProperty.java:529) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeWithErrorWrapping(BeanDeserializer.java:528) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeUsingPropertyBased(BeanDeserializer.java:417) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializerBase.deserializeFromObjectUsingNonDefault(BeanDeserializerBase.java:1280) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserializeFromObject(BeanDeserializer.java:326) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer._deserializeOther(BeanDeserializer.java:194) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.BeanDeserializer.deserialize(BeanDeserializer.java:161) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer._deserializeTypedForId(AsPropertyTypeDeserializer.java:130) at org.apache.hive.druid.com.fasterxml.jackson.databind.jsontype.impl.AsPropertyTypeDeserializer.deserializeTypedFromObject(AsPropertyTypeDeserializer.java:97) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.AbstractDeserializer.deserializeWithType(AbstractDeserializer.java:254) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.impl.TypeWrappedDeserializer.deserialize(TypeWrappedDeserializer.java:68) at org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:4001) at org.apache.hive.druid.com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2992) at org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.initialize(DruidQueryRecordReader.java:104) at org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.initialize(DruidQueryRecordReader.java:123) at org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.getRecordReader(DruidQueryBasedInputFormat.java:297) at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:435) ... 24 moreCaused by: java.lang.NoClassDefFoundError: org/antlr/v4/runtime/CharStream at org.apache.hive.druid.io.druid.segment.virtual.ExpressionVirtualColumn.&lt;init&gt;(ExpressionVirtualColumn.java:60) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hive.druid.com.fasterxml.jackson.databind.introspect.AnnotatedConstructor.call(AnnotatedConstructor.java:124) at org.apache.hive.druid.com.fasterxml.jackson.databind.deser.std.StdValueInstantiator.createFromObjectWith(StdValueInstantiator.java:283) ... 57 moreCaused by: java.lang.ClassNotFoundException: org.antlr.v4.runtime.CharStream at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 64 more</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-3 01:00:00" id="20069" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix reoptimization in case of DPP and Semijoin optimization</summary>
      <description>reported by t3rmin4t0rIn case dynamic partition pruning; the operator statistics became partial; to only reflect the actually scanned partitions; but they are being used as an information about the "full" table...which leads to the exchange of the 2 tables being joined...which is really unfortunate...</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.OperatorStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSources.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-7 01:00:00" id="20335" opendate="2018-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for materialized view rewriting with composite aggregation functions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-8 01:00:00" id="20336" opendate="2018-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Masking and filtering policies for materialized views</summary>
      <description>Implement masking and filtering policies for materialized views.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-10 01:00:00" id="20527" opendate="2018-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intern table descriptors from spark task</summary>
      <description>Table descriptors from MR tasks and Tez tasks are interned. This fix is to intern table desc from spark tasks as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-3 01:00:00" id="21082" opendate="2019-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In HPL/SQL, declare statement does not support variable of type character</summary>
      <description>In the following HPL/SQL programs:DECLARE a character(5); SET a = 'b';when the type of variable 'a' is CHARACTER, it cannot be assigned a value successfully. The support for the character type should be added to DECLARE statement.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-10 01:00:00" id="21113" opendate="2019-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>For HPL/SQL that contains boolean expression with NOT, incorrect SQL may be generated.</summary>
      <description>In HPL/SQL, ' SELECT * FROM a WHERE NOT (1 = 2) ' will generate to incorrect SQL ' SELECT * FROM a WHERE (1 = 2) ', the 'NOT' in boolean expression is missing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.out.txt</file>
      <file type="M">hplsql.src.test.queries.offline.select.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-5-16 01:00:00" id="21739" opendate="2019-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make metastore DB backward compatible with pre-catalog versions of hive.</summary>
      <description>Since the addition of foreign key constraint between Database ('DBS') table and catalogs ('CTLGS') table in HIVE-18755 we are unable to run a simple create database command with an older version of Metastore Server. This is due to older versions having JDO schema as per older schema of 'DBS' which did not have an additional 'CTLG_NAME' column.The error is as follows: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Exception thrown flushing changes to datastore)....java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails ("metastore_1238"."DBS", CONSTRAINT "CTLG_FK1" FOREIGN KEY ("CTLG_NAME") REFERENCES "CTLGS" ("NAME"))</description>
      <version>1.2.0,2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.2.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.2.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.2.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-2-13 01:00:00" id="22647" opendate="2019-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable session pool by default</summary>
      <description>Non pooled session may leak when the client doesn't close the connection.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-26 01:00:00" id="23083" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable fast serialization in xprod edge</summary>
      <description>select count(*) from store_sales, store, customer, customer_address where ss_store_sk = s_store_sk and s_market_id=10 and ss_customer_sk = c_customer_sk and c_birth_country &lt;&gt; upper(ca_country);This uses "org/apache/hadoop/io/serializer/WritableSerialization" instead of TezBytesWritableSerialization. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-30 01:00:00" id="23109" opendate="2020-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query-based compaction omits database</summary>
      <description>E.g. MM major compaction query looks like:insert into tmp_table select * from src_table;it should beinsert into tmp_table select * from src_db.src_table;Therefore compaction fails if db of source table isn't default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactionQueryBuilder.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestMmCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorOnTezTest.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-1 01:00:00" id="23119" opendate="2020-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test sort_acid should be run by TestMiniLlapLocalCliDriver only</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-1 01:00:00" id="23120" opendate="2020-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey related tests should be run by TestMiniLlapLocalCliDriver only</summary>
      <description>TopNKey optimization is only used when the execution framework is Tez.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.windowing.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.grouping.sets.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.grouping.sets.order.q</file>
      <file type="M">ql.src.test.queries.clientpositive.topnkey.grouping.sets.functions.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-18 01:00:00" id="23494" opendate="2020-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Apache parent POM to version 23</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-20 01:00:00" id="23519" opendate="2020-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Read Ranger Configs from Classpath</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerLoadTask.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestRangerDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.RangerDumpTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-8-6 01:00:00" id="2354" opendate="2011-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support automatic rebuilding of indexes when they go stale</summary>
      <description>Support a mode where indexes will be automatically rebuilt when the table/partition they are based on is modified. So if index foo is built on table bar, and bar has it's contents overwritten, we should support a mode where index foo will automatically rebuild itself.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-1-5 01:00:00" id="2695" opendate="2012-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add PRINTF() Udf</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
</bugrepository>