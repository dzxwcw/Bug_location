<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-5-1 01:00:00" id="10568" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-13 01:00:00" id="1537" opendate="2010-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow users to specify LOCATION in CREATE DATABASE statement</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-14 01:00:00" id="15921" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-order the slider stop command to avoid a force if possible</summary>
      <description>A graceful stop is required for slider --service llapstatus to work properly</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-30 01:00:00" id="16340" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Kerberos + SSL connections to HMS</summary>
      <description>It should be possible to connect to HMS with Kerberos authentication and SSL enabled, at the same time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-31 01:00:00" id="16341" opendate="2017-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Task Execution Summary has incorrect input record counts on some operators</summary>
      <description>Task Execution Summary-------------------------------------------------------------------------------------------------------------------------------- VERTICES TOTAL_TASKS FAILED_ATTEMPTS KILLED_TASKS DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS-------------------------------------------------------------------------------------------------------------------------------- Map 1 167 0 0 17640.00 2,109,200 23,068 150,000,004 11,995,136 Map 11 5 0 0 10559.00 71,960 633 4,023,690 799,900 Map 13 1 0 0 2244.00 6,090 29 25 3 Map 3 1 0 0 2849.00 7,080 99 25 3 Map 5 271 0 0 55834.00 12,934,890 358,376 1,500,000,001 1,500,000,161 Map 7 241 0 0 91243.00 5,020,860 71,182 1,827,250,341 652,413,443Reducer 10 1 0 0 1010.00 1,900 0 4 0Reducer 12 1 0 0 3854.00 1,320 0 799,900 1Reducer 14 1 0 0 1420.00 3,790 45 3 1 Reducer 2 1 0 0 9720.00 6,220 122 11,995,136 1 Reducer 4 1 0 0 810.00 2,100 105 3 1 Reducer 6 1 0 0 24863.00 3,260 5 1,500,000,161 1 Reducer 8 412 0 0 88215.00 17,106,440 184,524 2,165,208,640 1,864 Reducer 9 2 0 0 29752.00 3,980 0 1,864 4--------------------------------------------------------------------------------------------------------------------Seeing this on queries using runtime filtering. Noticed the INPUT_RECORDS look incorrect for the reducers that are responsible for aggregating the min/max/bloomfilter (Reducers 12, 14, 2, 6). For example Reducer 2 shows 12M input records. However looking at the task logs for Reducer 2, there were only 167 input records.It looks like Map 1 has 2 different output vertices (Reducer 2 and Reducer 8), but the total output rows for Map 1 (rather than just the rows going to each specific vertex) is being counted in the input rows for both Reducer 2 and Reducer 8.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-27 01:00:00" id="18150" opendate="2017-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark Version to 2.2.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-27 01:00:00" id="18151" opendate="2017-11-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP external client: Better error message propagation during submission failures</summary>
      <description>During failed submissions, the original error message should be added to the error message that is eventually propagated to the user.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-9-28 01:00:00" id="20020" opendate="2018-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive contrib jar should not be in lib</summary>
      <description>Currently the way hive is packaged it includes hive-contrib-&lt;version&gt;.jar in lib, we should not include it here because it is picked up by services like HS2. This creates a situation in which experimental features such as the MultiDelimitSerDe are accessible without understanding how to really install and use it. For example you can create a table using HS2 via beeline with the aforementioned SerDe and it will work as long you do not do M/R jobs. The M/R jobs do not work because the SerDe is not in aux to get shipped into distcache. I propose we do not package it this way and if someone would like to leverage an experimental feature they can add it manually to their environment. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-4 01:00:00" id="20090" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend creation of semijoin reduction filters to be able to discover new opportunities</summary>
      <description>Assume the following plan:TS[0] - RS[1] - JOIN[4] - RS[5] - JOIN[8] - FS[9]TS[2] - RS[3] - JOIN[4] TS[6] - RS[7] - JOIN[8]Currently, TS[6] may only be reduced with the output of RS[5], i.e., input to join between both subplans.However, it may be useful to consider other possibilities too, e.g., reduced by the output of RS[1] or RS[3]. For instance, this is important when, given a large plan, an edge between RS&amp;#91;5&amp;#93; and TS&amp;#91;0&amp;#93; would create a cycle, while an edge between RS&amp;#91;1&amp;#93; and TS&amp;#91;6&amp;#93; would not.This patch comprises two parts. First, it creates additional predicates when possible. Secondly, it removes duplicate semijoin reduction branches/predicates, e.g., if another semijoin that consumes the output of the same expression already reduces a certain table scan operator (heuristic, since this may not result in most efficient plan in all cases). Ultimately, the decision on whether to use one or another should be cost-driven (follow-up).</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-6 01:00:00" id="20322" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlakyTest: TestMiniDruidCliDriver</summary>
      <description>TestMiniDruidCliDriver is failing intermittently but I'm seeing it fail a significant percentage of the time.druid_timestamptzdruidmini_joinsdruidmini_maskingdruidmini_test1</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-3-22 01:00:00" id="2069" opendate="2011-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NullPointerException on getSchemas</summary>
      <description>Calling getSchemas will cause a nullpointerexception</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveMetaDataResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-5 01:00:00" id="20692" opendate="2018-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable folding of NOT x IS (NOT) [TRUE|FALSE] expressions</summary>
      <description>Expressions like: not ((a&gt;0) is not true) could be rewritten to (a&gt;0) is true.However currently this doesn't happen because some of these functions are not translated for Calcite.create table t (a integer);explain select not ((a&gt;0) is not true) from t group by a;[...]expressions: (not (_col0 &gt; 0) is not true) (type: boolean) |[...]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-5-30 01:00:00" id="21670" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replacing mockito-all with mockito-core dependency</summary>
      <description>The mockito-all dependency contains an old version of Hamcrest core which can collide with other Hamcrest dependencies. Replacint it with mockito-core should be straightforward.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-24 01:00:00" id="2178" opendate="2011-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log related Check style Comments fixes</summary>
      <description>Fix Log related Check style Comments</description>
      <version>0.5.0,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.TBinarySortableProtocol.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.SimpleCharStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDataSource.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-7-10 01:00:00" id="21981" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>When LlapDaemon capacity is set to 0 and the waitqueue is not empty then the queries are stuck</summary>
      <description>When an LlapDaemon executor capacity is set to 0 then the already queued tasks are not handled causing the queries to stuck</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-30 01:00:00" id="22270" opendate="2019-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-io to 2.6</summary>
      <description>Hive's currently using commons-io 2.4 and according to HIVE-21273, a number of issues are present in it, which can be resolved by upgrading to 2.6:IOUtils copyLarge() and skip() methods are performance hogs affectsVersions:2.3;2.4 https://issues.apache.org/jira/projects/IO/issues/IO-355?filter=allopenissues CharSequenceInputStream#reset() behaves incorrectly in case when buffer size is not dividable by data size affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-356?filter=allopenissues &amp;#91;Tailer&amp;#93; InterruptedException while the thead is sleeping is silently ignored affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-357?filter=allopenissues IOUtils.contentEquals* methods returns false if input1 == input2; should return true affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-362?filter=allopenissues Apache Commons - standard links for documents are failing affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-369?filter=allopenissues FileUtils.sizeOfDirectoryAsBigInteger can overflow affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-390?filter=allopenissues Regression in FileUtils.readFileToString from 2.0.1 affectsVersions:2.1;2.2;2.3;2.4 https://issues.apache.org/jira/projects/IO/issues/IO-453?filter=allopenissues Correct exception message in FileUtils.getFile(File; String...) affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-479?filter=allopenissues org.apache.commons.io.FileUtils#waitFor waits too long affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-481?filter=allopenissues FilenameUtils should handle embedded null bytes affectsVersions:2.4 https://issues.apache.org/jira/projects/IO/issues/IO-484?filter=allopenissues Exceptions are suppressed incorrectly when copying files. affectsVersions:2.4;2.5 https://issues.apache.org/jira/projects/IO/issues/IO-502?filter=allopenissues </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-6-29 01:00:00" id="22681" opendate="2019-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-20 01:00:00" id="2296" opendate="2011-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bad compressed file names from insert into</summary>
      <description>When INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:Before INSERT INTO:000000_0.gzAfter INSERT INTO:000000_0.gz000000_0.gz_copy_1This causes corrupted output when doing a SELECT * on the table.Correct behavior should be to pick a valid filename such as:000000_0_copy_1.gz</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-3 01:00:00" id="23132" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test of Explain CBO of Merge statements</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sort.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.acid.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-3 01:00:00" id="23134" opendate="2020-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive &amp; Kudu interaction not available on ARM</summary>
      <description>Currently, we have set up an ARM CI to test out how Hive works on ARM platform:https://builds.apache.org/view/H-L/view/Hive/job/Hive-linux-ARM-trunk/According to the results, Hive &amp; Kudu interaction is not available on ARM platform:https://builds.apache.org/view/H-L/view/Hive/job/Hive-linux-ARM-trunk/25/testReport/org.apache.hadoop.hive.kudu/this is because that we use Kudu version 1.10 and that version does not come with ARM workable packages.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">itests.qtest-kudu.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-4 01:00:00" id="23136" opendate="2020-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not compare q test result if test.output.overwrite is specified</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-4 01:00:00" id="23139" opendate="2020-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve q test result masking</summary>
      <description>Q Test result masking can take a long time, in extreme cases up to 5 times as much as the test itself, because it is inefficient. Use contains, startsWith instead of patterns for masking lines containing or starting with specified strings.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QOutProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-15 01:00:00" id="23211" opendate="2020-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix metastore schema differences between init scripts, and upgrade scripts</summary>
      <description>There are some differences (character encoding, defaults etc..) in metastore schema if we initialize using the init scripts, or upgrade using the upgrade scripts. The schema should be identical.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.0.0-to-2.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.0.0-to-2.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-2.0.0-to-2.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.0.0-to-2.1.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.034-HIVE-13395.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.039-HIVE-12274.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.035-HIVE-13395.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.0.0-to-2.1.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.035-HIVE-13395.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.020-HIVE-13395.mssql.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-19 01:00:00" id="2398" opendate="2011-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive server doesn&amp;#39;t return schema for &amp;#39;set&amp;#39; command</summary>
      <description>The Hive server does process the CLI commands like 'set', 'set -v' sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.</description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-9-8 01:00:00" id="2431" opendate="2011-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrading thrift version didn&amp;#39;t upgrade libthrift.jar symlink correctly</summary>
      <description>libthrift.jar and libfb303.jar are symlinks to the current thrift version. With the upgrade to 0.7, there's a bug in the symlink creation.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-3-8 01:00:00" id="2439" opendate="2011-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade antlr version to 3.4</summary>
      <description>Upgrade antlr version to 3.4</description>
      <version>0.8.0</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.parse.sample6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.union.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf.when.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf.case.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.udf1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.subq.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample7.q.out</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ASTNode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.test.results.clientnegative.archive.partspec3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.clusterbyorderby.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.column.rename3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lateral.view.join.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.select.charliteral.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.table.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.columns2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.show.tables.bad2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.cast1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input20.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input9.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.part1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join7.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.join8.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample1.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.sample5.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-9 01:00:00" id="2440" opendate="2011-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make hive mapper initialize faster when having tons of input files</summary>
      <description>when one hive job has tons of input files, a lot of mappers may fail because of slow initialization.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-12 01:00:00" id="2442" opendate="2011-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore upgrade script and schema DDL for Hive 0.8.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-18 01:00:00" id="2455" opendate="2011-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass correct remoteAddress in proxy user authentication</summary>
      <description/>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-12-26 01:00:00" id="2468" opendate="2011-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive compile against Hadoop 0.23</summary>
      <description>Due to restructure of Hadoop 0.22 branch compared to Hadoop 0.20 Hive does not compile against 0.22</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.fs.ProxyLocalFileSystem.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.fs.ProxyFileSystem.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.build.xml</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">service.build.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobTrackerURLResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-7 01:00:00" id="2492" opendate="2011-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PDK PluginTest failing on Hudson</summary>
      <description/>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pdk.scripts.build-plugin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-12 01:00:00" id="2498" opendate="2011-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
      <description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-20 01:00:00" id="2519" opendate="2011-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-10 01:00:00" id="2568" opendate="2011-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-2246 upgrade script needs to drop foreign key in COLUMNS_OLD</summary>
      <description>One more bug in the MySQL metastore upgrade script: the foreign key in COLUMNS needs to be dropped, otherwise drop_partition will fail because the SDS row cannot be deleted due to the foreign key constraint.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-10 01:00:00" id="2569" opendate="2011-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too much debugging info on console if a job failed</summary>
      <description>When a job failed and Hive client tries to get the error message from failed task, it printed the following info on console for each task:Examining task ID: task_201110112120_773499_m_000037 from job job_201110112120_773499This should be shorten significantly.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-10 01:00:00" id="2572" opendate="2011-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-2246 upgrade script changed the COLUMNS_V2.COMMENT length</summary>
      <description>This changes from varchar(4000) to varchar(128) is backward incompatible.</description>
      <version>0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.008-HIVE-2246.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-11 01:00:00" id="2574" opendate="2011-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ivy offline mode broken by changingPattern and checkmodified attributes</summary>
      <description>As described here:http://www.mail-archive.com/ivy-user@ant.apache.org/msg03534.htmlThis wasn't the case formerly (maybe the upgrade to ivy 1.2?)</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-14 01:00:00" id="2578" opendate="2011-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Debug mode in some situations doesn&amp;#39;t work properly when child JVM is started from MapRedLocalTask</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">bin.ext.debug.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2011-5-12 01:00:00" id="2646" opendate="2011-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Ivy dependencies on Hadoop should depend on jars directly, not tarballs</summary>
      <description>The current Hive Ivy dependency logic for its Hadoop dependencies is problematic - depending on the tarball and extracting the jars from there, rather than depending on the jars directly. It'd be great if this was fixed to actually have the jar dependencies defined directly.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">shims.build.xml</file>
      <file type="M">service.ivy.xml</file>
      <file type="M">service.build.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">pdk.scripts.build-plugin.xml</file>
      <file type="M">pdk.ivy.xml</file>
      <file type="M">pdk.build.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">ivy.common-configurations.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
      <file type="M">builtins.ivy.xml</file>
      <file type="M">builtins.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-10 01:00:00" id="2703" opendate="2012-1-10 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>ResultSetMetaData.getColumnType() always returns VARCHAR(string) for partition columns irrespective of partition column type</summary>
      <description>ResultSetMetaData.getColumnType() always returns VARCHAR(string) as column type, no matter what the column type is for the partition column.However DatabaseMetadata.getColumnType() returns correct type. Create a table with a partition column having a type other than string, you will see that ResultSet.getColumnType() always returns string as the type for int or boolean or float columns...</description>
      <version>0.8.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-12-17 01:00:00" id="2723" opendate="2012-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>should throw "Ambiguous column reference key" Exception in particular join condition</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-22 01:00:00" id="2735" opendate="2012-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-24 01:00:00" id="2746" opendate="2012-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore client doesn&amp;#39;t log properly in case of connection failure to server</summary>
      <description>LOG.error(e.getStackTrace()) in current code prints memory location of StackTraceElement[] instead of message.</description>
      <version>0.8.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-25 01:00:00" id="2748" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-2-30 01:00:00" id="2762" opendate="2012-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter Table Partition Concatenate Fails On Certain Characters</summary>
      <description>Alter table partition concatenate creates a Java URI object for the location of a partition. If the partition name contains certain characters, such as } or space ' ', the object constructor fails, causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-2-9 01:00:00" id="2792" opendate="2012-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
      <description/>
      <version>0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.substr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ba.table.udfs.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-12-9 01:00:00" id="2794" opendate="2012-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregations without grouping should return NULL when applied to partitioning column of a partitionless table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-8-13 01:00:00" id="2804" opendate="2012-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Task log retrieval fails on Hadoop 0.23</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JobDebugger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.errors.TaskLogProcessor.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-6-1 01:00:00" id="3079" opendate="2012-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2989</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.tablelink.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.table.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.tablelink.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure5.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableIdentifier.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-2989.mysql.sql</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-12-14 01:00:00" id="3140" opendate="2012-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Comment indenting is broken for "describe" in CLI</summary>
      <description>Just go into the CLI and type "describe &amp;#91;TABLE_NAME&amp;#93;". If a comment has multiple lines, it is completely unreadable due to poor comment indenting. For example:birthdayParam string 1 = comment12 = comment23 = comment3But it supposed to display as:birthdayParam string 1 = comment1 2 = comment2 3 = comment3Comments should be indented the same amount on each line, i.e., if the comment starts at row k for the first line of the comment, it should be indented by k on line 2.</description>
      <version>None</version>
      <fixedVersion>0.10.0,0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.split.sample.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.columns.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.sahooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.14.managed.location.over.existing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.13.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.12.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.11.managed.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.10.external.managed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.08.nonpart.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.03.nonpart.over.compat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.01.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.00.nonpart.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.partition.filter.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation0.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.set.hiveconf.validation1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.archive.corrupt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.change.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.evolved.schemas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.joins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.sanity.test.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.error.message.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.bincolserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.table.colserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-8-24 01:00:00" id="3189" opendate="2012-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cast ( &lt;string type&gt; as bigint) returning null values</summary>
      <description>select rnum, c1, cast(c1 as bigint) from cert.tsdchar tsdchar where rnum in (0,1,2)create table if not exists CERT.TSDCHAR ( RNUM int , C1 string)row format sequencefilernum c1 _c20 -1 &lt;null&gt;1 0 &lt;null&gt;2 10 &lt;null&gt;</description>
      <version>0.8.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFDateAdd.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-24 01:00:00" id="3191" opendate="2012-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamp - timestamp causes null pointer exception</summary>
      <description>select tts.rnum, tts.cts - tts.cts from cert.tts ttsError: Query returned non-zero code: 12, cause: FAILED: Hive Internal Error: java.lang.NullPointerException(null)SQLState: 42000ErrorCode: 12create table if not exists CERT.TTS ( RNUM int , CTS timestamp) stored as sequencefile;</description>
      <version>0.8.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.NumericOpMethodResolver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-7-16 01:00:00" id="3261" opendate="2012-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter the number of buckets for a non-empty partitioned table should not be allowed</summary>
      <description>This is dangerous since the code uses the table metadata everywhere to get the number of buckets</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.exim.04.evolved.parts.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.groupby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-2-24 01:00:00" id="3937" opendate="2013-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Profiler</summary>
      <description>Adding a Hive Profiler implementation which tracks inclusive wall times and call counts of the operators</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-2-7 01:00:00" id="3995" opendate="2013-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL upgrade scripts are not valid</summary>
      <description>I've noticed that scripts for upgrading metastore backed up on PostgreSQL are not valid.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.012-HIVE-1362.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.011-HIVE-3649.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.010-HIVE-3072.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-7-2 01:00:00" id="4807" opendate="2013-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore hangs</summary>
      <description>Hive metastore hangs (does not accept any new connections) due to a bug in DBCP. The root cause analysis is here https://issues.apache.org/jira/browse/DBCP-398. The fix is to change Hive connection pool to BoneCP which is natively supported by DataNucleus.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-1-4 01:00:00" id="5946" opendate="2013-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-30 01:00:00" id="6338" opendate="2014-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve exception handling in createDefaultDb() in Metastore</summary>
      <description>There is a suggestion on HIVE-5959 comment list on possible improvements.</description>
      <version>0.8.0,0.9.0,0.10.0,0.11.0,0.12.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>