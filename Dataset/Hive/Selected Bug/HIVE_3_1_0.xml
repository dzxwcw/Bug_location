<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  
  <bug fixdate="2018-6-6 01:00:00" id="18874" opendate="2018-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HiveConnection shades log4j interfaces</summary>
      <description>This prevents Hive JDBC from being instantiated into a regular SLF4J logger env.java.lang.IncompatibleClassChangeError: Class org.apache.logging.slf4j.Log4jLoggerFactory does not implement the requested interface org.apache.hive.org.slf4j.ILoggerFactory at org.apache.hive.org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:285)</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-6 01:00:00" id="18875" opendate="2018-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable SMB Join by default in Tez</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.notin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-11 01:00:00" id="19168" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger changes for llap commands</summary>
      <description>New llap commands "llap cluster -info" and "llap cache -purge" require some changes so that Ranger can log the commands for auditing.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapClusterResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapCacheResourceProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-11 01:00:00" id="19175" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.testCliDriver update_access_time_non_current_db failing</summary>
      <description>Caused by HIVE-18060. Instead of generating golden file under clientpositive/llap it is under clientpositive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.update.access.time.non.current.db.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-4-13 01:00:00" id="19210" opendate="2018-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for streaming ingest</summary>
      <description>This will retain the old hcat streaming API for old clients. The new streaming ingest API will be separate module under hive.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-13 01:00:00" id="19212" opendate="2018-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs yetus pre-commit checks</summary>
      <description>Follow up from HIVE-18883, the committed patch isn't working and Findbugs is still not working.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-18 01:00:00" id="19239" opendate="2018-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check for possible null timestamp fields during SerDe from Druid events</summary>
      <description>Currently we do not check for possible null timestamp events.This might lead to NPE.This Patch add addition check for such case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidTimeseriesQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidGroupByQueryRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-19 01:00:00" id="19243" opendate="2018-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop.version to 3.1.0</summary>
      <description>Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is required for HIVE-18037 since it depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0).</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-20 01:00:00" id="19257" opendate="2018-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-19157 commit references wrong jira</summary>
      <description>1eea5a80ded2df33d57b2296b3bed98cb18383fd on master references wrong jira.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-23 01:00:00" id="19277" opendate="2018-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active/Passive HA web endpoints does not allow cross origin requests</summary>
      <description>CORS is not allowed with web endpoints added for active/passive HA. Enable CORS by default for all web endpoints.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2Peers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="19323" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore SQL install and upgrade scripts for 3.1</summary>
      <description>Now that we've branched for 3.0 we need to create SQL install and upgrade scripts for 3.1</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade.order.postgres</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade.order.oracle</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade.order.mysql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade.order.mssql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade.order.derby</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-26 01:00:00" id="19324" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve YARN queue check error message in Tez pool</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.YarnQueueHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-27 01:00:00" id="19344" opendate="2018-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of msck.repair.batch.size</summary>
      <description>msck.repair.batch.size default to 0 which means msck will try to add all the partitions in one API call to HMS. This can potentially add huge memory pressure on HMS. The default value should be changed to a reasonable number so that in case of large number of partitions we can batch the addition of partitions. Same goes for msck.repair.batch.max.retries</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-30 01:00:00" id="19362" opendate="2018-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable LLAP cache affinity by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-30 01:00:00" id="19363" opendate="2018-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove cryptic metrics from LLAP IO output</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.LLAPioSummary.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-1 01:00:00" id="19367" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Load Data should fail for empty Parquet files.</summary>
      <description>Load data does not validate the input for Parquet tables. This results in query failures.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-2 01:00:00" id="19385" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional hive env variable to redirect bin/hive to use Beeline</summary>
      <description>With beeline-site and beeline-user-site, the user can easily specify default hs2 urls to connect. We can use an optional env variable, which when set, will enable bin/hive to use beeline.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-2 01:00:00" id="19387" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables conflicts with ResultSet cache</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-2 01:00:00" id="19389" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool: For Hive&amp;#39;s Information Schema, use embedded HS2 as default</summary>
      <description>Currently, for initializing/upgrading Hive's information schema, we require a full jdbc url (for HS2). It will be good to have it connect using embedded HS2 by default.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-12-22 01:00:00" id="194" opendate="2008-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support specifying decimal places for ROUND function</summary>
      <description>Standard behavior:ROUND( number, [ decimal_places ] )decimal_places can be negative, which rounds digits to the left of the decimal point. NULL is returned if either argument is NULL.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRound.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-3 01:00:00" id="19415" opendate="2018-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CORS for all HS2 web endpoints</summary>
      <description>HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2018-5-10 01:00:00" id="19495" opendate="2018-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow SerDe itest failure</summary>
      <description>"You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowColumnarBatchSerDe.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-12 01:00:00" id="19510" opendate="2018-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add performance metric to find the total time spend in rsync</summary>
      <description>I think we are spending a lot of time copying logs from worker nodes to the server. We should add some logging to print aggregate time spent in rsync to confirm.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.MockRSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncResult.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommandExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ssh.RSyncCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.Phase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.LocalCommand.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-13 01:00:00" id="19517" opendate="2018-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable/delete TestNegativeCliDriver merge_negative_5 and mm_concatenate</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-3 01:00:00" id="1952" opendate="2011-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix some outputs and make some tests deterministic</summary>
      <description>Some of the tests are un-deterministic, and are causing intermediate diffs</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.vs.table.metadata.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input.part7.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-14 01:00:00" id="19529" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Date/Timestamp NULL issues</summary>
      <description>Wrong results found for: date_add/date_subUT areas: date_add/date_subdatediffto_dateinterval_year_month + interval_year_month interval_day_time + interval_day_time interval_day_time + timestamp timestamp + interval_day_time date + interval_day_time interval_day_time + date interval_year_month + date date + interval_year_month interval_year_month + interval_year_month timestamp + interval_year_monthdate - date interval_year_month - interval_year_month interval_day_time - interval_day_time timestamp - interval_day_time timestamp - timestamp date - timestamp timestamp - date date - interval_day_time date - interval_year_month timestamp - interval_year_month</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomBatchSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-14 01:00:00" id="19530" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix JDBCSerde and re-enable vectorization</summary>
      <description>According to jcamachorodriguez there is a big switch statement in the code that has might have missing types. This can lead to the string types seen.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-15 01:00:00" id="19560" opendate="2018-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retry test runner and retry rule for flaky tests</summary>
      <description>Implement custom test runner that retries failed tests as a workaround for flakiness. Also a test rule for retrying failed tests (for cases where custom test runner is not possible, e.g ParametrizedTests which already is a customer TestRunner).</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersMoveWorkloadManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-15 01:00:00" id="19562" opendate="2018-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flaky test: TestMiniSparkOnYarn FileNotFoundException in spark-submit</summary>
      <description>Seeing sporadic failures during test setup. Specifically, when spark-submit runs this error (or a similar error) gets thrown:2018-05-15T10:55:02,112 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: Exception in thread "main" java.io.FileNotFoundException: File file:/tmp/spark-56e217f7-b8a5-4c63-9a6b-d737a64f2820/__spark_libs__7371510645900072447.zip does not exist2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:867)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:442)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:365)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:316)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.copyFileToRemote(Client.scala:356)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:478)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:565)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:863)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:169)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.Client.run(Client.scala:1146)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1518)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136)2018-05-15T10:55:02,113 INFO [RemoteDriver-stderr-redir-27d3dcfb-2a10-4118-9fae-c200d2e095a5 main] client.SparkSubmitSparkClient: at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Essentially, Spark is writing some files for container localization to a tmp dir, and that tmp dir is getting deleted. We have seen a lot of issues with writing files to /tmp/ in the past, so its probably best to write these files to a test-specific dir.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-cluster.hive-site.xml</file>
      <file type="M">data.conf.spark.standalone.hive-site.xml</file>
      <file type="M">data.conf.spark.local.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-5-18 01:00:00" id="19615" opendate="2018-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Proper handling of is null and not is null predicate when pushed to Druid</summary>
      <description>Recent development in Druid introduced new semantic of null handling hereBased on those changes when need to honer push down of expressions with is null/ is not null predicates.The prosed fix overrides the mapping of Calcite Function to Druid Expression to much the correct semantic.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-21 01:00:00" id="19620" opendate="2018-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tmp directory used by PigServer in HCat tests</summary>
      <description/>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatBaseTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-21 01:00:00" id="19632" opendate="2018-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove webapps directory from standalone jar</summary>
      <description>JDBC standalone jar contains webapps static files which just adds to the jar size and are not required by the clients.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-21 01:00:00" id="19633" opendate="2018-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove/Migrate Minimr tests</summary>
      <description>MR has been deprecated for a long time. Minimr tests are incredibly slow. We should remove the tests or migrate to faster options for coverage.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.root.dir.external.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.using.q</file>
      <file type="M">ql.src.test.queries.clientpositive.root.dir.external.table.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifyNumReducersHook.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMinimrCliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-21 01:00:00" id="19641" opendate="2018-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sync up hadoop version used by storage-api with hive</summary>
      <description>There is hadoop version mismatch between hive and storage-api and hence different transitive dependency versions gets pulled.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-22 01:00:00" id="19643" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM table conversion doesn&amp;#39;t need full ACID structure checks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.conversions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.conversions.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-22 01:00:00" id="19654" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change tmp staging mapred directory for TestBlobstoreCliDriver</summary>
      <description>Similar to HIVE-19626.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-blobstore.src.test.resources.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="19669" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC to 1.5.1</summary>
      <description/>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-23 01:00:00" id="19677" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable sample6.q</summary>
      <description>Flaky test, already found similar behavior with sample2.q and sample4.q (HIVE-19657). More info to reproduce and try to fix the issue in HIVE-19673.</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-29 01:00:00" id="19727" opendate="2018-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Signature matching of table aliases</summary>
      <description>there is a probable problem with alias matching: "t1 as a" is matched to "t2 as a"</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-2 01:00:00" id="19775" opendate="2018-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should use HS2 embedded mode in privileged auth mode</summary>
      <description>Follow up of HIVE-19389.Authorization checks don't make sense for embedded mode and since it is not used in that mode it leads to issues if authorization is enabled (eg, username not set).</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.EmbeddedThriftBinaryCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.ShutdownHookManager.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-3 01:00:00" id="19778" opendate="2018-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable a flaky test: TestCliDriver#input31</summary>
      <description>Noticed this one has been failing occasionally on precommit test runs.Running: diff -a /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/itests/qtest/target/qfile-results/clientpositive/input31.q.out /home/hiveptest/35.193.227.186-hiveptest-1/apache-github-source-source/ql/src/test/results/clientpositive/input31.q.out128c128&lt; 496---&gt; 242</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-5 01:00:00" id="19796" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push Down TRUNC Fn to Druid Storage Handler</summary>
      <description>Push down Queries with TRUNC date function such as SELECT SUM((`ssb_druid_100`.`discounted_price` * `ssb_druid_100`.`net_revenue`)) AS `sum_calculation_4998925219892510720_ok`, CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE) AS `tmn___time_ok`FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY CAST(TRUNC(CAST(`ssb_druid_100`.`__time` AS TIMESTAMP),'MM') AS DATE)</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DruidSqlOperatorConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveExtractDate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-5 01:00:00" id="19799" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove jasper dependency</summary>
      <description>jasper dependency version looks old and unwanted. There is a comment which says it is required by thrift but I don't see jasper as thrift dependency. Try removing it to see if its safe (after precommit test run). </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenSelector.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-9 01:00:00" id="1980" opendate="2011-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merging using mapreduce rather than map-only job failed in case of dynamic partition inserts</summary>
      <description>In dynamic partition insert and if merge is set to true and hive.mergejob.maponly=false, the merge MapReduce job will fail.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-5 01:00:00" id="19801" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes</summary>
      <description/>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-8 01:00:00" id="19827" opendate="2018-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hiveserver2 startup should provide a way to override TEZ_CONF_DIR</summary>
      <description>HS2 should use /etc/tez/conf, HSI should use /etc/tez_llap/conf</description>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-3-10 01:00:00" id="1983" opendate="2011-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j configuration files in Hive JARs</summary>
      <description>Splitting this off as a subtask so that it can be resolved independently of the hive-default.xml issue.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">conf.hive-log4j.properties</file>
      <file type="M">conf.hive-exec-log4j.properties</file>
      <file type="M">common.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="19851" opendate="2018-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade jQuery version</summary>
      <description>jQuery version seems to be very old. Update to latest stable version. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.static.js.jquery.min.js</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="19852" opendate="2018-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update jackson to latest</summary>
      <description>Update jackson version to latest 2.9.5</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-12 01:00:00" id="19875" opendate="2018-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>increase LLAP IO queue size for perf</summary>
      <description>According to gopalv queue limit has perf impact, esp. during hashtable load for mapjoin where in the past IO used to queue up more data for processing to process.1) Overall the default limit could be adjusted higher.2) Depending on Decimal64 availability, the weight for decimal columns could be reduced.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-13 01:00:00" id="19881" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow metadata-only dump for database which are not source of replication</summary>
      <description>If the dump is meta data only then allow dump even if the db is not source of replication</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-13 01:00:00" id="19884" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalidation cache may throw NPE when there is no data in table used by materialized view</summary>
      <description/>
      <version>3.1.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-13 01:00:00" id="19885" opendate="2018-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Kafka Ingestion - Allow user to set kafka consumer properties via table properties</summary>
      <description>Allow users to set kafka consumer properties via table properties.</description>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.basic.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidkafkamini.basic.q</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.qtest.src.test.java.org.apache.hadoop.hive.cli.TestMiniDruidKafkaCliDriver.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-14 01:00:00" id="19902" opendate="2018-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Metastore micro-benchmarks</summary>
      <description>It would be very useful to have metastore benchmarks to be able to track perf issues.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-18 01:00:00" id="19938" opendate="2018-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade scripts for information schema</summary>
      <description>To make schematool -upgradeSchema work for information schema.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade.order.hive</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.1.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-21 01:00:00" id="19956" opendate="2018-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include yarn registry classes to jdbc standalone jar</summary>
      <description>HS2 Active/Passive HA requires some yarn registry classes. Include it in JDBC standalone jar. </description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-22 01:00:00" id="19972" opendate="2018-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Followup to HIVE-19928 : Fix the check for managed table</summary>
      <description>The check for managed table should use ENUM comparison rather than string comparison.The check in the patch will always return false, thus maintaining existing behavior.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-30 01:00:00" id="20040" opendate="2018-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HTTP listen queue is 50 and SYNs are lost</summary>
      <description>When testing with 5000 concurrent users, the JDBC HTTP port ends up overflowing on SYNs when the HS2 gc pauses.This is because each getQueryProgress request is an independent HTTP request, so unlike the BINARY mode, there are more connections being established &amp; closed in HTTP mode.LISTEN 0 50 *:10004 *:* This turns into connection errors when enabling net.ipv4.tcp_abort_on_overflow=1, but the better approach is to enqueue the connections until the HS2 is done with its GC pause.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-7-9 01:00:00" id="20123" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-2 01:00:00" id="20299" opendate="2018-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>potential race in LLAP signer unit test</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-3 01:00:00" id="20300" opendate="2018-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>VectorFileSinkArrowOperator</summary>
      <description>Bypass the row-mode FileSinkOperator for pushing Arrow format to the LlapOutputFormatService.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapRow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlapArrow.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-3 01:00:00" id="20301" opendate="2018-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for materialized view rewriting tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.ssb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.empty.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rebuild.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.time.window.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.rebuild.dummy.q</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.mv.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-6 01:00:00" id="20316" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip external table file listing for create table event.</summary>
      <description>We are currently skipping external table replication. We shall also skip listing all the files in create table event generation for external tables. External tables might have very large number of files, so it would take long time to list them.</description>
      <version>3.1.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-3-8 01:00:00" id="2034" opendate="2011-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport HIVE-1991 after overridden by HIVE-1950</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-8 01:00:00" id="20340" opendate="2018-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String</summary>
      <description>Druid time expressions return numeric values in form of ms (instead of formatted timestamp). Because of this expressions/function which expects its argument as string type ended up returning different values for time expressions input.e.g. SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;| 4-07-25 |SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;| 0022400000 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;| 2004-07-17 00:00:00 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;| 1090454400000 00:00:00 | Druid needs explicit cast to make this work</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-9 01:00:00" id="20357" opendate="2018-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce initOrUpgradeSchema option to schema tool</summary>
      <description>Currently, schematool has two option: initSchema/upgradeSchema. User needs to use different command line for different action. However, from the schema version stored in db, we shall able to figure out if there's a need to init/upgrade, and choose the right action automatically.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SchemaToolCommandLine.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.0.0-to-3.1.0.hive.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-21 01:00:00" id="20431" opendate="2018-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>txn stats write ID check triggers on set location</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.stats4.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-4 01:00:00" id="20499" opendate="2018-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTablesOperation pull all the tables meta irrespective of auth.</summary>
      <description>GetTablesOperation pull all the tables meta irrespective of auth.dbvisualizer and other ui based jdbc client pull tableemta similar to following operation:ResultSet res = con.getMetaData().getTables("", "", "%", new String[] { "TABLE", "VIEW" });https://github.com/rajkrrsingh/HiveServer2JDBCSample/blob/master/src/main/java/TestConnection.java#L20</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreFilterHook.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-5 01:00:00" id="20503" opendate="2018-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use datastructure aware estimations during mapjoin selection</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestVectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-9-5 01:00:00" id="20508" opendate="2018-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not support user names of type "user@realm"</summary>
      <description>Hive does not support user names of type "user@realm". This causes authorization problems with Ranger for user names containing realms in Kerberos environment. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-11-12 01:00:00" id="20545" opendate="2018-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to exclude potentially large parameters in HMS Notifications</summary>
      <description>Clients can add large-sized parameters in Table/Partition objects. So we need to enable adding regex patterns through HiveConf to match parameters to be filtered from table and partition objects before serialization in HMS notifications.</description>
      <version>3.1.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.MessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-10-21 01:00:00" id="20618" opendate="2018-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>During join selection BucketMapJoin might be choosen for non bucketed tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-29 01:00:00" id="20659" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-4-22 01:00:00" id="2068" opendate="2011-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Speed up query "select xx,xx from xxx LIMIT xxx" if no filtering or aggregation</summary>
      <description>Currently, "select xx,xx from xxx where ...(only partition conditions) LIMIT xxx" will start a MapReduce job with input to be the whole table or partition. The latency can be huge if the table or partition is big. We could reduce number of input files to speed up the queries.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LimitDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-5-22 01:00:00" id="2070" opendate="2011-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW GRANT grantTime field should be a human-readable timestamp</summary>
      <description>Unix timestamps are not super useful when trying to interpret metadatahive&gt; show grant user foo on table bar;database default table bar principalName foo principalType USER privilege Select grantTime 1300828549 grantor natty</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-8 01:00:00" id="20714" opendate="2018-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW tblproperties for a single property returns the value in the name column</summary>
      <description>show tblproperties default.tmpfoo("bar") returns:+------------+-------------+| prpt_name | prpt_value |+------------+-------------+| bar value | NULL |+------------+-------------+It should return+------------+-------------+| prpt_name | prpt_value |+------------+-------------+| bar | bar value |+------------+-------------+</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-9 01:00:00" id="20715" opendate="2018-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable test: udaf_histogram_numeric</summary>
      <description>this qtest is breaking a lot of testruns latelyI think it should be disabled</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-12 01:00:00" id="20735" opendate="2018-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address some of the review comments plus Kerberos support</summary>
      <description>As part of the review comments we agreed to: remove start and end offsets columns remove the best effort mode make the 2pc as default protocol for EOSAlso this patch will include an additional enhancement to add kerberos support.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.SimpleKafkaWriterTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaWritableTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaUtilsTest.java</file>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaRecordIteratorTest.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.SimpleKafkaWriter.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.MetadataColumn.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaWritable.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaUtils.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaTableProperties.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaStorageHandler.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaOutputFormat.java</file>
      <file type="M">kafka-handler.README.md</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-12-20 01:00:00" id="20785" opendate="2018-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong key name in the JDBC DatabaseMetaData.getPrimaryKeys method</summary>
      <description>According to the documentation (1) the key should be KEY_SEQ, not KEQ_SEQ.Pull request available: https://github.com/apache/hive/pull/440 (1) https://docs.oracle.com/javase/8/docs/api/java/sql/DatabaseMetaData.html#getPrimaryKeys-java.lang.String-java.lang.String-java.lang.String- </description>
      <version>3.1.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetPrimaryKeysOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-23 01:00:00" id="20793" opendate="2018-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add RP namespacing to workload management</summary>
      <description>The idea is to be able to use the same warehouse for multiple clusters in the cloud use cases. This scenario is not currently supported by WM.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMAlterResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMCreateOrDropTriggerToPoolMappingRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropPoolRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMDropTriggerRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetActiveResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMMapping.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullablePool.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullableResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMPool.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMPoolTrigger.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMTrigger.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MWMResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-11-7 01:00:00" id="20881" opendate="2018-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation oversimplifies projections</summary>
      <description>create table cx2(bool1 boolean);insert into cx2 values (true),(false),(null);set hive.cbo.enable=true;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+--------+| _c0 |+--------+| true || false || NULL |+--------+set hive.cbo.enable=false;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+-------+| _c0 |+-------+| true || NULL || NULL |+-------+from explain it seems the expression was simplified to: (_col0 is true or null)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-12-4 01:00:00" id="21004" opendate="2018-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less object creation for Hive Kafka reader</summary>
      <description>Reduce the amount of un-needed object allocation by using a row boat as way to carry data around.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.VectorizedKafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-5 01:00:00" id="21007" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join + Union can lead to wrong plans</summary>
      <description>Tez compiler has the ability to push JOIN within UNION (by replicating join on each branch). If this JOIN had a SJ branch outgoing (or incoming) it could mess up the plan and end up generating incorrect or wrong plan.As a safe measure any SJ branch after UNION should be removed (until we improve the logic to better handle SJ branches)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-13 01:00:00" id="21038" opendate="2018-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix checkstyle for standalone-metastore</summary>
      <description>Since HIVE-17506 checkstyle is not working for standalone-metastore and it's sub projects.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-15 01:00:00" id="21124" opendate="2019-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL does not support the CREATE TABLE LIKE statement</summary>
      <description>Hive supports the CREATE TABLE LIKE statement but HPL/SQL does not support.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-3-4 01:00:00" id="21383" opendate="2019-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC storage handler: Use catalog and schema to retrieve tables if specified</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.Constants.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-13 01:00:00" id="21444" opendate="2019-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional tests for materialized view rewriting</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.rewrite.1.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-18 01:00:00" id="21462" opendate="2019-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrading SQL server backed metastore when changing data type of a column with constraints</summary>
      <description>SQL server does not allow changing data type of a column which has a constraint or an index on it. The constraint or the index needs to be dropped before changing the data type and needs to be recreated after that. Metastore upgrade scripts aren't doing this and thus upgrade fails.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.1.0-to-2.2.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-29 01:00:00" id="21541" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing asf headers from HIVE-15406</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloatNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimalNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromDate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-29 01:00:00" id="21543" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use FilterHooks for show compactions</summary>
      <description>Use FilterHooks for checking dbs/tables/partitions for showCompactions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-29 01:00:00" id="21544" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation corrupts coalesce/case/when expressions during folding</summary>
      <description>set hive.fetch.task.conversion=none;set hive.optimize.ppd=false;create table t (s1 string,s2 string);insert into t values (null,null);explainselect coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;select coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;incorrect result is:null_value null_value NULL noteqexpected result:null_value null_value true eq</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-6 01:00:00" id="21587" opendate="2019-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain formatted CBO should write row type in JSON</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelWriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-24 01:00:00" id="21645" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-30 01:00:00" id="21668" opendate="2019-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove tomcat dependencies even from tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-6-5 01:00:00" id="21836" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-1 01:00:00" id="21939" opendate="2019-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>protoc:2.5.0 dependence has broken building on aarch64</summary>
      <description>When I try to build master of Hive from source code on "aarch64" server, I met following error:&amp;#91;ERROR&amp;#93; Failed to execute goal com.github.os72:protoc-jar-maven-plugin:3.5.1.1:run (default) on project hive-standalone-metastore-common: Error resolving artifact: com.google.protobuf:protoc:2.5.0: Could not find artifact com.google.protobuf:protoc:exe:linux-aarch_64:2.5.0 in central (https://repo.maven.apache.org/maven2)that is because Hive using the "com.google.protobuf:protoc:2.5.0" as required artifact, which does not have released package for "aarch64" platform. In order to fix this, I bumped the protobuf used in standalone-metadata to 2.6.1 and added a new profile, this profile will identifythe hardware architecture and if it is Aarch64, it will override the protobuf group.id and package to com.github.os72 whichincludes ARM support. For X86 platform, Hive will still download the protobuf packages from org.google repo. With this method,we can make Hive able to run on Aarch64 and keep the influence to existing x86 users to the minimum.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-2 01:00:00" id="21941" opendate="2019-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use checkstyle ruleset in Pre Upgrade Tool project</summary>
      <description>The project upgrade-acid/pre-upgrade does not uses the same checkstyle ruleset as hive root project</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-3 01:00:00" id="21948" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement parallel processing in Pre Upgrade Tool</summary>
      <description>Pre Upgrade Tool scans for all databases and tables in the warehouse sequentially which can be very slow in case of lots of tables.Example: It took the process 8-10 hours to complete on ~500k tables.</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.src.main.java.org.apache.hadoop.hive.upgrade.acid.RunOptions.java</file>
      <file type="M">upgrade-acid.pre-upgrade.src.main.java.org.apache.hadoop.hive.upgrade.acid.PreUpgradeTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-3 01:00:00" id="21949" opendate="2019-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-21232 LLAP: Add a cache-miss friendly split affinity provider</summary>
      <description>HDFS skew issues become LLAP skew issues and issues of skew need to be fixed by running "hdfs balancer" instead of getting a uniform distribution via LLAP.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-29 01:00:00" id="22059" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-5 01:00:00" id="22084" opendate="2019-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement exchange partitions related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to exchanging partitions:Partition exchange_partition(Map&lt;String, String&gt; partitionSpecs, String sourceDb, String sourceTable, String destdb, String destTableName);Partition exchange_partition(Map&lt;String, String&gt; partitionSpecs, String sourceCat, String sourceDb, String sourceTable, String destCat, String destdb, String destTableName);List&lt;Partition&gt; exchange_partitions(Map&lt;String, String&gt; partitionSpecs, String sourceDb, String sourceTable, String destdb, String destTableName);List&lt;Partition&gt; exchange_partitions(Map&lt;String, String&gt; partitionSpecs, String sourceCat, String sourceDb, String sourceTable, String destCat, String destdb, String destTableName);In order to support partitions on temporary tables, these methods must be implemented. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22089" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description/>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22090" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-21 01:00:00" id="22132" opendate="2019-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-lang3 version to 3.9</summary>
      <description>Upgrade commons lang 3 to 3.9</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-21 01:00:00" id="22134" opendate="2019-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 3.1 driver includes org.glassfish.jersey.* which can interfer with an application</summary>
      <description>An application that uses JAX-RS 1.1 can be broken by the Hive 3.1 standalone JAR.For example, an application is running in IBM Websphere Liberty  Profile (WLP) which detects the classes packaged in the Apache Hive standalone JAR.  This results in WLP assuming that the application is providing it's implementation and should not use the default in WLP. Can the Apache Hive JDBC team confirm why these classes are in the JAR.Can the Apache Hive JDBC team schedule to remove them if they are not mandatory.Can the Apache Hive JDBC team confirm which individual JAR files can be copied instead of the uber-standalone JAR which would not include these conflicting classes.This is the class which triggers the problem if all of the jersey stuff is deleted the issue will go awayorg.glassfish.jersey.server.internal.RuntimeDelegateImplorg.glassfish.jersey.*</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-5 01:00:00" id="22170" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-12 01:00:00" id="22199" opendate="2019-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ugrade findbugs to 3.0.5</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">findbugs.findbugs-exclude.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-1-10 01:00:00" id="222" opendate="2009-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by on a combination of disitinct and non distinct aggregates can return serialization errors with map side aggregations.</summary>
      <description>For queries of the form (groupby2_map.q in the source)SELECT x, count(DISTINCT y), SUM FROM t GROUP BY xwhen map side aggregation is on hive.map.aggr=true (This is off by default)The following exception can occur: &amp;#91;junit&amp;#93; Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to java.lang.Double &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeTypeDouble.serialize(DynamicSerDeTypeDouble.java:60) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeFieldList.serialize(DynamicSerDeFieldList.java:235) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDeStructBase.serialize(DynamicSerDeStructBase.java:81) &amp;#91;junit&amp;#93; at org.apache.hadoop.hive.serde2.dynamic_type.DynamicSerDe.serialize(DynamicSerDe.java:174)</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-17 01:00:00" id="22211" opendate="2019-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change maven phase to generate test sources</summary>
      <description>Some protobuf files are generated in the wrong phase; so I get compile errors because they are not there for eclipse...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-17 01:00:00" id="22212" opendate="2019-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement append partition related methods on temporary tables</summary>
      <description>The following methods must be implemented in SessionHiveMetastoreClient, in order to support partition append on temporary tables: Partition appendPartition(String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, List&lt;String&gt; partVals) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException; Partition appendPartition(String catName, String dbName, String tableName, String name) throws InvalidObjectException, AlreadyExistsException, MetaException, TException;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-18 01:00:00" id="22213" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TxnHander cleanupRecords should only clean records belonging to default catalog</summary>
      <description>Currently it removes record for given database and given table without checking for the catalog, as a result it can end up removing records when it shouldn't.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-18 01:00:00" id="22214" opendate="2019-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain vectorization should disable user level explain</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.topnkey.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-10-16 01:00:00" id="22357" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema mismatch between the Hive table definition and the "hive.sql.query" Parameter</summary>
      <description>The problem is that, for certain types of schema mismatch between the Hive table definition and the "hive.sql.query" parameter, Hive simply returns no rows rather than throwing an error when queried.Ideally Hive would check for schema compatibility during table definition and throw an error if there's a problem - the earlier the better. But even if that cannot be made a requirement, I'd definitely expect an error rather than a silent failure (i.e. zero rows returned) at runtime.I'm attaching investigation of this issue in "Hive-JDBC schema matching sensitivity.txt".</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.JdbcRecordIterator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-16 01:00:00" id="22358" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schedule shorthands for convinience</summary>
      <description>Add shorthands for schedulesevery minuteevery 10 minutesevery 10 secondsevery 4 hoursevery 4 hours offset by '2:03:04'every day offset by '2:03:04'every day at '2:03:04'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryStatements.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ScheduledQueryAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.HookContext.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-1-12 01:00:00" id="22635" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-24 01:00:00" id="22770" opendate="2020-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip interning of MapWork fields during deserialization</summary>
      <description>HIVE-19937 introduced some interning logic into mapwork deserialization process, but it's only related to spark, maybe we should skip this for tez, reducing the cpu pressure in tez tasks.UPDATE: Hive on spark is not supported anymore, the MapWorkSerializer can be completely removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-31 01:00:00" id="22803" opendate="2020-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mark scheduled queries executions to help end-user identify it easier</summary>
      <description>scheduled queries are executed as-is;it might probably help a lot in field deployments to have a hint where that query is coming from; I'm thinking of prefixing the query with some comment:/*schedule:sc1*/ select 1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-31 01:00:00" id="22804" opendate="2020-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure ANSI quotes are used for mysql connections</summary>
      <description>Recent changes in direct sql queries to resolve postgres issues(e.g. TxnHandler in HIVE-22663 ) break compatibility with mysql backend db. A workaround for these issues is to add a session config to the mysql connection string, e.g.:jdbc:mysql://localhost:3306/db?sessionVariables=sql_mode=ANSI_QUOTES </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.HikariCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.DbCPDataSourceProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.datasource.BoneCPDataSourceProvider.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-4-10 01:00:00" id="23004" opendate="2020-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Decimal64 operations across multiple vertices</summary>
      <description>Support Decimal64 operations across multiple vertices</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.fast.LazySimpleDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.fast.LazyBinaryDeserializeRead.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.fast.BinarySortableDeserializeRead.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.trailing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.case.when.nvl.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal64.case.when.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.vectorized.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-11 01:00:00" id="23011" opendate="2020-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shared work optimizer should check residual predicates when comparing joins</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="23035" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-18 01:00:00" id="23048" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use sequences for TXN_ID generation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Oracle.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.DatabaseRule.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.DbInstallBase.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.SQLGenerator.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.ITestDbTxnManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-20 01:00:00" id="23058" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction task reattempt fails with FileAlreadyExistsException</summary>
      <description>Issue occurs when compaction attempt is relaunched after first task attempt failure due to preemption by Scheduler or any other reason.Since _tmp directory was created by first attempt and was left uncleaned after task attempt failure. Second attempt of the the task fails with "FileAlreadyExistsException" exception.Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.fs.FileAlreadyExistsException): /warehouse/tablespace/managed/hive/default.db/compaction_test/_tmp_3670bbef-ba7a-4c10-918d-9a2ee17cbd22/base_0000186/bucket_00005 for client 10.xx.xx.xxx already exists</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-20 01:00:00" id="23059" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In constraint name uniqueness query use the MTable instead of it&amp;#39;s id</summary>
      <description>The constraint name uniqueness query uses the MTable instead of it's id. This will solve the immediate problem of not being able to create the sys db in Oracle, but not solve the same problem about the rest of the queries with integer number parameters (Long, Integer, etc).</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-20 01:00:00" id="23062" opendate="2020-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive to check Yarn RM URL in TLS and Yarn HA mode for custom Tez queue</summary>
      <description>Currently if custom Tez queue is used, Hive will only check the Http port, so it is not handling TLS and Yarn HA mode URL. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.YarnQueueHelper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-21 01:00:00" id="23065" opendate="2020-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-service</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveCommandOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-10 01:00:00" id="23178" opendate="2020-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description/>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-11 01:00:00" id="23181" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-8-28 01:00:00" id="2319" opendate="2011-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-17 01:00:00" id="23239" opendate="2020-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution via transitive dependency</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-19 01:00:00" id="23241" opendate="2020-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce transitive dependencies</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-19 01:00:00" id="23243" opendate="2020-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accept SQL type like pattern for Show Databases</summary>
      <description>Show Databases pattern accepts java like pattern with * and ., use SQL like instead with % and _.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.describe.database.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.database.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.describe.database.json.q</file>
      <file type="M">ql.src.test.queries.clientpositive.database.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.show.ShowDatabasesOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-21 01:00:00" id="23267" opendate="2020-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on groovy</summary>
      <description>Transitively pulled where its unneeded.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-22 01:00:00" id="23268" opendate="2020-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate beanutils transitive dependency</summary>
      <description>Transitively retrieved from hadoop-commons</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-22 01:00:00" id="23272" opendate="2020-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix and reenable timestamptz_2.q</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestTimestampTZWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampLocalTZWritable.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-23 01:00:00" id="23278" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on bouncycastle</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-30 01:00:00" id="23347" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSCK REPAIR cannot discover partitions with upper case directory names.</summary>
      <description>For the following scenario, we expect MSCK REPAIR to discover partitions but it couldn't.1. Have partitioned data path as follows.hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=10hdfs://mycluster/datapath/t1/Year=2020/Month=03/Day=112. create external table t1 (key int, value string) partitioned by (Year int, Month int, Day int) stored as orc location hdfs://mycluster/datapath/t1'';3. msck repair table t1;4. show partitions t1; --&gt; Returns zero partitions5. select * from t1; --&gt; Returns empty data.When the partition directory names are changed to lower case, this works fine.hdfs://mycluster/datapath/t1/year=2020/month=03/day=10hdfs://mycluster/datapath/t1/year=2020/month=03/day=11</description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.Msck.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreChecker.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.CheckResult.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestMsckCreatePartitionsInBatches.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-1 01:00:00" id="23351" opendate="2020-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Replication Scheduling</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-8 01:00:00" id="23414" opendate="2020-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Detail Hive Java Compatibility</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">README.md</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-11 01:00:00" id="23435" opendate="2020-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Full outer join result is missing rows</summary>
      <description>Full Outer join result has missing rows. Appears to be a bug with the full outer join logic. Expected output is receiving when we do a left and right outer join.Reproducible steps are mentioned below.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~SUPPORT ANALYSISSteps to Reproduce:1. Create a table and insert data:create table x (z char(5), x int, y int);insert into x values ('one', 1, 50), ('two', 2, 30), ('three', 3, 30), ('four', 4, 60), ('five', 5, 70), ('six', 6, 80);2. Try full outer with the below command. The result is incomplete, it is missing the row:NULL NULL NULL three 3 30.0 Full Outer Join:select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 full outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`);Result:----------------------------------+x1.z x1.x x1.y x2.z x2.x x2.y ----------------------------------+one 1 50 NULL NULL NULL NULL NULL NULL one 1 50 two 2 30 NULL NULL NULL NULL NULL NULL two 2 30 three 3 30 NULL NULL NULL four 4 60 NULL NULL NULL NULL NULL NULL four 4 60 five 5 70 NULL NULL NULL NULL NULL NULL five 5 70 six 6 80 NULL NULL NULL NULL NULL NULL six 6 80 ----------------------------------+3. Expected output is coming when we use left/right join + union:select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 left outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`) union select x1.`z`, x1.`x`, x1.`y`, x2.`z`, x2.`x`, x2.`y` from `x` x1 right outer join `x` x2 on (x1.`x` &gt; 3) and (x2.`x` &lt; 4) and (x1.`x` = x2.`x`);Result:------------------------------------+z x y _col3 _col4 _col5 ------------------------------------+NULL NULL NULL five 5 70 NULL NULL NULL four 4 60 NULL NULL NULL one 1 50 four 4 60 NULL NULL NULL one 1 50 NULL NULL NULL six 6 80 NULL NULL NULL three 3 30 NULL NULL NULL two 2 30 NULL NULL NULL NULL NULL NULL six 6 80 NULL NULL NULL three 3 30 NULL NULL NULL two 2 30 five 5 70 NULL NULL NULL ------------------------------------+ </description>
      <version>3.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.1to1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-6-9 01:00:00" id="23668" opendate="2020-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up Task for Hive Metrics</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestReplicationMetrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MReplicationMetrics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.metric.MetricSink.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-12-8 01:00:00" id="2634" opendate="2011-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-2566</summary>
      <description>This is leading to some problems.I will upload the offending testcase in a new jira.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-12 01:00:00" id="2650" opendate="2011-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel test commands that include cd fail</summary>
      <description>Parallel test commands that include cd fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.Ssh.py</file>
    </fixedFiles>
  </bug>
</bugrepository>