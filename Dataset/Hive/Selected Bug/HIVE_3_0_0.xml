<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-4-16 01:00:00" id="10364" opendate="2015-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HMS upgrade script test does not publish results when prepare.sh fails.</summary>
      <description>The HMS upgrade script must publish succeed or failure results to JIRA. This bug is not publishing any results on JIRA is the prepare.sh script fails.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-3-18 01:00:00" id="1555" opendate="2010-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC Storage Handler</summary>
      <description>With the Cassandra and HBase Storage Handlers I thought it would make sense to include a generic JDBC RDBMS Storage Handler so that you could import a standard DB table into Hive. Many people must want to perform HiveQL joins, etc against tables in other systems etc.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-15 01:00:00" id="16225" opendate="2017-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in webhcat service (FileSystem CACHE entries)</summary>
      <description>This is a known beast. here are detailsThe problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-27 01:00:00" id="16305" opendate="2017-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional Datanucleus ClassLoaderResolverImpl leaks causing HS2 OOM</summary>
      <description>This is a followup for HIVE-16160. We see additional ClassLoaderResolverImpl leaks even with the patch.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="16316" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Prepare master branch for 3.0.0 development.</summary>
      <description>master branch is now being used for 3.0.0 development. The build files will need to reflect this change.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.2.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.2.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.2.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.2.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.2.0-to-3.0.0.derby.sql</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-3-28 01:00:00" id="16319" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Better handling of an empty wait queue, should try scheduling checks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-13 01:00:00" id="1633" opendate="2010-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CombineHiveInputFormat fails with "cannot find dir for emptyFile"</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-30 01:00:00" id="16334" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query lock contains the query string, which can cause OOM on ZooKeeper</summary>
      <description>When there are big number of partitions in a query this will result in a huge number of locks on ZooKeeper. Since the query object contains the whole query string this might cause serious memory pressure on the ZooKeeper services.It would be good to have the possibility to truncate the query strings that are written into the locks</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestHiveLockObject.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestEmbeddedLockManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDummyTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DummyTxnManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-5 01:00:00" id="16383" opendate="2017-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to HikariCP as default connection pooling</summary>
      <description>Since 3.0 is planned to move to JDK8, we can now switch to HikariCP as default connection pooling for DN because of its improved performance over others.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-6 01:00:00" id="16394" opendate="2017-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS does not support queue name change in middle of session</summary>
      <description>The mapreduce.job.queuename only effects when HoS executes its query first time. After that, changing mapreduce.job.queuename won't change the query yarn scheduler queue name.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-6 01:00:00" id="16402" opendate="2017-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 2.8.0</summary>
      <description>Hadoop 2.8.0 has been out since March, we should upgrade to it. Release notes for Hadoop 2.8.x are here: http://hadoop.apache.org/docs/r2.8.0/index.htmlIt has a number of useful features, improvements for S3 support, ADLS support, etc. along with a bunch of other fixes. This should also help us on our way to upgrading to Hadoop 3.x (HIVE-15016).</description>
      <version>None</version>
      <fixedVersion>2.2.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-6 01:00:00" id="16403" opendate="2017-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP UI shows the wrong number of executors</summary>
      <description>Queued tasks are added twice.</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16425" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: unload old hashtables before reloadHashTable</summary>
      <description>@Override protected void reloadHashTable(byte pos, int partitionId) throws IOException, HiveException, SerDeException, ClassNotFoundException { // The super method will reload a hash table partition of one of the small tables. // Currently, for native vector map join it will only be one small table. super.reloadHashTable(pos, partitionId); MapJoinTableContainer smallTable = spilledMapJoinTables[pos]; vectorMapJoinHashTable = VectorMapJoinOptimizedCreateHashTable.createHashTable(conf, smallTable); needHashTableSetup = true; LOG.info("Created " + vectorMapJoinHashTable.getClass().getSimpleName() + " from " + this.getClass().getSimpleName()); if (isLogDebugEnabled) { LOG.debug(CLASS_NAME + " reloadHashTable!"); } }The super call causes an OOM because of existing memory usage by vectorMapJoinHashTable.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-13 01:00:00" id="16436" opendate="2017-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Response times in "Task Execution Summary" at the end of the job is not correct</summary>
      <description>"Task execution summary" is printed at the of running a hive query. E.gTask Execution Summary---------------------------------------------------------------------------------------------- VERTICES DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS---------------------------------------------------------------------------------------------- Map 1 277869.00 0 0 1,500,000,000 1,500,000,000 Map 2 277868.00 0 0 5,999,989,709 31,162,299 Reducer 3 59875.00 0 0 1,531,162,299 2,018 Reducer 4 2436.00 0 0 2,018 2 Reducer 5 375.00 0 0 2 0----------------------------------------------------------------------------------------------Response times reported here for Map-1/Map-2 is not correct. Not sure if this is broken due to any other patch. Creating this jira for tracking purpose.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.RenderStrategy.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-17 01:00:00" id="16460" opendate="2017-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In the console output, show vertex list in topological order instead of an alphabetical sort</summary>
      <description>cc prasanth_j</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.monitoring.TestTezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezProgressMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-17 01:00:00" id="16462" opendate="2017-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Enabling hybrid grace disables specialization of all reduce side joins</summary>
      <description>Observed by gopalv.Having grace hash join enabled prevents the specialized vector hash joins during the vectorizer stage of query planning. However hive.llap.enable.grace.join.in.llap will later disable grace hash join (LlapDecider runs after Vectorizer). If we can disable the grace hash join before vectorization kicks in then we can still benefit from the specialized vector hash joins.This can be special cased for the llap.execution.mode=only case.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-26 01:00:00" id="16540" opendate="2017-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>dynamic_semijoin_user_level is failing on MiniLlap</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-27 01:00:00" id="16545" opendate="2017-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: bug in arena size determination logic</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-27 01:00:00" id="16547" opendate="2017-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: may not unlock buffers in some cases</summary>
      <description>Actually this is a pretty major bug, no idea how it slipped before.If last RG is not selected, dictionary buffers will not be unlocked because of bad assumptions about what isLastRg means (last in processing vs last in the stripe).</description>
      <version>None</version>
      <fixedVersion>2.2.0,2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-28 01:00:00" id="16559" opendate="2017-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet schema evolution for partitioned tables may break if table and partition serdes differ</summary>
      <description>Parquet schema evolution should make it possible to have partitions/tables backed by files with different schemas. Hive should match the table columns with file columns based on the column name if possible.However if the serde for a table is missing columns from the serde of a partition Hive fails to match the columns together.Steps to reproduce:CREATE TABLE myparquettable_parted( name string, favnumber int, favcolor string, age int, favpet string)PARTITIONED BY (day string)STORED AS PARQUET;INSERT OVERWRITE TABLE myparquettable_partedPARTITION(day='2017-04-04')SELECT 'mary' as name, 5 AS favnumber, 'blue' AS favcolor, 35 AS age, 'dog' AS favpet;alter table myparquettable_partedREPLACE COLUMNS(favnumber int,age int); &lt;!--- No cascade option, so the partition will not be altered. SELECT * FROM myparquettable_parted where day='2017-04-04';will fail with:java.lang.UnsupportedOperationException: Cannot inspect org.apache.hadoop.io.IntWritableHive should either match the columns together or prevent the user from dropping columns from the table.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-3 01:00:00" id="16573" opendate="2017-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In-place update for HoS can&amp;#39;t be disabled</summary>
      <description>hive.spark.exec.inplace.progress has no effect</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-4 01:00:00" id="16581" opendate="2017-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>a bug in HIVE-16523</summary>
      <description>A bug</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-5-5 01:00:00" id="16599" opendate="2017-5-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE in runtime filtering cost when handling SMB Joins</summary>
      <description>A test with SMB joins failed with NPE in runtime filtering costing logic.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.semijoin.reduction.2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-11 01:00:00" id="166" opendate="2008-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create eclipse project template</summary>
      <description>In order to make it easier for developers using Eclipse to get started we could provide project templates much like Hadoop does. These can then be copied into place by an ant target. We should also add the copies into svn:ignore as per HIVE-101 to avoid having them pop up when committing code.See HADOOP-1228 for details.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.get.ivy.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-8 01:00:00" id="16605" opendate="2017-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enforce NOT NULL constraints</summary>
      <description>Since NOT NULL is so common it would be great to have tables start to enforce that.ekoifman described a possible approach in HIVE-16575:One way to enforce not null constraint is to have the optimizer add enforce_not_null UDF which throws if it sees a NULL, otherwise it's pass through.So if 'b' has not null constraint,Insert into T select a,b,c... would becomeInsert into T select a, enforce_not_null(b), c.....This would work for any table type.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.with.constraints.enforced.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.with.constraints.enable.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.create.with.constraints.enforced.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.with.constraints.enable.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-11 01:00:00" id="16639" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Derive shuffle thread counts and keep-alive connections from instance count</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-11 01:00:00" id="16646" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alias in transform ... as clause shouldn&amp;#39;t be case sensitive</summary>
      <description>Create a table like below:CREATE TABLE hive_bug(col1 string);Run below query in Hive:from hive_bug select transform(col1) using '/bin/cat' as (AAAA string);The result would be:0: jdbc:hive2://localhost:10000&gt; from hive_bug select transform(col1) using '/bin/cat' as (AAAA string);......INFO : OK+-------+--+| AAAA |+-------+--++-------+--+The output column name is AAAA instead of the lowercase aaaa.</description>
      <version>None</version>
      <fixedVersion>2.3.2,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-12 01:00:00" id="16660" opendate="2017-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Not able to add partition for views in hive when sentry is enabled</summary>
      <description>Repro:create table tesnit (a int) partitioned by (p int);insert into table tesnit partition (p = 1) values (1);insert into table tesnit partition (p = 2) values (1);create view test_view partitioned on (p) as select * from tesnit where p =1;alter view test_view add partition (p = 2);Error: Error while compiling statement: FAILED: SemanticException &amp;#91;Error 10056&amp;#93;: The query does not reference any valid partition. To run this query, set hive.mapred.mode=nonstrict (state=42000,code=10056)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-15 01:00:00" id="16667" opendate="2017-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL metastore handling of CLOB types for COLUMNS_V2.TYPE_NAME and other field is incorrect</summary>
      <description>The CLOB JDO type introduced with HIVE-12274 does not work correctly with PostgreSQL. The value is written out-of-band and the LOB handle is written,as an INT, into the table. SELECTs return the INT value, which should had been read via the lo_get PG built-in, and then cast into string.Furthermore, the behavior is different between fields upgraded from earlier metastore versions (they retain their string storage) vs. values inserted after the upgrade (inserted as LOB roots).Teh code in MetasoreDirectSql.getPartitionsFromPartitionIds/extractSqlClob expects the underlying JDO/Datanucleus to map the column to a Clob but that does not happen, the value is a Java String containing the int which is the LOB root saved by PG.This manifests at runtime with errors like:hive&gt; select * from srcpart;Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Error: type expected at the position 0 of '24030:24031' but '24030' is found.the 24030:24031 should be 'string:string'.repro:CREATE TABLE srcpart (key STRING COMMENT 'default', value STRING COMMENT 'default') PARTITIONED BY (ds STRING, hr STRING) STORED AS TEXTFILE;LOAD DATA LOCAL INPATH "${hiveconf:test.data.dir}/kv1.txt" OVERWRITE INTO TABLE srcpart PARTITION (ds="2008-04-09", hr="11");select * from srcpart;I did not see the issue being hit by non-partitioned/textfile tables, but that is just the luck of the path taken by the code. Inspection of my PG metastore shows all the CLOB fields suffering from this issue.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-16 01:00:00" id="16671" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: BufferUnderflowException may happen in very rare(?) cases due to ORC end-of-CB estimation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-16 01:00:00" id="16673" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test for HIVE-16413</summary>
      <description>unit test for HIVE-16413</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-17 01:00:00" id="16697" opendate="2017-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema table validator should return a sorted list of missing tables</summary>
      <description>SchemaTool's validate feature has a schema table validator that checks to see if the HMS schema is missing tables. This validator reports a list of tables that are deemed to be missing. This list is currently unsorted (depends on the order of create table statements in the schema file, which is different for different DB schema files). This makes it hard to write a unit test that parses the results.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-7-31 01:00:00" id="167" opendate="2008-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: add a RegularExpressionDeserializer</summary>
      <description>We need a RegularExpressionDeserializer to read data based on a regex. This will be very useful for reading files like apache log.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-25 01:00:00" id="1670" opendate="2010-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapJoin throws EOFExeption when the mapjoined table has 0 column selected</summary>
      <description>select /*+mapjoin(b) */ sum(a.key) from src a join src b on (a.key=b.key); throws EOFException</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-17 01:00:00" id="16700" opendate="2017-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log ZK discovery info (hostname &amp; port) for HTTP mode when connection is established</summary>
      <description>Currently it seems we only do this for binary mode. See here. For debugging purpose, it would be good to do this for http mode too.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-6-24 01:00:00" id="16744" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP index update may be broken after ORC switch</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcStripeMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-24 01:00:00" id="16755" opendate="2017-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: incorrect assert may trigger in tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2017-4-2 01:00:00" id="16815" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from error for the rest of modules</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsRebuildLockHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.Service.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ProfileServlet.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.GenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseStructValue.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ConsumerFeedback.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.CustomQueryFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.SearchResultHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PasswdAuthenticationProvider.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="16819" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MM test for temporary table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-4-7 01:00:00" id="16850" opendate="2017-6-7 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Converting table to insert-only acid may open a txn in an inappropriate place</summary>
      <description>This would work for unit-testing, but would need to be fixed for production.HiveTxnManager txnManager = SessionState.get().getTxnMgr(); if (txnManager.isTxnOpen()) { mmWriteId = txnManager.getCurrentTxnId(); } else { mmWriteId = txnManager.openTxn(new Context(conf), conf.getUser()); txnManager.commitTxn(); }</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.conversions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.conversions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-6-9 01:00:00" id="16871" opendate="2017-6-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CachedStore.get_aggr_stats_for has side affect</summary>
      <description>Every get_aggr_stats_for accumulates the stats and propagated to the first partition stats object. It accumulates and gives wrong result in the follow up invocations.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="16888" opendate="2017-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite to 1.13 and Avatica to 1.10</summary>
      <description>I'm creating this early to be able to ptest the current Calcite 1.13.0-SNAPSHOT</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveTypeSystemImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveDruidProjectFilterTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.HiveMaterializedViewFilterScanRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.MaterializedViewSubstitutionVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.views.SubstitutionVisitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.q</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fouter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-7-15 01:00:00" id="16911" opendate="2017-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade groovy version to 2.4.11</summary>
      <description>Hive currently uses groovy 2.4.4 which has security issue (https://access.redhat.com/security/cve/cve-2016-6814). Need to upgrade to 2.4.8 or later.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-15 01:00:00" id="16912" opendate="2017-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve table validator&amp;#39;s performance against Oracle</summary>
      <description>Currently, this validator uses DatabaseMetaData.getTables() that takes in the order of minutes to return because of the number of SYSTEM tables present in Oracle.Providing a schema name via a system property would limit the number of tables being returned and thus improve performance.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-16 01:00:00" id="16915" opendate="2017-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition column count is not determined correctly in LLAP IO non-vectorized wrapper</summary>
      <description>May be related to HIVE-16761</description>
      <version>None</version>
      <fixedVersion>2.3.0,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-6-23 01:00:00" id="16947" opendate="2017-6-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin Reduction : Task cycle created due to multiple semijoins in conjunction with hashjoin</summary>
      <description>Typically a semijoin branch and a mapjoin may create a cycle when on same operator tree. This is already handled, however, a semijoin branch can serve more than one filters and the cycle detection logic currently only handles the 1st one causing cycles preventing the queries from running.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-6 01:00:00" id="1697" opendate="2010-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migration scripts should increase size of PARAM_VALUE in PARTITION_PARAMS</summary>
      <description>The migration scripts should increase the size of column PARAM_VALUE in the table PARTITION_PARAMS to 4000 chars to follow the description in package.jdo.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.6.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.6.0.derby.sql</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-27 01:00:00" id="16974" opendate="2017-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the sort key for the schema tool validator to be &lt;ID&gt;</summary>
      <description>In HIVE-16729, we introduced ordering of results/failures returned by schematool's validators. This allows fault injection testing to expect results that can be verified. However, they were sorted on NAME values which in the HMS schema can be NULL. So if the introduced fault has a NULL/BLANK name column value, the result could be different depending on the backend database(if they sort NULLs first or last).So I think it is better to sort on a non-null column value.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-7-5 01:00:00" id="17034" opendate="2017-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The spark tar for itests is downloaded every time if md5sum is not installed</summary>
      <description>I think we should either skip verifying md5, or fail the build to let developer know md5sum is required.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-1-6 01:00:00" id="17055" opendate="2017-7-6 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Flaky test: TestMiniLlapCliDriver.testCliDriver[llap_smb]</summary>
      <description>Following tests are also failing with same symptoms: TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;tez_smb_1&amp;#93; TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;tez_smb_main&amp;#93;Client Execution succeeded but contained differences (error code = 1) after executing llap_smb.q 324,325c324,325&lt; 2000 9 52&lt; 2001 0 139630&amp;#8212;&gt; 2001 4 139630&gt; 2001 6 52</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-7-12 01:00:00" id="17079" opendate="2017-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use FQDN by default for work submission</summary>
      <description>HIVE-14624 added FDQN for work submission. We should enable it by default to avoid DNS issues.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="17086" opendate="2017-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: JMX Metric for max file descriptors used so far</summary>
      <description>LlapDaemonMaxFileDescriptorCount shows max file descriptors that system will allow. For debugging purpose we could also store the max value that was seen so far to know if we have hit the limit.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-13 01:00:00" id="17088" opendate="2017-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 WebUI throws a NullPointerException when opened</summary>
      <description>After bumping the Jetty version to 3.9 and excluding several other dependencies on HIVE-16049, the HS2 webui stopped working and throwing a NPE error.HTTP ERROR 500Problem accessing /hiveserver2.jsp. Reason: Server ErrorCaused by:java.lang.NullPointerException at org.apache.hive.generated.hiveserver2.hiveserver2_jsp._jspService(hiveserver2_jsp.java:181) at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:70) at javax.servlet.http.HttpServlet.service(HttpServlet.java:790) at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:840) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:584) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:512) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134) at org.eclipse.jetty.server.Server.handle(Server.java:534) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:320) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:251) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.ssl.SslConnection.onFillable(SslConnection.java:240) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:108) at org.eclipse.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148) at org.eclipse.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671) at org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589) at java.lang.Thread.run(Thread.java:748)Powered by Jetty:// 9.3.19.v20170502</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-13 01:00:00" id="1709" opendate="2010-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide Postgres metastore schema migration scripts (0.5 -&gt; 0.6)</summary>
      <description>Yuanjun Li provided a schema upgrade script for postgres. This should be included in the 0.6 release.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-14 01:00:00" id="17090" opendate="2017-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark.only.query.files are not being run by ptest</summary>
      <description>Checked a recent run of Hive QA and it doesn't look like qtests specified in spark.only.query.files are being run.I think some modifications to ptest config files are required to get this working - e.g. the deployed master-m2.properties file for ptest should contain mainProperties.${spark.only.query.files} in the qFileTest.miniSparkOnYarn.groups.normal key.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-14 01:00:00" id="17093" opendate="2017-7-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP ssl configs need to be localized to talk to a wire encrypted hdfs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-17 01:00:00" id="17109" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove calls to RelMetadataQuery.instance() after Calcite 1.13 upgrade</summary>
      <description>After CALCITE-1812 the code should retrieve the RelMetadataQuery from the planner, if needed. Calling RelMetadatQuery.instance() invalidates the Calcite RelNode properties memoization cache.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-13 01:00:00" id="1711" opendate="2010-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE LIKE should not set stats in the new table</summary>
      <description>CREATE TABLE T LIKE S; will copy every parameters from T to S. It should not copy table level stats.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.stats13.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hbase-handler.src.test.results.hbase.stats.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-17 01:00:00" id="17111" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add TestLocalSparkCliDriver</summary>
      <description>The TestSparkCliDriver sets the spark.master to local-cluster&amp;#91;2,2,1024&amp;#93; but the HoS still uses decides to use the RemoteHiveSparkClient rather than the LocalHiveSparkClient.The issue is with the following check in HiveSparkClientFactory: if (master.equals("local") || master.startsWith("local[")) { // With local spark context, all user sessions share the same spark context. return LocalHiveSparkClient.getInstance(generateSparkConf(sparkConf)); } else { return new RemoteHiveSparkClient(hiveconf, sparkConf); }When master.startsWith("local[") it checks the value of spark.master and sees that it doesn't start with local[ and then decides to use the RemoteHiveSparkClient.We should fix this so that the LocalHiveSparkClient is used. It should speed up some of the tests, and also makes qtests easier to debug since everything will now be run in the same process.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-18 01:00:00" id="17114" opendate="2017-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: Possible skew in shuffling when data is not really skewed</summary>
      <description>Observed in HoS and may apply to other engines as well.When we join 2 tables on a single int key, we use the key itself as hash code in ObjectInspectorUtils.hashCode: case INT: return ((IntObjectInspector) poi).get(o);Suppose the keys are different but are all some multiples of 10. And if we choose 10 as #reducers, the shuffle will be skewed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.join.filter.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constprog.semijoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-8-25 01:00:00" id="17167" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create metastore specific configuration tool</summary>
      <description>As part of making the metastore a separately releasable module we need configuration tools that are specific to that module. It cannot use or extend HiveConf as that is in hive common. But it must take a HiveConf object and be able to operate on it.The best way to achieve this is using Hadoop's Configuration object (which HiveConf extends) together with enums and static methods.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-25 01:00:00" id="17168" opendate="2017-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for stand alone metastore</summary>
      <description>We need to create a separate maven module for the stand alone metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-15 01:00:00" id="1717" opendate="2010-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ant clean should delete stats database</summary>
      <description>If a test failed, the derby database used for storing intermediate stats may be in an inconsistent state. This database is shared with future tests and prevent them from connecting to the database.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-28 01:00:00" id="17190" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema changes for bitvectors for unpartitioned tables</summary>
      <description>Missed in HIVE-16997</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.043-HIVE-16997.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.044-HIVE-16997.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.044-HIVE-16997.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.029-HIVE-16997.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.044-HIVE-16997.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.022-HIVE-11107.derby.sql</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.perf-reg.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-28 01:00:00" id="17191" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for StorageHandler APIs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.InputEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStoragePredicateHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-28 01:00:00" id="17192" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for Stats Collection APIs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsCollectionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsAggregator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ClientStatsPublisher.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-28 01:00:00" id="17193" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: don&amp;#39;t combine map works that are targets of different DPPs</summary>
      <description>Suppose srcpart is partitioned by ds. The following query can trigger the issue:explainselect * from (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.key) ajoin (select srcpart.ds,srcpart.key from srcpart join src on srcpart.ds=src.value) bon a.key=b.key;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkPartitionPruningSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.CombineEquivalentWorkResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkDynamicPartitionPruner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-7-28 01:00:00" id="17202" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for HMS Listener APIs</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStorePreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreReadTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreReadDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreLoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreDropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreCreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreCreateDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAuthorizationCallEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAlterIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAddPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreAddIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.LoadPartitionDoneEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.ListenerEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.InsertEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropFunctionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.DropDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateFunctionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.CreateDatabaseEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.ConfigChangeEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterTableEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AlterIndexEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AddPartitionEvent.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.AddIndexEvent.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-28 01:00:00" id="17203" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add InterfaceAudience and InterfaceStability annotations for HCat APIs</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.Command.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataSerializer.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatPartition.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatNotificationEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatDatabase.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateTableDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatCreateDBDesc.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatAddPartitionDesc.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageFactory.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.MessageDeserializer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.InsertMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.HCatEventMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.DropDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateFunctionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.CreateDatabaseMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterTableMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterPartitionMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AlterIndexMessage.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.AddPartitionMessage.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatStorer.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatLoader.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.OutputJobInfo.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecord.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatContext.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-2 01:00:00" id="17228" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump tez version to 0.9.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-2 01:00:00" id="17229" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetastore HMSHandler locks during initialization, even though its static variable threadPool is not null</summary>
      <description>A thread pool is used to accelerate adding partitions operation, since HIVE-13901. However, HMSHandler needs a lock during initialization every time, even though its static variable threadPool is not null</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-2 01:00:00" id="17233" opendate="2017-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set "mapred.input.dir.recursive" for HCatInputFormat-based jobs.</summary>
      <description>This has to do with HIVE-15575. TezCompiler seems to set mapred.input.dir.recursive to true. This is acceptable for Hive jobs, since this allows Hive to consume its peculiar UNION ALL output, where the output of each relation is stored in a separate sub-directory of the output-dir.For such output to be readable through HCatalog (via Pig/HCatLoader), mapred.input.dir.recursive should be set from HCatInputFormat as well. Otherwise, one gets zero records for that input.</description>
      <version>2.2.0,3.0.0</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-7 01:00:00" id="17263" opendate="2017-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce debug logging for S3 tables</summary>
      <description>When log level is set to debug operations accessing tables on amazon s3 will output a significant amount of logs, a lot of which is about the http communication (http headers and requests) which may not be that useful even for debugging purposes.Since some ZooKeeper, Hadoop, DataNucleus etc. loggers are by default set to INFO+ levels I suggest we do the same for Apache Http and AWS.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-9 01:00:00" id="17276" opendate="2017-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check max shuffle size when converting to dynamically partitioned hash join</summary>
      <description>Currently we only check that the max number of entries in the hashmap for a MapJoin surpasses a certain threshold to decide whether to execute a dynamically partitioned hash join.We would like to factor the size of the large input that we will shuffle for the dynamically partitioned hash join into the cost model too.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.join.max.hashtable.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-8-10 01:00:00" id="17285" opendate="2017-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes for bit vector retrievals and merging</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-10 01:00:00" id="17286" opendate="2017-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid expensive String serialization/deserialization for bitvectors</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.double.q.out</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StringColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LongColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DoubleColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DecimalColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DateColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BooleanColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BinaryColumnStatsData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.varchar.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exec.parallel.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.long.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.empty.table.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.FMSketch.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.fm.FMSketchUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.hll.HyperLogLog.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.NumDistinctValueEstimator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ndv.NumDistinctValueEstimatorFactory.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.ndv.fm.TestFMSketchSerialization.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreStatsMerge.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.ColumnStatsAggregatorFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.aggr.StringColumnStatsAggregator.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.ColumnStatsMergerFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.columnstats.merge.StringColumnStatsMerger.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestOldSchema.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.quoting.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.decimal.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-18 01:00:00" id="1729" opendate="2010-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Satisfy ASF release management requirements</summary>
      <description>We need to make sure we satisfy the ASF release requirements: http://www.apache.org/dev/release.html http://incubator.apache.org/guides/releasemanagement.html</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.build.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">lib.json-LICENSE.txt</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-11 01:00:00" id="17297" opendate="2017-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow AM to use LLAP guaranteed tasks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tez.LlapProtocolClientProxy.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-8-12 01:00:00" id="17308" opendate="2017-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvement in join cardinality estimation</summary>
      <description>Currently during logical planning join cardinality is estimated assuming no correlation among join keys (This estimation is done using exponential backoff). Physical planning on the other hand consider correlation for multi keys and uses different estimation. We should consider correlation during logical planning as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.TestCBORuleFiredOnlyOnce.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HivePlannerContext.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-15 01:00:00" id="17322" opendate="2017-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Serialise BeeLine qtest execution to prevent flakyness</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.Parallelized.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-17 01:00:00" id="17347" opendate="2017-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniSparkOnYarnCliDriver[spark_dynamic_partition_pruning_mapjoin_only] is failing every time</summary>
      <description>As lirui identified there was a missing file from this patch: HIVE-17247 - HoS DPP: UDFs on the partition column side does not evaluate correctly</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.mapjoin.only.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-17 01:00:00" id="17351" opendate="2017-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use new slider package installation command in run.sh</summary>
      <description>The old syntax does not include some perf improvements in newer versions of Slider.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-18 01:00:00" id="17354" opendate="2017-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix "alter view" for incremental replication</summary>
      <description>There is a bug that "alter view" operation is resulting in a view creation operation instead of a overwriting/replacement operation.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-20 01:00:00" id="1736" opendate="2010-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove "-dev" suffix from release package name and generate MD5 checksum using Ant</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2017-11-23 01:00:00" id="17376" opendate="2017-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade snappy version to 1.1.4</summary>
      <description>Upgrade the snappy java version to 1.1.4. The older version has some issues like memory leak (https://github.com/xerial/snappy-java/issues/91).</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-9-25 01:00:00" id="17389" opendate="2017-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus is always failing on rat checks</summary>
      <description>Rat checks are failing on metastore_db/dblock and files under patchprocess created by Yetus itself.Both directories should be excluded from rat checks.CC: pvary kgyrtkirk</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-9-31 01:00:00" id="17420" opendate="2017-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bootstrap - get replid before object dump</summary>
      <description>when we do the bootstrap dump, the idea is that we take note of the replication id of a object (table in case of this bug) and then request the table metadata from the rdbms which is then serialized to _metadata file for the table containing lastReplicationId + table metadata.This is to make sure that we only apply events after the event id in _metadata on the table, which implies that its best to order the call to get currentReplicationId and tablemetadata retrieval in the event of multiple concurrent operations happening on the same table along with bootstrap.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-31 01:00:00" id="17421" opendate="2017-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clear incorrect stats after replication</summary>
      <description>After replication, some stats summary are incorrect. If hive.compute.query.using.stats set to true, we will get wrong result on the destination side.This will not happen with bootstrap replication. This is because stats summary are in table properties and will be replicated to the destination. However, in incremental replication, this won't work. When creating table, the stats summary are empty (eg, numRows=0). Later when we insert data, stats summary are updated with update_table_column_statistics/update_partition_column_statistics, however, both events are not captured in incremental replication. Thus on the destination side, we will get count&amp;#40;*&amp;#41;=0. The simple solution is to remove COLUMN_STATS_ACCURATE property after incremental replication.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-6 01:00:00" id="17464" opendate="2017-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix to be able to disable max shuffle size DHJ config</summary>
      <description>Setting hive.auto.convert.join.shuffle.max.size to -1 does not work as expected.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-9-7 01:00:00" id="17475" opendate="2017-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable mapjoin using hint</summary>
      <description>Using hint disable mapjoin for a given query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-25 01:00:00" id="1748" opendate="2010-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Statistics broken for tables with size in excess of Integer.MAX_VALUE</summary>
      <description>ANALYZE TABLE x COMPUTE STATISTICS would fail to update the table size if it exceeded Integer.MAX_VALUE because it used parseInt instead of parseLong.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-9-9 01:00:00" id="17493" opendate="2017-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve PKFK cardinality estimation in Physical planning</summary>
      <description>Cardinality estimation of a join, after PK-FK relation has been ascertained, could be improved if parent of the join operator is LEFT outer or RIGHT outer join.Currently estimation is done by estimating reduction of rows occurred on PK side, then multiplying the reduction to FK side row count. This estimation of reduction currently doesn't distinguish b/w INNER or OUTER joins. This could be improved to handle outer joins better.TPC-DS query45 is impacted by this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-11 01:00:00" id="17504" opendate="2017-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip ACID table for replication</summary>
      <description>Currently we are not supporting replicate ACID tables (which will be future work).</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-9-12 01:00:00" id="17514" opendate="2017-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use SHA-256 for cookie signer to improve security</summary>
      <description>See HIVE-17226 for detailed description.</description>
      <version>None</version>
      <fixedVersion>2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-12 01:00:00" id="17519" opendate="2017-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transpose column stats display</summary>
      <description>currently describe formatted table1 insert_num shows the column informations in a table like format...which is very hard to read - because there are to many columns# col_name data_type min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses comment bitVector insert_num int from deserializer I think it would be better to show the same information like this:col_name insert_num data_type int min max num_nulls distinct_count avg_col_len max_col_len num_trues num_falses comment from deserializer bitVector</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.table.stats.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.updateAccessTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.notation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tunable.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.statsfs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.reported.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.3.exim.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.repl.2.exim.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.table.update.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.external.partition.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.with.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.part.inherit.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.timestamp2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.schema1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.date2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.array.null.element.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.orderby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.named.column.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.fast.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fsstat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.infer.bucket.sort.bucketed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.skewtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.20.part.managed.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.19.00.part.external.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.18.part.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.15.external.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.09.part.spec.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.07.all.part.over.nonoverlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.06.one.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.05.some.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.evolved.parts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.04.all.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.02.00.part.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.move.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.desc.tbl.part.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.nonascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.comment.indent.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.default.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.skewed.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.alter.list.bucketing.table1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.enum.to.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compustat.avro.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.infinity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.colstats.all.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.add.column3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.tbl.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.analyze.table.null.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.local.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.blobstore.to.warehouse.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.addpartition.local.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.blobstore.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.local.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.blobstore.to.warehouse.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.import.local.to.blobstore.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataPrettyFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.2columns.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidcolname.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.coltype.invalidtype.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.partition.partial.spec.dyndisabled.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.desc.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.change.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.onto.nocurrent.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.skewed.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.not.sorted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-12 01:00:00" id="17522" opendate="2017-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cleanup old &amp;#39;repl dump&amp;#39; dirs</summary>
      <description>We want to clean up the old dump dirs to save space and reduce scan time when needed.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-27 01:00:00" id="1755" opendate="2010-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBM diff in test caused by Hive-1641</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-18 01:00:00" id="17552" opendate="2017-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable bucket map join by default for Tez</summary>
      <description>Currently bucket map join is disabled by default, however, it is potentially most optimal join we have. Need to enable it by default.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.5.q</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-19 01:00:00" id="17553" opendate="2017-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO wrongly type cast decimal literal to int</summary>
      <description>explain select 100.000BD from fSTAGE PLANS: Stage: Stage-0 Fetch Operator limit: -1 Processor Tree: TableScan alias: f Select Operator expressions: 100 (type: int) outputColumnNames: _col0 ListSinkNotice that the expression 100.000BD is of type int instead of decimal.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.wrong.column.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-19 01:00:00" id="17558" opendate="2017-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip non-native/temporary tables for constraint related scenarios</summary>
      <description>The change would be similar to HIVE-17422.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddUniqueConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPrimaryKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddNotNullConstraintHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddForeignKeyHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-10-28 01:00:00" id="1757" opendate="2010-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test cleanup for Hive-1641</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JDBMSinkOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-20 01:00:00" id="17570" opendate="2017-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix view deletion related test failures (create_view.q etc)</summary>
      <description>Fixing the bug introduced by HIVE-17459. Sorry that did not capture that in a timely fashion.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.DropTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-11-28 01:00:00" id="1758" opendate="2010-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize group by hash map memory</summary>
      <description>Group By map side's hash map consumes a lot of memory, thereby decreasing its effectiveness.We can use some of the optimizations from map-join to reduce the memory footprint: class KeyWrapper { int hashcode; ArrayList&lt;Object&gt; keys; // decide whether this is already in hashmap (keys in hashmap are deepcopied // version, and we need to use 'currentKeyObjectInspector'). boolean copy = false;1. Changes keys to Array2. Optimize the scenario when keys is of a small size (1,2) etcLet us start profiling it and take it from there</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-22 01:00:00" id="17585" opendate="2017-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve thread safety when loading dynamic partitions in parallel</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.metastore.SynchronizedMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-9-25 01:00:00" id="17601" opendate="2017-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling in LlapServiceDriver</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapStatusOptionsProcessor.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-1 01:00:00" id="1761" opendate="2010-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support show locks for a particular table</summary>
      <description>Currently, only show locks is supported - it would be very useful to show locks for a particular table</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lock2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lock1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.lock2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.lock1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowLocksDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-26 01:00:00" id="17610" opendate="2017-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO: an exception in exception handling can hide the original exception</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2017-10-28 01:00:00" id="17633" opendate="2017-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make it possible to override the query results directory in TestBeeLineDriver</summary>
      <description>It would be good to have the possibility to override where the TestBeeLineDriver looks for the golden files</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-28 01:00:00" id="17639" opendate="2017-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t reuse planner context when re-parsing the query</summary>
      <description>The error is "java.lang.AssertionError: Unexpected type UNEXPECTED", e.g. on CTAS</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-2 01:00:00" id="17665" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update netty-all to latest 4.0.x.Final</summary>
      <description>Update netty version to latest 4.0.x.Final to address http://www.cvedetails.com/cve/CVE-2016-4970/</description>
      <version>2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-4 01:00:00" id="1767" opendate="2010-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge files does not work with dynamic partition</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-3 01:00:00" id="17682" opendate="2017-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: IF stmt produces wrong results</summary>
      <description>A query using with a vectorized IF(condition, thenExpr, elseExpr) function can produce wrong results.</description>
      <version>1.2.2,2.3.0,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IfExprColumnScalar.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-5 01:00:00" id="17706" opendate="2017-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a possibility to run the BeeLine tests on the default database</summary>
      <description>Currently it is possible to run the BeeLine tests sequentially but it still relies on cleaning up after the tests by cleaning up the database. Some of the tests could be run only against the default database. We need a cleanup mechanism between the tests</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFileBeeLineClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-5 01:00:00" id="17708" opendate="2017-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade surefire to 2.20.1</summary>
      <description>with the current 2.18.1 jdk9 test execution result in:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test (default-test) on project hive-common: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:2.18.1:test failed: java.lang.NoClassDefFoundError: java/sql/Timestamp: java.sql.Timestamp -&gt; [Help 1]make sure that it works reliably (2.19.1 had a bug which made idea debuging problematic)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-6 01:00:00" id="17720" opendate="2017-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bitvectors are not shown in describe statement on beeline</summary>
      <description>Describe statement takes different codepath for HS2 where bit vectors weren't displayed.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-11-6 01:00:00" id="1774" opendate="2010-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>merge_dynamic_part&amp;#39;s result is not deterministic</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.merge.dynamic.partition.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-10 01:00:00" id="17749" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple class have missed the ASF header</summary>
      <description>Multiple class have missed the ASF header</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.calcite.rules.TestHiveReduceExpressionsWithStatsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.QueryPlanPostProcessor.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MetaStoreTestUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.repl.DumpDirCleanerTask.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestAMReporter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-10 01:00:00" id="17756" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable subquery related Qtests for Hive on Spark</summary>
      <description>HIVE-15456 and HIVE-15192 using Calsite to decorrelate and plan subqueries. This JIRA is to indroduce subquery test and verify the subqueries plan for Hive on Spark</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-10 01:00:00" id="17762" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude older jackson-annotation.jar from druid-handler shaded jar</summary>
      <description>hive-druid-handler.jar is shading jackson core dependencies in hive-17468 but older versions are brought in from the transitive dependencies.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-11 01:00:00" id="17781" opendate="2017-10-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map MR settings to Tez settings via DeprecatedKeys</summary>
      <description>Here's one that cdrome and thiruvel worked on:We found that certain Hadoop Map/Reduce settings that are set in site config files do not take effect in Hive jobs, because the Tez site configs do not contain the same settings.In Yahoo's case, the problem was that, at the time, there was no mapping between MRJobConfig.COMPLETED_MAPS_FOR_REDUCE_SLOWSTART and TEZ_SHUFFLE_VERTEX_MANAGER_MAX_SRC_FRACTION. There were situations where significant capacity on production clusters were being used up doing nothing, while waiting for slow tasks to complete. This would have been avoided, were the mappings in place.Tez provides a DeprecatedKeys utility class, to help map MR settings to Tez settings. Hive should use this to ensure that the mappings are in sync.(Note to self: YHIVE-883)</description>
      <version>3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-12 01:00:00" id="17782" opendate="2017-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inconsistent cast behavior from string to numeric types with regards to leading/trailing spaces</summary>
      <description>select cast(' 1 ' as tinyint), cast(' 1 ' as smallint), cast(' 1 ' as int), cast(' 1 ' as bigint), cast(' 1 ' as float), cast(' 1 ' as double), cast(' 1 ' as decimal(10,2))NULL NULL NULL NULL 1.0 1.0 1Looks like integer types (short, int, etc) fail the conversion due to the leading/trailing spaces and return NULL, while float/double/decimal do not. In fact, Decimal used to also return NULL in previous versions up until HIVE-10799.Let's try to make this behavior consistent across all of these types, should be simple enough to strip spaces before passing to number formatter.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyPrimitive.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyShort.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyLong.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyInteger.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-10 01:00:00" id="1779" opendate="2010-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement GenericUDF str_to_map</summary>
      <description>People need way to load their data to map.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-12 01:00:00" id="17793" opendate="2017-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parameterize Logging Messages</summary>
      <description>Use SLF4J parameterized logging Remove use of archaic Util's "stringifyException" and simply allow logging framework to handle formatting of output. Also saves having to create the error message and then throwing it away when the logging level is set higher than the logging message Add some LOG.isDebugEnabled around complex debug messages</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReplCopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-13 01:00:00" id="17799" opendate="2017-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Ellipsis For Truncated Query In Hive Lock</summary>
      <description>HIVE-16334 introduced truncation for storing queries in ZK lock nodes. This Jira is to add ellipsis into the query to let the operator know that truncation has occurred and therefore they will not find the specific query in their logs, only a prefix match will work.-- Truncation of query may be confusing to operator-- Without truncationSELECT * FROM TABLE WHERE COL=1-- With truncation (operator will not find this query in workload)SELECT * FROM TABLE-- With truncation (operator will know this is only a prefix match)SELECT * FROM TABLE...</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-10-13 01:00:00" id="17807" opendate="2017-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Execute maven commands in batch mode for ptests</summary>
      <description>No need to run in interactive mode in CI environment.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-16 01:00:00" id="17822" opendate="2017-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide an option to skip shading of jars</summary>
      <description>Maven shade plugin does not have option to skip. Adding it under a profile can help with skip shade reducing build times.Maven build profile shows druid and jdbc shade plugin to be slowest (also hive-exec). For devs not working on druid or jdbc, it will be good to have an option to skip shading via a profile. With this it will be possible to get a subminute dev build.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-11-19 01:00:00" id="17834" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix flaky triggers test</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-12631?focusedCommentId=16209803&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16209803</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-19 01:00:00" id="17839" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot generate thrift definitions in standalone-metastore</summary>
      <description>mvn clean install -Pthriftif -Dthrift.home=... does not regenerate the thrift sources. This is after the https://issues.apache.org/jira/browse/HIVE-17506 fix.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-20 01:00:00" id="17864" opendate="2017-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTestClient cannot start during Precommit tests</summary>
      <description>HIVE-17807 has bumped version number in testutils/ptest2/pom.xml from 1.0 to 3.0 resulting failure to start PTestClient during Precommit runs:java -cp '/home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target/hive-ptest-1.0-classes.jar:/home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target/lib/*' org.apache.hive.ptest.api.client.PTestClient --command testStart --outputDir /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/target --password '[*******]' --testHandle PreCommit-HIVE-Build-7389 --endpoint http://104.198.109.242:8080/hive-ptest-1.0 --logsEndpoint http://104.198.109.242/logs/ --profile master-mr2 --patch https://issues.apache.org/jira/secure/attachment/12893016/HIVE-17842.0.patch --jira HIVE-17842Error: Could not find or load main class org.apache.hive.ptest.api.client.PTestClient</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-21 01:00:00" id="17871" opendate="2017-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add non nullability flag to druid time column</summary>
      <description>Druid time column is non null all the time.Adding the non nullability flag will enable extra calcite goodness like transforming select count(`__time`) from table to select count(*) from table</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druid.timeseries.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-22 01:00:00" id="17873" opendate="2017-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>External LLAP client: allow same handleID to be used more than once</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-24 01:00:00" id="17879" opendate="2017-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Datanucleus Maven Plugin</summary>
      <description>when build hive with jdk9got following error[ERROR] Failed to execute goal org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance (default) on project hive-standalone-metastore: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer: InvocationTargetException: java/sql/Date: java.sql.Date -&gt; [Help 1]org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.datanucleus:datanucleus-maven-plugin:3.3.0-release:enhance (default) on project hive-standalone-metastore: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:212) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:153) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:145) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:116) at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:80) at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:51) at org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:307) at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:193) at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:106) at org.apache.maven.cli.MavenCli.execute(MavenCli.java:863) at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:288) at org.apache.maven.cli.MavenCli.main(MavenCli.java:199) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:289) at org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229) at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:415) at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356)Caused by: org.apache.maven.plugin.MojoExecutionException: Error executing DataNucleus tool org.datanucleus.enhancer.DataNucleusEnhancer at org.datanucleus.maven.AbstractDataNucleusMojo.executeInJvm(AbstractDataNucleusMojo.java:350) at org.datanucleus.maven.AbstractEnhancerMojo.enhance(AbstractEnhancerMojo.java:266) at org.datanucleus.maven.AbstractEnhancerMojo.executeDataNucleusTool(AbstractEnhancerMojo.java:72) at org.datanucleus.maven.AbstractDataNucleusMojo.execute(AbstractDataNucleusMojo.java:126) at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:134) at org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:207) ... 20 moreCaused by: java.lang.reflect.InvocationTargetException at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.base/java.lang.reflect.Method.invoke(Method.java:564) at org.datanucleus.maven.AbstractDataNucleusMojo.executeInJvm(AbstractDataNucleusMojo.java:333) ... 25 moreCaused by: java.lang.NoClassDefFoundError: java/sql/Date at org.datanucleus.ClassConstants.&lt;clinit&gt;(ClassConstants.java:66) at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensions(NonManagedPluginRegistry.java:206) at org.datanucleus.plugin.NonManagedPluginRegistry.registerExtensionPoints(NonManagedPluginRegistry.java:155) at org.datanucleus.plugin.PluginManager.&lt;init&gt;(PluginManager.java:63) at org.datanucleus.plugin.PluginManager.createPluginManager(PluginManager.java:430) at org.datanucleus.AbstractNucleusContext.&lt;init&gt;(AbstractNucleusContext.java:85) at org.datanucleus.enhancer.EnhancementNucleusContextImpl.&lt;init&gt;(EnhancementNucleusContextImpl.java:48) at org.datanucleus.enhancer.EnhancementNucleusContextImpl.&lt;init&gt;(EnhancementNucleusContextImpl.java:37) at org.datanucleus.enhancer.DataNucleusEnhancer.&lt;init&gt;(DataNucleusEnhancer.java:161) at org.datanucleus.enhancer.CommandLineHelper.createDataNucleusEnhancer(CommandLineHelper.java:153) at org.datanucleus.enhancer.DataNucleusEnhancer.main(DataNucleusEnhancer.java:1108) ... 30 moreCaused by: java.lang.ClassNotFoundException: java.sql.Date at java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:466) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:563) at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:496) ... 41 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2008-1-16 01:00:00" id="179" opendate="2008-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR function should work like other databases</summary>
      <description>Positions start at 1, not 0. Negative positions start at the end of the string and count backwards.Oracle returns null for lengths less than 1 or non-existent substrings (any empty strings are null). MySQL and PostgreSQL return empty strings, never null. PostgreSQL errors for negative lengths. I suggest we follow the MySQL behavior.Oracle treats position 0 the same as 1. Perhaps we should too? SUBSTR('ABCDEFG',3,4): CDEFSUBSTR('ABCDEFG',-5,4): CDEF SUBSTR('ABCDEFG',3): CDEFG SUBSTR('ABCDEFG',-5): CDEFG SUBSTR('ABC',1,1): AMySQL: SUBSTR('ABC',0,1): &lt;empty&gt; SUBSTR('ABC',0,2): &lt;empty&gt; SUBSTR('ABC',1,0): &lt;empty&gt; SUBSTR('ABC',1,-1): &lt;empty&gt;Oracle: SUBSTR('ABC',0,1): A SUBSTR('ABC',0,2): AB SUBSTR('ABC',1,0): &lt;null&gt; SUBSTR('ABC',1,-1): &lt;null&gt;</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby6.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby5.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby4.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby3.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.parse.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.queries.positive.groupby6.q</file>
      <file type="M">ql.src.test.queries.positive.groupby5.q</file>
      <file type="M">ql.src.test.queries.positive.groupby4.q</file>
      <file type="M">ql.src.test.queries.positive.groupby3.q</file>
      <file type="M">ql.src.test.queries.positive.groupby2.q</file>
      <file type="M">ql.src.test.queries.positive.groupby1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-25 01:00:00" id="17905" opendate="2017-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>propagate background LLAP cluster changes to WM</summary>
      <description>HIVE-17841 already adds a method, it just needs to be called when there are relevant cluster changes that HS2 can detect</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-25 01:00:00" id="17906" opendate="2017-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use kill query mechanics to kill queries in WM</summary>
      <description>Right now it just closes the session (see HIVE-17841). The sessions would need to be reused after the kill, or closed after the kill if the total QP has decreased</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-11-31 01:00:00" id="17945" opendate="2017-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support column projection for index access when using Parquet Vectorization</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-11-2 01:00:00" id="17969" opendate="2017-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore to alter table in batches of partitions when renaming table</summary>
      <description>I'm currently trying to speed up the alter table rename to feature of HMS. The recently submitted change (HIVE-9447) already helps a lot especially on Oracle HMS DBs.This time I intend to gain throughput independently of DB types by enabling HMS to execute this alter table command on batches of partitions (rather than 1by1)</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-12-6 01:00:00" id="17988" opendate="2017-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace patch utility usage with git apply in ptest</summary>
      <description>It would be great to replace the standard diff util because git can do a 3-way merge - which in most cases successfull.This could reduce the ptest results which are erroring out because of build failure.error: patch failed: ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java:7003Falling back to three-way merge...Applied patch to 'ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java' cleanly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-7 01:00:00" id="17996" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ASF headers</summary>
      <description>Yetus check reports some ASF header related issues in Hive code. Let's fix them up.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.resources.log4j2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterResourcePlanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumEmptyIsZeroAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-16 01:00:00" id="180" opendate="2008-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Data Generator for thrift-serialized sequence files</summary>
      <description>Add a data generator to create thrift-serialized sequence files.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-11-7 01:00:00" id="18007" opendate="2017-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address maven warnings</summary>
      <description>[WARNING] Some problems were encountered while building the effective model for org.apache.hive:hive-metastore:jar:3.0.0-SNAPSHOT[WARNING] 'build.plugins.plugin.(groupId:artifactId)' must be unique but found duplicate declaration of plugin org.apache.maven.plugins:maven-jar-plugin @ line 299, column 15[WARNING] Some problems were encountered while building the effective model for org.apache.hive:hive-standalone-metastore:jar:3.0.0-SNAPSHOT[WARNING] 'build.plugins.plugin.version' for org.antlr:antlr3-maven-plugin is missing. @ line 538, column 15[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-11-9 01:00:00" id="18028" opendate="2017-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix WM based on cluster smoke test; add logging</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManagerFederation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-11-10 01:00:00" id="18046" opendate="2017-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore: default IS_REWRITE_ENABLED=false instead of NULL</summary>
      <description>The materialized view impl breaks old metastore sql write access, by complaining that the new table creation does not set this column up. `IS_REWRITE_ENABLED` bit(1) NOT NULL,NOT NULL DEFAULT 0 would allow old metastore direct sql compatibility (not thrift).2017-11-09T07:11:58,331 ERROR [HiveServer2-Background-Pool: Thread-2354] metastore.RetryingHMSHandler: Retrying HMSHandler after 2000 ms (attempt 1 of 10) with error: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MTable@249dbf1" using statement "INSERT INTO `TBLS` (`TBL_ID`,`CREATE_TIME`,`DB_ID`,`LAST_ACCESS_TIME`,`OWNER`,`RETENTION`,`SD_ID`,`TBL_NAME`,`TBL_TYPE`,`VIEW_EXPANDED_TEXT`,`VIEW_ORIGINAL_TEXT`) VALUES (?,?,?,?,?,?,?,?,?,?,?)" failed : Field 'IS_REWRITE_ENABLED' doesn't have a default value        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543)        at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:720)        at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:740)        at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:1038)</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.3.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.2.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.036-HIVE-14496.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.3.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.2.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.037-HIVE-14496.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.3.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.2.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.037-HIVE-14496.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.3.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.2.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.022-HIVE-14496.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.3.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.2.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.037-HIVE-14496.derby.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-13 01:00:00" id="18053" opendate="2017-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support different table types for MVs</summary>
      <description>MVs backed by MM tables, managed tables, external tables. This might work already, but we need to add tests.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-15 01:00:00" id="18067" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove extraneous golden files</summary>
      <description>TestDanglingQouts makes sure that there are no unneeded files in repo. This is currently failing.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.windowspec4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.exim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.6b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.column.mixcase.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-15 01:00:00" id="18068" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Calcite 1.15</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveReduceExpressionsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregatePullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateProjectMergeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelDistribution.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMaterializedViewsRegistry.java</file>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-15 01:00:00" id="18071" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add HS2 jmx information about pools and current resource plan</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-15 01:00:00" id="18072" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix various WM bugs based on cluster testing - part 2</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-15 01:00:00" id="18073" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AM may assert when its guaranteed task count is reduced</summary>
      <description>Sometimes it asserts that it doesn't have so many ducks to give away. This should never happen, need to debug.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-15 01:00:00" id="18076" opendate="2017-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>killquery doesn&amp;#39;t actually work for non-trigger WM kills</summary>
      <description>Not sure what's wrong with it, need to take a look.It dumps a lot of info about everything being cancelled, instead of a nice message like triggers do.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-16 01:00:00" id="18084" opendate="2017-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade checkstyle version to support lambdas</summary>
      <description>Current version does not support lambdas in source files so it skips them. We need to upgrade checkstyle version to fix this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-11-17 01:00:00" id="18089" opendate="2017-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for few tests</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorize.grant.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-23 01:00:00" id="1809" opendate="2010-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive comparison operators are broken for NaN values</summary>
      <description>Comparisons between NaN values and doubles do not work as expected:hive&gt; select 'NaN' = 4.3 from data_one limit 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorExecution log at: /tmp/pbutler/pbutler_20101123145656_d23f9b77-8907-4ed3-aef9-8b99a1cc3138.logJob running in-process (local Hadoop)2010-11-23 14:56:40,488 null map = 100%, reduce = 0%Ended Job = job_local_0001OKtrueTime taken: 9.47 secondshive&gt; select 4 &lt;&gt; 'NaN' from data_one limit 1;Total MapReduce jobs = 1Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorExecution log at: /tmp/pbutler/pbutler_20101123145858_0d243ac2-f745-4e25-9a38-509bef3bb370.logJob running in-process (local Hadoop)2010-11-23 14:58:45,689 null map = 100%, reduce = 0%Ended Job = job_local_0001OKfalseTime taken: 3.938 seconds</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-21 01:00:00" id="18109" opendate="2017-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix identifier usage in parser</summary>
      <description>HIVE-17902 broke exposed this</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-22 01:00:00" id="18123" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain formatted improve column expression map display</summary>
      <description>HIVE-17898 introduced columnExprMap in explain formatted. Formatting of that map was a little off. This jira is to improve the formatting.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AbstractOperatorDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-22 01:00:00" id="18131" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-22 01:00:00" id="18134" opendate="2017-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>some alter resource plan fixes</summary>
      <description>Part of HIVE-18075</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-23 01:00:00" id="18138" opendate="2017-11-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix columnstats problem in case schema evolution</summary>
      <description>column stats are kept in case the main table schema is altered; and this causes all kind of problems.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2017-12-30 01:00:00" id="18188" opendate="2017-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestSSL failures in master</summary>
      <description>HIVE-18170 broke TestSSL tests.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-1 01:00:00" id="18193" opendate="2017-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Migrate existing ACID tables to use write id per table rather than global transaction id</summary>
      <description>dependent upon HIVE-18192For existing ACID Tables we need to update the table level write id metatables/sequences so any new operations on these tables works seamlessly without any conflicting data in existing base/delta files.1. Need to create metadata tables such as NEXT_WRITE_ID and TXN_TO_WRITE_ID.2. Add entries for each ACID/MM tables into NEXT_WRITE_ID where NWI_NEXT is set to current value of NEXT_TXN_ID.NTXN_NEXT.3. All current open/abort transactions to have an entry in TXN_TO_WRITE_ID such that T2W_TXNID=T2W_WRITEID=Open/AbortedTxnId.4. Added new column TC_WRITEID in TXN_COMPONENTS and CTC_WRITEID in COMPLETED_TXN_COMPONENTS to store the write id which should be set as respective values of TC_TXNID and CTC_TXNID from the same row.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-1 01:00:00" id="18202" opendate="2017-12-1 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Automatically migrate hbase.table.name to hbase.mapreduce.hfileoutputformat.table.name for hbase-based table</summary>
      <description>The property name for Hbase table mapping is changed from hbase.table.name to hbase.mapreduce.hfileoutputformat.table.name in HBase 2.We can include such upgrade for existing hbase-based tables in DB upgrade script to automatically change such values.For the new tables, the query will be like:create table hbase_table(key int, val string) stored by 'org.apache.hadoop.hive.hbase.HBaseStorageHandler' with serdeproperties ('hbase.columns.mapping' = ':key,cf:val') tblproperties ('hbase.mapreduce.hfileoutputformat.table.name' = 'positive_hbase_handler_bulk')</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.046-HIVE-18202.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.047-HIVE-18202-oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.047-HIVE-18202.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.032-HIVE-18202.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.047-HIVE-18202.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="18213" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests: YARN Minicluster times out if the disks are &gt;90% full</summary>
      <description>Increase YARN minicluster threshold to timeout only at 99% full instead of 90%.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-6 01:00:00" id="18232" opendate="2017-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Packaging: add dfs-init script in package target</summary>
      <description>As discussed with Ashutosh Chauhan this change is to include init-hive-dfs.sh in the hive package.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-7 01:00:00" id="18248" opendate="2017-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up parameters</summary>
      <description>Clean up of parameters that need not change at run time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-12 01:00:00" id="18266" opendate="2017-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: /system references wrong file for THP</summary>
      <description>copy paste error in /system endpoint. THP references same files again.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.SystemConfigurationServlet.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-12-6 01:00:00" id="1830" opendate="2010-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mappers in group followed by joins may die OOM</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-19 01:00:00" id="18317" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error messages in TransactionalValidationListerner</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.TransactionalValidationListener.java</file>
      <file type="M">ql.src.test.results.clientnegative.create.not.acid.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-19 01:00:00" id="18318" opendate="2017-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP record reader should check interrupt even when not blocking</summary>
      <description>Hive operators don't check interrupts, and may not do blocking operations.LLAP record reader only blocks in IO is slower than processing; so, if IO is fast enough, it will not ever block (at least not interruptibly, the sync w/IO on the object does not check interrupts), and thus never catch interrupts.So, the task would be impossible to terminate.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-20 01:00:00" id="18319" opendate="2017-12-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 3.0.0</summary>
      <description>Hadoop 3.0.0 has been released, we should upgrade to it:http://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-common/release/3.0.0/RELEASENOTES.3.0.0.htmlhttps://hadoop.apache.org/docs/r3.0.0/index.htmlSome test failures can be found when we tried to upgrade to Hadoop 3.0.0 in HIVE-17684</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-1-2 01:00:00" id="18356" opendate="2018-1-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixing license headers in checkstyle</summary>
      <description>The checkstyle header contains the following ASF header:/** * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file[..]Even if we undecided what to do with the already existing headers (HIVE-17952), the new ones should use the proper one with 1 '*' in the first line:/* * Licensed to the Apache Software Foundation (ASF) under one * or more contributor license agreements. See the NOTICE file[..]</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.checkstyle.asf.header</file>
      <file type="M">standalone-metastore.checkstyle.asf.header</file>
      <file type="M">checkstyle.asf.header</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-1-5 01:00:00" id="18384" opendate="2018-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ConcurrentModificationException in log4j2.x library</summary>
      <description>In one of the internal testing, observed the following exceptionjava.util.ConcurrentModificationException at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:909) ~[?:1.8.0_152] at java.util.ArrayList$Itr.next(ArrayList.java:859) ~[?:1.8.0_152] at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1042) ~[?:1.8.0_152] at org.apache.logging.log4j.message.ParameterFormatter.appendCollection(ParameterFormatter.java:596) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.appendPotentiallyRecursiveValue(ParameterFormatter.java:504) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.recursiveDeepToString(ParameterFormatter.java:429) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterFormatter.formatMessage2(ParameterFormatter.java:189) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterizedMessage.formatTo(ParameterizedMessage.java:224) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.message.ParameterizedMessage.getFormattedMessage(ParameterizedMessage.java:200) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEvent.setMessage(RingBufferLogEvent.java:126) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEvent.setValues(RingBufferLogEvent.java:104) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:56) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.RingBufferLogEventTranslator.translateTo(RingBufferLogEventTranslator.java:34) ~[log4j-core-2.6.2.jar:2.6.2] at com.lmax.disruptor.RingBuffer.translateAndPublish(RingBuffer.java:930) ~[disruptor-3.3.0.jar:?] at com.lmax.disruptor.RingBuffer.tryPublishEvent(RingBuffer.java:456) ~[disruptor-3.3.0.jar:?] at org.apache.logging.log4j.core.async.AsyncLoggerDisruptor.tryPublish(AsyncLoggerDisruptor.java:190) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.publish(AsyncLogger.java:160) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.logWithThreadLocalTranslator(AsyncLogger.java:156) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.core.async.AsyncLogger.logMessage(AsyncLogger.java:126) ~[log4j-core-2.6.2.jar:2.6.2] at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:2011) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1884) ~[log4j-api-2.6.2.jar:2.6.2] at org.apache.logging.slf4j.Log4jLogger.info(Log4jLogger.java:189) ~[log4j-slf4j-impl-2.6.2.jar:2.6.2] at org.apache.hadoop.hive.druid.security.KerberosHttpClient.inner_go(KerberosHttpClient.java:96) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient.access$100(KerberosHttpClient.java:50) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:144) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hadoop.hive.druid.security.KerberosHttpClient$2.onSuccess(KerberosHttpClient.java:134) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at org.apache.hive.druid.com.google.common.util.concurrent.Futures$4.run(Futures.java:1181) ~[hive-druid-handler-2.1.0.2.6.4.0-91.jar:2.1.0.2.6.4.0-91] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_152] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_152] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_152]The fix for this went into 2.9.1 LOG4J2-1988 onwards. Updating log4j to latest version should have a fix for this issue.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18414" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-3-10 01:00:00" id="18433" opendate="2018-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version of com.fasterxml.jackson</summary>
      <description>Let's upgrade to version 2.9.4</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="18434" opendate="2018-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Type is not determined correctly for comparison between decimal column and string constant</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-11 01:00:00" id="18436" opendate="2018-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Spark 2.3.0</summary>
      <description>Branching has been completed. Release candidates should be published soon. Might be a while before the actual release, but at least we get to identify any issues early.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestRpc.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestKryoMessageCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-11 01:00:00" id="18437" opendate="2018-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>use plan parallelism for the default pool if both are present</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-13 01:00:00" id="18450" opendate="2018-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support TABLE function in CBO</summary>
      <description>Follow-up of HIVE-18416 to support TABLE function in CBO.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tablevalues.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveTableFunctionScan.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-13 01:00:00" id="18452" opendate="2018-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>work around HADOOP-15171</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-16 01:00:00" id="18456" opendate="2018-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add some tests for HIVE-18367 to check that the table information contains the query correctly</summary>
      <description>This cannot be tested with a CliDriver test so add a java test to check the output of 'describe extended', which is changed by HIVE-18367 </description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-17 01:00:00" id="18469" opendate="2018-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2UI: Introduce separate option to show query on web ui</summary>
      <description>currently ConfVars.HIVE_LOG_EXPLAIN_OUTPUT enables 2 features: log the query to the console (even thru beeline) shows the query on the web uiI've enabled it...and ever since then my beeline is always flooded with an explain extended output...which is very verbose; even for simple queries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.jamon.org.apache.hive.tmpl.QueryProfileTmpl.jamon</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-17 01:00:00" id="18473" opendate="2018-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Infer timezone information correctly in DruidSerde</summary>
      <description>Currently timezone information is not being processed by DruidSerde (contrary to other SerDes).</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-18 01:00:00" id="18489" opendate="2018-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Automatically migrate s3n URIs to s3a URIs</summary>
      <description>s3n has been removed from Hadoop 3.x, we should auto-migrate tables with s3n URIs to the s3a URIs</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.048-HIVE-18489.postgres.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-1-19 01:00:00" id="18499" opendate="2018-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Amend point lookup tests to check for data</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.pointlookup.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-22 01:00:00" id="18514" opendate="2018-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add service output for ranger to WM DDL operations</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-23 01:00:00" id="18516" opendate="2018-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements for ACID Tables</summary>
      <description>load data should rename files consistent with insert statements for ACID Tables.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.data.into.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.7.q</file>
      <file type="M">ql.src.test.queries.clientnegative.load.data.into.acid.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveCopyFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-24 01:00:00" id="18521" opendate="2018-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: query failing in reducer VectorUDAFAvgDecimalPartial2 java.lang.ClassCastException StructTypeInfo --&gt; DecimalTypeInfo</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvgDecimalMerge.txt</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-24 01:00:00" id="18531" opendate="2018-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Vectorized PTF operator should not set the initial type infos</summary>
      <description>The Vectorized PTF operator is mistakenly setting the initial type infos for its output VectorizationContext.  It should not.  It is only creating a projection of the initial column names/type infos from ReduceSink (i.e. keys, values) plus scratch columns for output columns for which the type infos are already known.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.ptf.VectorPTFOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-25 01:00:00" id="18546" opendate="2018-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary code introduced in HIVE-14498</summary>
      <description>HIVE-14498 introduced some code to check the invalidation of materialized views that can be simplified, relying instead on existing transaction ids.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetTableMeta.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.resources.package.jdo</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsInvalidationCache.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.UniqueConstraintsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TxnsSnapshot.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SetPartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PrimaryKeysResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionWithoutSD.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRow.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionValuesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpecWithSharedSD.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionListComposingSpec.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotNullConstraintsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Materialization.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">metastore.scripts.upgrade.derby.048-HIVE-14498.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.033-HIVE-14498.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.048-HIVE-14498.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-txn-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.048-HIVE-14498.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-3.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.047-HIVE-14498.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-txn-schema-3.0.0.postgres.sql</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ImportTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddForeignKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddNotNullConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPrimaryKeyRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddUniqueConstraintRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AggrStats.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.BasicTxnInfo.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ForeignKeysResponse.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-26 01:00:00" id="18557" opendate="2018-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>q.outs: fix issues caused by q.out_spark files</summary>
      <description>HIVE-18061 caused some issues in yetus check by introducing q.out_spark files.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-26 01:00:00" id="18558" opendate="2018-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade orc version to 1.4.2</summary>
      <description>Upgrade orc version to latest 1.4.2 release.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-30 01:00:00" id="18578" opendate="2018-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some class has missed the ASF header</summary>
      <description>Some class has missed the ASF header</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.DruidNode.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-31 01:00:00" id="18590" opendate="2018-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Assertion error on transitive join inference in the presence of NOT NULL constraint</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinPushTransitivePredicatesRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-1 01:00:00" id="18601" opendate="2018-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Power platform by updating protoc-jar-maven-plugin version</summary>
      <description>Below is error is seen while building standalone-metastore project[ [1;34mINFO [m] [1m--- [0;32mprotoc-jar-maven-plugin:3.0.0-a3:run [m [1m(default) [m @ [36mhive-standalone-metastore [0;1m --- [m[ [1;34mINFO [m] Protoc version: 2.5.0[ [1;34mINFO [m] Input directories:[ [1;34mINFO [m] /var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore[ [1;34mINFO [m] Output targets:[ [1;34mINFO [m] java: /var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources (add: none, clean: false)[ [1;34mINFO [m] /var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources does not exist. Creating...[ [1;34mINFO [m] Processing (java): metastore.protoprotoc-jar: protoc version: 250, detected platform: linux/ppc64leprotoc-jar: executing: [/tmp/protoc1841305810088884216.exe, -I/var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore, --java_out=/var/lib/jenkins/workspace/hive/standalone-metastore/target/generated-sources, /var/lib/jenkins/workspace/hive/standalone-metastore/src/main/protobuf/org/apache/hadoop/hive/metastore/metastore.proto]/tmp/protoc1841305810088884216.exe: 1: /tmp/protoc1841305810088884216.exe: &#127;ELF : not found/tmp/protoc1841305810088884216.exe: 1: /tmp/protoc1841305810088884216.exe: Cȁ: not found/tmp/protoc1841305810088884216.exe: 2: /tmp/protoc1841305810088884216.exe: �: not found/tmp/protoc1841305810088884216.exe: 3: /tmp/protoc1841305810088884216.exe: ��_�c�� �jnP� ��R��� ?��Y@�9� � Ch � �߳yIk�� : not found/tmp/protoc1841305810088884216.exe: 2: /tmp/protoc1841305810088884216.exe: Syntax error: Unterminated quoted string The protoc-jar-maven-plugin version used is 3.0.0-a3 whereas Power (ppc64le) support was added in 3.5.1.1. </description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-2-2 01:00:00" id="18612" opendate="2018-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build subprocesses under Yetus in Ptest use 1.7 jre instead of 1.8</summary>
      <description>As per this jira comment made by Yetus maven plugins that want to use java executable are seeing a 1.7 java binary. In this particular case Yetus sets JAVA_HOME to a 1.8 JDK installation, and thus maven uses that, but any subsequent java executes will use the JRE which they see on PATH.This should be fixed by adding the proper java/bin (that of JAVA_HOME setting) to PATH.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-2 01:00:00" id="18614" opendate="2018-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix sys db creation in Hive</summary>
      <description>Sys db can not be created due to several server side issues.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-3.0.0.hive.sql</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-2 01:00:00" id="18616" opendate="2018-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>work around HADOOP-15171 p2</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-5 01:00:00" id="18620" opendate="2018-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve error message while dropping a table that is part of a materialized view</summary>
      <description>When we want to drop a table used by a materialized view, we prevent dropping that table. However, the message shown is not very meaningful (FK-PK violation).</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.drop.table.used.by.mv.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-6 01:00:00" id="18627" opendate="2018-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD: Handle FLOAT boxing differently for single/double precision constants</summary>
      <description>Constants like 0.1 and 0.3 are differently boxed based on intermediate precision of the compiler codepath.Disabling CBO produces 0.1BD constants which fail to box correctly to Double/Float.Enabling CBO fixes this issue, but cannot be applied all queries in Hive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-2-7 01:00:00" id="18643" opendate="2018-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t check for archived partitions for ACID ops</summary>
      <description>This removes the slowness associated with pointless metastore calls when ACID update/delete queries affect a large number of partitions.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-8 01:00:00" id="18654" opendate="2018-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Hiveserver2 specific HADOOP_OPTS environment variable</summary>
      <description>HIVE-2665 added support to include metastore specific HADOOP_OPTS variable. This is helpful in debugging especially if you want to add some jvm parameters to metastore's process. A similar setting for Hiveserver2 is missing and could be very helpful in debugging.</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.hiveserver2.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-8 01:00:00" id="18658" opendate="2018-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM: allow not specifying scheduling policy when creating a pool</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-8 01:00:00" id="18660" opendate="2018-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesn&amp;#39;t distinguish between partition and virtual columns</summary>
      <description>As a result transforms a filter INPUT_FILE_NAME is not null; to false causing wrong results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.boolexpr.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-9 01:00:00" id="18663" opendate="2018-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logged Spark Job Id contains a UUID instead of the actual id</summary>
      <description>We have logs like Spark Job&amp;#91;job-id&amp;#93; but the &amp;#91;job-id&amp;#93; is set to a UUID that is created by the RSC ClientProtocol. It should be pretty easy to print out the actual job id instead.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18668" opendate="2018-2-9 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Really shade guava in ql</summary>
      <description>After HIVE-15393 a test started to fail in druid; after some investigation it turned out that ql doesn't shade it's guava artifact at all...because it shades 'com.google.guava' instead 'com.google.common'</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-9 01:00:00" id="18675" opendate="2018-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make HIVE_LOCKS.HL_TXNID NOT NULL</summary>
      <description>In Hive 3.0 all statements that may need locks run in a transaction</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-txn-schema-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-12 01:00:00" id="18686" opendate="2018-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Installation on Postgres and Oracle broken</summary>
      <description>HIVE-18614 broke the installation and upgrade on Postgres and Oracle.  It calls Connection.setSchema in the JDBC driver.  But the JDBC drivers for these databases don't support that call.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.MetastoreSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-8-18 01:00:00" id="187" opendate="2008-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ODBC driver</summary>
      <description>We need to provide the a small number of functions to get basic queryexecution and retrieval of results. This is based on the tutorial providedhere: http://www.easysoft.com/developer/languages/c/odbc_tutorial.htmlThe minimum set of ODBC functions required are:SQLAllocHandle - for environment, connection, statementSQLSetEnvAttrSQLDriverConnectSQLExecDirectSQLNumResultColsSQLFetchSQLGetDataSQLDisconnectSQLFreeHandleIf required the plan would be to do the following:1. generate c++ client stubs for thrift server2. implement the required functions in c++ by calling the c++ client3. make the c++ functions in (2) extern C and then use those in the odbcSQL* functions4. provide a .so (in linux) which can be used by the ODBC clients.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.include.thrift.TReflectionLocal.h</file>
      <file type="M">service.include.thrift.transport.TZlibTransport.h</file>
      <file type="M">service.include.thrift.transport.TTransportUtils.h</file>
      <file type="M">service.include.thrift.transport.TTransportException.h</file>
      <file type="M">service.include.thrift.transport.TTransport.h</file>
      <file type="M">service.include.thrift.transport.TSocketPool.h</file>
      <file type="M">service.include.thrift.transport.TSocket.h</file>
      <file type="M">service.include.thrift.transport.TServerTransport.h</file>
      <file type="M">service.include.thrift.transport.TServerSocket.h</file>
      <file type="M">service.include.thrift.transport.THttpClient.h</file>
      <file type="M">service.include.thrift.transport.TFileTransport.h</file>
      <file type="M">service.include.thrift.TProcessor.h</file>
      <file type="M">service.include.thrift.TLogging.h</file>
      <file type="M">service.include.thrift.Thrift.h</file>
      <file type="M">service.include.thrift.server.TThreadPoolServer.h</file>
      <file type="M">service.include.thrift.server.TThreadedServer.h</file>
      <file type="M">service.include.thrift.server.TSimpleServer.h</file>
      <file type="M">service.include.thrift.server.TServer.h</file>
      <file type="M">service.include.thrift.server.TNonblockingServer.h</file>
      <file type="M">service.include.thrift.protocol.TProtocolException.h</file>
      <file type="M">service.include.thrift.protocol.TProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TOneWayProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TDenseProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TDebugProtocol.h</file>
      <file type="M">service.include.thrift.protocol.TBinaryProtocol.h</file>
      <file type="M">service.include.thrift.processor.StatsProcessor.h</file>
      <file type="M">service.include.thrift.processor.PeekProcessor.h</file>
      <file type="M">service.include.thrift.fb303.ServiceTracker.h</file>
      <file type="M">service.include.thrift.fb303.fb303.types.h</file>
      <file type="M">service.include.thrift.fb303.FacebookService.h</file>
      <file type="M">service.include.thrift.fb303.FacebookBase.h</file>
      <file type="M">service.include.thrift.config.h</file>
      <file type="M">service.include.thrift.concurrency.Util.h</file>
      <file type="M">service.include.thrift.concurrency.TimerManager.h</file>
      <file type="M">service.include.thrift.concurrency.ThreadManager.h</file>
      <file type="M">service.include.thrift.concurrency.Thread.h</file>
      <file type="M">service.include.thrift.concurrency.PosixThreadFactory.h</file>
      <file type="M">service.include.thrift.concurrency.Mutex.h</file>
      <file type="M">service.include.thrift.concurrency.Monitor.h</file>
      <file type="M">service.include.thrift.concurrency.Exception.h</file>
      <file type="M">service.if.hive.service.thrift</file>
      <file type="M">serde.if.serde.thrift</file>
      <file type="M">serde.build.xml</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-12-28 01:00:00" id="1870" opendate="2010-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestRemoteHiveMetaStore.java accidentally deleted during commit of HIVE-1845</summary>
      <description>TestRemoteHiveMetaStore.java was removed by the commit of HIVE-1845. This change was not part ofthe patch for HIVE-1845.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-14 01:00:00" id="18713" opendate="2018-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize: Transform IN clauses to = when there&amp;#39;s only one element</summary>
      <description>(col1) IN (col2) can be transformed to (col1) = (col2), to avoid the hash-set implementation.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.simple.select.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.simple.select.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.rp.simple.select.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-15 01:00:00" id="18717" opendate="2018-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid transitive dependency on jetty 6.x</summary>
      <description>Although Hive is using jetty 9.3, transitive dependencies bring in 6.2.x which should be avoided.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-15 01:00:00" id="18718" opendate="2018-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integer like types throws error when there is a mismatch</summary>
      <description>If a value is saved with long type and read as int type it results inFAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedPrimitiveColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedListColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-15 01:00:00" id="18721" opendate="2018-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket Map Join : Handle empty buckets</summary>
      <description>Bucket Map Join needs to bluff Tez by sending empty task list for DataMovementEvent for those buckets for which there is no data.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionEdge.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-20 01:00:00" id="18748" opendate="2018-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename table impacts the ACID behavior as table names are not updated in meta-tables.</summary>
      <description>ACID implementation uses metatables such as TXN_COMPONENTS, COMPLETED_TXN_COMPONENTS, COMPACTION_QUEUE, COMPLETED_COMPCTION_QUEUE etc to manage ACID operations.Per table write ID implementation (HIVE-18192) introduces couple of metatables such as NEXT_WRITE_ID and TXN_TO_WRITE_ID to manage write ids allocated per table.Now, when we rename any tables, it is necessary to update the corresponding table names in these metatables as well. Otherwise, ACID table operations won't work properly.Since, this change is significant and have other side-effects, we propose to disable rename tables on ACID tables until a fix is figured out.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.AcidEventListener.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-20 01:00:00" id="18751" opendate="2018-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID table scan through get_splits UDF doesn&amp;#39;t receive ValidWriteIdList configuration.</summary>
      <description>Per table write ID (HIVE-18192) have replaced global transaction ID with write ID to version data files in ACID/MM tables,To ensure snapshot isolation, need to generate ValidWriteIdList for the given txn/table and use it when scan the ACID/MM tables.In case of get_splits UDF which runs on ACID table scan query won't receive it properly through configuration (hive.txn.tables.valid.writeids) and hence throws exception. TestAcidOnTez.testGetSplitsLocks is the test failing for the same. Need to fix it. </description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-20 01:00:00" id="18754" opendate="2018-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL STATUS should support &amp;#39;with&amp;#39; clause</summary>
      <description>We have support for "WITH" clause in "REPL LOAD" command, but we don't have that for "REPL STATUS" command. With the cloud replication model , HiveServer2 is only running in the source on-prem cluster. "REPL LOAD"'s with clause is currently used to pass the remote cloud clusters metastore uri, using "hive.metastore.uri" parameter.Once "REPL LOAD" is run, "REPL STATUS" needs to be run to determine where the next incremental replication should start from. Since "REPL STATUS" is also going to run on source cluster, we need to add support for the "WITH" clause for it.We should also change the privilege required for "REPL STATUS" command to what is required by "REPL LOAD" command as now arbitrary configs can be set for "REPL STATUS" using the WITH clause.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-21 01:00:00" id="18757" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO for text fails for empty files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-21 01:00:00" id="18764" opendate="2018-2-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ELAPSED_TIME resource plan setting is not getting honored</summary>
      <description>Trigger validation for ELAPSED_TIME counter should happen even if session is not created. Currently ELAPSED_TIME counter is populated only after session creation but a query can be waiting to get a session for a long time by the time trigger might have been violated.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.wm.WmContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TriggerValidatorRunnable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersTezSessionPoolManager.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-22 01:00:00" id="18771" opendate="2018-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor tests, so only 1 MetaStore instance will be started per test class and test configuration</summary>
      <description>It takes too much time to start a MetaStore for every test instance.To reduce the running time, start only 1 MetaStore instance, and run every test against this instance</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesList.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesGetExists.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestListPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetTableMeta.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestGetListIndexes.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestFunctions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestExchangePartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAlterPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddAlterDropIndexes.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.MetaStoreFactoryForTests.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-24 01:00:00" id="18794" opendate="2018-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl load "with" clause does not pass config to tasks for non-partition tables</summary>
      <description>Miss one scenario in HIVE-18626.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-24 01:00:00" id="18796" opendate="2018-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix TestSSL</summary>
      <description>broken by HIVE-18203</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.hadoop.hive.jdbc.SSLTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-26 01:00:00" id="18805" opendate="2018-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ConstantPropagate before stats annotation</summary>
      <description>this seems to also make a few more optimizations identify more cases</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce.groupby.duplicate.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.join.transpose.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="18819" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Optimize IF statement expression evaluation of THEN/ELSE</summary>
      <description>Currently, all the rows of a batch are evaluated for the THEN and ELSE expressions even though only a value from one of them is needed for any particular row.</description>
      <version>3.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf.adaptor.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.udf.adaptor.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">data.files.student.2.lines</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="18824" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ValidWriteIdList config should be defined on tables which has to collect stats after insert</summary>
      <description>In HIVE-18192 , per table write ID was introduced where snapshot isolation is built using ValidWriteIdList on tables which are read with in a txn. ReadEntity list is referred to decide which table is read within a txn.For insert operation, table will be found only in WriteEntity, but the table is read to collect stats.So, it is needed to build the ValidWriteIdList for tables/partition part of WriteEntity as well.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="18825" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define ValidTxnList before starting query optimization</summary>
      <description>Consider a set of tables used by a materialized view where inserts happened after the materialization was created. To compute incremental view maintenance, we need to be able to filter only new rows from those base tables. That can be done by inserting a filter operator with condition e.g. ROW&amp;#95;&amp;#95;ID.transactionId &lt; highwatermark and ROW&amp;#95;&amp;#95;ID.transactionId NOT IN(&lt;open txns&gt;) on top of the MVs query definition and triggering the rewriting (which should in turn produce a partial rewriting). However, to do that, we need to have a value for ValidTxnList during query compilation so we know the snapshot that we are querying.This patch aims to generate ValidTxnList before query optimization. There should not be any visible changes for end user.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-28 01:00:00" id="18828" opendate="2018-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve error handling for codecs in LLAP IO</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-1 01:00:00" id="18835" opendate="2018-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC standalone jar download link in ambari</summary>
      <description>Let HS2 offer the file for download, so that Ambari can create link on it.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-3-2 01:00:00" id="18848" opendate="2018-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve readability of filter conditions in explain plan when CBO is run</summary>
      <description>CBO might return comparison operands in any non-deterministic order. Try to show &lt;reference&gt; &lt;cmp&gt; &lt;literal&gt; when possible, i.e., c &lt; 10 rather than 10 &gt; c.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-5 01:00:00" id="18861" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>druid-hdfs-storage is pulling in hadoop-aws-2.7.x and aws SDK, creating classpath problems on hadoop 3.x</summary>
      <description>druid-hdfs-storage JAR is transitively pulling in hadoop-aws JAR 2.7.3, which creates classpath problems as a set of aws-sdk 1.10.77 JARs get on the CP, even with Hadoop 3 &amp; its move to a full aws-sdk-bundle JAR.Two options exclude the dependency force it up to whatever ${hadoop.version} is, so make it consistent</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-5 01:00:00" id="18866" opendate="2018-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semijoin and analyze: Implement a Long -&gt; Hash64 vector fast-path</summary>
      <description>A significant amount of CPU is wasted with JMM restrictions on byte[] arrays.To transform from one Long -&gt; another Long, this goes into a byte[] array, which shows up as a hotspot.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.ndv.hll.HyperLogLog.java</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-6 01:00:00" id="18883" opendate="2018-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add findbugs to yetus pre-commit checks</summary>
      <description>We should enable FindBugs for our YETUS pre-commit checks, this will help overall code quality and should decrease the overall number of bugs in Hive.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-7 01:00:00" id="18888" opendate="2018-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace synchronizedMap with ConcurrentHashMap</summary>
      <description>There are a bunch of places that use Collections.synchronizedMap instead of ConcurrentHashMap which are better. We should search/replace the uses.</description>
      <version>2.3.3,3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-7 01:00:00" id="18889" opendate="2018-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update all parts of Hive to use the same Guava version</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.metrics.JvmPauseMonitor.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2018-3-15 01:00:00" id="18967" opendate="2018-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone metastore SQL upgrade scripts do not properly set schema version</summary>
      <description>The new combined upgrade scripts for Hive 2.3 to 3.0 transition do not properly set the schema version after they have completed the upgrade.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-3-19 01:00:00" id="18992" opendate="2018-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable synthetic file IDs by default in LLAP</summary>
      <description>The file IDs are much more reliable than they were initially (hash+len+date instead of just one hash of everything) so they should be enabled by default.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-3-21 01:00:00" id="19012" opendate="2018-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support builds for ARM and PPC arch</summary>
      <description>Hive standalone metastore uses protoc-jar-maven-plugin 3.5.1.1 which supports downloading from maven repo.   Artifact download should be supported for ARM and PPC architecture since some protobuf versions do not exist in ARM/PPC.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-21 01:00:00" id="19013" opendate="2018-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix some minor build issues in storage-api</summary>
      <description>Currently, the storage-api tests complain that there isn't a log4j2.xml and the javadoc fails.</description>
      <version>None</version>
      <fixedVersion>storage-2.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidWriteIdList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidCompactorWriteIdList.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-21 01:00:00" id="19014" opendate="2018-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>utilize YARN-8028 (queue ACL check) in Hive Tez session pool</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLoggedInUser.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-21 01:00:00" id="19016" opendate="2018-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization and Parquet: Disable vectorization for nested complex types</summary>
      <description>Original title: Vectorization and Parquet: When vectorized, parquet_nested_complex.q produces RuntimeException: Unsupported type used Adding "SET hive.vectorized.execution.enabled=true;" to parquet_nested_complex.q triggers this call stack:Caused by: java.lang.RuntimeException: Unsupported type used in list:array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;array&lt;int&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkListColumnSupport(VectorizedParquetRecordReader.java:589) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.buildVectorizedParquetReader(VectorizedParquetRecordReader.java:525) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.checkEndOfRowGroup(VectorizedParquetRecordReader.java:440) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:401) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:353) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.next(VectorizedParquetRecordReader.java:92) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:360) ~[hive-exec-3.0.0-SNAPSHOT.jar:3.0.0-SNAPSHOT]FYI: vihangk1</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-3-22 01:00:00" id="19032" opendate="2018-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable GROUP BY aggregations with DISTINCT</summary>
      <description>Vectorized GROUP BY does not support DISTINCT aggregation functions.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.groupby.grouping.sets.grouping.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.groupby.cube1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.sort.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.rollup1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets.grouping.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.sets1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.grouping.id1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.cube1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-24 01:00:00" id="19042" opendate="2018-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>set MALLOC_ARENA_MAX for LLAP</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-26 01:00:00" id="19049" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for Alter table add columns for Druid</summary>
      <description>Add support for Alter table add columns for Druid. Currently it is not supported and throws exception.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.non.native.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">hbase-handler.src.test.results.negative.hbase.ddl.q.out</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-26 01:00:00" id="19052" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Disable Vector Pass-Thru SMB MapJoin in the presence of old-style MR FilterMaps</summary>
      <description>Pass-Thru VectorSMBMapJoinOperator was not designed to handle old-style MR FilterMaps.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-26 01:00:00" id="19054" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function replication shall use "hive.repl.replica.functions.root.dir" as root</summary>
      <description>It's wrongly use fs.defaultFS as the root, ignore "hive.repl.replica.functions.root.dir" definition, thus prevent replicating to cloud destination.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.repl.load.message.TestPrimaryToReplicaResourceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateFunctionHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-26 01:00:00" id="19055" opendate="2018-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM alter may fail if the name is not changed</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMNullableResourcePlan.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-27 01:00:00" id="19059" opendate="2018-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support DEFAULT keyword with INSERT and UPDATE</summary>
      <description>Support DEFAULT keyword in INSERT e.g.INSERT INTO TABLE t values (DEFAULT, DEFAULT)or with UPDATEUPDATE TABLE t SET col1=DEFAULT WHERE col2 &gt; 4</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-4-3 01:00:00" id="19091" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hive 3.0.0 Release] Rat check failure fixes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-3 01:00:00" id="19092" opendate="2018-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Somne improvement in bin shell scripts</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.lineage.sh</file>
      <file type="M">bin.ext.fixacidkeyindex.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-13 01:00:00" id="1912" opendate="2011-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Double escaping special chars when removing old partitions in rmr</summary>
      <description>If a partition column value contains special characters such as ':', it will be escaped to '%3A' in the partition path. However in FsShell.rmr(oldPath.toUri().toString()), toUri() will double escape '%' to '%25'. This will make the removal fail.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-5 01:00:00" id="19120" opendate="2018-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>catalog not properly set for some tables in SQL upgrade scripts</summary>
      <description>A catalog column is added to the PARTITION_EVENTS and NOTIFICATION_LOG but the upgrade scripts do not include an UPDATE statement to set this to the default value.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-5 01:00:00" id="19123" opendate="2018-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNegativeCliDriver nopart_insert failing</summary>
      <description>Looks like HIVE-19083 caused this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.nopart.insert.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-7 01:00:00" id="19128" opendate="2018-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for spark perf tests</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-9 01:00:00" id="19135" opendate="2018-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need tool to allow admins to create catalogs and move existing dbs to catalog during upgrade</summary>
      <description>As part of upgrading to Hive 3 admins may wish to create new catalogs and move some existing databases into those catalogs.  We can do this by adding options to schematool.  This guarantees that only admins can do these operations.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-9 01:00:00" id="19138" opendate="2018-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Results cache: allow queries waiting on pending cache entries to check cache again if pending query fails</summary>
      <description>HIVE-18846 allows the results cache to refer to currently executing queries so that another query can wait for these results to become ready in the results cache. If the pending query fails then Hive will automatically skip the cache and do the full query compilation. Make a fix here so that if the pending query fails, Hive will still try to check the cache again in case the cache has another cached/pending result that can be used to answer the query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-10 01:00:00" id="19143" opendate="2018-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for negative tests</summary>
      <description>Missed in HIVE-18859</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subquery.subquery.chain.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.table.grant.nosuchrole.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.grant.nosuchrole.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.case.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.grant.table.dup.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.caseinsensitivity.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-10 01:00:00" id="19147" opendate="2018-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix PerfCliDrivers: Tpcds30T missed CAT_NAME change</summary>
      <description>it seems the baked metastore dump misses the CAT_NAME field added by some recent metastore change</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">data.conf.perf-reg.spark.hive-site.xml</file>
      <file type="M">data.conf.perf-reg.tez.hive-site.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CorePerfCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-10 01:00:00" id="19153" opendate="2018-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for few tests</summary>
      <description>Some golden file updates which were missed since many tests were failing.Following test golden files were updated:acid_table_statsbucket_map_join_tez_emptydefault_constraintinsert_values_orig_table_use_metadatatez_smb_1</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-10 01:00:00" id="19156" opendate="2018-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.vectorized_dynamic_semijoin_reduction.q is broken</summary>
      <description>Looks like this test has been broken for some time</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicValueRegistryTez.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-11 01:00:00" id="19168" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger changes for llap commands</summary>
      <description>New llap commands "llap cluster -info" and "llap cache -purge" require some changes so that Ranger can log the commands for auditing.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapClusterResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.LlapCacheResourceProcessor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.authorization.TestJdbcWithSQLAuthorization.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-11 01:00:00" id="19175" opendate="2018-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver.testCliDriver update_access_time_non_current_db failing</summary>
      <description>Caused by HIVE-18060. Instead of generating golden file under clientpositive/llap it is under clientpositive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.update.access.time.non.current.db.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test.insert.q</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2018-4-13 01:00:00" id="19210" opendate="2018-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create separate module for streaming ingest</summary>
      <description>This will retain the old hcat streaming API for old clients. The new streaming ingest API will be separate module under hive.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-13 01:00:00" id="19212" opendate="2018-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix findbugs yetus pre-commit checks</summary>
      <description>Follow up from HIVE-18883, the committed patch isn't working and Findbugs is still not working.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.YetusPhase.java</file>
      <file type="M">dev-support.yetus-wrapper.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-16 01:00:00" id="19219" opendate="2018-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL DUMP should throw error if requested events are cleaned-up.</summary>
      <description>This is the case where the events were deleted on source because of old event purging and hence min(source event id) &gt; target event id (last replicated event id).Repl dump should fail in this case so that user can drop the database and bootstrap again.Cleaner thread is concurrently removing the expired events from NOTIFICATION_LOG table. So, it is necessary to check if the current dump missed any event while dumping. After fetching events in batches, we shall check if it is fetched in contiguous sequence of event id. If it is not in contiguous sequence, then likely some events missed in the dump and hence throw error.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-16 01:00:00" id="19222" opendate="2018-4-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestNegativeCliDriver tests are failing due to "java.lang.OutOfMemoryError: GC overhead limit exceeded"</summary>
      <description>TestNegativeCliDriver tests are failing with OOM recently. Not sure why. I will try to increase the memory to test out.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-17 01:00:00" id="19226" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend storage-api to print timestamp values in UTC</summary>
      <description>Related to HIVE-12192. Create new method that prints values in UTC.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.exec.vector.TestStructColumnVector.java</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.TimestampUtils.java</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-17 01:00:00" id="19227" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for negative tests</summary>
      <description>Now that we are able to run TestNegativeCliDriver some of golden files are found to be outdated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subq.insert.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.JoinWithAmbigousAlias.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.joinneg.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="19228" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove commons-httpclient 3.x usage</summary>
      <description>Commons-httpclient is not supported well anymore. Remove dependency and move to Apache HTTP client.</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.3.9,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-17 01:00:00" id="19233" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add utility for acid 1.0 to 2.0 migration</summary>
      <description>Click to add description</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-17 01:00:00" id="19235" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update golden files for Minimr tests</summary>
      <description>stats update needed for 3 tests.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-19 01:00:00" id="19243" opendate="2018-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hadoop.version to 3.1.0</summary>
      <description>Given that Hadoop 3.1.0 has been released, we need to upgrade hadoop.version to 3.1.0. This change is required for HIVE-18037 since it depends on YARN Service which had its first release in 3.1.0 (and is non-existent in 3.0.0).</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-4-23 01:00:00" id="19271" opendate="2018-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMiniLlapLocalCliDriver default_constraint and check_constraint failing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-23 01:00:00" id="19277" opendate="2018-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Active/Passive HA web endpoints does not allow cross origin requests</summary>
      <description>CORS is not allowed with web endpoints added for active/passive HA. Enable CORS by default for all web endpoints.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2Peers.java</file>
      <file type="M">service.src.java.org.apache.hive.service.servlet.HS2LeadershipStatus.java</file>
      <file type="M">service.src.java.org.apache.hive.http.LlapServlet.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-23 01:00:00" id="19280" opendate="2018-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invalid error messages for UPDATE/DELETE on insert-only transactional tables</summary>
      <description>UPDATE/DELETE on MM tables fails with "FAILED: SemanticException Error 10297: Attempt to do update or delete on table tpch.tbl_default_mm that is not transactional". This is invalid since the MM table is transactional.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-24 01:00:00" id="19282" opendate="2018-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>don&amp;#39;t nest delta directories inside LB directories for ACID tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-25 01:00:00" id="19306" opendate="2018-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow batch serializer</summary>
      <description>Leverage the ThriftJDBCBinarySerDe code path that already exists in SemanticAnalyzer/FileSinkOperator to create a serializer that batches rows into Arrow vector batches.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="19312" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM tables don&amp;#39;t work with BucketizedHIF</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-26 01:00:00" id="19334" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use actual file size rather than stats for fetch task optimization with external tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="19335" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable runtime filtering (semijoin reduction opt with bloomfilter) for external tables</summary>
      <description>Even with good stats runtime filtering can cause issues, if they are out of date things are even worse. Disable by default for external tables.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-26 01:00:00" id="19336" opendate="2018-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable SMB/Bucketmap join for external tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-27 01:00:00" id="1934" opendate="2011-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table rename messes the location</summary>
      <description>create table tmptmp(a string) partitioned by (b string);alter table tmptmp add partition (b="1:2:3");alter table tmptmp rename to tmptmp_test;The location for tmptmp_test partition (b="1:2:3) is unescaped due to rename, and hence it cannot be dropped.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter3.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-27 01:00:00" id="19344" opendate="2018-4-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default value of msck.repair.batch.size</summary>
      <description>msck.repair.batch.size default to 0 which means msck will try to add all the partitions in one API call to HMS. This can potentially add huge memory pressure on HMS. The default value should be changed to a reasonable number so that in case of large number of partitions we can batch the addition of partitions. Same goes for msck.repair.batch.max.retries</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-5-1 01:00:00" id="19373" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add test cases that verifies ALTER change owner type on the HMS</summary>
      <description>Subtask to add test cases that check the owner type of a table is changed on the HMS</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.client.builder.TableBuilder.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-1 01:00:00" id="19381" opendate="2018-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Function replication in cloud fail when download resource from AWS</summary>
      <description>Another case replication shall use the config in with clause.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-2 01:00:00" id="19383" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ArrayList$SubList kryo serializer</summary>
      <description>Otherwise failure is encountered while trying to deserialize such a plan.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-2 01:00:00" id="19385" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optional hive env variable to redirect bin/hive to use Beeline</summary>
      <description>With beeline-site and beeline-user-site, the user can easily specify default hs2 urls to connect. We can use an optional env variable, which when set, will enable bin/hive to use beeline.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-2 01:00:00" id="19387" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Truncate table for Acid tables conflicts with ResultSet cache</summary>
      <description>How should this work? Should it work like Insert Overwrite T select * from T where 1=2?This should create a new empty base_x/ and thus operate w/o violating Snapshot Isolation semantics.This makes sense for specific partition or unpartitioned table. What about "Truncate T" where T is partitioned? Is the expectation to wipe out all partition info or to make each partition empty?</description>
      <version>None</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TruncateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-2 01:00:00" id="19389" opendate="2018-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool: For Hive&amp;#39;s Information Schema, use embedded HS2 as default</summary>
      <description>Currently, for initializing/upgrading Hive's information schema, we require a full jdbc url (for HS2). It will be good to have it connect using embedded HS2 by default.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveSchemaHelper.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-3 01:00:00" id="19400" opendate="2018-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust Hive 1.0 to 2.0 conversion utility to the upgrade</summary>
      <description>Conversion utility should allow specification of the output dir, and create files only if there is actually something to do.</description>
      <version>3.0.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-3 01:00:00" id="19415" opendate="2018-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support CORS for all HS2 web endpoints</summary>
      <description>HIVE-19277 changes alone are not sufficient to support CORS. CrossOriginFilter has to be added to jetty which will serve appropriate response for OPTIONS pre-flight request.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestActivePassiveHA.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-4 01:00:00" id="19421" opendate="2018-5-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade version of Jetty to 9.3.20.v20170531</summary>
      <description>Move Jetty up to 9.3.20.v20170531</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-7 01:00:00" id="19435" opendate="2018-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental replication cause data loss if a table is dropped followed by create and insert-into with different partition type.</summary>
      <description>If the incremental dump have drop of partitioned table followed by create/insert on non-partitioned table with same name, doesn't replicate the data. Explained below.Let's say we have a partitioned table T1 which was already replicated to target.DROP_TABLE(T1)-&gt;CREATE_TABLE(T1) (Non-partitioned) -&gt; INSERT(T1)(10) After REPL LOAD, T1 doesn't have any data.Same is valid for non-partitioned to partitioned and partition spec mismatch case as well. </description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-8 01:00:00" id="19465" opendate="2018-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade ORC to 1.5.0</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.typechangetest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.file.dump.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.ConsumerFileMetadata.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.metadata.OrcFileMetadata.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedTreeReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcSerDeStats.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-8 01:00:00" id="19466" opendate="2018-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update constraint violation error message</summary>
      <description>Currently for both CHECK and NOT NULL constraint violation hive throws NOT NULL Constraint violated.</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.update.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.overwrite.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.multi.into.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.notnull.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into.acid.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.notnull.constraint.violation.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFEnforceNotNullConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFEnforceNotNullConstraint.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-5-9 01:00:00" id="19477" opendate="2018-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 in http mode not emitting metric default.General.open_connections</summary>
      <description>Instances in binary mode are emitting the metric default.General.open_connections but the instances operating in http mode are not emitting this metric.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-5-10 01:00:00" id="19495" opendate="2018-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow SerDe itest failure</summary>
      <description>"You tried to write a Bit type when you are using a ValueWriter of type NullableMapWriter."</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.ArrowColumnarBatchSerDe.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2008-6-23 01:00:00" id="195" opendate="2008-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>map-side joins are not supported</summary>
      <description>map-side joins are not supported.If all the tables but one are small and can fit in memory, the join should be performed on the map-side. Ideally, it should be all cost-based, butto start with, the user should be given an option to specify that he needs a map-side join.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">data.scripts.dumpdata.script.py</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExtractOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRRedSink3.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.fetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.selectDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-14 01:00:00" id="19529" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Date/Timestamp NULL issues</summary>
      <description>Wrong results found for: date_add/date_subUT areas: date_add/date_subdatediffto_dateinterval_year_month + interval_year_month interval_day_time + interval_day_time interval_day_time + timestamp timestamp + interval_day_time date + interval_day_time interval_day_time + date interval_year_month + date date + interval_year_month interval_year_month + interval_year_month timestamp + interval_year_monthdate - date interval_year_month - interval_year_month interval_day_time - interval_day_time timestamp - interval_day_time timestamp - timestamp date - timestamp timestamp - date date - interval_day_time date - interval_year_month timestamp - interval_year_month</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomBatchSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIfStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringGroupColConcatCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprVarCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnVarCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringGroupColumnCharScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprCharScalarStringGroupColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CharScalarConcatStringGroupCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToDecimal.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.StringGroupColumnCompareTruncStringScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringScalarCompareStringGroupColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTruncStringColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterStringGroupColumnCompareTruncStringScalar.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-14 01:00:00" id="19530" opendate="2018-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Fix JDBCSerde and re-enable vectorization</summary>
      <description>According to jcamachorodriguez there is a big switch statement in the code that has might have missing types. This can lead to the string types seen.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-16 01:00:00" id="19569" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table db1.t1 rename db2.t2 generates MetaStoreEventListener.onDropTable()</summary>
      <description>When renaming a table within the same DB, this operation causes MetaStoreEventListener.onAlterTable() to fire but when changing DB name for a table it causes MetaStoreEventListener.onDropTable() + MetaStoreEventListener.onCreateTable().The files from original table are moved to new table location. This creates confusing semantics since any logic in onDropTable() doesn't know about the larger context, i.e. that there will be a matching onCreateTable().In particular, this causes a problem for Acid tables since files moved from old table use WriteIDs that are not meaningful with the context of new table.Current implementation is due to replication. This should ideally be changed to raise a "not supported" error for tables that are marked for replication.cc sankarh</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestTablesCreateDropAlterTruncate.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-16 01:00:00" id="19577" opendate="2018-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TEMPORARY TABLE LIKE and INSERT generate output format mismatch errors</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-5-17 01:00:00" id="19583" opendate="2018-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some yetus working dirs are left on hivepest-server-upstream disk after test</summary>
      <description>PTest's PrepPhase is creating a yetus working folder for each build after checking out source code. The source code is then copied into that for yetus. This folder is cleaned up after the test executed, so if that doesn't happen e.g. due to patch not being applicable the folder is left on the disk. We need to remove it in this case too.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="19598" opendate="2018-5-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Acid V1 to V2 upgrade module</summary>
      <description>The on-disk layout for full acid (transactional) tables has changed 3.0.Any transactional table that has any update/delete events in any deltas that have not been Major compacted, must go through a Major compaction before upgrading to 3.0.  No more update/delete/merge should be run after/during major compaction.Not doing so will result in data corruption/loss. Need to create a utility tool to help with this process.  HIVE-19233 started this but it needs more work.</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.tools.HiveMetaTool.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-18 01:00:00" id="19605" opendate="2018-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TAB_COL_STATS table has no index on db/table name</summary>
      <description>The TAB_COL_STATS table is missing an index on (CAT_NAME, DB_NAME, TABLE_NAME). The getTableColumnStatistics call queries based on this tuple. This makes those queries take a significant amount of time in large metastores since they do a full table scan.</description>
      <version>None</version>
      <fixedVersion>3.1.0,2.4.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.upgrade-3.0.0-to-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-22 01:00:00" id="19659" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update master to version 4.0</summary>
      <description>Currently we have branch-3.0 for presumably branch-3.0.X, but branch-3 is marked as 3.0 and master as 3.1-SNAPSHOT.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">testutils.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">classification.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">itests.custom-udfs.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf1.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-udf2.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-classloader-util.pom.xml</file>
      <file type="M">itests.custom-udfs.udf-vectorized-badexample.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.hive-blobstore.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-3-7 01:00:00" id="1966" opendate="2011-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin operator should not load hashtable for each new inputfile if the hashtable to be loaded is already there.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-22 01:00:00" id="19661" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>switch Hive UDFs to use Re2J regex engine</summary>
      <description>Java regex engine can be very slow in some cases e.g. https://bugs.java.com/bugdatabase/view_bug.do?bug_id=JDK-8203458</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRegExp.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">LICENSE</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-23 01:00:00" id="19674" opendate="2018-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by Decimal Constants push down to Druid tables.</summary>
      <description>Queries like following gets generated by Tableau.SELECT SUM(`ssb_druid_100`.`lo_revenue`) AS `sum_lo_revenue_ok` FROM `druid_ssb`.`ssb_druid_100` `ssb_druid_100`GROUP BY 1.1000000000000001;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.drill.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constGby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2018-5-25 01:00:00" id="19713" opendate="2018-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>itests/hive-jmh should not reference a concreate storage-api version</summary>
      <description>this is a bigger problem on branch-3; where storage-api is 2.6.1; but hive-jmh references 2.7.0 (which is for master)</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-jmh.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-31 01:00:00" id="19758" opendate="2018-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set hadoop.version=3.1.0 in standalone-metastore</summary>
      <description>When HIVE-19243 set hadoop.version=3.1.0 it did not change the value used in standalone-metastore which still uses 3.0.0-beta1. At the moment standalone-metastore is still a module of hive and so this can suck in the wrong code.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-1 01:00:00" id="19769" opendate="2018-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create dedicated objects for DB and Table names</summary>
      <description>Currently table names are always strings.  Sometimes that string is just tablename, sometimes it is dbname.tablename.  Sometimes the code expects one or the other, sometimes it handles either.  This is burdensome for developers and error prone.  With the addition of catalog to the hierarchy, this becomes even worse.I propose to add two objects, DatabaseName and TableName.  These will track full names of each object.  They will handle inserting default catalog and database names when those are not provided.  They will handle the conversions to and from strings.These will need to be added to storage-api because ValidTxnList will use it.</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUpdaterThread.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-5 01:00:00" id="19801" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Add some missing classes to jdbc standalone jar and remove hbase classes</summary>
      <description/>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-5 01:00:00" id="19806" opendate="2018-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Several tests do not properly sort their output</summary>
      <description>A number of the tests produce unsorted output that happens to come out the same on people's laptops and the ptest infrastructure.  But when run on a separate linux box the sort differences show up.  </description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.schema.evolution.float.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.bround.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.interval.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.case.when.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.bround.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.parquet.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union38.q</file>
      <file type="M">ql.src.test.queries.clientpositive.selectindate.q</file>
      <file type="M">ql.src.test.queries.clientpositive.parquet.ppd.multifiles.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.schema.evolution.float.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.dynamic.partitions.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.overwrite.dynamic.partitions.q</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.overwrite.directory.q</file>
      <file type="M">itests.hive-blobstore.src.test.queries.clientpositive.insert.into.dynamic.partitions.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-7 01:00:00" id="19826" opendate="2018-6-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>OrcRawRecordMerger doesn&amp;#39;t work for more than one file in non vectorized case</summary>
      <description>Key object in the map is reused and reset, leading to bizarre merges and wrong results.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-11 01:00:00" id="19862" opendate="2018-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres init script has a glitch around UNIQUE_DATABASE</summary>
      <description>ALTER TABLE ONLY "DBS" ADD CONSTRAINT "UNIQUE_DATABASE" UNIQUE ("NAME");Should also include "CTLG_NAME".</description>
      <version>None</version>
      <fixedVersion>3.1.0,3.0.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-14 01:00:00" id="19890" opendate="2018-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Inherit bucket-id from original ROW_ID for delete deltas</summary>
      <description>The ACID delete deltas for unbucketed tables are written to arbitrary files, which should instead be shuffled using the bucket-id instead of hash(ROW__ID).</description>
      <version>3.0.0</version>
      <fixedVersion>3.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-2-14 01:00:00" id="1991" opendate="2011-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Shell to output number of mappers and number of reducers</summary>
      <description>Number of mappers and number of reducers are nice information to be outputted for users to know.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.history.HiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-22 01:00:00" id="19967" opendate="2018-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Need Optraits for PTFOperator ala GBY Op</summary>
      <description>The SMB join on one or more PTF Ops should reset the optraits keys just like GBY Op does.Currently there is no implementation of PTFOp optraits.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-22 01:00:00" id="19969" opendate="2018-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency order (dirlist) assessment fails in yetus run</summary>
      <description>As seen here, the dirlist step of yetus fails to determine order of modules to be built. It silently falls back to alphabetical order which may or may not work depending on the patch.Thu Jun 21 02:43:04 UTC 2018cd /data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958mvn -q exec:exec -Dexec.executable=pwd -Dexec.args=''/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/storage-api/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/upgrade-acid/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/classification/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/common/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/0.23/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/scheduler/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/shims/aggregator/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/common/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/service-rpc/data/hiveptest/working/yetus_PreCommit-HIVE-Build-11958/serdeUsage: java [-options] class [args...] (to execute a class) or java [-options] -jar jarfile [args...] (to execute a jar file)where options include:The problem is in standalone-metastore module: maven plugin 'exec' has a global config set executable=java disregarding the dirlist task's -Dexec.executable=pwd and causing the above error.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-6-26 01:00:00" id="20002" opendate="2018-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shipping jdbd-storage-handler dependency jars in LLAP</summary>
      <description>Shipping the following jars to LLAP to make jdbc storage-handler work: commons-dbcp, commons-pool, db specific jdbc jar whichever exists in classpath.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-27 01:00:00" id="20009" opendate="2018-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix runtime stats for merge statement</summary>
      <description>pushed to branch-3 as well.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-28 01:00:00" id="20021" opendate="2018-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fall back to Synthetic File-ids when getting a HdfsConstants.GRANDFATHER_INODE_ID</summary>
      <description>HDFS client implementations have multiple server implementations, which do not all support the inodes for file locations.If the client returns a 0 InodeId, fall back to the synthetic ones.</description>
      <version>3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-30 01:00:00" id="20039" opendate="2018-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning: Left Outer Join on bucketed table gives wrong result</summary>
      <description>Left outer join on bucketed table on certain cases gives wrong results.Depending on the order in which the table-scans are walked through, the FilterPruner might end up using the wrong table scan's table properties on the other table.</description>
      <version>2.3.2,3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-30 01:00:00" id="20040" opendate="2018-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: HTTP listen queue is 50 and SYNs are lost</summary>
      <description>When testing with 5000 concurrent users, the JDBC HTTP port ends up overflowing on SYNs when the HS2 gc pauses.This is because each getQueryProgress request is an independent HTTP request, so unlike the BINARY mode, there are more connections being established &amp; closed in HTTP mode.LISTEN 0 50 *:10004 *:* This turns into connection errors when enabling net.ipv4.tcp_abort_on_overflow=1, but the better approach is to enqueue the connections until the HS2 is done with its GC pause.</description>
      <version>3.1.0,3.0.0</version>
      <fixedVersion>3.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-1 01:00:00" id="20044" opendate="2018-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Arrow Serde should pad char values and handle empty strings correctly</summary>
      <description>When Arrow Serde serializes char values, it loses padding. Also when it counts empty strings, sometimes it makes a smaller number. It should pad char values and handle empty strings correctly.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringExpr.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.arrow.TestArrowColumnarBatchSerDe.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.arrow.Serializer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-7-3 01:00:00" id="20073" opendate="2018-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Additional tests for to_utc_timestamp function based on HIVE-20068</summary>
      <description>I have the following script and I'm at loss to explain the behavior.  Possibly it's an older bug as we are using the 2.1.1 drivers .  We noticed this issue when converting from US/Eastern into UTC and then back to US/Eastern.  Everything that was in Status Date / Status Hour on 3/11/17 21:00:00 shifted 6 hours ahead into UTC ... then shifted back to 3/11/17 22:00:00 back in US/Eastern.  The behavior appears to be the same using the constant EST5EDT.  EDT was effective on 3/12 2 am, so the issue appears only at this boundary condition when we "spring ahead", but it at least on the surface seems incorrect.--------------------------------------------------------------------------------------------------------------------------&amp;#8211; Potential Issue with to_utc_timestamp---------------------------------------------------------------------------------------------------------------------------SELECT '2017-03-11 18:00:00', to_utc_timestamp(timestamp '2017-03-11 18:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 19:00:00', to_utc_timestamp(timestamp '2017-03-11 19:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 20:00:00', to_utc_timestamp(timestamp '2017-03-11 20:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 21:00:00', to_utc_timestamp(timestamp '2017-03-11 21:00:00','US/Eastern'); &amp;#8211; Shifts ahead 6 hours (???)_c0                                   _c12017-03-11 21:00:00       2017-03-12 03:00:00SELECT '2017-03-11 22:00:00', to_utc_timestamp(timestamp '2017-03-11 22:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-11 23:00:00', to_utc_timestamp(timestamp '2017-03-11 23:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 00:00:00', to_utc_timestamp(timestamp '2017-03-12 00:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 01:00:00', to_utc_timestamp(timestamp '2017-03-12 01:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 02:00:00', to_utc_timestamp(timestamp '2017-03-12 02:00:00','US/Eastern'); &amp;#8211; Shifts ahead 5 hours as expectedSELECT '2017-03-12 03:00:00', to_utc_timestamp(timestamp '2017-03-12 03:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expectedSELECT '2017-03-12 04:00:00', to_utc_timestamp(timestamp '2017-03-12 04:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expectedSELECT '2017-03-12 05:00:00', to_utc_timestamp(timestamp '2017-03-12 05:00:00','US/Eastern'); &amp;#8211; Shifts ahead 4 hours as expected</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-6 01:00:00" id="20111" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</summary>
      <description>Similar to HIVE-20085. HBase-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.ppd.key.ranges.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.viewjoins.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.format.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseMetaHook.java</file>
      <file type="M">hbase-handler.src.test.queries.negative.cascade.dbdrop.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.generatehfiles.require.family.path.q</file>
      <file type="M">hbase-handler.src.test.queries.negative.hbase.ddl.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.external.table.ppd.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbasestats.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.binary.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.map.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.map.queries.prefix.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.binary.storage.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key2.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.custom.key3.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ddl.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.decimal.decimal.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.handler.bulk.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.joins.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.null.first.col.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ppd.join.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.ppd.key.range.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.pushdown.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.queries.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.scan.params.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.single.sourced.multi.insert.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.timestamp.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.timestamp.format.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.hbase.viewjoins.q</file>
      <file type="M">hbase-handler.src.test.queries.positive.ppd.key.ranges.q</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.generatehfiles.require.family.path.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.external.table.ppd.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.binary.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.map.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.map.queries.prefix.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.binary.storage.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key2.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.custom.key3.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ddl.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.decimal.decimal.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.handler.bulk.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.joins.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.null.first.col.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.join.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.scan.params.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-6 01:00:00" id="20112" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulo-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</summary>
      <description>Similar to HIVE-20085 and HIVE-20111. Accumulo-Hive (managed) table creation fails with strict managed table checks: Table is marked as a managed table but is not transactional</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.joins.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.index.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.custom.key2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.custom.key.q.out</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.single.sourced.multi.insert.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.queries.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.predicate.pushdown.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.joins.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.index.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.custom.key2.q</file>
      <file type="M">accumulo-handler.src.test.queries.positive.accumulo.custom.key.q</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.TestAccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-6 01:00:00" id="20113" opendate="2018-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shuffle avoidance: Disable 1-1 edges for sorted shuffle</summary>
      <description>The sorted shuffle avoidance can have some issues when the shuffle data gets broken up into multiple chunks on disk.The 1-1 edge cannot skip the tez final merge - there's no reason for 1-1 to have a final merge at all, it should open a single compressed file and write a single index entry.Until the shuffle issue is resolved &amp; a lot more testing, it is prudent to disable the optimization for sorted shuffle edges and stop rewriting the RS(sorted) = = = RS(sorted) into RS(sorted) = = = RS(FORWARD).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkJoinDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.no.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.is.not.distinct.from.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partition.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.shared.scan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedwork.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-9 01:00:00" id="20123" opendate="2018-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix masking tests after HIVE-19617</summary>
      <description>Masking tests results were changed inadvertently when HIVE-19617 went in, since table names were changed.</description>
      <version>3.1.0,3.0.0,3.2.0,4.0.0</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.newdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-10 01:00:00" id="20131" opendate="2018-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL Script changes for creating txn write notification in 3.2.0 files</summary>
      <description>1. Change partition name size from 1024 to 767 . (mySQL 5.6 and before that supports max 767 length keys) 2. Remove the create txn_write_notification_log table creation from 3.1.0 scripts and add a new scripts for 3.2.03. Remove the file 3.1.0-to-4.0.0 and instead add file for 3.2.0-to-4.0.0 and 3.1.0-to-3.2.04. Change in metastore init schema  xml file to take 4.0.0 instead of 3.1.0 as current version. </description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade.order.postgres</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.1.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.0.0-to-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade.order.oracle</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.1.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.0.0-to-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade.order.mysql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.1.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.0.0-to-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade.order.mssql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.1.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.0.0-to-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade.order.derby</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.1.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.0.0-to-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-13 01:00:00" id="20164" opendate="2018-7-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Murmur Hash : Make sure CTAS and IAS use correct bucketing version</summary>
      <description>With the migration to Murmur hash, CTAS and IAS from old table version to new table version does not work as intended and data is hashed using old hash logic.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-16 01:00:00" id="20183" opendate="2018-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Inserting from bucketed table can cause data loss, if the source table contains empty buckets</summary>
      <description>Could be reproduced by the following:set hive.enforce.bucketing=true;set hive.enforce.sorting=true;set hive.optimize.bucketingsorting=true;create table bucket1 (id int, val string) clustered by (id) sorted by (id ASC) INTO 4 BUCKETS;insert into bucket1 values (1, 'abc'), (3, 'abc');select * from bucket1;+-------------+--------------+| bucket1.id | bucket1.val |+-------------+--------------+| 3 | abc || 1 | abc |+-------------+--------------+create table bucket2 like bucket1;insert overwrite table bucket2 select * from bucket1;select * from bucket2;+-------------+--------------+| bucket2.id | bucket2.val |+-------------+--------------+| 1 | abc |+-------------+--------------+</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-17 01:00:00" id="20191" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit patch application doesn&amp;#39;t fail if patch is empty</summary>
      <description>I've created some backport tickets to branch-3 (e.g. HIVE-20181) and made the mistake of uploading the patch files with wrong filename (. instead of - between version and branch).These get applied on master, where they're already present, since git apply with -3 won't fail if patch is already there. Tests are run on master instead of failing.I think the patch application should fail if the patch is empty and branch selection logic should probably fail too if the patch name is malformed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-7-26 01:00:00" id="20242" opendate="2018-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query results cache: Improve ability of queries to use pending query results</summary>
      <description>HIVE-19138 allowed a currently running query to wait on the pending results of an already running query. gopalv, after testing with high concurrency, suggested further improving this by having a way to use the switch to using the results cache even at the end of query compilation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-8-26 01:00:00" id="20246" opendate="2018-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configurable collecting stats by using DO_NOT_UPDATE_STATS table property</summary>
      <description>By default, Hive collects stats when running operations like alter table partition(s), create table, and create external table. However, collecting stats requires Metastore lists all files under the table directory and the file listing operation can be very expensive particularly on filesystems like S3.HIVE-18743 made DO_NOT_UPDATE_STATS table property could be selectively prevent stats collection. This Jira aims at introducing DO_NOT_UPDATE_STATS table property into the MetaStoreUtils.updatePartitionStatsFast. By adding this, user can be selectively prevent stats collection when doing alter table partition(s) operation at table level. For example, set 'Alter Table S3_Table set tblproperties('DO_NOT_UPDATE_STATS'='TRUE');' MetaStore will not collect stats for the specified S3_Table when alter table add partition(key1=val1, key2=val2);</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-7-26 01:00:00" id="20248" opendate="2018-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up some TODOs after txn stats merge</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-3 01:00:00" id="20306" opendate="2018-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement projection spec for fetching only requested fields from partitions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java.orig</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java.orig</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MSerDeInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-3-8 01:00:00" id="2034" opendate="2011-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Backport HIVE-1991 after overridden by HIVE-1950</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-8 01:00:00" id="20340" opendate="2018-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Druid Needs Explicit CASTs from Timestamp to STRING when the output of timestamp function is used as String</summary>
      <description>Druid time expressions return numeric values in form of ms (instead of formatted timestamp). Because of this expressions/function which expects its argument as string type ended up returning different values for time expressions input.e.g. SELECT SUBSTRING(to_date(datetime0),4) FROM tableau_orc.calcs;| 4-07-25 |SELECT SUBSTRING(to_date(datetime0),4) FROM druid_tableau.calcs;| 0022400000 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM tableau_orc.calcs;| 2004-07-17 00:00:00 |SELECT CONCAT(to_date(datetime0),' 00:00:00') FROM druid_tableau.calcs;| 1090454400000 00:00:00 | Druid needs explicit cast to make this work</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.alt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.test1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-17 01:00:00" id="20413" opendate="2018-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"cannot insert NULL" for TXN_WRITE_NOTIFICATION_LOG in Oracle</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.1.0-to-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-22 01:00:00" id="20439" opendate="2018-8-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use the inflated memory limit during join selection for llap</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.unionDistinct.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join32.lessSize.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-29 01:00:00" id="20659" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-4-11 01:00:00" id="2106" opendate="2011-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase the number of operator counter</summary>
      <description>Currently Hadoop counters have to be defined as enum (hardcoded) and we support up to 400 counters now. This limit the number of operators to 100 (each operator has 4 counters). We need to increase the hadoop counters or change the Hive code to use Hadoop 0.20 API.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-7 01:00:00" id="21095" opendate="2019-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>&amp;#39;Show create table&amp;#39; should not display a time zone for timestamp with local time zone</summary>
      <description>SHOW CREATE TABLE shows the time zone that the table was created in (if it contains a TIMESTAMPTZ column). This is also misleading, since it has nothing to do with the actual data or server or user time zone.e.g.hive&gt; set time zone America/Los_Angeles;hive&gt; create table text_local (ts timestamp with local time zone) stored as textfile;hive&gt; show create table text_local;CREATE TABLE `text_local`(  `ts` timestamp with local time zone('America/Los_Angeles'))should be:hive&gt; show create table text_local;CREATE TABLE `text_local`(  `ts` timestamp with local time zone)This was discussed in the community doc Consistent timestamp types in Hadoop SQL engines</description>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TimestampLocalTZTypeInfo.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-9 01:00:00" id="21107" opendate="2019-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cannot find field" error during dynamically partitioned hash join</summary>
      <description>This occurs in non-CBO path with dynamic partitioned join + constant propagation ON.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-24 01:00:00" id="21159" opendate="2019-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify Merge statement logic to perform Update split early</summary>
      <description/>
      <version>3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RewriteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MergeSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-11 01:00:00" id="21239" opendate="2019-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline help LDAP connection example incorrect</summary>
      <description>There's the following connection example string in the beeline -h command output: 5. Connect using LDAP authentication$ beeline -u jdbc:hive2://hs2.local:10013/default &lt;ldap-username&gt; &lt;ldap-password&gt;When a user attempts to connect like above, it'll fail with LDAP authentication failure. This is because username and passwords are not picked up in the shown form. A working example would be:$ beeline -n &lt;ldap-username&gt; -p &lt;ldap-password&gt; -u jdbc:hive2://hs2.local:10013/default </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-24 01:00:00" id="21645" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-6-5 01:00:00" id="21836" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-16 01:00:00" id="21998" opendate="2019-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-21823 commit message is wrong</summary>
      <description>https://github.com/apache/hive/commit/4853a44b2fcfa702d23965ab0d3835b6b57954c4The Jira message is wrong. Reuses previous commit message.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-1 01:00:00" id="22072" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Altering table to make a column change does not update constraints references</summary>
      <description>The constraint will still point to old column descriptor incorrectly.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22089" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description/>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22090" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2019-1-17 01:00:00" id="22652" opendate="2019-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey push through Group by with Grouping sets</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.topnkey.TopNKeyPushdownProcessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-1-24 01:00:00" id="22770" opendate="2020-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip interning of MapWork fields during deserialization</summary>
      <description>HIVE-19937 introduced some interning logic into mapwork deserialization process, but it's only related to spark, maybe we should skip this for tez, reducing the cpu pressure in tez tasks.UPDATE: Hive on spark is not supported anymore, the MapWorkSerializer can be completely removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="23035" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-12-28 01:00:00" id="2611" opendate="2011-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make index table output of create index command if index is table based</summary>
      <description>If an index is table based, when that index is created a table is created to contain that index. This should be listed in the output of the command.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.merge.negative.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.bitmap.no.map.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-1-22 01:00:00" id="2674" opendate="2011-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_partitions_ps throws TApplicationException if table doesn&amp;#39;t exist</summary>
      <description>If the table passed to get_partition_ps doesn't exist, a NPE is thrown by getPartitionPsQueryResults. There should be a check here, which throws a NoSuchObjectException if the table doesn't exist.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-15 01:00:00" id="8472" opendate="2014-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ALTER DATABASE SET LOCATION</summary>
      <description>Similarly to ALTER TABLE tablename SET LOCATION, it would be helpful if there was an equivalent for databases.</description>
      <version>2.2.0,2.4.0,3.0.0</version>
      <fixedVersion>2.2.1,2.4.0,3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.events.PreEventContext.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestAuthorizationPreEventListener.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
    </fixedFiles>
  </bug>
</bugrepository>