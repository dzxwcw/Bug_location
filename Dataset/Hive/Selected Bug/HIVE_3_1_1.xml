<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2016-5-18 01:00:00" id="13782" opendate="2016-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile async query asynchronously</summary>
      <description>Currently, when an async query is submitted to HS2, HS2 does the preparation synchronously. One of the preparation step is to compile the query, which may take some time. It will be helpful to provide an option to do the compilation asynchronously.</description>
      <version>None</version>
      <fixedVersion>2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-12 01:00:00" id="14737" opendate="2016-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem accessing /logs in a Kerberized Hive Server 2 Web UI</summary>
      <description>The /logs menu fails with error &amp;#91;1&amp;#93; when the cluster is Kerberized. Other menu items are working properly.&amp;#91;1&amp;#93; HTTP ERROR: 401Problem accessing /logs/. Reason: Unauthenticated users are not authorized to access this page.Powered by Jetty://</description>
      <version>1.1.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-11-7 01:00:00" id="20881" opendate="2018-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation oversimplifies projections</summary>
      <description>create table cx2(bool1 boolean);insert into cx2 values (true),(false),(null);set hive.cbo.enable=true;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+--------+| _c0 |+--------+| true || false || NULL |+--------+set hive.cbo.enable=false;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+-------+| _c0 |+-------+| true || NULL || NULL |+-------+from explain it seems the expression was simplified to: (_col0 is true or null)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-1-5 01:00:00" id="209" opendate="2009-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable assertions for unit tests</summary>
      <description>We should enable assertions while running unit tests. Many assertions currently fail while running tests.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.typeinfo.StructTypeInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-11-19 01:00:00" id="20940" opendate="2018-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bridge cases in which Calcite&amp;#39;s type resolution is more stricter than Hive.</summary>
      <description>Calcite is more stricter w.r.t common types than Hive.For example in case a CASE with different types on the branches is not something Calcite likes:CASE WHEN cond1 THEN booleanCol WHEN cond2 THEN stringCol WHEN cond3 THEN floatCol ELSE doubleColENDissues will only happen with 1.18-SNAPSHOT version</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.kryo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.conversion.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-11-20 01:00:00" id="20951" opendate="2018-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set Xms to 50% always</summary>
      <description>The lack of GC pauses is killing LLAP containers whenever the significant amount of memory is consumed by the off-heap structures which aren't cleaned up automatically until the GC runs.There's a java.nio.DirectByteBuffer.Deallocator which runs when the Direct buffers are garbage collected, which actually does the cleanup of the underlying off-heap buffers.The lack of Garbage collection activity for several hours while responding to queries triggers a build-up of these off-heap structures which end up forcing YARN to kill the process instead.It is better to hit a GC pause occasionally rather than to lose a node every few hours.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-12-4 01:00:00" id="21004" opendate="2018-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less object creation for Hive Kafka reader</summary>
      <description>Reduce the amount of un-needed object allocation by using a row boat as way to carry data around.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.VectorizedKafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-5 01:00:00" id="21007" opendate="2018-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join + Union can lead to wrong plans</summary>
      <description>Tez compiler has the ability to push JOIN within UNION (by replicating join on each branch). If this JOIN had a SJ branch outgoing (or incoming) it could mess up the plan and end up generating incorrect or wrong plan.As a safe measure any SJ branch after UNION should be removed (until we improve the logic to better handle SJ branches)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-1-10 01:00:00" id="21113" opendate="2019-1-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>For HPL/SQL that contains boolean expression with NOT, incorrect SQL may be generated.</summary>
      <description>In HPL/SQL, ' SELECT * FROM a WHERE NOT (1 = 2) ' will generate to incorrect SQL ' SELECT * FROM a WHERE (1 = 2) ', the 'NOT' in boolean expression is missing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.out.txt</file>
      <file type="M">hplsql.src.test.queries.offline.select.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-25 01:00:00" id="21167" opendate="2019-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucketing: Bucketing version 1 is incorrectly partitioning data</summary>
      <description>Using murmur hash for bucketing columns was introduced in HIVE-18910, following which 'bucketing_version'='1' stands for the old behaviour (where for example integer columns were partitioned based on mod values). Looks like we have a bug in the old bucketing scheme now. I could repro it when modified the existing schema using an alter table add column and adding new data. Repro:0: jdbc:hive2://localhost:10010&gt; create transactional table acid_ptn_bucket1 (a int, b int) partitioned by(ds string) clustered by (a) into 2 buckets stored as ORC TBLPROPERTIES('bucketing_version'='1', 'transactional'='true', 'transactional_properties'='default');No rows affected (0.418 seconds)0: jdbc:hive2://localhost:10010&gt; insert into acid_ptn_bucket1 partition (ds) values(1,2,'today'),(1,3,'today'),(1,4,'yesterday'),(2,2,'yesterday'),(2,3,'today'),(2,4,'today');6 rows affected (3.695 seconds)Data from ORC file (data as expected):/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00000{"operation": 0, "originalTransaction": 1, "bucket": 536870912, "rowId": 0, "currentTransaction": 1, "row": {"a": 2, "b": 4}}{"operation": 0, "originalTransaction": 1, "bucket": 536870912, "rowId": 1, "currentTransaction": 1, "row": {"a": 2, "b": 3}}/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00001{"operation": 0, "originalTransaction": 1, "bucket": 536936448, "rowId": 0, "currentTransaction": 1, "row": {"a": 1, "b": 3}}{"operation": 0, "originalTransaction": 1, "bucket": 536936448, "rowId": 1, "currentTransaction": 1, "row": {"a": 1, "b": 2}}Modifying table schema and inserting new data:0: jdbc:hive2://localhost:10010&gt; alter table acid_ptn_bucket1 add columns(c int);No rows affected (0.541 seconds)0: jdbc:hive2://localhost:10010&gt; insert into acid_ptn_bucket1 partition (ds) values(3,2,1000,'yesterday'),(3,3,1001,'today'),(3,4,1002,'yesterday'),(4,2,1003,'today'), (4,3,1004,'yesterday'),(4,4,1005,'today');6 rows affected (3.699 seconds)Data from ORC file (wrong partitioning):/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00000{"operation": 0, "originalTransaction": 3, "bucket": 536870912, "rowId": 0, "currentTransaction": 3, "row": {"a": 3, "b": 3, "c": 1001}}/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00001{"operation": 0, "originalTransaction": 3, "bucket": 536936448, "rowId": 0, "currentTransaction": 3, "row": {"a": 4, "b": 4, "c": 1005}}{"operation": 0, "originalTransaction": 3, "bucket": 536936448, "rowId": 1, "currentTransaction": 3, "row": {"a": 4, "b": 2, "c": 1003}}As seen above, the expected behaviour is that new data with column 'a' being 3 should go to bucket1 and column 'a' being 4 should go to bucket0, but the partitioning is wrong.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.murmur.hash.migration.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.murmur.hash.migration.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-8-21 01:00:00" id="2122" opendate="2011-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of deprecated methods from org.apache.hadoop.io package</summary>
      <description>Serde code uses some deprecated methods from org.apache.hadoop.io. package. We should remove them.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-3-8 01:00:00" id="21416" opendate="2019-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log git apply tries with p0, p1, and p2</summary>
      <description>Currently when the PreCommit-HIVE-Build Jenkins job is trying to apply the patch it tries it first with -p0, then if it wasn't successful with -p1, then finally if it still wasn't successful with -p2. The 3 tries are not separated by anything, so the error messages of  the potential failures are mixed together. There should be a log message before each try.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-11 01:00:00" id="21423" opendate="2019-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not check for whitespace issues in generated code</summary>
      <description/>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hive-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-11 01:00:00" id="21424" opendate="2019-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable AggregateStatsCache by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.updateBasicStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-13 01:00:00" id="21440" opendate="2019-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test_teradatabinaryfile to not run into stackoverflows</summary>
      <description>this test seems to be failing in recent runs; taking a closer look shows that it might be some kryo related stackoverflowCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.TeradataBinaryFileOutputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:490) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97) at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:321) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:314) at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ... 104 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:54) ... 115 moreCaused by: java.lang.StackOverflowError at java.util.HashMap.hash(HashMap.java:338) at java.util.HashMap.get(HashMap.java:556) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:61) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-5-3 01:00:00" id="2146" opendate="2011-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block Sampling should adjust number of reducers accordingly to make it useful</summary>
      <description>Now number of reducers of block sampling is not modified, so that queries like:select c from tab tablesample(1 percent) group by c;can generate huge number of reducers although the input is sampled to be small.We need to shrink number of reducers to make block sampling more useful.Since now number of reducers are determined before get splits, the way to do it probably is not clean enough, but we can do a good guess.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-2-21 01:00:00" id="21487" opendate="2019-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>COMPLETED_COMPACTIONS and COMPACTION_QUEUE table missing appropriate indexes</summary>
      <description>Looking at a MySQL install where HMS is pointed on Hive 3.1, I see a constant stream of queries of the form:select CC_STATE from COMPLETED_COMPACTIONS where CC_DATABASE = 'tpcds_orc_exact_1000' and CC_TABLE = 'catalog_returns' and CC_PARTITION = 'cr_returned_date_sk=2452851' and CC_STATE != 'a' order by CC_ID desc;but the COMPLETED_COMPACTIONS table has no index. In this case it's resulting in a full table scan over 115k rows, which takes around 100ms.</description>
      <version>3.1.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-4-27 01:00:00" id="21518" opendate="2019-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GenericUDFOPNotEqualNS does not run in LLAP</summary>
      <description>GenericUDFOPNotEqualNS (Not equal nullsafe operator) does not run in LLAP mode, because it is not registered as a built-in function.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-29 01:00:00" id="21541" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing asf headers from HIVE-15406</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloatNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimalNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromDate.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-29 01:00:00" id="21543" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use FilterHooks for show compactions</summary>
      <description>Use FilterHooks for checking dbs/tables/partitions for showCompactions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-3-29 01:00:00" id="21544" opendate="2019-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation corrupts coalesce/case/when expressions during folding</summary>
      <description>set hive.fetch.task.conversion=none;set hive.optimize.ppd=false;create table t (s1 string,s2 string);insert into t values (null,null);explainselect coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;select coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;incorrect result is:null_value null_value NULL noteqexpected result:null_value null_value true eq</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-22 01:00:00" id="21641" opendate="2019-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Llap external client returns decimal columns in different precision/scale as compared to beeline</summary>
      <description>Llap external client gives different precision/scale as compared to when the query is executed beeline. Consider the following results:Query: select avg(ss_ext_sales_price) my_avg from store_sales; Result from Beeline +----------------------------+| my_avg |+----------------------------+| 37.8923531030581611189434 |+----------------------------+ Result from Llap external client+---------+| my_avg|+---------+|37.892353|+---------+ This is due to Driver(beeline path) calls analyzeInternal() for getting result set schema which initializes resultSchema after some more transformations as compared to llap-ext-client which calls genLogicalPlan()Replacing genLogicalPlan() by analyze() resolves this.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-24 01:00:00" id="21645" opendate="2019-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-3-29 01:00:00" id="21660" opendate="2019-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong result when union all and later view with explode is used</summary>
      <description>There is a data loss when the data is inserted to a partitioned table using union all and lateral view with explode.  Steps to reproduce: create table t1 (id int, dt string);insert into t1 values (2, '2019-04-01');create table t2( id int, dates array&lt;string&gt;);insert into t2 select 1 as id, array('2019-01-01','2019-01-02','2019-01-03') as dates;create table dst (id int) partitioned by (dt string);set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.dynamic.partition=true;insert overwrite table dst partition (dt)select t.id, t.dt from (select id, dt from t1union allselect id, dts as dt from t2 tt2 lateral view explode(tt2.dates) dd as dts ) t;select * from dst;  Actual Result:+--------------+--------------+| 2 | 2019-04-01 |+--------------+--------------+ Expected Result (Run only the select part from the above insert query): +-------+------------+| 2     | 2019-04-01 || 1     | 2019-01-01 || 1     | 2019-01-02 || 1     | 2019-01-03 |+-------+------------+ Data retrieved using union all and lateral view with explode from second table is missing. </description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-29 01:00:00" id="21663" opendate="2019-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metastore Translation Layer</summary>
      <description>This task is for the implementation of the default provider for translation, that is extensible if needed for a custom translator. Please refer the spec for additional details on the translation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableValidWriteIds.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WriteNotificationLogRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CommitTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsFilterSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsProjectionSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpecWithSharedSD.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionWithoutSD.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RenamePartitionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplLastIdInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplTblWriteIdStateRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-5-22 01:00:00" id="21777" opendate="2019-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven jar goal is producing warning due to missing dependency</summary>
      <description>org.apache.directory.client.ldap:ldap-client-directory is a test scope dependecy. Hive is using version 0.1 but 0.1-SNAPSHOT is also there as transitive dependency (omitted for collision with 0.1 which is already there on top level) causing warning in the maven default lifecycle execution:&amp;#91;WARNING&amp;#93; The POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT is missing, no dependency information availableThe warning appears in the jar goal logs and it can easily be removed by excluding this transitive dependency. </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-5-24 01:00:00" id="21791" opendate="2019-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Surrogate Key issue for insert with select with limit operations</summary>
      <description/>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2019-6-5 01:00:00" id="21836" opendate="2019-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-27 01:00:00" id="21927" opendate="2019-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer Web UI: Setting the HttpOnly option in the cookies</summary>
      <description>Intend of this JIRA is to introduce the HttpOnly option in the cookie.cookie: before changehdp32b FALSE / FALSE 0 JSESSIONID 8dkibwayfnrc4y4hvpu3vh74after change:#HttpOnly_hdp32b FALSE / FALSE 0 JSESSIONID e1npdkbo3inj1xnd6gdc6ihws</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-8 01:00:00" id="21968" opendate="2019-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove index related codes</summary>
      <description>Hive doesn't support indexes since 3.0.0, still some index related tests were left behind, and some code to disable them. Also some index related code is still in the codebase. They should be removed.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.cbo.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.cbo.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-9 01:00:00" id="21972" opendate="2019-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"show transactions" display the header twice</summary>
      <description>show transactions;+-----------------+--------------------+----------------+----------------------+-------+-------------------+| txnid | state | startedtime | lastheartbeattime | user | host |+-----------------+--------------------+----------------+----------------------+-------+-------------------+| Transaction ID | Transaction State | Started Time | Last Heartbeat Time | User | Hostname || 896 | ABORTED | 1560209607000 | 1560209607000 | hive | hostname |+-----------------+--------------------+----------------+----------------------+-------+-------------------+</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.ShowTransactionsOperation.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-10 01:00:00" id="21979" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestReplication tests time out regularily</summary>
      <description>I think we should add TestTableLevelReplicationScenarios and friends to be executed in isolationfrom a recent ptest execution:[INFO] Running org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 150.413 s - in org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.084 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.062 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Running org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 57.568 s - in org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 348.769 s - in org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Running org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.089 s - in org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Running org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,044.666 s - in org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 225.734 s - in org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplAcrossInstancesWithJsonMessageFormat</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-10 01:00:00" id="21986" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer Web UI: Setting the Strict-Transport-Security in default response header</summary>
      <description>Currently, HiveServer UI HTTP response header doesn't have Strict-Transport-Security set so will be adding this to default header.expected response after patch:HTTP/1.1 200 OKDate: Wed, 10 Jul 2019 22:47:34 GMTContent-Type: text/html;charset=utf-8Strict-Transport-Security: max-age=31536000; includeSubDomainsX-Content-Type-Options: nosniffX-FRAME-OPTIONS: SAMEORIGINX-XSS-Protection: 1; mode=blockSet-Cookie: JSESSIONID=fby9p6p5olb12xui7kj93uys;Path=/;HttpOnlyExpires: Thu, 01 Jan 1970 00:00:00 GMTContent-Length: 3824Server: Jetty(9.3.25.v20180904)</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-11 01:00:00" id="21988" opendate="2019-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not consider nodes with 0 capacity when calculating host affinity</summary>
      <description>When a node is blacklisted (capacity set to 0) then we should not assign any more tasks to it</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.InactiveServiceInstance.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-6 01:00:00" id="2199" opendate="2011-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect success flag passed to jobClose</summary>
      <description>For block level merging of RCFiles, jobClose is passed the incorrect variable as the success flag</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-9-29 01:00:00" id="22059" opendate="2019-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-8-1 01:00:00" id="22075" opendate="2019-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the max-reducers=1 regression from HIVE-14200</summary>
      <description>The condition does not kick in when minPartition=1, maxPartition=1, nReducers=1, maxReducers=1</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22089" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description/>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-8 01:00:00" id="22090" opendate="2019-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-15 01:00:00" id="22118" opendate="2019-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log the table name while skipping the compaction because it&amp;#39;s sorted table/partitions</summary>
      <description>for debugging perspective it's good if we log the full table name while skipping the table for compaction otherwise it's tedious to know why the compaction is not happening for the target table.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-8-27 01:00:00" id="22151" opendate="2019-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off hybrid grace hash join by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.join.noncbo.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-5 01:00:00" id="22170" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2011-7-15 01:00:00" id="2224" opendate="2011-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to add partitions atomically</summary>
      <description>I'd like to see an atomic version of the add_partitions() call.Whether this is to be done by config to affect add_partitions() behaviour (not my preference) or just changing add_partitions() default behaviour (my preference, but likely to affect current behaviour, so will need others' input) or by making a new add_partitions_atomic() call depends on discussion.This looks relatively doable to implement (will need a dependent add_partition_core to not do a ms.commit_partition() early, and to cache list of directories created to remove on rollback, and a list of AddPartitionEvent to trigger in one shot later)Thoughts? This also seems like something to implement for allowing HIVE-1805.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-25 01:00:00" id="22241" opendate="2019-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement UDF to interpret date/timestamp using its internal representation and Gregorian-Julian hybrid calendar</summary>
      <description>UDF that converts a date/timestamp to new proleptic Gregorian calendar (ISO 8601 standard), which is produced by extending the Gregorian calendar backward to dates preceding its official introduction in 1582, assuming that its internal days/milliseconds since epoch is calculated using legacy Gregorian-Julian hybrid calendar, i.e., calendar that supports both the Julian and Gregorian calendar systems with the support of a single discontinuity, which corresponds by default to the Gregorian date when the Gregorian calendar was instituted.</description>
      <version>None</version>
      <fixedVersion>3.1.3,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-9-25 01:00:00" id="22243" opendate="2019-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align Apache Thrift version to 0.9.3-1 in standalone-metastore as well</summary>
      <description>Thrift was bumped to 0.9.3-1 in HIVE-21173, but standalone-metastore was left out of this.Thanks for pointing this out Ashutosh Bapat!</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-25 01:00:00" id="22244" opendate="2019-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added default ACLs for znodes on a non-kerberized cluster</summary>
      <description>Set default ACLs for znodes on a non-kerberized cluster: Create/Read/Delete/Write/Admin to the world</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-10-16 01:00:00" id="22354" opendate="2019-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP status driver may look for worker registration on &amp;#39;unsecure&amp;#39; ZK nodes</summary>
      <description>HIVE-22195 introduced a change in determining secure/unsecure environments:public static boolean isKerberosEnabled(Configuration conf) { try { return UserGroupInformation.getLoginUser().isFromKeytab() &amp;&amp; HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_ZOOKEEPER_USE_KERBEROS); } catch (IOException e) { return false; }} This won't work for cases where the JVM process was started after kinit (e.g. in a launcher shell script), where Kerberos authentication is not 'fromKeytab' but rather 'fromTicket' - it will return false even if we have a successfully authenticated principal.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.registry.impl.TestZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-7-5 01:00:00" id="2257" opendate="2011-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TestHadoop20SAuthBridge</summary>
      <description>Looks like this test was accidentally disabled in HIVE-818.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="22635" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-18 01:00:00" id="22659" opendate="2019-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JClouds needs to be updated to 2.1.3 in ptest</summary>
      <description>Since a couple of days ptest responded 404 to test queries coming in from jenkins side.I took a look into the issue and saw this exception on hiveptest-server-upstream side:Caused by: java.lang.IllegalStateException: zone https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-d not present in [https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-fhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-a] Debugging into the issue, it seems like that when we we put our cloud context in place for test execution, some default values originating from default templates are matched with actual GCP capabilities - and I guess zone us-central-1d was decommissioned in real-life, hence the exception.I upgraded jclouds version from 2.1.0 to 2.1.3 on server side and retried running Tomcat with this new installation. It seems to have fixed the issue. NO PRECOMMIT TESTS </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-8 01:00:00" id="2276" opendate="2011-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Inconsistency between RB and JIRA patches for HIVE-2194</summary>
      <description>The RB and JIRA patches for HIVE-2194 were out of sync. An outdated patch for HIVE-2194 was committed. This patch updates that patch to include the changes from RB.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-1-22 01:00:00" id="22761" opendate="2020-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor fails to report query state as errored if session initialization fails</summary>
      <description>right now the info object is only initialized after the sessionstate is inited - which might get into trouble...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-3 01:00:00" id="22966" opendate="2020-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Consider including waitTime for comparing attempts in same vertex</summary>
      <description>When attempts are compared within same vertex, it should pick up the attempt with longest wait time to avoid starvation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-17 01:00:00" id="23035" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-10 01:00:00" id="23178" opendate="2020-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description/>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-11 01:00:00" id="23181" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-8-28 01:00:00" id="2319" opendate="2011-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
</bugrepository>