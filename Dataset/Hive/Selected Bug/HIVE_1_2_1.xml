<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  
  
  
  <bug fixdate="2015-6-16 01:00:00" id="11014" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some MiniTez tests have result changes compared to master</summary>
      <description>vector_binary_join_groupby, vector_outer_join1, vector_outer_join2 and cbo_windowing</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.binary.join.groupby.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-2-26 01:00:00" id="1104" opendate="2010-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Suppress Checkstyle warnings for generated files</summary>
      <description>Suppress Checkstyle warnings for generated files.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">checkstyle.checkstyle.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-18 01:00:00" id="11040" opendate="2015-6-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Derby dependency version to 10.10.2.0</summary>
      <description>We don't see this on the Apache pre-commit tests because it uses PTest, but running the entire TestCliDriver suite results in failures in some of the partition-related qtests (partition_coltype_literals, partition_date, partition_date2). I've only really seen this on Linux (I was using CentOS).HIVE-8879 changed the Derby dependency version from 10.10.1.1 to 10.11.1.1. Testing with 10.10.1.1 or 10.20.2.0 seems to allow the partition related tests to pass. I'd like to change the dependency version to 10.20.2.0, since that version should also contain the fix for HIVE-8879.</description>
      <version>None</version>
      <fixedVersion>1.2.1,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-19 01:00:00" id="11059" opendate="2015-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcatalog-server-extensions tests scope should depend on hive-exec</summary>
      <description>(causes test failures in Windows due to the lack of WindowsPathUtil being available otherwise)</description>
      <version>1.2.1</version>
      <fixedVersion>1.2.2</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-27 01:00:00" id="1106" opendate="2010-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "Alter table t add if not exists partition ..."</summary>
      <description>It will be useful to add such a command so that users don't see an error when adding an existing partition.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.addpart1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AddPartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-24 01:00:00" id="11090" opendate="2015-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ordering issues with windows unit test runs</summary>
      <description/>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.orig.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.2.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.short.regress.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.casts.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.date.funcs.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.decimal.expressions.q</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.orig.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-11-8 01:00:00" id="11201" opendate="2015-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog is ignoring user specified avro schema in the table definition</summary>
      <description>HCatalog is ignoring user specified avro schema in the table definition , instead generating its own avro based from hive meta store. By generating its own schema will result in mismatch names. For exmple Avro fields name are Case Sensitive. By generating it's own schema will result in incorrect schema written to the avro file , and result select fail on read. And also Even if user specified schema does not allow null , when data is written using Hcatalog , it will write a schema that will allow null. For example in the table , user specified , all CAPITAL letters in the schema , and record name as LINEITEM. The schema should be written as it is. Instead Hcatalog ignores it and generated its own avro schema from the hive table case.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-8 01:00:00" id="11206" opendate="2015-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join translation should update all ExprNode recursively</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-19 01:00:00" id="11312" opendate="2015-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC format: where clause with CHAR data type not returning any rows</summary>
      <description>Test case:Setup: create table orc_test( col1 string, col2 char(10)) stored as orc tblproperties ("orc.compress"="NONE");insert into orc_test values ('val1', '1');Query:select * from orc_test where col2='1'; Query returns no row.Problem is introduced with HIVE-10286, class RecordReaderImpl.java, method evaluatePredicateRange.Old code: Object baseObj = predicate.getLiteral(PredicateLeaf.FileFormat.ORC); Object minValue = getConvertedStatsObj(min, baseObj); Object maxValue = getConvertedStatsObj(max, baseObj); Object predObj = getBaseObjectForComparison(baseObj, minValue);New code:+ Object baseObj = predicate.getLiteral();+ Object minValue = getBaseObjectForComparison(predicate.getType(), min);+ Object maxValue = getBaseObjectForComparison(predicate.getType(), max);+ Object predObj = getBaseObjectForComparison(predicate.getType(), baseObj);The values for min and max are of type String which contain as many characters as the CHAR column indicated. For example if the type is CHAR(10), and the row has value 1, the value of String min is "1 ";Before Hive 1.2, the method getConvertedStatsObj would call StringUtils.stripEnd(statsObj.toString(), null); which would remove the trailing spaces from min and max. Later in the compareToRange method, it was able to compare "1" with "1".In Hive 1.2 with the use getBaseObjectForComparison method, it simply returns obj.String if the data type is String, which means minValue and maxValue are still "1 ".As a result, the compareToRange method will return a wrong value ("1".compareTo("1 ") -9 instead of 0.</description>
      <version>1.2.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.ppd.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.char.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-23 01:00:00" id="11354" opendate="2015-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL extending compatibility with Transact-SQL</summary>
      <description>Although HPL/SQL already supports some Transact-SQL language elements (declarations, flow-of-control stmts, assignments and so on) some other widely used constructs are not supported yet.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.declare.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.function.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Signal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionOra.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionMisc.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Converter.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-31 01:00:00" id="11429" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase default JDBC result set fetch size (# rows it fetches in one RPC call) to 1000 from 50</summary>
      <description>This is in addition to HIVE-10982 which plans to make the fetch size customizable. This just bumps the default to 1000.</description>
      <version>0.14.0,1.0.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-1 01:00:00" id="11432" opendate="2015-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive macro give same result for different arguments</summary>
      <description>If you use hive macro more than once while processing same row, hive returns same result for all invocations even if the argument are different. Example : &gt; CREATE TABLE macro_testing( a int, b int, c int)&gt; select * from macro_testing;1 2 34 5 67 8 910 11 12&gt; create temporary macro math_square(x int) x*x;&gt; select math_square(a), b, math_square(c) from macro_testing;9 2 936 5 3681 8 81144 11 144</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-6 01:00:00" id="11484" opendate="2015-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-9-20 01:00:00" id="11613" opendate="2015-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool should return non zero exit status for info command, if state is inconsistent</summary>
      <description>schematool -info just prints the version information, but it is not easy to consume the validity of the state from a tool as the exit code is 0 even if the schema version has mismatch.</description>
      <version>1.0.0,1.1.1,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-21 01:00:00" id="11614" opendate="2015-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): ctas after order by has problem</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForReturnPath.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-25 01:00:00" id="11641" opendate="2015-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: switch the branch to Tez master from branch</summary>
      <description>Current Tez master is pretty close to what 0.8 release is going to be.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-25 01:00:00" id="11645" opendate="2015-8-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add in-place updates for dynamic partitions loading</summary>
      <description>Currently, updates go to log file and on console there is no visible progress.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-26 01:00:00" id="11655" opendate="2015-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean build on the branch appears to be broken</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-27 01:00:00" id="11664" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make tez container logs work with new log4j2 changes</summary>
      <description>MiniTezCliDriver should log container logs to syslog file. With new log4j2 changes this file is not created anymore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-9-4 01:00:00" id="11743" opendate="2015-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Port conflict for MiniHBaseCluster</summary>
      <description>HMaster http port conflict. Presumably a bug in HBaseTestingUtility. It suppose not to use default port for everything.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.HBaseIntegrationTests.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-9 01:00:00" id="11766" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Remove MiniLlapCluster from shim layer after hadoop-1 removal</summary>
      <description>Remove HIVE-11732 changes after HIVE-11378 goes in.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">data.conf.llap.llap-daemon-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-10 01:00:00" id="11775" opendate="2015-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement limit push down through union all in CBO</summary>
      <description>Enlightened by HIVE-11684 (Kudos to jcamachorodriguez), we can actually push limit down through union all, which reduces the intermediate number of rows in union branches.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.join.transpose.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-11 01:00:00" id="11802" opendate="2015-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Float-point numbers are displayed with different precision in Beeline/JDBC</summary>
      <description>When inserting float-point numbers to a table, the values displayed on beeline or jdbc are with different precision.How to reproduce:0: jdbc:hive2://localhost:10000&gt; create table decimals (f float, af array&lt;float&gt;, d double, ad array&lt;double&gt;) stored as parquet;No rows affected (0.294 seconds)0: jdbc:hive2://localhost:10000&gt; insert into table decimals select 1.10058, array(cast(1.10058 as float)), 2.0133, array(2.0133) from dummy limit 1;...No rows affected (20.089 seconds)0: jdbc:hive2://localhost:10000&gt; select f, af, af[0], d, ad[0] from decimals;+---------------------+------------+---------------------+---------+---------+--+| f | af | _c2 | d | _c4 |+---------------------+------------+---------------------+---------+---------+--+| 1.1005799770355225 | [1.10058] | 1.1005799770355225 | 2.0133 | 2.0133 |+---------------------+------------+---------------------+---------+---------+--+When displaying arrays, the values are displayed correctly, but if I print a specific element, it is then displayed with more decimal positions.</description>
      <version>1.2.1</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.Column.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-14 01:00:00" id="11816" opendate="2015-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade groovy to 2.4.4</summary>
      <description>Groovy 2.4.4 is the latest release and the first done under ASF.Also there are some issues with old Groovy like CVE-2015-3253, which doesn't seem to affect Hive itself but might affect applications depending on Hive that get leaked classpath artifacts.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-15 01:00:00" id="11831" opendate="2015-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TXN tables in Oracle should be created with ROWDEPENDENCIES</summary>
      <description>These frequently-updated tables may otherwise suffer from spurious deadlocks.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-18 01:00:00" id="11881" opendate="2015-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Supporting HPL/SQL Packages</summary>
      <description>HPL/SQL should support packages similar to Oracle PL/SQL.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.resources.hplsql-site.xml</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Scope.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionOra.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Copy.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Cmp.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-10-22 01:00:00" id="11925" opendate="2015-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive file format checking breaks load from named pipes</summary>
      <description>Opening the file and mucking with it when hive.fileformat.check is true (the default) breaks the LOAD command from a named pipe. Right now, it's done for all the text files blindly to see if they might be in some other format. Files.getAttribute can be used to figure out if the input is a named pipe (or a socket) and skip the format check.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.VectorizedRCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SequenceFileInputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.InputFormatChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-3-24 01:00:00" id="1194" opendate="2010-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sorted merge join</summary>
      <description>If the input tables are sorted on the join key, and a mapjoin is being performed, it is useful to exploit the sorted properties of the table.This can lead to substantial cpu savings - this needs to work across bucketed map joins also.Since, sorted properties of a table are not enforced currently, a new parameter can be added to specify to use the sort-merge join.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-24 01:00:00" id="11950" opendate="2015-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat status file doesn&amp;#39;t show UTF8 character</summary>
      <description>If we do a select on a UTF8 table and store the console output into the status file (enablelog=true), the UTF8 character is garbled. The reason is we don't specify encoding when opening stdout/stderr in statusdir. This will cause problem especially on Windows, when the default OS encoding is not UTF8.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-10-29 01:00:00" id="11990" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loading data inpath from a temporary table dir fails on Windows</summary>
      <description>The query runs:load data inpath 'wasb:///tmp/testtemptable/temptablemisc_5/data' overwrite into table temp2;It fails with:FAILED: SemanticException [Error 10028]: Line 2:37 Path is not legal ''wasb:///tmp/testtemptable/temptablemisc_5/data'': Move from: wasb://humb23-hive1@humboldttesting3.blob.core.windows.net/tmp/testtemptable/temptablemisc_5/data to: hdfs://headnode0.humb23-hive1-ssh.h2.internal.cloudapp.net:8020/tmp/hive/hrt_qa/0d5f8b31-5908-44bf-ae4c-9eee956da066/_tmp_space.db/75b44252-42a7-4d28-baf8-4977daa5d49c is not valid. Please check that values for params "default.fs.name" and "hive.metastore.warehouse.dir" do not conflict.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-30 01:00:00" id="12002" opendate="2015-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>correct implementation typo</summary>
      <description>The term "implemenation" is seen in HiveMetaScore INFO logs. Correcting.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.AlternateFailurePreListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">hcatalog.src.packages.templates.conf.hive-site.xml.template</file>
      <file type="M">hcatalog.conf.proto-hive-site.xml</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-30 01:00:00" id="12003" opendate="2015-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Streaming API : Add check to ensure table is transactional</summary>
      <description>Check if TBLPROPERTIES ('transactional'='true') is set when opening connection</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.InvalidTable.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-2 01:00:00" id="12013" opendate="2015-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable most llap tests before merge</summary>
      <description>Tests cannot be parallelized before we merge, and tests cannot run fast enough (they did once, so I guess cannot always run fast enough) when they are not parallelized, and we need tests to pass before merging.LLAP is off by default, we did see tests pass recently (with the exception of a few out file diffs), and some tests will still be run, so it should be ok to proceed as follows.We will disable most of the LLAP q tests for now, merge, enable paralllelism and re-enable.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-5 01:00:00" id="12032" opendate="2015-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test for HIVE-9855</summary>
      <description/>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-5 01:00:00" id="12034" opendate="2015-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-4243 broke things for llap branch</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcFile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-6 01:00:00" id="12042" opendate="2015-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update some out files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-8 01:00:00" id="12073" opendate="2015-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable session reuse for MiniTez cluster</summary>
      <description>See HIVE-12072</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-10-22 01:00:00" id="12232" opendate="2015-10-22 00:00:00" resolution="Duplicate">
    <buginformation>
      <summary>Create external table failed when enabled StorageBasedAuthorization</summary>
      <description>Please look at the stacktrace, when enabled StorageBasedAuthorization, creating external table will failed with write permission about the default warehouse path "/user/hive/warehouse": &gt; CREATE EXTERNAL TABLE test(id int) LOCATION '/tmp/wangmeng/test' ;Error: Error while compiling statement: FAILED: HiveException java.security.AccessControlException: Permission denied: user=wangmeng, access=WRITE, inode="/user/hive/warehouse":hive:hive:drwxr-x--t.</description>
      <version>1.2.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insertoverwrite.bucket.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.insertoverwrite.bucket.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-22 01:00:00" id="12235" opendate="2015-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve beeline logging for dynamic service discovery</summary>
      <description>It maybe nice to see which host it tried to, and ended up, connecting to.</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">beeline.src.main.resources.beeline-log4j2.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-10-28 01:00:00" id="12280" opendate="2015-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveConnection does not try other HS2 after failure for service discovery</summary>
      <description>Found this while mocking some bad connection data in znode.. will try to add a test for this.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-11-3 01:00:00" id="12329" opendate="2015-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on limit pushdown optimization by default</summary>
      <description>Whenever applicable, this will always help, so this should be on by default.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.select.as.omitted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-9 01:00:00" id="12367" opendate="2015-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lock/unlock database should add current database to inputs and outputs of authz hook</summary>
      <description>Click to add description</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.lock.db.in.use.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.db.lock.conflict.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.query.tbl.in.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dbtxnmgr.nodbunlock.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dbtxnmgr.nodblock.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-12 01:00:00" id="12388" opendate="2015-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GetTables cannot get external tables when TABLE type argument is given</summary>
      <description>By regression of HIVE-7575, external tables are not shown when "TABLE" type is specified as argument. I'm working on this. Sorry.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.HiveTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTablesOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-16 01:00:00" id="12417" opendate="2015-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for exclamation mark missing in regexp</summary>
      <description>with HIVE-6013 gets support for regular expressions. However, die ! character is valid, too. It is needed for expressions like set hive.support.quoted.identifiers = none;select `^(?!donotuseme).*$` from table;which is the idiom to select all but column donotuseme .See http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html for a reference of supported chars in Java regexp.The patch simply fixes the lexer to support '!' as REGEX char. And does simply work.Please review. If you like to have an iTest for it, I beg you to help me. I tried several days on a different issue to figure out how it is supposed to work and failed miserably.</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-19 01:00:00" id="12469" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump Commons-Collections dependency from 3.2.1 to 3.2.2. to address vulnerability</summary>
      <description>Currently the commons-collections (3.2.1) library allows for invocation of arbitrary code through InvokerTransformer, need to bump the version of commons-collections from 3.2.1 to 3.2.2 to resolve this issue.Results of mvn dependency:tree:[INFO] ------------------------------------------------------------------------[INFO] Building Hive HPL/SQL 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-hplsql ---[INFO] org.apache.hive:hive-hplsql:jar:2.0.0-SNAPSHOT[INFO] +- com.google.guava:guava:jar:14.0.1:compile[INFO] +- commons-collections:commons-collections:jar:3.2.1:compile[INFO] ------------------------------------------------------------------------[INFO] Building Hive Packaging 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] +- org.apache.hive:hive-hbase-handler:jar:2.0.0-SNAPSHOT:compile[INFO] | +- org.apache.hbase:hbase-server:jar:1.1.1:compile[INFO] | | +- commons-collections:commons-collections:jar:3.2.1:compile[INFO] ------------------------------------------------------------------------[INFO] Building Hive Common 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-common ---[INFO] +- org.apache.hadoop:hadoop-common:jar:2.6.0:compile[INFO] | +- commons-collections:commons-collections:jar:3.2.1:compileHadoop-Common dependency also found in: LLAP, Serde, Storage, Shims, Shims Common, Shims Scheduler)[INFO] ------------------------------------------------------------------------[INFO] Building Hive Ant Utilities 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] [INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-ant ---[INFO] | +- commons-collections:commons-collections:jar:3.1:compile[INFO] [INFO] ------------------------------------------------------------------------[INFO] Building Hive Accumulo Handler 2.0.0-SNAPSHOT[INFO] ------------------------------------------------------------------------[INFO] +- org.apache.accumulo:accumulo-core:jar:1.6.0:compile[INFO] | +- commons-collections:commons-collections:jar:3.2.1:compile</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">ant.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-3-16 01:00:00" id="1247" opendate="2010-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hints cannot be passed to transform statements</summary>
      <description>Statements like:select /*+ MAPJOIN(a) */ transform(c1, c2) using '..' as ..does not work</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-19 01:00:00" id="12470" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow splits to provide custom consistent locations, instead of being tied to data locality</summary>
      <description>LLAP instances may not run on the same nodes as HDFS, or may run on a subset of the cluster.Using split locations based on FileSystem locality is not very useful in such cases - since that guarantees not getting any locality.Allow a split to map to a specific location - so that there's a chance of getting cache locality across different queries.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.ServiceInstanceSet.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapYarnRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapRegistryService.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-19 01:00:00" id="12471" opendate="2015-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HS2 web UI with SSL</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-20 01:00:00" id="12485" opendate="2015-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HS2 web UI with kerberos</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-26 01:00:00" id="12531" opendate="2015-11-26 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Implement fast-path for Year/Month UDFs for dates between 1999 and 2038</summary>
      <description>Current codepath goes into the JDK Calendar implementation, which is very slow for the simple cases in the current decade.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-1-14 01:00:00" id="12664" opendate="2015-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in reduce deduplication optimization causing ArrayOutOfBoundException</summary>
      <description>The optimisation check for reduce deduplication only checks the first child node for join and the check itself also contains a major bug causing ArrayOutOfBoundException no matter what.Sample data table form:timeuserhostpathreferercodeagentsizemethodintstringstringstringstringbigintstringbigintstringSample querySELECT t1.host, COUNT(DISTINCT t1.`date`) AS login_count, MAX(t2.code) AS code, unix_timestamp() AS timeFROM ( SELECT HOST, MIN(time) AS DATE FROM www_access WHERE HOST IS NOT NULL GROUP BY HOST ) t1JOIN ( SELECT HOST, MIN(time) AS code FROM www_access WHERE HOST IS NOT NULL GROUP BY HOST ) t2 ON t1.host = t2.hostGROUP BY t1.host</description>
      <version>1.1.1,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-17 01:00:00" id="12706" opendate="2015-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect output from from_utc_timestamp()/to_utc_timestamp when local timezone has DST</summary>
      <description>Getting wrong output with the local timezone set to PST (which has DST). I don't think this happens when the local timezone does not observe DST.select from_utc_timestamp('2015-03-28 17:00:00', 'Europe/London')2015-03-28 17:00:00select from_utc_timestamp('2015-03-28 18:00:00', 'Europe/London')2015-03-28 19:00:00 &lt;= Wrong, should be 2015-03-28 18:00:00select from_utc_timestamp('2015-03-28 19:00:00', 'Europe/London')2015-03-28 20:00:00 &lt;= Wrong, should be 2015-03-28 19:00:00Also to_utc_timestamp():select to_utc_timestamp('2015-03-28 17:00:00', 'Europe/London')2015-03-28 17:00:00select to_utc_timestamp('2015-03-28 18:00:00', 'Europe/London')2015-03-28 17:00:00 &lt;= Wrongselect to_utc_timestamp('2015-03-28 19:00:00', 'Europe/London')2015-03-28 18:00:00 &lt;= Wrongselect to_utc_timestamp('2015-03-28 20:00:00', 'Europe/London')2015-03-28 19:00:00 &lt;= Wrong</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-21 01:00:00" id="12719" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>As a hive user, I am facing issues using permanent UDAF&amp;#39;s.</summary>
      <description>Currently function name for the permanent function is getting formed as dbName.WINDOW_FUNC_PREFIX+functionName where as function registry has function name as WINDOW_FUNC_PREFIX+dbName.functionName. This is leading to invalid function error when we use permanent function with window function. Fix has been done such that we form the permanent function name rightly as WINDOW_FUNC_PREFIX+dbName+.+functionName so that it matches with the name in function registry. The functionality for built-in/temporary function would remain the same.</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-24 01:00:00" id="12742" opendate="2015-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL table comparison within CASE does not work as previous hive versions</summary>
      <description>drop table test_1; create table test_1 (id int, id2 int); insert into table test_1 values (123, NULL);SELECT cast(CASE WHEN id = id2 THEN FALSE ELSE TRUE END AS BOOLEAN) AS b FROM test_1; --NULLBut the output should be true (confirmed with postgres.)</description>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-1-30 01:00:00" id="12761" opendate="2015-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add stack trace servlet to HS2 web ui</summary>
      <description>To confirm the state of HS2, I add the servlet which prints stack trace.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.resources.hive-webapps.hiveserver2.hiveserver2.jsp</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-4 01:00:00" id="12772" opendate="2016-1-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline/JDBC output of decimal values is not 0-padded, does not match with CLI output</summary>
      <description>HIVE-12063 changed the output of decimal values to pad zeros to the column's full scale for Hive CLI.It looks like Beeline and JDBC still have the old behavior that strips trailing 0s.Beeline:+---------------+---------------+--+| c1 | c2 |+---------------+---------------+--+| 1.9999999999 | 1.9999999999 || 9.9999999999 | 9.9999999999 |+---------------+---------------+--+HiveCli:1.99999999990 1.99999999999.99999999990 9.9999999999</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TableSchema.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.RowBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnValue.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.ColumnBasedSet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Column.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">data.files.datatypes.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-6 01:00:00" id="12793" opendate="2016-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Address TestSparkCliDriver.testCliDriver_order2 failure due to HIVE-12782</summary>
      <description/>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-6 01:00:00" id="12794" opendate="2016-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cannot run queries against HBase due to missing HBase jars</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
      <file type="M">llap-server.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-6 01:00:00" id="12796" opendate="2016-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Switch to 32-bits containers for HMS upgrade tests</summary>
      <description>The Hive metastore upgrade tests create LXC containers for each of the databases server supported by HMS. These containers are default to Ubuntu 64-bits. The Oracle database libraries are correctly executed on 32-bits only. We should switch to 32-bits containers for all the database servers to allow tests being executed for Oracle as well.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-13 01:00:00" id="12863" opendate="2016-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix test failure for TestMiniTezCliDriver.testCliDriver_tez_union</summary>
      <description/>
      <version>1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-13 01:00:00" id="12867" opendate="2016-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semantic Exception Error Msg should be with in the range of "10000 to 19999"</summary>
      <description>At many places errors encountered during semantic exception is translated as generic error(GENERIC_ERROR, 40000) msg as opposed to semantic error msg.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-19 01:00:00" id="12888" opendate="2016-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSparkNegativeCliDriver does not run in Spark mode[Spark Branch]</summary>
      <description>During test, i found TestSparkNegativeCliDriver run in MR mode actually, it should be fixed.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-8 01:00:00" id="13020" opendate="2016-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK</summary>
      <description>HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.</description>
      <version>1.2.0,1.2.1,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.Utils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-13 01:00:00" id="13056" opendate="2016-2-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>delegation tokens do not work with HS2 when used with http transport and kerberos</summary>
      <description>We're getting a HiveSQLException on secure windows clusters.2016-02-08 13:48:09,535|beaver.machine|INFO|6114|140264674350912|MainThread|Job ID : 0000000-160208134528402-oozie-oozi-W2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Workflow Name : hive2-wf2016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|App Path : wasb://oozie1-hbs24@humbtestings5jp.blob.core.windows.net/user/hrt_qa/test_hiveserver22016-02-08 13:48:09,536|beaver.machine|INFO|6114|140264674350912|MainThread|Status : KILLED2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Run : 02016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|User : hrt_qa2016-02-08 13:48:09,537|beaver.machine|INFO|6114|140264674350912|MainThread|Group : -2016-02-08 13:48:09,547|beaver.machine|INFO|6114|140264674350912|MainThread|Created : 2016-02-08 13:47 GMT2016-02-08 13:48:09,548|beaver.machine|INFO|6114|140264674350912|MainThread|Started : 2016-02-08 13:47 GMT2016-02-08 13:48:09,552|beaver.machine|INFO|6114|140264674350912|MainThread|Last Modified : 2016-02-08 13:48 GMT2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|Ended : 2016-02-08 13:48 GMT2016-02-08 13:48:09,553|beaver.machine|INFO|6114|140264674350912|MainThread|CoordAction ID: -2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|2016-02-08 13:48:09,566|beaver.machine|INFO|6114|140264674350912|MainThread|Actions2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|ID Status Ext ID Ext Status Err Code2016-02-08 13:48:09,567|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,571|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@:start: OK - OK -2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@hive-node ERROR - ERROR HiveSQLException2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------2016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|0000000-160208134528402-oozie-oozi-W@fail OK - OK E07292016-02-08 13:48:09,572|beaver.machine|INFO|6114|140264674350912|MainThread|------------------------------------------------------------------------------------------------------------------------------------</description>
      <version>1.2.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-20 01:00:00" id="13108" opendate="2016-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Operators: SORT BY randomness is not safe with network partitions</summary>
      <description>SORT BY relies on a transient Random object, which is initialized once per deserialize operation.This results in complications during a network partition and when Tez/Spark reuses a cached plan.</description>
      <version>1.2.1,1.3.0,2.0.0,2.0.1</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-24 01:00:00" id="13144" opendate="2016-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HS2 can leak ZK ACL objects when curator retries to create the persistent ephemeral node</summary>
      <description>When the node gets deleted from ZK due to connection loss and curator tries to recreate the node, it might leak ZK ACL.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-3-18 01:00:00" id="13313" opendate="2016-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TABLESAMPLE ROWS feature broken for Vectorization</summary>
      <description>For vectorization, the ROWS clause is ignored causing many rows to be returned.SELECT * FROM source TABLESAMPLE(10 ROWS);</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-21 01:00:00" id="13320" opendate="2016-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
      <description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
      <version>1.2.1,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-28 01:00:00" id="13372" opendate="2016-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Macro overwritten when multiple macros are used in one column</summary>
      <description>When multiple macros are used in one column, results of the later ones are over written by that of the first.For example:Suppose we have created a table called macro_test with single column x in STRING type, and with data as:"a""bb""ccc"We also create three macros:CREATE TEMPORARY MACRO STRING_LEN(x string) length(x);CREATE TEMPORARY MACRO STRING_LEN_PLUS_ONE(x string) length(x)+1;CREATE TEMPORARY MACRO STRING_LEN_PLUS_TWO(x string) length(x)+2;When we ran the following query, SELECT CONCAT(STRING_LEN(x), ":", STRING_LEN_PLUS_ONE(x), ":", STRING_LEN_PLUS_TWO(x)) aFROM macro_testSORT BY a DESC;We get result:3:3:32:2:21:1:1instead of expected:3:4:52:3:41:2:3Currently we are using Hive 1.2.1, and have applied both HIVE-11432 and HIVE-12277 patches.</description>
      <version>1.2.1</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-30 01:00:00" id="13385" opendate="2016-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Cleanup] Streamline Beeline instantiation</summary>
      <description>Janitorial. Remove circular dependencies in BeelineCommandLineCompleter. Stream line code readability.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-30 01:00:00" id="13393" opendate="2016-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: Print help message for the --incremental option</summary>
      <description>beeline --help doesn't print the usage tips for the --incremental option.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-6 01:00:00" id="13433" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fixes for additional incompatible changes in tez-0.8.3</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskCommunicator.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-6 01:00:00" id="13439" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: provide a way to retrieve GUID to query Yarn ATS</summary>
      <description>HIVE-9673 added support for passing base64 encoded operation handles to ATS. We should a method on client side to retrieve that.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-6 01:00:00" id="13442" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: refactor submit API to be amenable to signing</summary>
      <description>This is going to be a wire compat breaking change.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-6 01:00:00" id="13443" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: signing for the second state of submit (the event)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.tez.dag.api.TaskSpecBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-6 01:00:00" id="13444" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add HMAC signatures to LLAP; verify them on LLAP side</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapTokenChecker.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestFirstInFirstOutComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenClientFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-7 01:00:00" id="13445" opendate="2016-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: token should encode application and cluster ids</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSecurityHelper.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenProvider.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-7 01:00:00" id="13446" opendate="2016-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: set default management protocol acls to deny all</summary>
      <description>The user needs to set the acls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-7 01:00:00" id="13448" opendate="2016-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: check ZK acls for ZKSM and fail if they are too permissive</summary>
      <description>Only the current user should have any access.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-7 01:00:00" id="13457" opendate="2016-4-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create HS2 REST API endpoints for monitoring information</summary>
      <description>Similar to what is exposed in HS2 webui in HIVE-12338, it would be nice if other UI's like admin tools or Hue can access and display this information as well. Hence, we will create some REST endpoints to expose this information.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-19 01:00:00" id="1351" opendate="2010-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tool to cat rcfiles</summary>
      <description>It will be useful for debugging</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-14 01:00:00" id="13520" opendate="2016-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t allow any test to run for longer than 60minutes in the ptest setup</summary>
      <description>Current timeout for batches is 2hours. This needs to be lowered. 1hour may be too much as well. We can start with this, and reduce timeouts further.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-14 01:00:00" id="13522" opendate="2016-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>regexp_extract.q hangs on master</summary>
      <description>Disable to unblock Hive QA runs.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-20 01:00:00" id="13561" opendate="2016-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
      <description>I can repo this on branch-1.2 and branch-2.0.It looks to be the same issues as: HIVE-11408The patch from HIVE-11408 looks to fix the issue as well.I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master</description>
      <version>1.2.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-29 01:00:00" id="13652" opendate="2016-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import table change order of dynamic partitions</summary>
      <description>Table with multiple dynamic partitions like year,month, day exported using "export table" command is imported (using "import table") such a way that order of partitions is changed to day, month, year.Export DB: Hive 0.14Import DB: Hive 1.2.1000.2.4.0.0-169Tables created as:create table T1( ... ) PARTITIONED BY (period_year string, period_month string, period_day string) STORED AS ORC TBLPROPERTIES ("orc.compress"="SNAPPY");export command:export table t1 to 'path'import command:import table t1 from 'path'HDFS file structure on both original table location and export path keeps the original partition order ../year/month/dayHDFS file structure after import is .../day/month/year</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-5-25 01:00:00" id="1366" opendate="2010-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>inputFileFormat error if the merge job takes a different input file format than the default output file format</summary>
      <description>If the input file format is say SequenceFileFormat and the default fileformat is RCFile. the merge job after the MR job assumes the input format is SequenceFile format rather than RCFile. This is probably introduced in HIVE-1357.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-30 01:00:00" id="13660" opendate="2016-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorizing IN expression with list of columns throws java.lang.ClassCastException ExprNodeColumnDesc cannot be cast to ExprNodeConstantDesc</summary>
      <description>Example:SELECT * FROM alltypesorc WHERE cint in (ctinyint, cbigint);</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-13 01:00:00" id="13756" opendate="2016-5-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map failure attempts to delete reducer _temporary directory on multi-query pig query</summary>
      <description>A pig script, executed with multi-query enabled, that reads the source data and writes it as-is into TABLE_A as well as performing a group-by operation on the data which is written into TABLE_B can produce erroneous results if any map fails. This results in a single MR job that writes the map output to a scratch directory relative to TABLE_A and the reducer output to a scratch directory relative to TABLE_B.If one or more maps fail it will delete the attempt data relative to TABLE_A, but it also deletes the _temporary directory relative to TABLE_B. This has the unintended side-effect of preventing subsequent maps from committing their data. This means that any maps which successfully completed before the first map failure will have its data committed as expected, other maps not, resulting in an incomplete result set.</description>
      <version>1.2.1,2.0.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-9-1 01:00:00" id="1378" opendate="2010-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return value for map, array, and struct needs to return a string</summary>
      <description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">data.scripts.input20.script</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-26 01:00:00" id="13866" opendate="2016-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>flatten callstack for directSQL errors</summary>
      <description>These errors look like final errors and confuse people. The callstack may be useful if it's some datanucleus/db issue, but it needs to be flattened and logged with a warning that this is not a final query error and that there's a fallback</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-6-8 01:00:00" id="1394" opendate="2010-6-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>do not update transient_lastDdlTime if the partition is modified by a housekeeping operation</summary>
      <description>Currently. purging looks at the hdfs time to see the last time the files got modified.It should look at the metastore property instead - these are facebook specific utilities, which do not require any changes to hive.However, in some cases, the operation might be performed by some housekeeping job, which should not modify the timestamp.Since, hive has no way of knowing the origin of the query, it might be a good idea to add a new hint which specifies that the operation is a cleanup operation, and the timestamp in the metastore need not be touched for that scenario.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-3 01:00:00" id="13941" opendate="2016-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve errors returned from SchemaTool</summary>
      <description>We've had feedback from Ambari folks on Schematool usage being opaque on errors.While, yes, the underlying error is present hidden in the stacktrace if you do a --verbose, that is often unwieldy and unusable. And without a --verbose, there is no indication of what actually went wrong.Thus, we need to fix this.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-6-12 01:00:00" id="1402" opendate="2010-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add parallel ORDER BY to Hive</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDAFPercentile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SkewJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.IndexWhereResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingInferenceOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.BucketingSortingCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-15 01:00:00" id="14027" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL values produced by left outer join do not behave as NULL</summary>
      <description>Consider the following setup:create table tbl (n bigint, t string); insert into tbl values (1, 'one'); insert into tbl values(2, 'two');select a.n, a.t, isnull(b.n), isnull(b.t) from (select * from tbl where n = 1) a left outer join (select * from tbl where 1 = 2) b on a.n = b.n;1 one false trueThe query should return true for isnull(b.n).I've tested by inserting a row with null value for the bigint column into tbl, and isnull returns true in that case.</description>
      <version>1.2.1,2.2.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-17 01:00:00" id="14052" opendate="2016-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup structures when external clients use LLAP</summary>
      <description>Per sseth: There's no cleanup at the moment, and structures used in LLAP to track a query will keep building up slowly over time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="14153" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: beeline history doesn&amp;#39;t work on Hive2</summary>
      <description>The up arrow on console is supposed to display history, which is broken currently. Changes in HIVE-6758 broke it.</description>
      <version>1.2.1,2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-4 01:00:00" id="14158" opendate="2016-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>deal with derived column names</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MaskAndFilterInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.8.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.masking.8.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-6 01:00:00" id="14175" opendate="2016-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix creating buckets without scheme information</summary>
      <description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-7 01:00:00" id="14188" opendate="2016-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: wrong user field is used from the token</summary>
      <description>realUser is not usually set in all cases for delegation tokens; we should use the owner.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTokenChecker.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-11 01:00:00" id="14207" opendate="2016-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Strip HiveConf hidden params in webui conf</summary>
      <description>HIVE-12338 introduced a new web ui, which has a page that displays the current HiveConf being used by HS2. However, before it displays that config, it does not strip entries from it which are considered "hidden" conf parameters, thus exposing those values from a web-ui for HS2. We need to add stripping to this.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-26 01:00:00" id="14348" opendate="2016-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for alter table exchange partition</summary>
      <description/>
      <version>1.2.1,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.exchgpartition2lel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exchange.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.exchange.partition.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-6-25 01:00:00" id="1435" opendate="2010-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgraded naming scheme causes JDO exceptions</summary>
      <description>We recently upgraded from Datanucleus 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. Because of this change, connecting to an existing database would throw exceptions such as:2010-06-24 17:59:09,854 ERROR exec.DDLTask (SessionState.java:printError(277)) - FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list'org.apache.hadoop.hive.ql.metadata.HiveException: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4ccd21c" using statement "INSERT INTO `SDS` (`SD_ID`,`NUM_BUCKETS`,`INPUT_FORMAT`,`OUTPUT_FORMAT`,`LOCATION`,`SERDE_ID`,`ISCOMPRESSED`) VALUES (?,?,?,?,?,?,?)" failed : Unknown column 'ISCOMPRESSED' in 'field list'NestedThrowables:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column 'ISCOMPRESSED' in 'field list' at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:325) at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:2012) at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:144) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:107) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:633) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:506) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:384) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:138) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:197) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:302) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:156)</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-4 01:00:00" id="14426" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extensive logging on info level in WebHCat</summary>
      <description>There is an extensive logging in WebHCat at info level, and even some sensitive information could be logged</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-4 01:00:00" id="14428" opendate="2016-8-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HadoopMetrics2Reporter leaks memory if the metrics sink is not configured correctly</summary>
      <description>About 80MB held after 7 hours of running. Metrics2Collector aggregates only when it's invoked by the Hadoop sink.Options - the first one is better IMO.1. Fix Metrics2Collector to aggregate more often, and fix the dependency in Hive accordingly2. Don't setup the metrics sub-system if a sink is not configured.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-29 01:00:00" id="1443" opendate="2010-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support to turn off bucketing with ALTER TABLE</summary>
      <description>Currently, there is an alter table command that can change the bucketing / sort columns, as well as the number of buckets (ALTER TABLE table_name CLUSTERED BY (col_name, col_name, ...) &amp;#91;SORTED BY (col_name, ...)&amp;#93; INTO num_buckets BUCKETS). However, this command does not provide a means to disable bucketing. This proposes to introduce a syntax likeALTER TABLE src NOT CLUSTERED;that would turn off bucketing.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-26 01:00:00" id="14651" opendate="2016-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a local cluster for Tez and LLAP</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">data.conf.llap.tez-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-29 01:00:00" id="14663" opendate="2016-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change ptest java language version to 1.7, other version changes and fixes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestParser.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-17 01:00:00" id="14786" opendate="2016-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline displays binary column data as string instead of byte array</summary>
      <description>In Beeline, doing a SELECT binaryColName FROM tableName; results in displays data as string type (which looks corrupted due to unprintable chars). Instead Beeline should display binary columns as byte array.</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestTableOutputFormat.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestIncrementalRowsWithNormalization.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBufferedRows.java</file>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Rows.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineOpts.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-19 01:00:00" id="14790" opendate="2016-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jenkins is not displaying test results because &amp;#39;set -e&amp;#39; is aborting the script too soon</summary>
      <description>NO PRECOMMIT TESTSJenkins is not displaying test results because 'set -e' is aborting the script too soon</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-19 01:00:00" id="14793" opendate="2016-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ptest branch to be specified, PROFILE override</summary>
      <description>Post HIVE-14734 - the profile is automatically determined. Add an option to override this via Jenkins. Also add an option to specify the branch from which ptest is built (This is hardcoded to github.com/apache/hive)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-28 01:00:00" id="14852" opendate="2016-9-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change qtest logging to not redirect all logs to console</summary>
      <description>A change was made recently to redirect all logs to console, to make IDE debugging of regular tests easier. That unfortunately makes qtest debugging tougher - since there's a lot of noise along with the diffs in the output file.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-29 01:00:00" id="14858" opendate="2016-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze command should support custom input formats</summary>
      <description>Currently analyze command with partialscan or noscan only applies to OrcInputFormat and MapredParquetInputFormat. However, if custom input formats extend these two they should also be able to use the same command to collect stats.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-14 01:00:00" id="14966" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Make cookie-auth work in HTTP mode</summary>
      <description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-10 01:00:00" id="15181" opendate="2016-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>buildQueryWithINClause didn&amp;#39;t properly handle multiples of ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_IN_CLAUSE</summary>
      <description>As there is a bug, we can still work around the issue by using the settings below (making sure the second setting is always at least 1000 times of the first setting):set hive.direct.sql.max.query.length=1;set hive.direct.sql.max.elements.in.clause=1000;</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-18 01:00:00" id="15247" opendate="2016-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass the purge option for drop table to storage handlers</summary>
      <description>This gives storage handler more control on how to handle drop table.</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-27 01:00:00" id="15517" opendate="2016-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOT (x &lt;=&gt; y) returns NULL if x or y is NULL</summary>
      <description>I created a table as following:create table test(x string, y string);insert into test values ('q', 'q'), ('q', 'w'), (NULL, 'q'), ('q', NULL), (NULL, NULL);Then I try to compare values taking NULLs into account:select *, x&lt;=&gt;y, not (x&lt;=&gt; y), (x &lt;=&gt; y) = false from test;OKq q true false falseq w false true trueq NULL false NULL trueNULL q false NULL trueNULL NULL true NULL falseI expected that 4th column will be the same as 5th one but actually got NULL as result of "not false" and "not true" expressions.Hive 1.2.1000.2.5.0.0-1245Subversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r da6c690d384d1666f5a5f450be5cbc54e2fe4bd6Compiled by jenkins on Fri Aug 26 01:39:52 UTC 2016From source with checksum c30648316a632f7a753f4359e5c8f4d6</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-28 01:00:00" id="15522" opendate="2016-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental ALTER_TABLE/ALTER_PTN including renames</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-1-24 01:00:00" id="15704" opendate="2017-1-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update the default logger for llap to query-routing</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.package.py</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-31 01:00:00" id="15765" opendate="2017-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support bracketed comments</summary>
      <description>C-style comments are in the SQL spec as well as supported by all major DBs. The are useful for inline annotation of the SQL. We should have them too.Example:select/*+ MAPJOIN(a) */ /* mapjoin hint */a /* column */from foo join bar;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin.negative3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketcontext.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SelectClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-4 01:00:00" id="15815" opendate="2017-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow to pass some Oozie properties to Spark in HoS</summary>
      <description>Oozie passes some of its properties (e.g. oozie.job.id) to Beeline/HS2 when it invokes Hive2 action. If we allow these properties to be passed to Spark in HoS, we can easily associate an Ooize workflow ID to an HoS client and Spark job in Spark history. It will be very helpful in diagnosing some issues involving Oozie Hive2/HoS/Spark.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-6 01:00:00" id="15817" opendate="2017-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix LlapDump classpath in llapdum.sh</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.llapdump.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-7 01:00:00" id="15834" opendate="2017-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit tests for org.json usage on master</summary>
      <description>Before switching implementation, we should add some tests that capture the current behavior.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ATSHook.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-4-15 01:00:00" id="16225" opendate="2017-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Memory leak in webhcat service (FileSystem CACHE entries)</summary>
      <description>This is a known beast. here are detailsThe problem seems to be similar to the one discussed in HIVE-13749. If we submit very large number of jobs like 1000 to 2000 then we can see increase in Configuration objects count.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StatusDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ListDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.DeleteDelegator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-28 01:00:00" id="16315" opendate="2017-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table doesn&amp;#39;t show num of partitions</summary>
      <description>This doesn't comply with our wiki: https://cwiki.apache.org/confluence/display/Hive/StatsDev#StatsDev-Examples</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.clusterby.sortby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.add.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.serde2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.avro.schema.evolution.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.table.like.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exim.hidden.files.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.mixed.partition.formats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2017-9-17 01:00:00" id="17107" opendate="2017-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Yetus to 0.5.0</summary>
      <description>Yetus 0.5.0 is released, and it contains our fixes.We should upgrade and remove our extra patched files.CC: kgyrtkirk</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.yetus-wrapper.sh</file>
      <file type="M">dev-support.maven.YETUS-506.sh</file>
      <file type="M">dev-support.findbugs.YETUS-471.sh</file>
      <file type="M">dev-support.checkstyle.YETUS-484.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-10-19 01:00:00" id="17845" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert fails if target table columns are not lowercase</summary>
      <description>eg., INSERT INTO TABLE EMP(ID,NAME) select * FROM SRC;FAILED: SemanticException 1:27 '&amp;#91;ID,NAME&amp;#93;' in insert schema specification are not found among regular columns of default.EMP nor dynamic partition columns.. Error encountered near token 'NAME'Whereas below insert is successful:INSERT INTO TABLE EMP(id,name) select * FROM SRC;</description>
      <version>1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-19 01:00:00" id="17847" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude net.hydromatic:aggdesigner-algorithm jar as compile and runtime dependency</summary>
      <description>Hive doesn't use this.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-12 01:00:00" id="1787" opendate="2010-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimize the code path when there are no outer joins</summary>
      <description>Currently, outer joins and joins are handled in the same manner - a special case for no outer joins would be useful</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-4 01:00:00" id="18207" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the test failure for TestCliDriver#vector_complex_join</summary>
      <description>The test result miss the info about bigTableKeyExpressions &amp; bigTableValueExpressions</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.join.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="18208" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB Join : Fix the unit tests to run SMB Joins.</summary>
      <description>Most of the SMB Join tests are actually not creating SMB Joins. Need them to test the intended join.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketsortoptimize.insert.7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.quotedid.smb.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.cache.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.quotedid.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketsortoptimize.insert.2.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-4 01:00:00" id="18210" opendate="2017-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create resource plan allows duplicates</summary>
      <description>Create resource plan allows duplicates. This was seen in a cluster:+----------+-----------+--------------------+| rp_name | status | query_parallelism |+----------+-----------+--------------------+| plan_2 | ACTIVE | 10 || plan_2 | DISABLED | NULL |+----------+-----------+--------------------+</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.046-HIVE-17566.mysql.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-1-8 01:00:00" id="18255" opendate="2017-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-client jar should be prefixed with hive-</summary>
      <description>Other Hive jars are prefixed with "hive-" except for the spark-client jar. Fixing this to make sure the jar name is consistent across all Hive jars.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-9 01:00:00" id="18257" opendate="2017-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement scheduling policy configuration instead of hardcoding fair scheduling</summary>
      <description>Not sure it makes sense to actually make it pluggable. At least the standard ones will be an enum; we don't expect people to implement custom classes - phase 2 if someone wants to</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">standalone-metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.resourceplan.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-12-9 01:00:00" id="18258" opendate="2017-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Reduce-Side GROUP BY MERGEPARTIAL with duplicate columns is broken</summary>
      <description>See Q file. Duplicate columns in key are not handled correctly.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-14 01:00:00" id="18273" opendate="2017-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LLAP-level counters for WM</summary>
      <description>On query fragment level (like IO counters)time queued as guaranteed;time running as guaranteed;time running as speculative.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.TezJobMonitor.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorTestHelpers.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-2-29 01:00:00" id="18347" opendate="2017-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow pluggable dynamic lookup of Hive Metastores from HiveServer2</summary>
      <description>In our organization, we have deployed HiveMetastore and HiveServer2 on Mesos as dynamic services for scalability and flexibility.In this architecture, we would like to allow HiveServer2 to dynamically load balance between Metastores (which may be scaled up and down or to different nodes) for different requests.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-17 01:00:00" id="184" opendate="2008-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tests fail due to including old hive jar files</summary>
      <description>Running "ant test" fails due to the old hive jar files from hadoopcore (which is inside the build directory by default) being included first in the classpath.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18400" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part2</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files srcbucket0 etc.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.scripts.q.test.init.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18406" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part8</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files empty1 etc</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join32.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18408" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part10</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files in1 etc and sortdp.txt</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.bucketing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.join.nulls.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.filters.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.opt.bucketing.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join.filters.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18409" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load data should rename files consistent with insert statements (bucketed tables only) Part11</summary>
      <description>As part of this effort, this JIRA tracks updating tests which use load data files which are of binary format. (dat, rc etc)This patch will fail the test run as the binary input files cant be patched, similar to HIVE-18403</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.exim.11.nonpart.noncompat.sorting.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.exim.11.nonpart.noncompat.sorting.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18414" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-9 01:00:00" id="18926" opendate="2018-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Imporve operator-tree matching</summary>
      <description>currently joins are not matched</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.stat.changes.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestReOptimization.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestOperatorCmp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.mapping.TestCounterMapping.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.optimizer.signature.TestOperatorSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReOptimizePlugin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.reexec.ReExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.StatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.SimpleRuntimeStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.RuntimeStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.PlanMapperProcess.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.PlanMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.GroupTransformer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapper.EmptyStatsSource.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LateralViewJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinCondDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CommonMergeJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.SignatureUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.Signature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.OpTreeSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.signature.OpSignature.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.IDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-3-13 01:00:00" id="18933" opendate="2018-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable ORC codec pool for now; remove clone</summary>
      <description>See ORC-310.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-6-26 01:00:00" id="19997" opendate="2018-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Batches for TestMiniDruidCliDriver</summary>
      <description>I have observed TestMiniDruidCliDriver takes a long time to execute. I verified that execution is not batched. We could batch tests as we do with TestHBaseCliDriver, i.e., 5 q files per batch, as there is only a small number of tests.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-6-26 01:00:00" id="19999" opendate="2018-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move precommit jobs to jdk 8</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.api.client.PTestClient.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-12-31 01:00:00" id="202" opendate="2008-12-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LINEAGE is not working for join quries</summary>
      <description>lineage is not giving input tables in case of join quires.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.tool.TestLineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.tools.LineageInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-17 01:00:00" id="20418" opendate="2018-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO may not handle ORC files that have row index disabled correctly for queries with no columns selected</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-10 01:00:00" id="20527" opendate="2018-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Intern table descriptors from spark task</summary>
      <description>Table descriptors from MR tasks and Tez tasks are interned. This fix is to intern table desc from spark tasks as well.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-29 01:00:00" id="20659" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-12-7 01:00:00" id="21018" opendate="2018-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Grouping/distinct on more than 64 columns should be possible</summary>
      <description>Earlier when more than 64 columns were in a group by or distinct an error was given</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-4-13 01:00:00" id="21613" opendate="2019-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries with join condition having timestamp or timestamp with local time zone literal throw SemanticException</summary>
      <description>Similar to HIVE-21540.</description>
      <version>None</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  
  
</bugrepository>