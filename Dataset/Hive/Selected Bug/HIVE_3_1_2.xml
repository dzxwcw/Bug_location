<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  
  
  
  
  <bug fixdate="2019-9-5 01:00:00" id="22170" opendate="2019-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-11 01:00:00" id="22195" opendate="2019-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure authentication type for Zookeeper when different from the default cluster wide</summary>
      <description>This could be useful in case when cluster is kerberized, but Zookeeper is not.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistryClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-16 01:00:00" id="2226" opendate="2011-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add API to retrieve table names by an arbitrary filter, e.g., by owner, retention, parameters, etc.</summary>
      <description>Create a function called get_table_names_by_filter that returns a list of table names in a database that match a certain filter. The filter should operate similar to the one HIVE-1609. Initially, you should be able to prune the table list based on owner, retention, or table parameter key/values. The filtering should take place at the JDO level for efficiency/speed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.Filter.g</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.constants.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Constants.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-27 01:00:00" id="22261" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for materialized view rewriting with window functions</summary>
      <description>Materialized views don't support window functions.  At a minimum, we should print a friendlier message when the rewrite fails (it can still be created with a "disable rewrite")Script is attached </description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query67.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.mv.query67.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-2-27 01:00:00" id="22263" opendate="2019-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV rewriting for distinct and count(distinct) not being triggered</summary>
      <description>Count distinct issues with materialized views.  Two scripts attached1) create materialized view base_aview stored as orc as select distinct c1 c1, c2 c2 from base;explain extended select count(distinct c1) from base group by c2 ;2)create materialized view base_aview stored as orc as SELECT c1 c1, c2 c2, sum(c2) FROM base group by 1,2;explain extended select count(distinct c1) from base group by c2;</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2019-10-3 01:00:00" id="22289" opendate="2019-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regenerate test output for tests broken due to commit race</summary>
      <description>HIVE-22042 got committed which changed the plans of a few tests (by enabling nonstrict partitioning mode by default) then HIVE-22269 got committed which fixes a bug with stats not being correctly calculated on some operators. Each patch got green runs individually but together causes test output differences.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-8 01:00:00" id="22305" opendate="2019-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the kudu-handler to the packaging module</summary>
      <description>The hive-kudu-handler needs to be added to the packaging module to ensure the jars are packaged into the tar distribution.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2019-10-15 01:00:00" id="22345" opendate="2019-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-21327 commit message is wrong</summary>
      <description>https://github.com/apache/hive/commit/a0ccbff838afb440461a4d6df335f824c1dccbccAccidentally used wrong commit message.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-15 01:00:00" id="22346" opendate="2019-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus is failing rat check</summary>
      <description>Lines that start with ????? in the ASF License report indicate files that do not have an Apache license header: !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/ad.example.com.ldif !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/example.com.ldif !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/microsoft.schema.ldif</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-18 01:00:00" id="22367" opendate="2019-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transaction type not retrieved from OpenTxnRequest</summary>
      <description>When opening a transaction, its type should be extracted from OpenTxnRequest object. Currently it's hardcoded with TxnType.DEFAULT.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-11-21 01:00:00" id="22374" opendate="2019-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress version to 1.19</summary>
      <description>As described in CVE-2019-12402, commons-compress:1.18 has an issue where certain inputs may cause an infinite loop which leads to a denial of service attack.This patch simply upgrades common-compress versions from 1.18 to 1.19 which is the latest minor version at the date of filing this issue (Maven repo).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-21 01:00:00" id="22378" opendate="2019-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove code duplicatoins from return path handling</summary>
      <description>Return path handling have some code duplications, they should be removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-10-23 01:00:00" id="22394" opendate="2019-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate Jars in druid classpath causing issues</summary>
      <description>hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. [hive@hiveserver2-1 lib]$ ls |grep druidcalcite-druid-1.19.0.7.0.2.0-163.jardruid-bloom-filter-0.15.1.7.0.2.0-163.jardruid-hdfs-storage-0.15.1.7.0.2.0-163.jarhive-druid-handler-3.1.2000.7.0.2.0-163.jarhive-druid-handler.jarException below - Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)  ... 22 moreCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)  ... 3 moreCaused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)  ... 6 moreCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-23 01:00:00" id="22395" opendate="2019-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to read Druid metastore password from jceks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2011-7-5 01:00:00" id="2257" opendate="2011-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TestHadoop20SAuthBridge</summary>
      <description>Looks like this test was accidentally disabled in HIVE-818.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-1-17 01:00:00" id="22652" opendate="2019-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey push through Group by with Grouping sets</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.topnkey.TopNKeyPushdownProcessor.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2020-1-9 01:00:00" id="22713" opendate="2020-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation shouldn&amp;#39;t be done for Join-Fil(*)-RS structure</summary>
      <description>Constant propagation shouldn't be done for Join-Fil-RS structure too. Since we output columns from the join if the structure is Join-Fil-RS, the expressions shouldn't be modified.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-13 01:00:00" id="22722" opendate="2020-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamptz_2 test failure</summary>
      <description>the min/max value seems to be off in some cases; this was highly non deterministic; and hard to reproduce - but in the recent QA runs it started failing more....</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.timestamptz.2.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-2-27 01:00:00" id="22780" opendate="2020-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade slf4j version to 1.7.30</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-7-12 01:00:00" id="2281" opendate="2011-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression introduced from HIVE-2155</summary>
      <description>EXPLAIN SELECT key, count(1) FROM src; throws a null pointer exception due to the operator stack not being checked prior to use for constructing the error message, due to the change introduced in HIVE-2155 to improve error message context tokens.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-1 01:00:00" id="22815" opendate="2020-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce the unnecessary file system object creation in MROutput</summary>
      <description>MROutput generates unnecessary file system object which may create long latency in Cloud environment. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-2-3 01:00:00" id="22818" opendate="2020-2-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preparation for jetty 9.4.26 upgrade</summary>
      <description>Make some code adjustment, before upgrading jetty to 9.4.26.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-18 01:00:00" id="22906" opendate="2020-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redundant checkLock Mutex blocks concurrent Lock requests</summary>
      <description>enqueueLocks is already serialising locks creation via (SELECT NL_NEXT FROM NEXT_LOCK_ID FOR UPDATE). Requested locks would be assigned 'W' state.checkLock is iterating over the sorted set of conflicting locks below current EXT_LOCK_ID. It does handle the situation when there is conflicting lock with lower ID in 'W' state - lock request would be denied and retried later.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-19 01:00:00" id="22908" opendate="2020-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AM caching connections to LLAP based on hostname and port does not work in kubernetes</summary>
      <description>AM is caching all connections to LLAP services using combination of hostname and port which does not work in kubernetes environment where hostname of pod and port can be same with statefulset. This causes AM to talk to old LLAP which could have died or OOM/Pod kill etc. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.AsyncPbRpcProxy.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-2-20 01:00:00" id="22914" opendate="2020-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive Connection ZK Interactions Easier to Troubleshoot</summary>
      <description>Add better logging and make errors more consistent and meaningful.Recently was trying to troubleshoot an issue where the ZK namespace of the client and the HS2 were different and it was way too difficult to diagnose.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-27 01:00:00" id="22937" opendate="2020-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP : Use unique names for the zip and tarball bundle for LLAP</summary>
      <description>LLAP : Use unique names for the zip and tarball bundle for LLAP</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-3-17 01:00:00" id="23035" opendate="2020-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-4-26 01:00:00" id="23079" opendate="2020-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-serde</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeSet.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-5-10 01:00:00" id="23178" opendate="2020-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description/>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-11 01:00:00" id="23181" opendate="2020-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-8-28 01:00:00" id="2319" opendate="2011-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-21 01:00:00" id="23267" opendate="2020-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on groovy</summary>
      <description>Transitively pulled where its unneeded.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-22 01:00:00" id="23268" opendate="2020-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate beanutils transitive dependency</summary>
      <description>Transitively retrieved from hadoop-commons</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-5-28 01:00:00" id="23314" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Kudu 1.12</summary>
      <description>we need KUDU-3044 because it could cause random failures...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-28 01:00:00" id="23315" opendate="2020-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove empty line from the end of SHOW EXTENDED TABLES and SHOW MATERIALIZED VIEWS</summary>
      <description>At the end of each SHOW EXTENDED TABLES; and SHOW MATERIALIZED VIEWS; command there is an empty line like this:+------------+----------------+|  tab_name  |   table_type   |+------------+----------------+| sample_07  | MANAGED_TABLE  || sample_08  | MANAGED_TABLE  || web_logs   | MANAGED_TABLE  ||            | NULL           |+------------+----------------+It should be removed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.show.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.show.materialized.views.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-19 01:00:00" id="23498" opendate="2020-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable HTTP Trace method on ThriftHttpCliService</summary>
      <description/>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-2-16 01:00:00" id="235" opendate="2009-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamicSerDe does not work with Thrift Protocols that can have missing fields for null values</summary>
      <description>The current DynamicSerDe code assumes all fields are there and no fields are missing.However Thrift Protocols can have missing fields, in case the field is null.In that case, DynamicSerDe may commit 2 behavior:1. array index out of bound error because DynamicSerDe assumes the number of fields in the record should be equal to that in the DDL;2. fields with null values will take the value from the last record. This may produce wrong result for queries.In order to fix this, we need to:1. Pass ObjectInspector/TypeInfo recursively so that we know the number of fields when deserializing the record.2. Clear out fields that are missing from the record.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-1 01:00:00" id="23585" opendate="2020-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retrieve replication instance metrics details</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-6-17 01:00:00" id="23706" opendate="2020-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix nulls first sorting behavior</summary>
      <description>INSERT INTO t(a) VALUES (1), (null), (3), (2), (2), (2);select a from t order by a desc;instead of 3, 2, 2, 2, 1, nullshould return null, 3, 2 ,2 ,2, 1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf3.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.decimal.1.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hypothetical.set.aggregates.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.percentile.cont.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.percentile.disc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.numeric.overflows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2011-12-28 01:00:00" id="2611" opendate="2011-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make index table output of create index command if index is table based</summary>
      <description>If an index is table based, when that index is created a table is created to contain that index. This should be listed in the output of the command.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.merge.negative.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.bitmap.no.map.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-12-2 01:00:00" id="2622" opendate="2011-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive POMs reference the wrong Hadoop artifacts</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
    </fixedFiles>
  </bug>
  
</bugrepository>