<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-3-19 01:00:00" id="10024" opendate="2015-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: q file test is broken again</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-26 01:00:00" id="10099" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for Decimal</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-2 01:00:00" id="101" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add various files and directories to svn:ignore</summary>
      <description>When creating patches or committing code it's nice to know that certain directories and files will never be included.I suggest we add the following to the svn:ignore variable (for more information see: http://svnbook.red-bean.com/en/1.1/ch07s02.html):build.classpath.project.settings</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-5-23 01:00:00" id="1010" opendate="2009-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement INFORMATION_SCHEMA in Hive</summary>
      <description>INFORMATION_SCHEMA is part of the SQL92 standard and would be useful to implement using our metastore.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.jdbc.handler.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.InputEstimatorTestClass.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UnparseTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DefaultStorageHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.ProjectionPusher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">jdbc-handler.src.test.java.org.apache.hive.config.JdbcStorageConfigManagerTest.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcStorageHandler.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcSerDe.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcRecordReader.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputFormat.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.MySqlDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.JdbcRecordIterator.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.dao.GenericJdbcDatabaseAccessor.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.JdbcStorageConfigManager.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.conf.DatabaseType.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestHiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-30 01:00:00" id="10140" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-1 01:00:00" id="10177" opendate="2015-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant folding for char &amp; varchar</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-4-13 01:00:00" id="10318" opendate="2015-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The HMS upgrade test does not test patches that affect the upgrade test scripts</summary>
      <description>When doing a change on the HMS upgrade scripts, such as adding a new DB server, then the HMS upgrade test does not test this change. In order to make it work, this patch needs to be committed to trunk.This is not desired, as we need to test the upgrade change first before doing the commit.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-8 01:00:00" id="1038" opendate="2010-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin dies if the join prunes all the columns</summary>
      <description>The query:select /*+ mapjoin(a) */ count(1) from src a join src b on a.key = b.keydies.It is a blocker for 0.5</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-1-11 01:00:00" id="1042" opendate="2010-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>function in a transform with more than 1 argument fails</summary>
      <description>select transform(substr(key, 1, 3)) USING '/bin/cat' FROM srcthrows an error:FAILED: Error in semantic analysis: AS clause has an invalid number of aliases</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-23 01:00:00" id="10451" opendate="2015-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTF deserializer fails if values are not used in reducer</summary>
      <description>In this particular case no values are needed from reducer to complete processing.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-1 01:00:00" id="10568" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-7 01:00:00" id="10639" opendate="2015-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create SHA1 UDF</summary>
      <description>Calculates an SHA-1 160-bit checksum for the string and binary, as described in RFC 3174 (Secure Hash Algorithm). The value is returned as a string of 40 hex digits, or NULL if the argument was NULL.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-12 01:00:00" id="10682" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-6-14 01:00:00" id="10705" opendate="2015-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update tests for HIVE-9302 after removing binaries</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveTestUtils.java</file>
      <file type="M">beeline.src.test.resources.postgresql-9.3.jdbc3.jar</file>
      <file type="M">beeline.src.test.resources.DummyDriver-1.0-SNAPSHOT.jar</file>
      <file type="M">beeline.src.test.org.apache.hive.beeline.TestBeelineArgParsing.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-6-26 01:00:00" id="10821" opendate="2015-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline-CLI: Implement CLI source command using Beeline functionality</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.test.org.apache.hive.beeline.cli.TestHiveCli.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-27 01:00:00" id="10835" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-6-2 01:00:00" id="10896" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-16 01:00:00" id="11027" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: Bucket map joins fail when hashcode goes negative</summary>
      <description>Seeing an issue when dynamic sort optimization is enabled while doing an insert into bucketed table. We seem to be flipping the negative sign on the hashcode instead of taking the complement of it for routing the data correctly. This results in correctness issues in bucket map joins in hive on tez when the hash code goes negative.</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="11029" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop.proxyuser.mapr.groups does not work to restrict the groups that can be impersonated</summary>
      <description>In the core-site.xml, the hadoop.proxyuser.&lt;user&gt;.groups specifies the user groups which can be impersonated by the HS2 &lt;user&gt;. However, this does not work properly in Hive. In my core-site.xml, I have the following configs:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.groups&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;I would expect with this configuration that 'mapr' can impersonate only members of the Unix group 'root'. However if I submit a query as user 'jon' the query is running as user 'jon' even though 'mapr' should not be able to impersonate this user.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-26 01:00:00" id="1103" opendate="2010-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add .gitignore file</summary>
      <description>Add a .gitignore file (equivalent to svn:ignore) for those using git-svn.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-19 01:00:00" id="11055" opendate="2015-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL - Implementing Procedural SQL in Hive (PL/HQL Contribution)</summary>
      <description>There is PL/HQL tool (www.plhql.org) that implements procedural SQL for Hive (actually any SQL-on-Hadoop implementation and any JDBC source).Alan Gates offered to contribute it to Hive under HPL/SQL name (org.apache.hive.hplsql package). This JIRA is to create a patch to contribute the PL/HQL code.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-9-26 01:00:00" id="11132" opendate="2015-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries using join and group by produce incorrect output when hive.auto.convert.join=false and hive.optimize.reducededuplication=true</summary>
      <description>Queries using join and group by produce multiple output rows with the same key when hive.auto.convert.join=false and hive.optimize.reducededuplication=true. This interaction between configuration parameters is unexpected and should be well documented at the very least and should likely be considered a bug.e.g. hive&gt; set hive.auto.convert.join = false;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 79XYZ 74XYZ 297XYZ 66hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 516</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-2 01:00:00" id="11172" opendate="2015-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization wrong results for aggregate query with where clause without group by</summary>
      <description>create table testvec(id int, dt int, greg_dt string) stored as orc;insert into table testvecvalues (1,20150330, '2015-03-30'),(2,20150301, '2015-03-01'),(3,20150502, '2015-05-02'),(4,20150401, '2015-04-01'),(5,20150313, '2015-03-13'),(6,20150314, '2015-03-14'),(7,20150404, '2015-04-04');hive&gt; select dt, greg_dt from testvec where id=5;OK20150313 2015-03-13Time taken: 4.435 seconds, Fetched: 1 row(s)hive&gt; set hive.vectorized.execution.enabled=true;hive&gt; set hive.map.aggr;hive.map.aggr=truehive&gt; select max(dt), max(greg_dt) from testvec where id=5;OK20150313 2015-03-30hive&gt; set hive.vectorized.execution.enabled=false;hive&gt; select max(dt), max(greg_dt) from testvec where id=5;OK20150313 2015-03-13</description>
      <version>0.14.0</version>
      <fixedVersion>1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-7-16 01:00:00" id="11273" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Register for finishable state change notifications when adding a task instead of when scheduling it</summary>
      <description>Registering when trying to execute is far too late. The task won't be considered for execution (queue may not be re-oredered) without the notification coming in.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-4 01:00:00" id="113" opendate="2008-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support of distribute by and sort by</summary>
      <description>Currently Hive supports "cluster by" which puts the columns into map-reduce key. Hadoop map-reduce actually allows different partitioning (we call distribute) keys and sorting keys.In Hive language, we will add "distribute by" for specifying the partition key, and "sort by" for specifying the sort key. "cluster by" will be a short-cut for "distribute by" and "sort by" with the same keys.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-20 01:00:00" id="11319" opendate="2015-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS with location qualifier overwrites directories</summary>
      <description>CTAS with location clause acts as an insert overwrite. This can cause problems when there sub directories with in a directory.This cause some users accidentally wipe out directories with very important data. We should ban CTAS with location to a non-empty directory. Reproduce:create table ctas1 location '/Users/ychen/tmp' as select * from jsmall limit 10;create table ctas2 location '/Users/ychen/tmp' as select * from jsmall limit 5;Both creates will succeed. But value in table ctas1 will be replaced by ctas2 accidentally.</description>
      <version>0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-5 01:00:00" id="1134" opendate="2010-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketing mapjoin where the big table contains more than 1 big partition</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.bucketmapjoin1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-28 01:00:00" id="11387" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : fix reduce_deduplicate optimization</summary>
      <description>The main problem is that, due to return path, now we may have (RS1-GBY2)&amp;#45;(RS3-GBY4) when map.aggr=false, i.e., no map aggr. However, in the non-return path, it will be treated as (RS1)-(GBY2-RS3-GBY4). The main problem is that it does not take into account of the setting.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-30 01:00:00" id="11409" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): add SEL before UNION</summary>
      <description>Two purpose: (1) to ensure that the data type of non-primary branch (the 1st branch is the primary branch) of union can be casted to that of the primary branch; (2) to make UnionProcessor optimizer work; (3) if the SEL is redundant, it will be removed by IdentidyProjectRemover optimizer.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-31 01:00:00" id="11429" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase default JDBC result set fetch size (# rows it fetches in one RPC call) to 1000 from 50</summary>
      <description>This is in addition to HIVE-10982 which plans to make the fetch size customizable. This just bumps the default to 1000.</description>
      <version>0.14.0,1.0.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-5 01:00:00" id="11461" opendate="2015-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transform flat AND/OR into IN struct clause</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.flatten.and.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.deep.filters.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-6 01:00:00" id="11484" opendate="2015-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-2-11 01:00:00" id="11526" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-26 01:00:00" id="11655" opendate="2015-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean build on the branch appears to be broken</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-27 01:00:00" id="11664" opendate="2015-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make tez container logs work with new log4j2 changes</summary>
      <description>MiniTezCliDriver should log container logs to syslog file. With new log4j2 changes this file is not created anymore.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-3 01:00:00" id="11724" opendate="2015-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHcat get jobs to order jobs on time order with latest at top</summary>
      <description>HIVE-5519 added pagination feature support to WebHcat. This implementation returns the jobs lexicographically resulting in older jobs showing at the top. Improvement is to order them on time with latest at top. Typically latest jobs (or running) ones are more relevant to the user. Time based ordering with pagination makes more sense.</description>
      <version>0.14.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-4 01:00:00" id="11729" opendate="2015-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update to use Tez 0.8</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-15 01:00:00" id="11825" opendate="2015-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_json_object(col,&amp;#39;$.a&amp;#39;) is null in where clause didn`t work</summary>
      <description>example:select attr from raw_kafka_item_dt0 where l_date='2015-09-06' and customer='Czgc_news' and get_json_object(attr,'$.title') is NULL limit 10;but in results,title is still not null!{"title":"思科Q4收入估$79.2亿 前景阴云笼罩","ItemType":"NewsBase","keywords":"思科Q4收入估\$79.2亿 前景阴云笼罩","random":"1420253511075","callback":"BCore.instances[2].callbacks[1]","user_agent":"Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_2_1 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML; like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5","is_newgid":"false","uuid":"DS.Input:b56c782bcb75035d:00002116:003dcd40:54a75947","ptime":"1.1549997E9"} attr is a dict</description>
      <version>0.14.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-29 01:00:00" id="11990" opendate="2015-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loading data inpath from a temporary table dir fails on Windows</summary>
      <description>The query runs:load data inpath 'wasb:///tmp/testtemptable/temptablemisc_5/data' overwrite into table temp2;It fails with:FAILED: SemanticException [Error 10028]: Line 2:37 Path is not legal ''wasb:///tmp/testtemptable/temptablemisc_5/data'': Move from: wasb://humb23-hive1@humboldttesting3.blob.core.windows.net/tmp/testtemptable/temptablemisc_5/data to: hdfs://headnode0.humb23-hive1-ssh.h2.internal.cloudapp.net:8020/tmp/hive/hrt_qa/0d5f8b31-5908-44bf-ae4c-9eee956da066/_tmp_space.db/75b44252-42a7-4d28-baf8-4977daa5d49c is not valid. Please check that values for params "default.fs.name" and "hive.metastore.warehouse.dir" do not conflict.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-7 01:00:00" id="12058" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive script to record errors when calling hbase fails</summary>
      <description>By default hive will try to find out which jars need to be added to the classpath in order to run MR jobs against an HBase cluster, however if hbase can't be found or if hbase mapredcp fails, the hive script will fail silently and ignore some of the jars to be included into the. That makes very difficult to analyze the real problem.Hive script should record the error not just simply redirect two hbase failures:HBASE_BIN=${HBASE_BIN:-"$(which hbase 2&gt;/dev/null)"}$HBASE_BIN mapredcp 2&gt;/dev/null</description>
      <version>0.14.0,1.1.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-7 01:00:00" id="12059" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up reference to deprecated constants in AvroSerdeUtils</summary>
      <description>AvroSerdeUtils contains several deprecated String constants that are used by other Hive modules. Those should be cleaned up.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerde.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.AvroHBaseValueFactory.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeParameters.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDeHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-7 01:00:00" id="12060" opendate="2015-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: create separate variable for llap tests</summary>
      <description>No real reason to just reuse tez one; also needed to parallelize the tests</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-10-23 01:00:00" id="12253" opendate="2015-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-12061</summary>
      <description>Breaks the build on HiveQA. I cannot repro these build errors.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.HBaseReadWrite.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.filemeta.OrcFileMetadataHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.FileMetadataHandler.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FileMetadataExprType.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-10-13 01:00:00" id="12408" opendate="2015-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLStdAuthorizer should not require external table creator to be owner of directory, in addition to rw permissions</summary>
      <description>When trying to create an external table via beeline in Hive using the SQLStdAuthorizer it expects the table creator to be the owner of the directory path and ignores the group rwx permission that is granted to the user.Error: Error while compiling statement: FAILED: HiveAccessControlException Permission denied: Principal [name=hari, type=USER] does not have following privileges for operation CREATETABLE [[INSERT, DELETE, OBJECT OWNERSHIP] on Object [type=DFS_URI, name=/etl/path/to/hdfs/dir]] (state=42000,code=40000)All it should be checking is read access to that directory.The directory owner requirement breaks the ability of more than one user to create external table definitions to a given location. For example this is a flume landing directory with json data, and the /etl tree is owned by the flume user. Even chowning the tree to another user would still break access to other users who are able to read the directory in hdfs but would still unable to create external tables on top of it.This looks like a remnant of the owner only access model in SQLStdAuth and is a separate issue to HIVE-11864 / HIVE-12324.</description>
      <version>0.14.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-16 01:00:00" id="12422" opendate="2015-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add security to Web UI endpoint</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-20 01:00:00" id="12485" opendate="2015-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure HS2 web UI with kerberos</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-1-21 01:00:00" id="12717" opendate="2015-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enabled to accept quoting of all character backslash qooting mechanism to json_tuple UDTF</summary>
      <description>Similar to HIVE-11825, we need to enable ALLOW_BACKSLASH_ESCAPING_ANY_CHARACTER property in json_tuple UDTFFor example in HIVE-11825, there are null return in below statement(https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-json_tuple)SELECT a.timestamp, b.* FROM log a LATERAL VIEW json_tuple(a.appevent, 'eventid', 'eventname') b as f1, f2;</description>
      <version>0.14.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-22 01:00:00" id="12907" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading - II</summary>
      <description>Remove unnecessary calls to metastore.</description>
      <version>0.14.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-22 01:00:00" id="12908" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading III</summary>
      <description>Remove unnecessary Namenode calls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-28 01:00:00" id="12954" opendate="2016-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE with str_to_map on null strings</summary>
      <description>Running str_to_map on a null string will return a NullPointerException.Workaround is to use coalesce.</description>
      <version>0.14.0,1.0.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-29 01:00:00" id="13648" opendate="2016-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC Schema Evolution doesn&amp;#39;t support same type conversion for VARCHAR, CHAR, or DECIMAL when maxLength or precision/scale is different</summary>
      <description>E.g. when a data file is copied in has a VARCHAR maxLength that doesn't match the DDL's maxLength. This error is produced:java.io.IOException: ORC does not support type conversion from file type varchar(145) (36) to reader type varchar(114) (36)</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">orc.src.java.org.apache.orc.impl.TreeReaderFactory.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.SchemaEvolution.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-5-24 01:00:00" id="1365" opendate="2010-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in SMBJoinOperator which may causes a final part of the results in some cases.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-18 01:00:00" id="13782" opendate="2016-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile async query asynchronously</summary>
      <description>Currently, when an async query is submitted to HS2, HS2 does the preparation synchronously. One of the preparation step is to compile the query, which may take some time. It will be helpful to provide an option to do the compilation asynchronously.</description>
      <version>None</version>
      <fixedVersion>2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-9 01:00:00" id="140" opendate="2008-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Event Based Infrastructure for Syntax Trees in the compiler</summary>
      <description>In order to refactor SemanticAnalyzer into a bunch of different components, we need to move more modular mechanisms of doing semantic analysis. One of the missing pieces here is the event based infrastructure for analyzing ASTs in the Semantic Analyzer. We will first use this infrastructure for lineage tools and then migrate the rest of the Semantic Analyzer to this infrastructure over time.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-12 01:00:00" id="14000" opendate="2016-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>(ORC) Changing a numeric type column of a partitioned table to lower type set values to something other than &amp;#39;NULL&amp;#39;</summary>
      <description>When an integer column is changed to a type that is smaller (e.g. bigint to int) and set hive.metastore.disallow.incompatible.col.type.changes=false, the data is clipped instead of being NULL.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.util.TimestampUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.vecrow.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.text.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.vec.mapwork.part.all.primitive.q</file>
      <file type="M">ql.src.test.queries.clientpositive.schema.evol.orc.nonvec.mapwork.part.all.primitive.q</file>
      <file type="M">orc.src.java.org.apache.orc.impl.ConvertTreeReaderFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-5 01:00:00" id="14896" opendate="2016-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stabilize golden files for currently failing tests</summary>
      <description>3 tests currently fail because of stats mismatch in golden files.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.hadoop20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.q</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.mapjoin.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-23 01:00:00" id="3405" opendate="2012-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2008-4-13 01:00:00" id="61" opendate="2008-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-15 01:00:00" id="6433" opendate="2014-2-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - allow grant/revoke roles if user has ADMIN OPTION</summary>
      <description>Follow up jira for HIVE-5952.If a user/role has admin option on a role, then user should be able to grant /revoke other users to/from the role.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.authorization.set.role.neg2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-3-6 01:00:00" id="6563" opendate="2014-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hdfs jar being pulled in when creating a hadoop-2 based hive tar ball</summary>
      <description>Looks like some dependency issue is causing hadoop-hdfs jar to be packaged in the hive tar ball.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-7 01:00:00" id="6587" opendate="2014-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow specifying additional Hive classpath for Hadoop</summary>
      <description>Allow users to add jars to hive's Hadoop classpath without explicitly modifying their Hadoop classpath</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-4-24 01:00:00" id="6732" opendate="2014-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Notes for Hive 0.13</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-25 01:00:00" id="6742" opendate="2014-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Outputs need to be started before accessing the writer</summary>
      <description>Accessing the writer on a Tez Output may not be safe until the Output has been started.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-25 01:00:00" id="6743" opendate="2014-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying the log level for Tez tasks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-26 01:00:00" id="6758" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline doesn&amp;#39;t work with -e option when started in background</summary>
      <description>In hive CLI you could easily integrate its use into a script and back ground the process like this: hive -e "some query" &amp;Beeline does not run when you do the same even with the -f switch.</description>
      <version>0.11.0,0.14.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-26 01:00:00" id="6760" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scalable dynamic partitioning should bail out properly for list bucketing</summary>
      <description>In case of list bucketing HIVE-6455 looks only at this config "hive.optimize.listbucketing" to bail out. There are cases when this config ("hive.optimize.listbucketing") is not set but list bucketing is enabled using SKEWED BY in CREATE TABLE statement.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-26 01:00:00" id="6761" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hashcode computation does not use maximum parallelism for scalable dynamic partitioning</summary>
      <description>Hashcode computation for HIVE-6455 should consider all the partitioning columns and bucket number to distribute the rows. The following code for (int i = 0; i &lt; partitionEval.length - 1; i++) {ignores the last partition column thereby generating lesser hashcodes.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-5-3 01:00:00" id="6828" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive tez bucket map join conversion interferes with map join conversion</summary>
      <description>The issue is that bucket count is used for checking the scaled down size of the hash tables but is used later on to convert to the map join as well which may be incorrect in cases where the entire hash table does not fit in the specified size.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-4-21 01:00:00" id="6944" opendate="2014-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e tests broken by HIVE-6432</summary>
      <description>HIVE-6432 removed templeton/v/queue REST endpoint and broke webhcat e2e testsNO PRECOMMIT TESTS</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-22 01:00:00" id="6947" opendate="2014-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>More fixes for tests on hadoop-2</summary>
      <description>Few more fixes for test cases on hadoop-2</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.16.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-22 01:00:00" id="6961" opendate="2014-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Drop partitions treats partition columns as strings</summary>
      <description>Discovered just now while testing HIVE-6945</description>
      <version>None</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.drop.partitions.filter2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.partitions.filter2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-28 01:00:00" id="698" opendate="2009-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create temporary function for GenericUDAF</summary>
      <description>We don't have that capability for GenericUDAF yet. We should add it.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-12-7 01:00:00" id="7032" opendate="2014-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Triple Negative in Error 10129</summary>
      <description>Error message 10129 currently reads:"Drop partitions for a non string partition columns is not allowed using non-equality"That has three negations and a singular/plural agreement error. Can it be changed to something like:"Drop partitions for a non-string partition column is only allowed using equality"</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-19 01:00:00" id="7084" opendate="2014-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestWebHCatE2e is failing on trunk</summary>
      <description>I am able to repro it consistently on fresh checkout.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-19 01:00:00" id="7085" opendate="2014-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestOrcHCatPigStorer.testWriteDecimal tests are failing on trunk</summary>
      <description>TestOrcHCatPigStorer.testWriteDecimal, TestOrcHCatPigStorer.testWriteDecimalX, TestOrcHCatPigStorer.testWriteDecimalXYare failing.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.HCatRecordSerDe.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-23 01:00:00" id="7118" opendate="2014-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Oracle upgrade schema scripts do not map Java long datatype columns correctly for transaction related tables</summary>
      <description>In Transaction related tables, Java long column fields are mapped to NUMBER(10) which results in failure to persist the transaction ids which are incompatible. Following error is seen:ORA-01438: value larger than specified precision allowed for this columnNO PRECOMMIT TESTS</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.oracle.hive-txn-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-29 01:00:00" id="7145" opendate="2014-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependence on apache commons-lang</summary>
      <description>We currently depend on both Apache commons-lang and commons-lang3. They are the same project, just at version 2.x vs 3.x. I propose that we move all of the references in Hive to commons-lang3 and remove the v2 usage.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-31 01:00:00" id="7158" opendate="2014-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Tez auto-parallelism in Hive</summary>
      <description>Tez can optionally sample data from a fraction of the tasks of a vertex and use that information to choose the number of downstream tasks for any given scatter gather edge.Hive estimates the count of reducers by looking at stats and estimates for each operator in the operator pipeline leading up to the reducer. However, if this estimate turns out to be too large, Tez can reign in the resources used to compute the reducer.It does so by combining partitions of the upstream vertex. It cannot, however, add reducers at this stage.I'm proposing to let users specify whether they want to use auto-parallelism or not. If they do there will be scaling factors to determine max and min reducers Tez can choose from. We will then partition by max reducers, letting Tez sample and reign in the count up until the specified min.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TezEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-1 01:00:00" id="7162" opendate="2014-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop-1 build broken by HIVE-7071</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-3 01:00:00" id="7173" opendate="2014-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support HIVE-4867 on mapjoin of MR Tasks</summary>
      <description>HIVE-4867 dedups columns in RS for reducer join and RS for order-by. But small aliases of mapjoin of MR tasks still contains key columns in value exprs.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.UnwrapRowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientnegative.sortmerge.mapjoin.mismatch.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-11-18 01:00:00" id="72" opendate="2008-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong results if partition pruning not strict and no mep-reduce job needed</summary>
      <description>Suppose T is a partitioned table on ds, where ds is a string column, the following queries: SELECT a.* FROM T a WHERE a.ds=2008-09-08 LIMIT 1; SELECT a.* FROM T a WHERE a.ds=2008-11-10 LIMIT 1;return the first row from the first partition.This is because of the typecast to double.for a.ds=2008-01-01 or anything (a.ds=1), evaluate (Double, Double) is invoked at partition pruning.Since '2008-11-01' is not a valid double, it is converted to a null, and therefore the result of pruning returns null (unknown) - not FALSE.All unknowns are also accepted, therefore all partitions are accepted which explains this behavior.filter is not invoked since it is a select * query, so map-reduce job is started.We just turn off this optimization if pruning indicates that there can be unknown partitions.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2014-6-20 01:00:00" id="7265" opendate="2014-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BINARY columns use BytesWritable::getBytes() without ::getLength()</summary>
      <description>The Text conversion for BINARY columns does case BINARY: t.set(((BinaryObjectInspector) inputOI).getPrimitiveWritableObject(input).getBytes()); return t;This omission was noticed while investigating a different String related bug, in a list of functions which call getBytes() without calling getSize/getLength().</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-25 01:00:00" id="7289" opendate="2014-6-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-6469</summary>
      <description>this task is to revert HIVE-6469 (see discussion on HIVE-7100)</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreFsImpl.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-6 01:00:00" id="729" opendate="2009-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>select expression with optional "as"</summary>
      <description>"as" should be optional in select expression. Query like the following should work:SELECT a, b FROM ( SELECT key a, value b FROM src) src1ORDER BY a LIMIT 1;</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.invalid.select.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-28 01:00:00" id="7313" opendate="2014-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow in-memory/ssd session-level temp-tables</summary>
      <description>With HDFS storage policies implementation, temporary tables can be written with different storage/reliability policies. In-session temporary tables can be targetted at both SSD and memory storage policies, with fallbacks onto the disk and the associated reliability trade-offs.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-7 01:00:00" id="7356" opendate="2014-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Table level stats collection fail for partitioned tables</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.columnstats.partlvl.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-7-8 01:00:00" id="7364" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk cannot be built on -Phadoop1 after HIVE-7144</summary>
      <description>Text.copyBytes() is introduced in hadoop-2</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-5-9 01:00:00" id="7375" opendate="2014-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option in test infra to compile in other profiles (like hadoop-1)</summary>
      <description>As we are seeing some commits breaking hadoop-1 compilation due to lack of pre-commit converage, it might be nice to add an option in the test infra to compile on optional profiles as a pre-step before testing on the main profile.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.java</file>
      <file type="M">testutils.ptest2.src.main.resources.source-prep.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-12 01:00:00" id="7395" opendate="2014-7-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Work around non availability of stats for partition columns</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-17 01:00:00" id="7432" opendate="2014-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove deprecated Avro&amp;#39;s Schema.parse usages</summary>
      <description>Schema.parse has been deprecated by Avro, however it is being used at multiple places in Hive.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestThatEvolvedSchemasActAsWeWant.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestSchemaReEncoder.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestGenericAvroRecordWritable.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerdeUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerde.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroObjectInspectorGenerator.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.SchemaResolutionProblem.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerdeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroGenericRecordWritable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.avro.AvroGenericRecordReader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-22 01:00:00" id="7477" opendate="2014-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hive to use tez 0.4.1</summary>
      <description>Tez has released 0.4.1 that has bug fixes we need.</description>
      <version>0.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-22 01:00:00" id="7478" opendate="2014-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: PPD push conditions to Join Operators</summary>
      <description>This was turned off in HivePushFilterPastJoin because non equality conditions were being pushed. Fix this so that equality predicates are pushed as join conditions.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-12 01:00:00" id="748" opendate="2009-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>test for extracting urls</summary>
      <description>This test is useful because it will help in progress of https://issues.apache.org/jira/browse/HIVE-396</description>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-23 01:00:00" id="7490" opendate="2014-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert ORC stripe size</summary>
      <description>HIVE-6037 reverted the changes to ORC stripe size introduced by HIVE-7231.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-7-24 01:00:00" id="7510" opendate="2014-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Add Greedy Algorithm(LucidDB) For Join Order</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-27 01:00:00" id="7528" opendate="2014-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support cluster by and distributed by [Spark Branch]</summary>
      <description>clustered by = distributed by + sort by, so this is related to HIVE-7527. If sort by is in place, the assumption is that we don't need to do anything about distributed by or clustered by. Still, we need to confirm and verify.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkEdgeProperty.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-28 01:00:00" id="7535" opendate="2014-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make use of number of nulls column statistics in filter rule</summary>
      <description>The filter rule does not make use of number of nulls column statistics for "IS NULL" and "IS NOT NULL" expression evaluation.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-28 01:00:00" id="7536" opendate="2014-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make use of decimal column statistics in statistics annotation</summary>
      <description>HIVE-6701 added decimal column statistics. The statistics annotation optimizer should make use of decimal column statistics as well.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-29 01:00:00" id="7538" opendate="2014-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix eclipse:eclipse after HIVE-7496</summary>
      <description>HIVE-7496 removes "hive-default.xml.template" from the source control. However, hive-common still refers to this file in the build-resources. Thus trying to build eclipse projects on fresh checkout of branch will get "hive-common missing required source-folder: ../conf/"NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-29 01:00:00" id="7544" opendate="2014-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes related to TEZ-1288 (FastTezSerialization)</summary>
      <description>Add ability to make use of TezBytesWritableSerialization.NO PRECOMMIT TESTS</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-9-14 01:00:00" id="755" opendate="2009-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Driver NullPointerException when calling getResults without first compiling</summary>
      <description>The Hive Driver will throw a NullPointerException when getResults() is called but the context has not yet been set by a call to compile().</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-30 01:00:00" id="7550" opendate="2014-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend cached evaluation to multiple expressions</summary>
      <description>Currently, hive.cache.expr.evaluation caches per expression. But cache context might be shared for multiple expressions.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-1 01:00:00" id="7592" opendate="2014-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>List Jars or Files are not supported by Beeline</summary>
      <description>Through adding jars or files are supported by Beeline, List jars or Files are still not supported.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-5 01:00:00" id="7621" opendate="2014-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for cte with cbo</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-7 01:00:00" id="7652" opendate="2014-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check OutputCollector after closing ExecMapper/ExecReducer</summary>
      <description>Some operators such as Group By are adding output records to OutputCollector after closing the ExecMapper/ExecReducer. Need to check if lastRecordOutput has any records after closing the ExecMapper/ExecReducer.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-8-13 01:00:00" id="7702" opendate="2014-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Start running .q file tests on spark [Spark Branch]</summary>
      <description>Spark can currently only support a few queries, however there are some .q file tests which will pass today. The basic idea is that we should get some number of these actually working (10-20) so we can actually start testing the project.A good starting point might be the udf*, varchar*, or alter* tests:https://github.com/apache/hive/tree/spark/ql/src/test/queries/clientpositiveTo generate the output file for test XXX.q, you'd do:mvn clean install -DskipTests -Phadoop-2cd itestsmvn clean install -DskipTests -Phadoop-2cd qtest-sparkmvn test -Dtest= TestSparkCliDriver -Dqfile=XXX.q -Dtest.output.overwrite=true -Phadoop-2which would generate XXX.q.out which we can check-in to source control as a "golden file".Multiple tests can be run at a give time as so:mvn test -Dtest= TestSparkCliDriver -Dqfile=X1.q,X2.q -Dtest.output.overwrite=true -Phadoop-2</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2014-8-14 01:00:00" id="7736" opendate="2014-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve the columns stats update speed for all the partitions of a table</summary>
      <description>The current implementation of columns stats update for all the partitions of a table takes a long time when there are thousands of partitions. For example, on a given cluster, it took 600+ seconds to update all the partitions' columns stats for a table with 2 columns but 2000 partitions.ANALYZE TABLE src_stat_part partition (partitionId) COMPUTE STATISTICS for columns;We would like to improve the columns stats update speed for all the partitions of a table</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RequestPartsSpec.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsStatsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.DropPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsResult.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddPartitionsRequest.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="776" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make div as infix operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.divider.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.divider.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-18 01:00:00" id="7767" opendate="2014-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.optimize.union.remove does not work properly [Spark Branch]</summary>
      <description>Turing on the hive.optimize.union.remove property generates wrong union all result. For Example:create table inputTbl1(key string, val string) stored as textfile;load data local inpath '../../data/files/T1.txt' into table inputTbl1;SELECT *FROM ( SELECT key, count(1) as values from inputTbl1 group by key UNION ALL SELECT key, count(1) as values from inputTbl1 group by key) a; when the hive.optimize.union.remove is turned on, the query result is like: 1 12 13 17 18 2when the hive.optimize.union.remove is turned off, the query result is like: 7 12 18 23 11 17 12 18 23 11 1The expected query result is:7 12 18 23 11 17 12 18 23 11 1</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GraphTran.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="777" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE with STRUCT type</summary>
      <description>Currently we only support map/array in type definition.We should support STRUCT as well.CREATE TABLE abc ( pageid INT, ads STRUCT&lt;adid:INT,location:STRING&gt;, userid INT);</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-19 01:00:00" id="7772" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add tests for order/sort/distribute/cluster by query [Spark Branch]</summary>
      <description>Now that these queries are supported, we should have tests to catch any problems we may have.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-19 01:00:00" id="7775" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable sample8.q.[Spark Branch]</summary>
      <description>sample8.q contain join query, should enable this qtest after hive on spark support join operation.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-19 01:00:00" id="7777" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add CSV Serde based on OpenCSV</summary>
      <description>There is no official support for csvSerde for hive while there is an open source project in github(https://github.com/ogrodnek/csv-serde). CSV is of high frequency in use as a data format.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-19 01:00:00" id="7784" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Created the needed indexes on Hive.PART_COL_STATS for CBO</summary>
      <description>With CBO we need the correct set of indexes to provide an efficient Read/Write access.These indexes improve performance of Explain plan and Analyzed table by 60% and 300%.MySQL CREATE INDEX PART_COL_STATS_N50 ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME) USING BTREE;MsSQLCREATE INDEX PART_COL_STATS_N50 ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME);Oracle CREATE INDEX PART_COL_STATS_N50 ON PART_COL_STATS (DB_NAME,TABLE_NAME,COLUMN_NAME);PostgresCREATE INDEX "PART_COL_STATS_N50" ON "PART_COL_STATS" USING btree ("DB_NAME","TABLE_NAME","COLUMN_NAME");</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.13.0-to-0.14.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.14.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.13.0-to-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.13.0-to-0.14.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.14.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.13.0-to-0.14.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.14.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-19 01:00:00" id="7785" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Projection Pruning needs to handle cross Joins</summary>
      <description>Projection pruning needs to handle cross joins. Ex: select r1.x from r1 join r2.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-20 01:00:00" id="7791" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (1) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. alter_merge_orc.q,\ alter_merge_stats_orc.q,\ auto_join0.q,\ auto_join1.q,\ bucket2.q,\ bucket3.q,\ bucket4.q,\ count.q,\ create_merge_compressed.q,\ cross_join.q,\ cross_product_check_1.q,\ cross_product_check_2.q,\ ctas.q,\custom_input_output_format.q,\ disable_merge_for_bucketing.q,\ dynpart_sort_opt_vectorization.q,\ dynpart_sort_optimization.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7792" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (2) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on.limit_pushdown.q,\ load_dyn_part1.q,\ load_dyn_part2.q,\ load_dyn_part3.q,\ mapjoin_mapjoin.q,\ mapreduce1.q,\ mapreduce2.q,\ merge1.q,\ merge2.q,\ metadata_only_queries.q,\ optimize_nullscan.q,\ orc_analyze.q,\ orc_merge1.q,\ orc_merge2.q,\ orc_merge3.q,\ orc_merge4.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7793" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (3) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. ptf.q,\ sample1.q,\ script_env_var1.q,\ script_env_var2.q,\ script_pipe.q,\ scriptfile1.q,\ stats_counter.q,\ stats_counter_partitioned.q,\ stats_noscan_1.q,\ subquery_exists.q,\ subquery_in.q,\ temp_table.q,\ transform1.q,\ transform2.q,\ transform_ppr1.q,\ transform_ppr2.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-20 01:00:00" id="7794" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (4) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. vector_cast_constant.q,\ vector_data_types.q,\ vector_decimal_aggregate.q,\ vector_left_outer_join.q,\ vector_string_concat.q,\ vectorization_12.q,\ vectorization_13.q,\ vectorization_14.q,\ vectorization_15.q,\ vectorization_9.q,\ vectorization_part_project.q,\ vectorization_short_regress.q,\ vectorized_mapjoin.q,\ vectorized_nested_mapjoin.q,\ vectorized_ptf.q,\ vectorized_shufflejoin.q,\ vectorized_timestamp_funcs.q</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="7795" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ptf.q and ptf_streaming.q.[Spark Branch]</summary>
      <description>ptf.q and ptf_streaming.q contains join queries, we should enable these qtests in milestone2.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-20 01:00:00" id="7814" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Predicate Push Down Enhancements</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-21 01:00:00" id="782" opendate="2009-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce progress reported incorrectly</summary>
      <description>its reporting map progress as reduce progress as well.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-21 01:00:00" id="7821" opendate="2014-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>StarterProject: enable groupby4.q [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2014-8-23 01:00:00" id="7866" opendate="2014-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge from trunk (1) [Spark Branch]</summary>
      <description>I will be creating JIRA's for the merge going forward to run tests beforehand.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-23 01:00:00" id="7867" opendate="2014-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-order spark.query.files in sorted order [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-26 01:00:00" id="7880" opendate="2014-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support subquery [Spark Branch]</summary>
      <description>While try to enable SubQuery qtests, I found that SubQuery cases return null value currently, we should enable subquery for Hive on Spark. We should enable subquery_exists.q and subquery_in.q in this task as Tez does.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-26 01:00:00" id="7881" opendate="2014-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable Qtest scriptfile1.q [Spark Branch]</summary>
      <description>scriptfile1.q failed due to script file not found, should verify whether add script file to SparkContext.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-8-29 01:00:00" id="7912" opendate="2014-8-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t add is not null filter for partitioning column</summary>
      <description>HIVE-7159 introduces optimization which introduces is not null filter on inner join columns which is wasteful for partitioning column.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-31 01:00:00" id="7925" opendate="2014-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend current partition status extrapolation to support all DBs</summary>
      <description>extend current partition status extrapolation only supports Derby.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-1 01:00:00" id="7927" opendate="2014-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checking sticky bit needs shim</summary>
      <description>Hive cannot be built on hadoop-1 after HIVE-7895, which checks sticky bit in FsPermission.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-2 01:00:00" id="7947" opendate="2014-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add message at the end of each testcase with timestamp in Webhcat system tests</summary>
      <description>Currently, Webhcat e2e testsuite only prints message while starting test run:Beginning test &lt;testcase&gt; at 1406716992It should also print ending message with timestamp similar to this:Ending test &lt;testcase&gt; at 1406717992This change will make log collection easy for failed test cases. NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-3 01:00:00" id="7948" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an E2E test to verify fix for HIVE-7155</summary>
      <description>E2E Test to verify webhcat property templeton.mapper.memory.mb correctly overrides mapreduce.map.memory.mb. The feature was added as part of HIVE-7155.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.env.sh</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.deploy.e2e.artifacts.sh</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-3 01:00:00" id="7971" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support alter table change/replace/add columns for existing partitions</summary>
      <description>ALTER TABLE CHANGE COLUMN is allowed for tables, but not for partitions. Same for add/replace columns.Allowing this for partitions can be useful in some cases. For example, one user has tables with Hive 0.12 Decimal columns, which do not specify precision/scale. To be able to properly read the decimal values from the existing partitions, the column types in the partitions need to be changed to decimal types with precision/scale.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-4 01:00:00" id="7977" opendate="2014-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid creating serde for partitions if possible in FetchTask</summary>
      <description>Currently, FetchTask creates SerDe instance thrice for each partition, which can be avoided if it's same with table SerDe.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedUnionObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedStructObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.DelegatedListObjectInspector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FetchWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">data.files.datatypes.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-4 01:00:00" id="7987" opendate="2014-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Storage based authorization - NPE for drop view</summary>
      <description>When storage based authorization is enabled, NullPointerException is thrown for 'drop view'.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationDrops.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-4 01:00:00" id="7988" opendate="2014-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Test changes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="7994" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BMJ test fails on tez</summary>
      <description>Problem is that mapjoin hangs on to the wrong hash table.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="8003" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Handle Literal casting, Restrict CBO to select queries, Translate Strings, Optiq Log</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUtcTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-8 01:00:00" id="8023" opendate="2014-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Code in HIVE-6380 eats exceptions</summary>
      <description>This code eats the stack trace LOG.error("Unable to load resources for " + dbName + "." + fName + ":" + e);</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-9 01:00:00" id="8025" opendate="2014-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle null-safe joins</summary>
      <description>We will not optimize query containing NS join expression because our join reordering algo cant optimize such joins.</description>
      <version>None</version>
      <fixedVersion>cbo-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-9 01:00:00" id="8026" opendate="2014-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: check for Grouping Sets, Cube and Rollup and bail to non cbo planning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-9-11 01:00:00" id="8046" opendate="2014-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: fix issues with Windowing queries</summary>
      <description>handle count handle lead/lag</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-11 01:00:00" id="8047" opendate="2014-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lazy char/varchar are not using escape char defined in serde params</summary>
      <description>A table with char/varchar columns that specifies an escape character for string data still has the escape characters in the char/varchar columns.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyPrimitiveObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyHiveCharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyString.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveVarchar.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveChar.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-9-11 01:00:00" id="8064" opendate="2014-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: decimal support is broken for some corner cases</summary>
      <description/>
      <version>None</version>
      <fixedVersion>cbo-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-12 01:00:00" id="8072" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TesParse_union is failing on trunk</summary>
      <description>Needs golden file update</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-14 01:00:00" id="8094" opendate="2014-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LIKE keyword support for SHOW FUNCTIONS</summary>
      <description>It would be nice to add LIKE keyword support for SHOW FUNCTIONS as below, and keep the patterns consistent to the way as SHOW DATABASES, SHOW TABLES.SHOW FUNCTIONS LIKE 'foo*';</description>
      <version>0.13.1,0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.show.functions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8097" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Reduce-Side [SMB] MapJoin operator fails</summary>
      <description>Fails attempting to getScratchColumnVectorTypes since mapWork is null on reduce-side.Fix by calling that method using reduceWork object.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-15 01:00:00" id="8106" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization for spark [spark branch]</summary>
      <description>Enable the vectorization optimization on spark</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8108" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle queries without any source tables</summary>
      <description>Disable CBO on such queries since there is nothing much to optimize for those.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-9-2 01:00:00" id="811" opendate="2009-9-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>javadoc broken</summary>
      <description>javadoc is broken</description>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryPrimitive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFPosMod.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.mapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.NonSyncDataInputBuffer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinObjectValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinObjectKey.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8110" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Improve casting of constants</summary>
      <description>Make it more type-specific</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8111" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO trunk merge: duplicated casts for arithmetic expressions in Hive and CBO</summary>
      <description>Original test failure: looks like column type changes to different decimals in most cases. In one case it causes the integer part to be too big to fit, so the result becomes null it seems.What happens is that CBO adds casts to arithmetic expressions to make them type compatible; these casts become part of new AST, and then Hive adds casts on top of these casts. This (the first part) also causes lots of out file changes. It's not clear how to best fix it so far, in addition to incorrect decimal width and sometimes nulls when width is larger than allowed in Hive.Option one - don't add those for numeric ops - cannot be done if numeric op is a part of compare, for which CBO needs correct types.Option two - unwrap casts when determining type in Hive - hard or impossible to tell apart CBO-added casts and user casts. Option three - don't change types in Hive if CBO has run - seems hacky and hard to ensure it's applied everywhere.Option four - map all expressions precisely between two trees and remove casts again after optimization, will be pretty difficult.Option five - somehow mark those casts. Not sure about how yet.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.HiveDecimalUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.decimal.udf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8112" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change reporting string to reflect update in Tez</summary>
      <description>We're now printing:Status: Running (application id: Executing on YARN cluster with App id application_1410822917153_0001)Where Tez just used to print the id it now has the full "Executing on YARN..." string.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="8126" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Standalone hive-jdbc jar is not packaged in the Hive distribution</summary>
      <description>With HIVE-538 we started creating the hive-jdbc-*-standalone.jar but the packaging/distribution does not contain the standalone jdbc jar. I would have expected it to locate under the lib folder of the distribution.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-9-3 01:00:00" id="813" opendate="2009-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the actual exception thrown in UDF evaluation</summary>
      <description>As title, we should show the actual exception thrown in UDF evaluation.User should be able to see both the message and the stacktrace.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="8130" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Date in Avro</summary>
      <description>Avro represents date as Avro int annotated with "date" logical type, where the int stores the number of days from the unix epoch (AVRO-739)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.resources.avro-struct.avsc</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-16 01:00:00" id="8131" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support timestamp in Avro</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="8139" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-lang from 2.4 to 2.6</summary>
      <description>Upgrade commons-lang version from 2.4 to latest 2.6</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-16 01:00:00" id="8148" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HDFS Path named with file:// instead of file:/// results in Unit test failures in Windows</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestStorageBasedMetastoreAuthorizationProviderWithACL.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">cli.src.test.org.apache.hadoop.hive.cli.TestRCFileCat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-16 01:00:00" id="8150" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Type coercion in union queries</summary>
      <description>If we can't get common type from Optiq, bail out for now.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-16 01:00:00" id="8152" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update with expression in set fails</summary>
      <description>Doing: update orctabsmall set age = age + 1; results in FAILED: ParseException line 1:33 missing EOF at '+' near 'age'We should be able to handle simple expressions in set clauses.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.values.tmp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.tmp.table.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.tmp.table.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-17 01:00:00" id="8156" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized reducers need to avoid memory build-up during a single key</summary>
      <description>When encountering a skewed key with a large number of values, the vectorized reducer will not release memory within the loop.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-17 01:00:00" id="8167" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn install command broken by HIVE-8126 commit</summary>
      <description>HIVE-8126 broke the mvn install command by referencing a jar that got removed as part of HIVE-8126 command.[INFO] Installing ~/hive/packaging/target/apache-hive-0.14.0-SNAPSHOT-jdbc.jar to ~/.m2/repository/org/apache/hive/hive-packaging/0.14.0-SNAPSHOT/hive-packaging-0.14.0-SNAPSHOT-standalone.jarThe file ~/hive/packaging/target/apache-hive-0.14.0-SNAPSHOT-jdbc.jar doesn't get created right now.Basically a copy goal for the above jar needs to be revived for this command to succeed.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-17 01:00:00" id="8169" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windows: alter table ..set location from hcatalog failed with NullPointerException</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-18 01:00:00" id="8174" opendate="2014-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Bailout if stats needed for VC from base table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-18 01:00:00" id="8175" opendate="2014-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive metastore upgrade from v0.13.0 to v0.14.0 script for Oracle is missing an upgrade step</summary>
      <description>Hive metastore upgrade script from 0.13.0 to 0.14.0 for Oracle is missing the required upgrade step from HIVE-7118.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.13.0-to-0.14.0.oracle.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-18 01:00:00" id="8185" opendate="2014-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-jdbc-0.14.0-SNAPSHOT-standalone.jar fails verification for signatures in build</summary>
      <description>In the current build, runningjarsigner --verify ./lib/hive-jdbc-0.14.0-SNAPSHOT-standalone.jarJar verification failed.unless that jar is removed from the lib dir, all hive queries throw the following error Exception in thread "main" java.lang.SecurityException: Invalid signature file digest for Manifest main attributes at sun.security.util.SignatureFileVerifier.processImpl(SignatureFileVerifier.java:240) at sun.security.util.SignatureFileVerifier.process(SignatureFileVerifier.java:193) at java.util.jar.JarVerifier.processEntry(JarVerifier.java:305) at java.util.jar.JarVerifier.update(JarVerifier.java:216) at java.util.jar.JarFile.initializeVerifier(JarFile.java:345) at java.util.jar.JarFile.getInputStream(JarFile.java:412) at sun.misc.URLClassPath$JarLoader$2.getInputStream(URLClassPath.java:775)</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-19 01:00:00" id="8187" opendate="2014-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Change Optiq Type System Precision/scale to use Hive Type System Precision/Scale</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-19 01:00:00" id="8194" opendate="2014-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: bail for having clause referring select expr aliases</summary>
      <description>This is non standard behavior. Not supporting in cbo mode.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-19 01:00:00" id="8198" opendate="2014-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle case where top level schema contains repeated alias</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-19 01:00:00" id="8201" opendate="2014-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hardwiring to HiveInputFormat in acid qfile tests</summary>
      <description>Now that HIVE-7812 is checked in we should remove the hardwiring to HiveInputFormat for the qfile tests.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.update.where.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.where.no.match.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.where.non.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.two.cols.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.tmp.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.non.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.after.multiple.inserts.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.tmp.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.non.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.values.dynamic.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.update.delete.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.whole.partition.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.where.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.where.no.match.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.where.non.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.tmp.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.orig.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.all.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.delete.all.non.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.update.own.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.update.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.delete.own.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.delete.q</file>
      <file type="M">ql.src.test.queries.clientpositive.acid.vectorization.q</file>
      <file type="M">ql.src.test.queries.clientnegative.update.partition.col.q</file>
      <file type="M">ql.src.test.queries.clientnegative.update.no.such.table.q</file>
      <file type="M">ql.src.test.queries.clientnegative.authorization.update.noupdatepriv.q</file>
      <file type="M">ql.src.test.queries.clientnegative.authorization.delete.nodeletepriv.q</file>
      <file type="M">ql.src.test.queries.clientnegative.acid.overwrite.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-20 01:00:00" id="8202" opendate="2014-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support SMB Join for Hive on Spark [Spark Branch]</summary>
      <description>SMB joins are used wherever the tables are sorted and bucketed. It's a map-side join. The join boils down to just merging the already sorted tables, allowing this operation to be faster than an ordinary map-join.The task is to research and support the conversion from regular SMB join to SMB map join for Spark execution engine.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-20 01:00:00" id="8207" opendate="2014-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add .q tests for multi-table insertion [Spark Branch]</summary>
      <description>Now that multi-table insertion is committed to branch, we should enable those related qtests.Here is a list of qfiles that should be activated (some of them may already be activated).The list may not be comprehensive.add_part_multiple.qauto_smb_mapjoin_14.qbucket5.qcolumn_access_stats.qdate_udf.qgroupby10.qgroupby11.qgroupby3_map_multi_distinct.qgroupby3_map.qgroupby3_map_skew.qgroupby3_noskew_multi_distinct.qgroupby3_noskew.qgroupby7_map_multi_single_reducer.qgroupby7_map.qgroupby7_map_skew.qgroupby7_noskew_multi_single_reducer.qgroupby7_noskew.qgroupby7.qgroupby8_map.qgroupby8_map_skew.qgroupby8_noskew.qgroupby8.qgroupby9.qgroupby_complex_types_multi_single_reducer.qgroupby_complex_types.qgroupby_cube1.qgroupby_map_ppr_multi_distinct.qgroupby_map_ppr.qgroupby_multi_insert_common_distinct.qgroupby_multi_single_reducer2.qgroupby_multi_single_reducer3.qgroupby_multi_single_reducer.qgroupby_position.qgroupby_ppr.qgroupby_rollup1.qgroupby_sort_1_23.qgroupby_sort_1.qgroupby_sort_skew_1_23.qinfer_bucket_sort_multi_insert.qinnerjoin.qinput12_hadoop20.qinput12.qinput13.qinput14.qinput17.qinput18.qinput1_limit.qinput_part2.qinsert_into3.qjoin_nullsafe.qload_dyn_part8.qmetadata_only_queries_with_filters.qmultigroupby_singlemr.qmulti_insert_gby2.qmulti_insert_gby3.qmulti_insert_gby.qmulti_insert_lateral_view.qmulti_insert_move_tasks_share_dependencies.qmulti_insert.qparallel.qpartition_date2.qpcr.qppd_multi_insert.qppd_transform.qsmb_mapjoin_11.qsmb_mapjoin_12.qsmb_mapjoin_13.qsmb_mapjoin_15.qsmb_mapjoin_16.qstats4.qsubquery_multiinsert.qtable_access_keys_stats.qtez_dml.qudaf_percentile_approx_20.qudaf_percentile_approx_23.qunion17.qunion18.qunion19.q There are some tests that cannot be enabled right now, due to various reasons:1. ForwardOperator Issue, includinggroupby7_noskew_multi_single_reducer.qgroupby8_map.qgroupby8_map_skew.qgroupby8_noskew.qgroupby8.qgroupby9.qgroupby10.qgroupby_multi_insert_common_distinct.q union17.qReason: currently, if the node to break in the operator tree is a ForwardOperator, we simple do nothing. However, we may have the following case: ... RS_0 | FOR | / \ GBY_1 GBY_2 | | ... ... | | RS_1 RS_2 | | ... ... | | FS_1 FS_2which may result to: RW / \ RW RWand because of the issue in HIVE-7731 and HIVE-8118, both downstream branches will get duplicated (and same) inputs.2. Stats issue, including:bucket5.qinfer_bucket_sort_multi_insert.qstats4.qsmb_mapjoin_13.qsmb_mapjoin_15.qReason: In these tests, I get diff error because numRows and rawDataSize are -1, but they are expected to be some positive value. I don't think this is related to multi-insertion.3. Join/SMB Join Issue, includingauto_smb_mapjoin_14.qauto_sortmerge_join_13.qsmb_mapjoin_11.qsmb_mapjoin_12.qsmb_mapjoin_13.qsmb_mapjoin_15.qsmb_mapjoin_16.qReason: These tests either failed with exception or failed with diff. I think it's because SMB Join (HIVE-8202) isn't supported right now.4. Result doesn't match, includinggroupby3_map_skew.qgroupby_map_ppr_multi_distinct.qgroupby_complex_types_multi_single_reducer.qgroupby_map_ppr.qpartition_date2.qudaf_percentile_approx_23.qReason: The results from these tests are different from MR's. For instance, test for groupby3_map_skew.q failed because:&lt; 130091.0 260.182 256.10355987055016 98.0 0.0 142.92680950752379 143.06995106518903 20428.07288 20469.0109---&gt; 130091.0 260.182 256.10355987055016 98.0 0.0 142.9268095075238 143.06995106518906 20428.07288 20469.0109I don't know why this will happen. But, I think they may not be related to multi-insertion.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-22 01:00:00" id="8227" opendate="2014-9-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE w/ hive on tez when doing unions on empty tables</summary>
      <description>We're looking at aliasToWork.values() to determine input paths etc. This can contain nulls when we're scanning empty tables.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-23 01:00:00" id="8232" opendate="2014-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO Trunk Merge: Address Review Comments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePartitionPrunerRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveMergeProjectRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveSortRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveCostUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveCost.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-24 01:00:00" id="8245" opendate="2014-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect table read entities at same time as view read entities</summary>
      <description/>
      <version>0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-9-26 01:00:00" id="8270" opendate="2014-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC uber jar is missing some classes required in secure setup.</summary>
      <description>JDBC uber jar is missing some required classes for a secure setup.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-26 01:00:00" id="8271" opendate="2014-9-26 00:00:00" resolution="Won&amp;#39;t Fix">
    <buginformation>
      <summary>Jackson incompatibility between hadoop-2.4 and hive-14</summary>
      <description>jackson-1.8 is not API compatible with jackson-1.9 (abstract classes).threw an Error. Shutting down now...java.lang.AbstractMethodError: org.codehaus.jackson.map.AnnotationIntrospector.findSerializer(Lorg/codehaus/jackson/map/introspect/Annotated;)Ljava/lang/Object;hadoop-common (2.4) depends on jackson-1.8 and hive-14 depends on jackson-1.9.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-27 01:00:00" id="8281" opendate="2014-9-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE with dynamic partition pruning on Tez</summary>
      <description>Dynamic partition pruning can generate incorrect query plans during join algorithm selection.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-29 01:00:00" id="8296" opendate="2014-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez ReduceShuffle Vectorization needs 2 data buffers (key and value) for adding rows</summary>
      <description>We reuse the keys for the vectorized row batch and need to use a separate buffer (for strings) for reuse the batch for new values.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-30 01:00:00" id="8300" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing guava lib causes IllegalStateException when deserializing a task [Spark Branch]</summary>
      <description>In spark-1.2, we have guava shaded in spark-assembly. And we only ship hive-exec to spark cluster. So spark executor won't have (original) guava in its class path.This can cause some problem when TaskRunner deserializes a task, and throws something like this:org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 3, node13-1): java.lang.IllegalStateException: unread block data java.io.ObjectInputStream$BlockDataInputStream.setBlockDataMode(ObjectInputStream.java:2421) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1382) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62) org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:164) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744)We may have to verify this issue and ship guava to spark cluster.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-30 01:00:00" id="8304" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Reduce-Side GROUP BY Vectorization doesn&amp;#39;t copy NULL keys correctly</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-30 01:00:00" id="8313" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize evaluation for ExprNodeConstantEvaluator and ExprNodeNullEvaluator</summary>
      <description>Consider the following query:SELECT foo, bar, goo, idFROM myTableWHERE id IN ( 'A', 'B', 'C', 'D', ... , 'ZZZZZZ' );One finds that when the IN clause has several thousand elements (and the table has several million rows), the query above takes orders-of-magnitude longer to run on Hive 0.12 than say Hive 0.10.I have a possibly incomplete fix.</description>
      <version>0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-30 01:00:00" id="8314" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore thrift string interning of HIVE-7975</summary>
      <description>In HIVE-7975 did string interning in thrift-generated code by having a google-replacer plugin run with -Pthriftif that does the replacements. In commit of HIVE-7482, it was removed as it was done without that plugin. Thrift code should be regenerated.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SerDeInfo.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FieldSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-30 01:00:00" id="8315" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO : Negate condition underestimates selectivity which results in an in-efficient plan</summary>
      <description>For TPC-DS Q64 the predicate cd1.cd_marital_status &lt;&gt; cd2.cd_marital_status under estimate the join selectivity by a huge margin and results in in-efficient join order.This is a subset of the logical plan showing that item was joined very last HiveJoinRel(condition=[=($0, $37)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017602518958E8 rows, 0.0 cpu, 0.0 io}, id = 3790 HiveJoinRel(condition=[=($0, $33)], joinType=[inner]): rowcount = 1.0, cumulative cost = {6.386017582518958E8 rows, 0.0 cpu, 0.0 io}, id = 3067 HiveFilterRel(condition=[&lt;&gt;($30, $32)]): rowcount = 1.8252236387887635, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 1153 HiveProjectRel(ss_item_sk=[$2], ss_customer_sk=[$3], ss_cdemo_sk=[$4], ss_hdemo_sk=[$5], ss_addr_sk=[$6], ss_store_sk=[$7], ss_promo_sk=[$8], ss_ticket_number=[$9], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$12], ss_sold_date_sk=[$13], sr_item_sk=[$0], sr_ticket_number=[$1], c_customer_sk=[$23], c_current_cdemo_sk=[$24], c_current_hdemo_sk=[$25], c_current_addr_sk=[$26], c_first_shipto_date_sk=[$27], c_first_sales_date_sk=[$28], d_date_sk=[$14], d_year=[$15], d_date_sk0=[$29], d_year0=[$30], d_date_sk1=[$31], d_year1=[$32], s_store_sk=[$18], s_store_name=[$19], s_zip=[$20], cd_demo_sk=[$16], cd_marital_status=[$17], cd_demo_sk0=[$21], cd_marital_status0=[$22]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2312 HiveJoinRel(condition=[AND(=($2, $0), =($9, $1))], joinType=[inner]): rowcount = 3.6246005783468924E7, cumulative cost = {6.386017554266721E8 rows, 0.0 cpu, 0.0 io}, id = 2310 HiveProjectRel(sr_item_sk=[$1], sr_ticket_number=[$8]): rowcount = 5.5578005E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 912 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_returns]]): rowcount = 5.5578005E7, cumulative cost = {0}, id = 62 HiveJoinRel(condition=[=($1, $21)], joinType=[inner]): rowcount = 1.2950939439433252E7, cumulative cost = {5.700728109872389E8 rows, 0.0 cpu, 0.0 io}, id = 2308 HiveJoinRel(condition=[=($5, $16)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.629812800658973E8 rows, 0.0 cpu, 0.0 io}, id = 2301 HiveJoinRel(condition=[=($2, $14)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.574895371445558E8 rows, 0.0 cpu, 0.0 io}, id = 2299 HiveJoinRel(condition=[=($11, $12)], joinType=[inner]): rowcount = 5491530.921341597, cumulative cost = {5.500772062232143E8 rows, 0.0 cpu, 0.0 io}, id = 1898 HiveProjectRel(ss_item_sk=[$1], ss_customer_sk=[$2], ss_cdemo_sk=[$3], ss_hdemo_sk=[$4], ss_addr_sk=[$5], ss_store_sk=[$6], ss_promo_sk=[$7], ss_ticket_number=[$8], ss_wholesale_cost=[$10], ss_list_price=[$11], ss_coupon_amt=[$18], ss_sold_date_sk=[$22]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 909 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200_orig.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 55Query select cs1.product_name ,cs1.store_name ,cs1.store_zip ,cs1.b_street_number ,cs1.b_streen_name ,cs1.b_city ,cs1.b_zip ,cs1.c_street_number ,cs1.c_street_name ,cs1.c_city ,cs1.c_zip ,cs1.syear ,cs1.cnt ,cs1.s1 ,cs1.s2 ,cs1.s3 ,cs2.s1 ,cs2.s2 ,cs2.s3 ,cs2.syear ,cs2.cntfrom(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3 FROM store_sales JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk JOIN store ON store_sales.ss_store_sk = store.s_store_sk JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk JOIN item ON store_sales.ss_item_sk = item.i_item_sk JOIN (select cs_item_sk ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund from catalog_sales JOIN catalog_returns ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk and catalog_sales.cs_order_number = catalog_returns.cr_order_number group by cs_item_sk having sum(cs_ext_list_price)&gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_uiON store_sales.ss_item_sk = cs_ui.cs_item_sk WHERE cd1.cd_marital_status &lt;&gt; cd2.cd_marital_status and i_color in ('maroon','burnished','dim','steel','navajo','chocolate') and i_current_price between 35 and 35 + 10 and i_current_price between 35 + 1 and 35 + 15group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year) cs1JOIN(select i_product_name as product_name ,i_item_sk as item_sk ,s_store_name as store_name ,s_zip as store_zip ,ad1.ca_street_number as b_street_number ,ad1.ca_street_name as b_streen_name ,ad1.ca_city as b_city ,ad1.ca_zip as b_zip ,ad2.ca_street_number as c_street_number ,ad2.ca_street_name as c_street_name ,ad2.ca_city as c_city ,ad2.ca_zip as c_zip ,d1.d_year as syear ,d2.d_year as fsyear ,d3.d_year as s2year ,count(*) as cnt ,sum(ss_wholesale_cost) as s1 ,sum(ss_list_price) as s2 ,sum(ss_coupon_amt) as s3 FROM store_sales JOIN store_returns ON store_sales.ss_item_sk = store_returns.sr_item_sk and store_sales.ss_ticket_number = store_returns.sr_ticket_number JOIN customer ON store_sales.ss_customer_sk = customer.c_customer_sk JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk JOIN date_dim d2 ON customer.c_first_sales_date_sk = d2.d_date_sk JOIN date_dim d3 ON customer.c_first_shipto_date_sk = d3.d_date_sk JOIN store ON store_sales.ss_store_sk = store.s_store_sk JOIN customer_demographics cd1 ON store_sales.ss_cdemo_sk= cd1.cd_demo_sk JOIN customer_demographics cd2 ON customer.c_current_cdemo_sk = cd2.cd_demo_sk JOIN promotion ON store_sales.ss_promo_sk = promotion.p_promo_sk JOIN household_demographics hd1 ON store_sales.ss_hdemo_sk = hd1.hd_demo_sk JOIN household_demographics hd2 ON customer.c_current_hdemo_sk = hd2.hd_demo_sk JOIN customer_address ad1 ON store_sales.ss_addr_sk = ad1.ca_address_sk JOIN customer_address ad2 ON customer.c_current_addr_sk = ad2.ca_address_sk JOIN income_band ib1 ON hd1.hd_income_band_sk = ib1.ib_income_band_sk JOIN income_band ib2 ON hd2.hd_income_band_sk = ib2.ib_income_band_sk JOIN item ON store_sales.ss_item_sk = item.i_item_sk JOIN (select cs_item_sk ,sum(cs_ext_list_price) as sale,sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit) as refund from catalog_sales JOIN catalog_returns ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk and catalog_sales.cs_order_number = catalog_returns.cr_order_number group by cs_item_sk having sum(cs_ext_list_price)&gt;2*sum(cr_refunded_cash+cr_reversed_charge+cr_store_credit)) cs_uiON store_sales.ss_item_sk = cs_ui.cs_item_sk WHERE cd1.cd_marital_status &lt;&gt; cd2.cd_marital_status and i_color in ('maroon','burnished','dim','steel','navajo','chocolate') and i_current_price between 35 and 35 + 10 and i_current_price between 35 + 1 and 35 + 15group by i_product_name ,i_item_sk ,s_store_name ,s_zip ,ad1.ca_street_number ,ad1.ca_street_name ,ad1.ca_city ,ad1.ca_zip ,ad2.ca_street_number ,ad2.ca_street_name ,ad2.ca_city ,ad2.ca_zip ,d1.d_year ,d2.d_year ,d3.d_year) cs2ON cs1.item_sk=cs2.item_skwhere cs1.syear = 2000 and cs2.syear = 2000 + 1 and cs2.cnt &lt;= cs1.cnt and cs1.store_name = cs2.store_name and cs1.store_zip = cs2.store_ziporder by cs1.product_name ,cs1.store_name ,cs2.cnt</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-2 01:00:00" id="8327" opendate="2014-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mvn site -Pfindbugs</summary>
      <description>HIVE-3099 originally added findbugs into the old ant build.Get basic findbugs working for the maven build.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-10-3 01:00:00" id="8340" opendate="2014-10-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windows: HiveServer2 service doesn&amp;#39;t stop backend jvm process, which prevents follow-up service start.</summary>
      <description>On stopping the HS2 service from the services tab, it only kills the root process and does not kill the child java process. As a result resources are not freed and this throws an error on restarting from command line.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">bin.hive.cmd</file>
      <file type="M">bin.ext.hiveserver2.cmd</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-10-4 01:00:00" id="8350" opendate="2014-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant folding should happen before group-by optimization</summary>
      <description>Constant folding should happen as early as possible, so later optimizations can take advantages of it.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GroupByOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-5 01:00:00" id="8354" opendate="2014-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-7156 introduced required dependency on tez</summary>
      <description>Should not be much work to fix, but HIVE-7156 introduced a require dep of tez-api.</description>
      <version>0.14.0,0.15.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-6 01:00:00" id="8358" opendate="2014-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant folding should happen before PCR</summary>
      <description>So, that partition pruning and transitive predicate propagation may take advantage of constant folding.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.threshold.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-6 01:00:00" id="8366" opendate="2014-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO fails if there is a table sample in subquery</summary>
      <description>Bail out from cbo in such cases.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-7 01:00:00" id="8374" opendate="2014-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool fails on Postgres versions &lt; 9.2</summary>
      <description>The upgrade script for HIVE-5700 creates an UDF with language 'plpgsql',which is available by default only for Postgres 9.2+.For older Postgres versions, the language must be explicitly created,otherwise schematool fails with the error:Error: ERROR: language "plpgsql" does not exist Hint: Use CREATE LANGUAGE to load the language into the database. (state=42704,code=0)</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreSchemaInfo.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-7 01:00:00" id="8377" opendate="2014-10-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable Kerberized SSL for HiveServer2 in http mode</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-12-8 01:00:00" id="8395" opendate="2014-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: enable by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.date.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.processor.namespaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.print.header.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">ql.src.test.queries.clientnegative.join.nonexistent.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ambiguous.col.q</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constantPropagateForSubQuery.q</file>
      <file type="M">ql.src.test.queries.clientpositive.filter.join.breaktask2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.vc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.optimize.nullscan.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.gby.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.test.queries.clientpositive.query.properties.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.exists.explain.rewrite.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.in.explain.rewrite.q</file>
      <file type="M">ql.src.test.results.clientnegative.join.nonexistent.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ansi.sql.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-8 01:00:00" id="8403" opendate="2014-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build broken by datanucleus.org being offline</summary>
      <description>datanucleus.org is not available, making it impossible to download jars.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-8 01:00:00" id="8407" opendate="2014-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle filters with non-boolean return type</summary>
      <description>e.g. select * from src where 'foo';</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-9 01:00:00" id="8413" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle ill-formed queries which have distinct, having in incorrect context</summary>
      <description>e.g., select hash (distinct key) from src;</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-9 01:00:00" id="8421" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Use OptiqSemanticException in error conditions</summary>
      <description>TestNegativeCliDriver</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-9 01:00:00" id="8422" opendate="2014-10-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on all join .q tests [Spark Branch]</summary>
      <description>With HIVE-8412, all join queries should work on Spark, whether they require a particular optimization or not.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2014-10-15 01:00:00" id="8463" opendate="2014-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add numPartitions info to SparkEdgeProperty [Spark Branch]</summary>
      <description>When we are debugging the Spark branch, it's better if we can add the number of partitions info associated with SparkEdgeProperty in the "explain" result.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-15 01:00:00" id="8478" opendate="2014-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Reduce-Side Group By doesn&amp;#39;t handle Decimal type correctly</summary>
      <description>Note that DecimalColumnVector is different than LongColumnVector because it keeps (an instance) reference to a Decimal128 class whereas the latter stores a long primitive value. So, trouble if you set the reference instead of updating the object.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-10-16 01:00:00" id="8490" opendate="2014-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant folding should happen before partition pruning</summary>
      <description>so that partition pruning see simplified expressions and present simpler expressions to metastore</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-10-17 01:00:00" id="8496" opendate="2014-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-enable statistics [Spark Branch]</summary>
      <description>AnnotatedWithStatistics is not activated due to a recent merge, and hence the SetSparkReducerParallelism is not activated either. This caused Hive to always use one reducer. We should re-enable statistics.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.clusterby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.distributeby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.orderby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.escape.sortby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join10.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-17 01:00:00" id="8500" opendate="2014-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline does not need to set hive.aux.jars.path</summary>
      <description>Since beeline may be run from a host not running HS2, there is no reason for it to set hive.aux.jars.path property. Beeline doesn't need these jars, HS2 does and should use hive.aux.jars.path property populated with jars on HS2 host (not the ones sent over by beeline connection.)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-20 01:00:00" id="8518" opendate="2014-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile time skew join optimization returns duplicated results</summary>
      <description>Compile time skew join optimization clones the join operator tree and unions the results.The problem here is that we don't properly insert the predicate for the cloned join (relying on an assert statement).To reproduce the issue, run the simple query:select * from tbl1 join tbl2 on tbl1.key=tbl2.key;And suppose there's some skew in tbl1 (specify skew with CREATE or ALTER statement).Duplicated results will be returned if you set hive.optimize.skewjoin.compiletime=true.</description>
      <version>0.14.0</version>
      <fixedVersion>1.0.2,1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SkewJoinOptimizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-21 01:00:00" id="8546" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Handle "add archive scripts.tar.gz" in Tez</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8547" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO and/or constant propagation breaks partition_varchar2 test</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8550" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive cannot load data into partitioned table with Unicode key</summary>
      <description>Steps to reproduce:1) Copy the file partitioned.txt to the root folder of your HDFS root dir. Copy the two hql files to your local directory.2) Open Hive CLI.3) Run:hive&gt; source &lt;path to CreatePartitionedTable.hql&gt;;4) Run hive&gt; source &lt;path to LoadIntoPartitionedTable.hql&gt;;The following error will be shown:hive&gt; source C:\Scripts\partition\LoadIntoPartitionedTable.hql;Loading data to table default.mypartitioned partition (tag=ä¶µ)Failed with exception nullFAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.MoveTask</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.13.0-to-0.14.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.14.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-22 01:00:00" id="8556" opendate="2014-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>introduce overflow control and sanity check to BytesBytesMapJoin</summary>
      <description>When stats are incorrect, negative or very large number can be passed to the map</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mapjoin.MapJoinMemoryExhaustionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-22 01:00:00" id="8558" opendate="2014-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: enable n-way joins after CBO join reordering</summary>
      <description>With CBO, we disable n-way joins. But for consecutive tables we can safely collapse into n-way joins. For e.g this is safe to collapse:select p1.p_name, p2.p_name, p3.p_name from part p1 join part p2 on p1.p_name = p2.p_name join part p3 on p3.p_name = p1.p_name;whereas, we shouldn't reorder in this case:select p1.p_name, p2.p_name, p3.p_name from part p1 join part p2 on p1.p_name = p2.p_name join part p3 on p3.p_size = p1.p_size and p3.p_size = p2.p_size join part p4 on p1.p_name = p4.p_name;</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-23 01:00:00" id="8582" opendate="2014-10-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Outer Join Simplification is broken</summary>
      <description>CLEAR LIBRARY CACHE</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-10-24 01:00:00" id="8596" opendate="2014-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 dynamic service discovery: ZK throws too many connections error</summary>
      <description>2014-10-23 07:55:44,221 - WARN [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxnFactory@193] - Too many connections from /172.31.47.11 - max is 60</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-24 01:00:00" id="8597" opendate="2014-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB join small table side should use the same set of serialized payloads across tasks</summary>
      <description>Each task sees all splits belonging to the bucket being processed by the task. At the moment, we end up using different instances of the same serialized split which adds unnecessary memory pressure.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.CustomPartitionVertex.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-24 01:00:00" id="8598" opendate="2014-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Push constant filters through joins</summary>
      <description>Will make NullScanOptimizer more effective.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.optimize.nullscan.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-12-29 01:00:00" id="86" opendate="2008-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop table should not delete data for external tables</summary>
      <description>We should not delete data for external tables.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestDrop.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RWTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-27 01:00:00" id="8614" opendate="2014-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hive to use tez version 0.5.2-SNAPSHOT</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-27 01:00:00" id="8617" opendate="2014-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn on all join .q tests #2 [Spark Branch]</summary>
      <description>There are still a few tests need to turned on (most of them need SORT_QUERY_RESULTS after we resolved HIVE-8422:auto_join26bucketmapjoin7date_join1join40skewjoinopt2vector_decimal_mapjoinvector_mapjoin_reduceauto_join_without_localtaskmapjoin1</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-11-27 01:00:00" id="8623" opendate="2014-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement HashTableLoader for Spark map-join [Spark Branch]</summary>
      <description>This is a sub-task of map-join for spark https://issues.apache.org/jira/browse/HIVE-7613This can use the baseline patch for map-joinhttps://issues.apache.org/jira/browse/HIVE-8616</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.HashTableLoaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinTableContainerSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-27 01:00:00" id="8624" opendate="2014-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Record counters don&amp;#39;t work with Tez container reuse</summary>
      <description>Problem is that the counters aren't re-initialized when containers are re-used.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-28 01:00:00" id="8631" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compressed transaction list cannot be parsed in job.xml</summary>
      <description>HIVE-8341 added code to compress the transaction list in the JobConf when it reaches a certain size. This breaks when the JobConf is converted to job.xml and sent to MR. The special characters are not correctly escaped. The proposed fix is to back out the compression (not the changes to ScriptOperator) for now and look later at compressing/encoding it in a way that works with job.xml.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestValidTxnImpl.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidTxnListImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-28 01:00:00" id="8634" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 fair scheduler queue mapping doesn&amp;#39;t handle the secondary groups rules correctly</summary>
      <description>The fair scheduler queue refresh in HiveServer2 (for non-impersonation mode), doesn't handle the primary/secondary queue mappings correctly. It's not reading primary and secondary rules from the scheduler rule file.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hive.jdbc.TestSchedulerQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-28 01:00:00" id="8635" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: ambiguous_col negative test no longer fails</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-28 01:00:00" id="8637" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In insert into X select from Y, table properties from X are clobbering those from Y</summary>
      <description>With a query like:insert into table X select * from Y;the table properties from table X are being sent to the input formats for table Y.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-28 01:00:00" id="8641" opendate="2014-10-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable skew joins in tez.</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-29 01:00:00" id="8649" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase level of parallelism in reduce phase [Spark Branch]</summary>
      <description>We calculate the number of reducers based on the same code for MapReduce. However, reducers are vastly cheaper in Spark and it's generally recommended we have many more reducers than in MR.Sandy Ryza who works on Spark has some ideas about a heuristic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-1 01:00:00" id="865" opendate="2009-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin: memory leak for same key with very large number of values</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-29 01:00:00" id="8653" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Push Semi Join through, Project/Filter/Join</summary>
      <description>CLEAR LIBRARY CACHE</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-29 01:00:00" id="8655" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: ppr_pushdown, udf_substr produces incorrect results due to broken tablesample handling</summary>
      <description>Bunch of rows were added. Looks like tablesample in subquery is no longer blocking CBO, probably broken by HIVE-8021</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-29 01:00:00" id="8656" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: auto_join_filters fails</summary>
      <description>Haven't looked why yet</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-29 01:00:00" id="8663" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fetching Vectorization scratch column map in Reduce-Side stop working</summary>
      <description>Recent changes (somewhere) caused scratch column types to not be fetched on reduce-side.Switching to use scratch column types from VectorizationContext.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-30 01:00:00" id="8665" opendate="2014-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix misc unit tests on Windows</summary>
      <description>Several junit tests failing on Windows for misc reasons (path issues, resources need to be closed before file can be deleted, etc).</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
      <file type="M">itests.hive-unit-hadoop2.src.test.java.org.apache.hadoop.hive.ql.security.TestPasswordWithCredentialProvider.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoaderStorer.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestUseDatabase.java</file>
      <file type="M">common.src.test.resources.hive-log4j-test.properties</file>
      <file type="M">common.src.test.resources.hive-exec-log4j-test.properties</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveLogging.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-31 01:00:00" id="8685" opendate="2014-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL operations in WebHCat set proxy user to "null" in unsecure mode</summary>
      <description>This makes DDL commands failThis was stupidly broken in HIVE-8643</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SecureProxySupport.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.ExecServiceImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-31 01:00:00" id="8686" opendate="2014-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable vectorization tests with query results sort [Spark Branch]</summary>
      <description>Hive-8573 added query results sort to some vectorization tests. Now since the patch is merged to the spark branch. We can enable these tests in the spark branch now.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-31 01:00:00" id="8688" opendate="2014-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>serialized plan OutputStream is not being closed</summary>
      <description>The OutputStream to which serialized plan is not being closed in several places.This can result in plan not getting written correctly.I have seen intermittent issues in deserializing the plan, and I think this could be the/a cause.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-1 01:00:00" id="8693" opendate="2014-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Separate out fair scheduler dependency from hadoop 0.23 shim</summary>
      <description>As part of HIVE-8424 HiveServer2 uses Fair scheduler APIs to determine resource queue allocation for non-impersonation case. This adds a hard dependency of Yarn server jars for Hive.</description>
      <version>0.14.0,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-2 01:00:00" id="8698" opendate="2014-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>default log4j.properties not included in jar files anymore</summary>
      <description>trunk and hive 0.14 based builds no longer have hive-log4j.properties in the jars. This means that in default tar, unless hive-log4j.properties is created in conf dir (from hive-log4j.properties.template file), hive cli is much more verbose in what is printed to console. Hiveserver2 fails to come up, as it errors out with - org.apache.hadoop.hive.common.LogUtils$LogInitializationException: Unable to initialize logging using hive-log4j.properties, not found on CLASSPATH!</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-2 01:00:00" id="8700" opendate="2014-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace ReduceSink to HashTableSink (or equi.) for small tables [Spark Branch]</summary>
      <description>With HIVE-8616 enabled, the new plan has ReduceSinkOperator for the small tables. For example, the follow represents the operator plan for the small table dec1 derived from query explain select /*+ MAPJOIN(dec)*/ * from dec join dec1 on dec.value=dec1.d; Map 2 Map Operator Tree: TableScan alias: dec1 Statistics: Num rows: 0 Data size: 107 Basic stats: PARTIAL Column stats: NONE Filter Operator predicate: d is not null (type: boolean) Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE Reduce Output Operator key expressions: d (type: decimal(5,2)) sort order: + Map-reduce partition columns: d (type: decimal(5,2)) Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE value expressions: i (type: int)With the new design for broadcasting small tables, we need to convert the ReduceSinkOperator with HashTableSinkOperator or equivalent in the new plan.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-3 01:00:00" id="8716" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partition filters are not pushed down with lateral view</summary>
      <description>Changes to HIVE-8454 revealed issues with partition filters not being pushed down in case of lateral view. For more info see discussion in HIVE-5718.</description>
      <version>0.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.JoinCondTypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-4 01:00:00" id="8725" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-client build failed sometimes.[Spark Branch]</summary>
      <description>Sometimes spark-client build failed due to miss spark dependency version, I'm not sure the root cause, but add dependency version definitely fix my problem.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-4 01:00:00" id="8726" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect Spark TaskMetrics and build job statistic[Spark Branch]</summary>
      <description>Implement SparkListener to collect TaskMetrics, and build SparkStatistic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SimpleSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.JobStateListener.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-4 01:00:00" id="8727" opendate="2014-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dag summary has incorrect row counts and duration per vertex</summary>
      <description>During the code review for HIVE-8495 some code was reworked which broke some of INPUT/OUTPUT counters and duration.Patch attached which fixes that.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-5 01:00:00" id="8740" opendate="2014-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sorted dynamic partition does not work correctly with constant folding</summary>
      <description>Sorted dynamic partition optimization looks for partition columns from the operator above FileSinkOperator. As per hive convention it expects partition columns at the last. But with HIVE-8585 equality filters on partition columns gets folded to constant. The column pruner then prunes the constant expression as they don't reference any columns. This in some cases will yield unexpected results (throw ArrayIndexOutOfBounds exception) with sorted dynamic partition insert optimization. In such cases we don't really need sorted dynamic partition optimization.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-5 01:00:00" id="8748" opendate="2014-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>jdbc uber jar is missing commons-logging</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-6 01:00:00" id="8754" opendate="2014-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Sqoop job submission via WebHCat doesn&amp;#39;t properly localize required jdbc jars in secure cluster</summary>
      <description>HIVE-8588 added support for this by copying jdbc jars to lib/ of localized/exploded Sqoop tar. Unfortunately, in a secure cluster, Dist Cache intentionally sets permissions on exploded tars such that they are not writable.this needs to be fixed, otherwise the users would have to modify their sqoop tar to include the relevant jdbc jars which is burdensome is different DBs are used and may create headache around licensing issuesNO PRECOMMIT TESTS</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.SqoopDelegator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-6 01:00:00" id="8768" opendate="2014-11-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Fix filter selectivity for "in clause" &amp; "&lt;&gt;"</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-15 01:00:00" id="877" opendate="2009-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>error with cast(null as short)</summary>
      <description>also cast(null as long)</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.negative.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-7 01:00:00" id="8779" opendate="2014-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez in-place progress UI can show wrong estimated time for sub-second queries</summary>
      <description>The in-place progress update UI added as part of HIVE-8495 can show wrong estimated time for AM only job which goes from INITED to SUCCEEDED DAG state directly without going to RUNNING state.</description>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-7 01:00:00" id="8781" opendate="2014-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Nullsafe joins are busted on Tez</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-7 01:00:00" id="8783" opendate="2014-11-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create some tests that use Spark counter for stats collection [Spark Branch]</summary>
      <description>Currently when .q tests are run with Spark, the default stats collection is "fs". We need to have some tests that use Spark counter for stats collection to enhance coverage.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-8 01:00:00" id="8797" opendate="2014-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Simultaneous dynamic inserts can result in "partition already exists" error</summary>
      <description>If two users attempt a dynamic insert into the same new partition at the same time, a possible race condition exists where both will attempt to create the partition and one will fail.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-9 01:00:00" id="8799" opendate="2014-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>boatload of missing apache headers</summary>
      <description>Adding missing apache headers to a number of files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.io.TestDateWritable.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroObjectInspectorException.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPNumeric.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetShortInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.serde.primitive.TestParquetByteInspector.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloHiveRow.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloStorageHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.LazyAccumuloRow.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableInputFormat.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.CompareOp.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.DoubleCompare.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.Equal.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThan.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.GreaterThanOrEqual.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.IntCompare.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.LessThan.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.LessThanOrEqual.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.Like.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.LongCompare.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.NotEqual.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.PrimitiveComparison.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.compare.StringCompare.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.PrimitiveComparisonFilter.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloSerDe.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.mr.TestHiveAccumuloTableInputFormat.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.compare.TestDoubleCompare.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.compare.TestIntCompare.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.compare.TestLongComparison.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.compare.TestStringCompare.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.TestAccumuloSerDe.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.Decimal128FastBuffer.java</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.TestHBaseKeyFactory3.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataJSONSerializer.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.MetadataSerializer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.partition.spec.CompositePartitionSpecProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionListComposingSpecProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecWithSharedSDProxy.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.MockPartitionExpressionForMetastore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinBytesTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFTopNHash.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.TimestampColumnStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.ForwardWalker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MergeJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.AnnotateWithOpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.TraitsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MergeJoinWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-15 01:00:00" id="880" opendate="2009-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>user group information not populated for pre and post hook</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.repair.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-9 01:00:00" id="8800" opendate="2014-11-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release notes and notice for hive .14</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
      <file type="M">NOTICE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-10 01:00:00" id="8805" opendate="2014-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO skipped due to SemanticException: Line 0:-1 Both left and right aliases encountered in JOIN &amp;#39;avg_cs_ext_discount_amt&amp;#39;</summary>
      <description>Queryset hive.cbo.enable=trueset hive.stats.fetch.column.stats=trueset hive.exec.dynamic.partition.mode=nonstrictset hive.tez.auto.reducer.parallelism=trueset hive.auto.convert.join.noconditionaltask.size=320000000set hive.exec.reducers.bytes.per.reducer=100000000set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DummyTxnManagerset hive.support.concurrency=falseset hive.tez.exec.print.summary=trueexplain SELECT sum(cs1.cs_ext_discount_amt) as excess_discount_amountFROM (SELECT cs.cs_item_sk as cs_item_sk, cs.cs_ext_discount_amt as cs_ext_discount_amt FROM catalog_sales cs JOIN date_dim d ON (d.d_date_sk = cs.cs_sold_date_sk) WHERE d.d_date between '2000-01-27' and '2000-04-27') cs1JOIN item i ON (i.i_item_sk = cs1.cs_item_sk)JOIN (SELECT cs2.cs_item_sk as cs_item_sk, 1.3 * avg(cs_ext_discount_amt) as avg_cs_ext_discount_amt FROM (SELECT cs.cs_item_sk as cs_item_sk, cs.cs_ext_discount_amt as cs_ext_discount_amt FROM catalog_sales cs JOIN date_dim d ON (d.d_date_sk = cs.cs_sold_date_sk) WHERE d.d_date between '2000-01-27' and '2000-04-27') cs2 GROUP BY cs2.cs_item_sk) tmp1ON (i.i_item_sk = tmp1.cs_item_sk)WHERE i.i_manufact_id = 436 and cs1.cs_ext_discount_amt &gt; tmp1.avg_cs_ext_discount_amtException14/11/07 19:15:38 [main]: ERROR parse.SemanticAnalyzer: CBO failed, skipping CBO. org.apache.hadoop.hive.ql.parse.SemanticException: Line 0:-1 Both left and right aliases encountered in JOIN 'avg_cs_ext_discount_amt' at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(SemanticAnalyzer.java:2369) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(SemanticAnalyzer.java:2293) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.parseJoinCondition(SemanticAnalyzer.java:2249) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genJoinTree(SemanticAnalyzer.java:8010) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9678) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9593) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9619) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9593) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9619) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9606) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10053) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:415) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:303) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1129) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1004) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:994) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345) at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:443) at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:459) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:739) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:616) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)</description>
      <version>0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-11 01:00:00" id="8813" opendate="2014-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow tests to be excluded based on pattern/regex</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-11 01:00:00" id="8827" opendate="2014-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove SSLv2Hello from list of disabled protocols</summary>
      <description>Turns out SSLv2Hello is not the same as SSLv2.</description>
      <version>0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-12 01:00:00" id="8831" opendate="2014-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>show roles appends dummy new line</summary>
      <description>hive&gt; show roles; OKADMINPUBLICadminnavispublicr1role1role2s1src_role2Time taken: 0.092 seconds, Fetched: 11 row(s)</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.roles.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.set.show.current.role.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.grant.public.role.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.admin.almighty1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.case.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.role.no.admin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.caseinsensitivity.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-12 01:00:00" id="8835" opendate="2014-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>identify dependency scope for Remote Spark Context.[Spark Branch]</summary>
      <description>While submit job through Remote Spark Context, spark RDD graph generation and job submit is executed in remote side, so we have to add hive related dependency into its classpath with spark.driver.extraClassPath. instead of add all hive/hadoop dependency, we should narrow the scope and identify what dependency remote spark context required.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-14 01:00:00" id="8875" opendate="2014-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.optimize.sort.dynamic.partition should be turned off for ACID</summary>
      <description>Turning this on causes ACID insert, updates, and deletes to produce non-optimal plans with extra reduce phases.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-14 01:00:00" id="8876" opendate="2014-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect upgrade script for Oracle (13-&gt;14)</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.oracle.020-HIVE-7784.oracle.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-14 01:00:00" id="8877" opendate="2014-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve context logging during job submission via WebHCat</summary>
      <description>currently the logging includes env variables. it should also include contents of cur dir (helpful in tar shipping scenarios). contents of sqoop/lib (for pre-installed sqoop scenarios)java props (for general debugging)</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TrivialExecService.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-14 01:00:00" id="8879" opendate="2014-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade derby version to address race candition</summary>
      <description>DERBY-4160 describes the race condition that i sometimes notice.. particularly on windows. Below is the stack traceError Messagejava.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientStacktracejava.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) ... 29 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) ... 34 moreCaused by: javax.jdo.JDODataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP""NestedThrowables:java.sql.SQLNonTransientConnectionException: No current connection. at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) ... 39 moreCaused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045) at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365) at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827) at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571) at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513) at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232) at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414) at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218) at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065) at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913) at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727) ... 57 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 80 moreStandard Output2014-11-14 12:56:43,043 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,047 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,048 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,162 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,240 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,242 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,278 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,281 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing2014-11-14 12:56:43,295 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,298 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.02014-11-14 12:56:43,311 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,328 (main) [WARN - org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:589)] Retrying creating default database after error: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP""javax.jdo.JDODataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)NestedThrowablesStackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045) at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365) at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827) at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571) at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513) at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232) at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414) at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218) at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065) at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913) at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 80 more2014-11-14 12:56:43,382 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,423 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,424 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,500 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,501 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,502 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,541 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,542 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,633 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,637 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing2014-11-14 12:56:43,647 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,649 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.02014-11-14 12:56:43,662 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more</description>
      <version>0.13.1,0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-16 01:00:00" id="8892" opendate="2014-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use MEMORY_AND_DISK for RDD caching [Spark Branch]</summary>
      <description>In HIVE-8844, we made the persistent policy for RDD caching configurable. We should do something simpler and don't add additional configurations.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ShuffleTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-17 01:00:00" id="8898" opendate="2014-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HIVE-8874 once HBASE-12493 is fixed</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-19 01:00:00" id="8917" opendate="2014-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-5679 adds two thread safety problems</summary>
      <description>HIVE-5679 adds two static SimpleDateFormat objects and SimpleDateFormat is not thread safe. These should be converted to thread locals.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.Filter.g</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-20 01:00:00" id="8922" opendate="2014-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: assorted date and timestamp issues</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.TaskGraphWalker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-20 01:00:00" id="8923" opendate="2014-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-8512 needs to be fixed also for CBO</summary>
      <description>Queries reverted to incorrect results.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.gby.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.semijoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.gby.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="8929" opendate="2014-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect error message for cbo path</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.JoinCondTypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-20 01:00:00" id="893" opendate="2009-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Thrift serde doesn&amp;#39;t work with the new version of thrift</summary>
      <description>The new version of thrift rename the __isset to __isset_bit_vector in the generated Thrift java code. This causes __isset_bit_vector passed as a field in ThriftSerDe.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ThriftStructObjectInspector.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-21 01:00:00" id="8931" opendate="2014-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test TestAccumuloCliDriver is not completing</summary>
      <description>Tests are taking 3 hours due to TestAccumuloCliDriver not finishing.Logs:http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1848/failed/TestAccumuloCliDriver/</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-21 01:00:00" id="8933" opendate="2014-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check release builds for SNAPSHOT dependencies</summary>
      <description>Hive 0.14.0 was released with SNAPSHOT dependencies. We should use the maven enforcer plugin to prevent this from happening again in the future.</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-2-20 01:00:00" id="894" opendate="2009-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UDAFs max_n(), min_n() to contrib</summary>
      <description>These 2 UDAFs should return the max n numbers, min n numbers in order.SELECT max_n(userid, 3) FROM src;[999,997,996]SELECT max_n(userid, 3)[1] FROM src;997SELECT min_n(userid, 3) FROM src;[0, 3, 8]SELECT min_n(userid, 3)[2] FROM src;8</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBridge.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DefaultUDAFEvaluatorResolver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-1 01:00:00" id="9001" opendate="2014-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ship with log4j.properties file that has a reliable time based rolling policy</summary>
      <description>The hive log gets locked by the hive process and cannot be rolled in windows OS.Install Hive in Windows, start hive, try and rename hive log while Hive is running. Wait for log4j tries to rename it and it will throw the same error as it is locked by the process.The changes in https://issues.apache.org/bugzilla/show_bug.cgi?id=29726 should be integrated to Hive for a reliable rollover.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.hive-log4j.properties</file>
      <file type="M">common.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-2 01:00:00" id="9003" opendate="2014-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized IF expr broken for the scalar and scalar case</summary>
      <description>SELECT IF (bool_col, 'first', 'second') FROM ...is broken for Vectorization.</description>
      <version>0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprStringScalarStringScalar.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-6 01:00:00" id="9035" opendate="2014-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Disable PPD when functions are non-deterministic (ppd_random.q - non-deterministic udf rand() pushed above join)</summary>
      <description>Not clear if this is a problem. Probably issue is in Optiq if it is, does it know the UDF is non-deterministic? We could disable Optiq for such UDFs if all else fails.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveFilterJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-1-7 01:00:00" id="9038" opendate="2014-12-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Join tests fail on Tez</summary>
      <description>Tez doesn't run all tests. But, if you run them, following tests fail with runt time exception pointing to bugs. auto_join21.q auto_join29.q auto_join30.q auto_join_filters.q auto_join_nulls.q</description>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-9 01:00:00" id="9048" opendate="2014-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build failed on hadoop-1 after HIVE-8828.</summary>
      <description>HIVE-8828 introduce org.apache.hadoop.tools.HadoopArchives which included in hadoop-tools(in hadoop-1)/hadoop-archives(in hadoop-2), while hadoop-tools is not added into hadoop-1 dependency. This lead to the following compile error:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[175,30] error: package org.apache.hadoop.tools does not exist[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,8] error: cannot find symbol[ERROR] class DDLTask[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,33] error: cannot find symbol[ERROR] -&gt; [Help 1]</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-12-12 01:00:00" id="9090" opendate="2014-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename "Tez File Merge Work" to smaller name</summary>
      <description>This is just a cosmetic change. The "Tez File Merge Work" vertex name is long that in-place update UI showing vertex name is out of place.</description>
      <version>0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-12-4 01:00:00" id="913" opendate="2009-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>no error if user specifies same output table mutliple times</summary>
      <description>The query:from srcinsert overwrite table dest1 select key, valueinsert overwrite table dest1 select key, value2;does not fail.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-16 01:00:00" id="9130" opendate="2014-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>vector_partition_diff_num_cols result is not updated after CBO upgrade</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-17 01:00:00" id="9140" opendate="2014-12-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Calcite&amp;#39;s ReduceExpressionRules to Hive</summary>
      <description>These rules provide a form of constant folding.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-1-22 01:00:00" id="9189" opendate="2014-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ProjectRemove rule on CBO path</summary>
      <description>When these rules fire we will get shorter and tighter operator pipeline.</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-23 01:00:00" id="9202" opendate="2014-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Choose Kryo as the serializer for pTest [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.spark.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-24 01:00:00" id="9207" opendate="2014-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more log information for debug RSC[Spark Branch]</summary>
      <description>Currently, error message in certain scenerio is lost in RSC, and we need more log info in DEBUG level for debugging.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcDispatcher.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-11-10 01:00:00" id="921" opendate="2009-11-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MapJoin output schema reordering</summary>
      <description>The FileSink operator following Map-side join sorts the output column names. This could results in wrong results if the number of columns of table is more than 10.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinFactory.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-24 01:00:00" id="9211" opendate="2014-12-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Research on build mini HoS cluster on YARN for unit test[Spark Branch]</summary>
      <description>HoS on YARN is a common use case in product environment, we'd better enable unit test for this case.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">data.conf.spark.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-30 01:00:00" id="9234" opendate="2014-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks FileSystem objects in FileSystem.CACHE</summary>
      <description>Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true), but creating new FileSystem objects is expensive.</description>
      <version>0.12.0,0.12.1,0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-6 01:00:00" id="9267" opendate="2015-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure custom UDF works with Spark [Spark Branch]</summary>
      <description>Create or add auto qtest if necessary.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-6 01:00:00" id="9272" opendate="2015-1-6 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Tests for utf-8 support</summary>
      <description>Including some test cases for utf8 support in webhcat. The first four tests invoke hive, pig, mapred and streaming apis for testing the utf8 support for data processed, file names and job name. The last test case tests the filtering of job name with utf8 character</description>
      <version>0.14.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.deployers.deploy.e2e.artifacts.sh</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-7 01:00:00" id="9278" opendate="2015-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cached expression feature broken in one case</summary>
      <description>Different query result depending on whether hive.cache.expr.evaluation is true or false. When true, no query results are produced (this is wrong).The q file:set hive.cache.expr.evaluation=true;CREATE TABLE cache_expr_repro (date_str STRING);LOAD DATA LOCAL INPATH '../../data/files/cache_expr_repro.txt' INTO TABLE cache_expr_repro;SELECT MONTH(date_str) AS `mon`, CAST((MONTH(date_str) - 1) / 3 + 1 AS int) AS `quarter`, YEAR(date_str) AS `year` FROM cache_expr_repro WHERE ((CAST((MONTH(date_str) - 1) / 3 + 1 AS int) = 1) AND (YEAR(date_str) = 2015)) GROUP BY MONTH(date_str), CAST((MONTH(date_str) - 1) / 3 + 1 AS int), YEAR(date_str) ;cache_expr_repro.txt2015-01-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-01-01 00:00:002015-02-01 00:00:002015-02-01 00:00:002015-01-01 00:00:002015-01-01 00:00:00</description>
      <version>0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-7 01:00:00" id="9292" opendate="2015-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Inline GroupBy, Properties</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteQueryUsingAggregateIndexCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-8 01:00:00" id="9316" opendate="2015-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSqoop tests in WebHCat testsuite hardcode libdir path to hdfs</summary>
      <description>Currently the TestSqoop tests in WebHCat Perl based testsuite has hdfs:// prefix in the jdbc jar path in libdir, we should remove this to enable it to run against other file systems.NO PRECOMMIT TESTS</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-8 01:00:00" id="9318" opendate="2015-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add UnionMerge rule on cbo path</summary>
      <description/>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-13 01:00:00" id="9353" opendate="2015-1-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make TABLE keyword optional in INSERT INTO TABLE foo...</summary>
      <description>standard SQL support INSERT INTO foo ...</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestIUD.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-1-15 01:00:00" id="9383" opendate="2015-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve schema verification error message</summary>
      <description>Currently the error message just says the schema found. It should say the schema expected as well.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-17 01:00:00" id="9402" opendate="2015-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create GREATEST and LEAST udf</summary>
      <description>GREATEST function returns the greatest value in a list of valuesSignature: T greatest(T v1, T v2, ...)all values should be the same type (like in COALESCE)LEAST returns the least value in a list of valuesSignature: T least(T v1, T v2, ...)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-17 01:00:00" id="9403" opendate="2015-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>File tests determinism with multiple reducers</summary>
      <description>If multiple reducers are used, some test result order need to be deterministic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.seqfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.register.tblfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.rcfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.seqfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.register.tblfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.rcfile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.multi.single.reducer2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.ppr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input.part2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join21.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.array.q</file>
      <file type="M">ql.src.test.queries.clientpositive.limit.partition.metadataonly.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.filter.on.outerjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.test.outer.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.transform.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ptf.matchpath.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ptf.rcfile.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ptf.register.tblfn.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ptf.seqfile.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.scriptfile1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.semijoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sort.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.transform.ppr1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.transform.ppr2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.union.ppr.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.java1.7.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-21 01:00:00" id="9438" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The standalone-jdbc jar missing some jars</summary>
      <description>The standalone-jdbc jar does not contain all the jars required for secure connections.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-21 01:00:00" id="9440" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Folders may not be pruned for Hadoop 2</summary>
      <description>HIVE-9367 is not a complete fix. It fixed for Hadoop 1. For Hadoop2, this method is not invoked.protected FileStatus[] listStatus(JobConf job) throws IOException;</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-22 01:00:00" id="9446" opendate="2015-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC DatabaseMetadata.getColumns() does not work for temporary tables</summary>
      <description>After creating temporary table, calling DatabaseMetadData.getColumns() hits error "UnknownTableException(message:default.tmp_07 table not found)"</description>
      <version>0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-1-27 01:00:00" id="9474" opendate="2015-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>truncate table changes permissions on the target</summary>
      <description>Create a table test(a string); Hive&gt; create table test(key string);Change the /user/hive/warehouse/test permission to something else other than the default, like 777.Hive&gt; dfs -chmod 777 /user/hive/warehouse/test;Hive&gt; dfs -ls -d /user/hive/warehouse/test;drwxrwxrwx - axu wheel 68 2015-01-26 18:45 /user/hive/warehouse/testThen truncate table test; Hive&gt; truncate table test;The permission goes back to the default.hive&gt; dfs -ls -d /user/hive/warehouse/test;drwxr-xr-x - axu wheel 68 2015-01-27 10:09 /user/hive/warehouse/test</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.FolderPermissionBase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-1-28 01:00:00" id="9493" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Failed job may not throw exceptions [Spark Branch]</summary>
      <description>Currently remote driver assumes exception will be thrown when job fails to run. This may not hold since job is submitted asynchronously. And we have to check the futures before we decide the job is successful.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-28 01:00:00" id="9496" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slf4j warning in hive command</summary>
      <description>Each time 'hive' command is ran, we have an Sl4J warning about multiple jars containing SL4J classes.This bug is similar to Hive-6162, but doesn't seems to be solved.Logging initialized using configuration in file:/etc/hive/conf/hive-log4j.propertiesSLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in &amp;#91;jar:file:/usr/hdp/2.2.0.0-1084/hadoop/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: Found binding in &amp;#91;jar:file:/usr/hdp/2.2.0.0-1084/hive/lib/hive-jdbc-0.14.0.2.2.0.0-1084-standalone.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type &amp;#91;org.slf4j.impl.Log4jLoggerFactory&amp;#93;</description>
      <version>0.14.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-28 01:00:00" id="9499" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.limit.query.max.table.partition makes queries fail on non-partitioned tables</summary>
      <description>If you use hive.limit.query.max.table.partition to limit the amount of partitions that can be queried it makes queries on non-partitioned tables fail.Example:CREATE TABLE tmp(test INT);SELECT COUNT(*) FROM TMP; -- works fineSET hive.limit.query.max.table.partition=20;SELECT COUNT(*) FROM TMP; -- generates NPE (FAILED: NullPointerException null)SET hive.limit.query.max.table.partition=-1;SELECT COUNT(*) FROM TMP; -- works fine again</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-2 01:00:00" id="95" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve cli error messages by lowering backtracking to 1</summary>
      <description>Stop antlr from backtracking so much should (and does) improve error messages since antlr will report the error closer to where it happened.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-29 01:00:00" id="9508" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MetaStore client socket connection should have a lifetime</summary>
      <description>Currently HiveMetaStoreClient (or SessionHMSC) is connected to one Metastore server until the connection is closed or there is a problem. I would like to introduce the concept of a MetaStore client socket life time. The MS client will reconnect if the socket lifetime is reached. This will help during rolling upgrade of Metastore.When there are multiple Metastore servers behind a VIP (load balancer), it is easy to take one server out of rotation and wait for 10+ mins for all existing connections will die down (if the lifetime is 5mins say) and the server can be updated.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-29 01:00:00" id="9509" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore partition spec validation removed by HIVE-9445</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-29 01:00:00" id="9513" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-31 01:00:00" id="9527" opendate="2015-1-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include dot files in tarball</summary>
      <description>Ideally the source tarball exactly matches the svn tag. On item that is missing is the dot files.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-9 01:00:00" id="9617" opendate="2015-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF from_utc_timestamp throws NPE if the second argument is null</summary>
      <description>UDF from_utc_timestamp throws NPE if the second argument is nullselect from_utc_timestamp('2015-02-06 10:30:00', cast(null as string));FAILED: NullPointerException null</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-1 01:00:00" id="962" opendate="2009-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"show functions" does not work with unquoted string</summary>
      <description>If the function name is provided without quotation marks, we should also show the function correctly.hive&gt; show functions substr;OKTime taken: 0.168 secondshive&gt; show functions 'substr';OKsubstrTime taken: 0.164 secondshive&gt; describe function substr;OKsubstr(str, pos[, len]) - returns the substring of str that starts at pos and is of length lenTime taken: 0.188 seconds</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-11 01:00:00" id="9652" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez in place updates should detect redirection of STDERR</summary>
      <description>Tez in place updates detects STDOUT redirection and logs using old logging method. Similarly it should detect STDERR redirection as well. This will make sure following will log using old methodhive -e '&lt;some_query&gt;' 2&gt; err.log</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-12-2 01:00:00" id="966" opendate="2009-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive command line should output log messages in 24-hour format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-12-3 01:00:00" id="968" opendate="2009-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>map join may lead to very large files</summary>
      <description>If the table under consideration is a very large file, it may lead to very large files on the mappers. The job may never complete, and the files will never be cleaned from the tmp directory. It would be great if the table can be placed in the distributed cache, but minimally the following should be added:If the table (source) being joined leads to a very big file, it should just throw an error.New configuration parameters can be added to limit the number of rows or for the size of the table.The mapper should not be tried 4 times, but it should fail immediately.I cant think of any better way for the mapper to communicate with the client, but for it to write in some well knownhdfs file - the client can read the file periodically (while polling), and if sees an error can just kill the job, but withappropriate error messages indicating to the client why the job died.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerOptions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.RecordManagerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.TransactionManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.RecordFile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.Provider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.jdbm.recman.BaseRecordManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-19 01:00:00" id="9727" opendate="2015-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GroupingID translation from Calcite</summary>
      <description>The translation from Calcite back to Hive might produce wrong results while interacting with other Calcite optimization rules.</description>
      <version>0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-19 01:00:00" id="9728" opendate="2015-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add heap mode to allocator (for q files, YARN w/o direct buffer accounting support)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.llap.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.Allocator.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.cache.LowLevelCache.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-2-23 01:00:00" id="9750" opendate="2015-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid log locks in operators</summary>
      <description>Basically wrap all LOG.xx calls in isLogXXXEnabled to avoid unnecessary locks on these calls.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-2 01:00:00" id="9831" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should use ConcurrentHashMap in ThreadFactory</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-3 01:00:00" id="9839" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks OperationHandle on async queries which fail at compile phase</summary>
      <description>Using beeline to connect to HiveServer2.And type the following:drop table if exists table_not_exists;select * from table_not_exists;There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .</description>
      <version>0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-7 01:00:00" id="9892" opendate="2015-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>various MSSQL upgrade scripts don&amp;#39;t work</summary>
      <description>Issue with GO statement when run through schematool - it results in syntax error. the create if not exists logic for PART_COL_STATS wasn't workingNO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.005-HIVE-9296.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.004-HIVE-8550.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.002-HIVE-7784.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.pre-0-upgrade-0.12.0-to-0.13.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-3-10 01:00:00" id="9909" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Specify hive branch to use on jenkins hms tests</summary>
      <description>The HMS metastore upgrade scripts work with 'trunk' branch only. We should allow to checkout any branch specified on Jenkins job in order to allow branch users test their changes.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-14 01:00:00" id="991" opendate="2009-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>union with 200 kids fail</summary>
      <description>It throws an array out of bound exception</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-10 01:00:00" id="9911" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Clean up structures and intermediate data when a query completes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.DirWatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapNodeId.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.configuration.LlapConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="9912" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Improvements to the Shuffle handler to avoid unnecessary disk scans</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapDaemonConfiguration.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="9914" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Post success comments on Jira from Jenkins metastore upgrades scripts</summary>
      <description>Currently, the HMS upgrade testing post failure comments on Jira only. We need to post success comments as well so that users know that their upgrade changes are working.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-12-18 01:00:00" id="999" opendate="2009-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sample pruning not working if denominator is not same as number of buckets</summary>
      <description>blocker for 0.5</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample6.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
</bugrepository>