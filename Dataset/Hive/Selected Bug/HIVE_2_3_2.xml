<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2017-9-1 01:00:00" id="17429" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-22 01:00:00" id="1743" opendate="2010-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-11-16 01:00:00" id="1794" opendate="2010-11-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>GenericUDFOr and GenericUDFAnd cannot receive boolean typed object</summary>
      <description>If a UDF returns Java's native boolean and passed into a logic AND or OR. The execution will break.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPAnd.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-21 01:00:00" id="18118" opendate="2017-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain Extended should indicate if a file being read is an EC file</summary>
      <description>We already print out the files Hive will read in the explain extended command, we just have to modify it to say whether or not its an EC file.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.utils.TestMetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.tblproperties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.describe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.multi.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.erasurecoding.erasure.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.table.property.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.erasure.simple.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.BasicStatsNoJobTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.Statistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-1-9 01:00:00" id="18414" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-1-26 01:00:00" id="18557" opendate="2018-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>q.outs: fix issues caused by q.out_spark files</summary>
      <description>HIVE-18061 caused some issues in yetus check by introducing q.out_spark files.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-2-1 01:00:00" id="18598" opendate="2018-2-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disallow NOT NULL constraints to be ENABLED/ENFORCED with EXTERNAL table</summary>
      <description>HIVE-16605 is enabling/enforcing NOT NULL constraint. But since Hive do not manage the data for external tables and can not enforce constraints it doesn't make sense to allow NOT NULL constraints to be enabled/enforced on external table.User can still specify RELY to signal optimizer for constraint related optimizations.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-2 01:00:00" id="18612" opendate="2018-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build subprocesses under Yetus in Ptest use 1.7 jre instead of 1.8</summary>
      <description>As per this jira comment made by Yetus maven plugins that want to use java executable are seeing a 1.7 java binary. In this particular case Yetus sets JAVA_HOME to a 1.8 JDK installation, and thus maven uses that, but any subsequent java executes will use the JRE which they see on PATH.This should be fixed by adding the proper java/bin (that of JAVA_HOME setting) to PATH.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.yetus-exec.vm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-2-6 01:00:00" id="18627" opendate="2018-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD: Handle FLOAT boxing differently for single/double precision constants</summary>
      <description>Constants like 0.1 and 0.3 are differently boxed based on intermediate precision of the compiler codepath.Disabling CBO produces 0.1BD constants which fail to box correctly to Double/Float.Enabling CBO fixes this issue, but cannot be applied all queries in Hive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2018-2-9 01:00:00" id="18674" opendate="2018-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>update Hive to use ORC 1.4.3</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-3-23 01:00:00" id="18788" opendate="2018-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up inputs in JDBC PreparedStatement</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.3,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-3-24 01:00:00" id="19042" opendate="2018-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>set MALLOC_ARENA_MAX for LLAP</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-30 01:00:00" id="19083" opendate="2018-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-3 01:00:00" id="19394" opendate="2018-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WM_TRIGGER trigger creation failed with type cast from Integer to Boolean</summary>
      <description>During testing of the new WM feature and the Hive metastore is created using Postgresql, I've discovered a bug when creating a new trigger. For exampleCREATE RESOURCE PLAN plan_1 WITH QUERY_PARALLELISM=4;CREATE POOL plan_1.slow WITH ALLOC_FRACTION=0.5, QUERY_PARALLELISM=2, SCHEDULING_POLICY='fair';ALTER POOL plan_1.default SET ALLOC_FRACTION=0.5, QUERY_PARALLELISM=2, SCHEDULING_POLICY='fifo';CREATE TRIGGER plan_1.trigger_1 WHEN S3A_BYTES_READ &gt; 268435456 DO MOVE TO slow;Right at the CREATE TRIGGER statement, an error will occurError while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Insert of object "org.apache.hadoop.hive.metastore.model.MWMTrigger@5c5ae5d8" using statement "INSERT INTO "WM_TRIGGER" ("TRIGGER_ID","ACTION_EXPRESSION","IS_IN_UNMANAGED","NAME","RP_ID","TRIGGER_EXPRESSION") VALUES (?,?,?,?,?,?)" failed : ERROR: column "IS_IN_UNMANAGED" is of type boolean but expression is of type integer Hint: You will need to rewrite or cast the expression. Position: 129) at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:543) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:729) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:749) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.apache.hadoop.hive.metastore.ObjectStore.createWMTrigger(ObjectStore.java:11218) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:97) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at com.sun.proxy.$Proxy37.createWMTrigger(Unknown Source) ~[?:?] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_wm_trigger(HiveMetaStore.java:7846) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at com.sun.proxy.$Proxy39.create_wm_trigger(Unknown Source) ~[?:?] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createWMTrigger(HiveMetaStoreClient.java:3062) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at com.sun.proxy.$Proxy40.createWMTrigger(Unknown Source) ~[?:?] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createWMTrigger(HiveMetaStoreClient.java:3062) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at com.sun.proxy.$Proxy40.createWMTrigger(Unknown Source) ~[?:?] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_151] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_151] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:2722) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] at com.sun.proxy.$Proxy40.createWMTrigger(Unknown Source) ~[?:?] at org.apache.hadoop.hive.ql.metadata.Hive.createWMTrigger(Hive.java:5048) ~[hive-exec-3.1.0-SNAPSHOT.jar:3.1.0-SNAPSHOT] ... 22 moreApparently, Postgres doesn't automatically cast int to boolean.hive=# create table example (active BOOLEAN);CREATE TABLEhive=# \d+ example; Table "public.example" Column | Type | Modifiers | Storage | Stats target | Description--------+---------+-----------+---------+--------------+------------- active | boolean | | plain | |hive=# insert into example (active) values (0);ERROR: column "active" is of type boolean but expression is of type integerLINE 1: insert into example (active) values (0); ^HINT: You will need to rewrite or cast the expression.Adding a ' quote and the insert statement will be okayhive=# insert into example (active) values ('0');INSERT 0 1hive=# select * from example; active-------- f(1 row)The fix is to change the IS_IN_UNMANAGED field in Postgres from boolean to integer (smallint) since that is what it's being done in derby schema.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-5 01:00:00" id="1962" opendate="2011-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make a libthrift.jar and libfb303.jar in dist package for backward compatibility</summary>
      <description>We have seen an internal user that relies on Hive's distribution library libthrift.jar. After the upgrade of thrift jar to 0.5.0, the jar file is renamed to thrift-0.5.0.jar and similarly for the fb303 jar. We can ask the user to change their dependency to thrift-0.5.0.jar, but later when we upgrade to a new version, the dependency breaks again. It's desirable to create a symlink in the dist/lib directory to link libthrift.jar to thrift-${thrift.version}.jar and the same for fb303.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-6-22 01:00:00" id="19649" opendate="2018-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up inputs in JDBC PreparedStatement. Add unit tests.</summary>
      <description>Add unit tests for feature that was implemented in HIVE-18788.The integration tests are present, but it will be useful to catch errors during module build.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hive.jdbc.TestHivePreparedStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-3-7 01:00:00" id="1965" opendate="2011-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Auto convert mapjoin should not throw exception if the top operator is union operator.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-20 01:00:00" id="19951" opendate="2018-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Need to disable encoded LLAP I/O for ORC when there is data type conversion (Schema Evolution)</summary>
      <description>Currently, reading encoded ORC data does not support data type conversion. So, encoded reading and cache populating needs to be disabled.</description>
      <version>None</version>
      <fixedVersion>3.1.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-30 01:00:00" id="20039" opendate="2018-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket pruning: Left Outer Join on bucketed table gives wrong result</summary>
      <description>Left outer join on bucketed table on certain cases gives wrong results.Depending on the order in which the table-scans are walked through, the FilterPruner might end up using the wrong table scan's table properties on the other table.</description>
      <version>2.3.2,3.0.0</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.FixedBucketPruningOptimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-17 01:00:00" id="20191" opendate="2018-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PreCommit patch application doesn&amp;#39;t fail if patch is empty</summary>
      <description>I've created some backport tickets to branch-3 (e.g. HIVE-20181) and made the mistake of uploading the patch files with wrong filename (. instead of - between version and branch).These get applied on master, where they're already present, since git apply with -3 won't fail if patch is already there. Tests are run on master instead of failing.I think the patch application should fail if the patch is empty and branch selection logic should probably fail too if the patch name is malformed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-19 01:00:00" id="20212" opendate="2018-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hiveserver2 in http mode emitting metric default.General.open_connections incorrectly</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-19 01:00:00" id="20213" opendate="2018-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite to 1.17.0</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.related.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.topn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.timeseries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.intervals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-16 01:00:00" id="20399" opendate="2018-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS w/a custom table location that is not fully qualified fails for MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-3-10 01:00:00" id="2040" opendate="2011-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>the retry logic in Hive&amp;#39;s concurrency is not working correctly.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObj.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockMode.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-16 01:00:00" id="20400" opendate="2018-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create table should always use a fully qualified path to avoid potential FS ambiguity</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.CLIServiceTest.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-10-29 01:00:00" id="20659" opendate="2018-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update commons-compress to 1.18 due to security issues</summary>
      <description>Currently most Hive version depends on commons-compress 1.9 or 1.4. Those versions have several security issues: https://commons.apache.org/proper/commons-compress/security-reports.htmlI propose to upgrade all commons-compress dependencies in all Hive (sub-)projects to at least 1.18. This will also make it easier for future extensions to Hive (serde, udfs, etc.) that have dependencies to commons-compress (e.g. https://github.com/zuinnote/hadoopoffice/wiki) to integrate into Hive without upgrading the commons-compress library manually in the Hive lib folder.</description>
      <version>1.2.1,2.3.2,3.1.0,3.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-10-29 01:00:00" id="20829" opendate="2018-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JdbcStorageHandler range split throws NPE</summary>
      <description>2018-10-29T06:37:14,982 ERROR [HiveServer2-Background-Pool: Thread-44466]: operation.Operation (:()) - Error running hive query:org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 at org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:335) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:228) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.access$700(SQLOperation.java:87) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:318) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_161] at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_161] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) ~[hadoop-common-3.1.1.3.0.3.0-150.jar:?] at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:338) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_161] at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_161] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_161] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_161]Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Vertex failed, vertexName=Map 1, vertexId=vertex_1540588928441_0121_2_00, diagnostics=[Vertex vertex_1540588928441_0121_2_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: employees initializer failed, vertex=vertex_1540588928441_0121_2_00 [Map 1], java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.initialize(HiveSplitGenerator.java:272) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269) at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1540588928441_0121_2_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1540588928441_0121_2_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:240) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:210) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:97) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:2707) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:2378) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:2054) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1752) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1746) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hadoop.hive.ql.reexec.ReExecDriver.run(ReExecDriver.java:157) ~[hive-exec-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:226) ~[hive-service-3.1.0.3.0.3.0-150.jar:3.1.0.3.0.3.0-150] ... 13 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputSplit.java</file>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-1-3 01:00:00" id="21082" opendate="2019-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In HPL/SQL, declare statement does not support variable of type character</summary>
      <description>In the following HPL/SQL programs:DECLARE a character(5); SET a = 'b';when the type of variable 'a' is CHARACTER, it cannot be assigned a value successfully. The support for the character type should be added to DECLARE statement.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-5-2 01:00:00" id="2142" opendate="2011-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jobs do not get killed even when they created too many files.</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-4-17 01:00:00" id="21621" opendate="2019-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Kafka Clients to recent release 2.2.0</summary>
      <description>all in the title update Kafka Storage Handler to the most recent clients library.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.test.org.apache.hadoop.hive.kafka.KafkaBrokerResource.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.HiveKafkaProducer.java</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-14 01:00:00" id="21874" opendate="2019-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement add partitions related methods on temporary table</summary>
      <description>IMetaStoreClient exposes the following add partition related methods:Partition add_partition(Partition partition);int add_partitions(List&lt;Partition&gt; partitions);int add_partitions_pspec(PartitionSpecProxy partitionSpec);List&lt;Partition&gt; add_partitions(List&lt;Partition&gt; partitions, boolean ifNotExists, boolean needResults);These methods should be implemented in order to handle addition of partitions to temporary tables.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitionsFromPartSpec.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAddPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-9-14 01:00:00" id="21875" opendate="2019-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement drop partition related methods on temporary tables</summary>
      <description>IMetaStoreClient exposes the following methods related to dropping partitions:boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, boolean deleteData);boolean dropPartition(String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);boolean dropPartition(String catName, String db_name, String tbl_name, List&lt;String&gt; part_vals, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, boolean deleteData, boolean ifExists, boolean needResults);List&lt;Partition&gt; dropPartitions(String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);List&lt;Partition&gt; dropPartitions(String catName, String dbName, String tblName, List&lt;ObjectPair&lt;Integer, byte[]&gt;&gt; partExprs, PartitionDropOptions options);boolean dropPartition(String db_name, String tbl_name, String name, boolean deleteData);boolean dropPartition(String catName, String db_name, String tbl_name, String name, boolean deleteData)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestDropPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-3 01:00:00" id="2191" opendate="2011-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow optional [inner] on equi-join.</summary>
      <description>Lot's of databases including mysql support an optional "inner" keyword to explicitely select an equi-join.As shown in the mysql docs: http://dev.mysql.com/doc/refman/5.1/en/join.htmlFor completeness/portability we should allow this.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">docs.xdocs.language.manual.joins.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-7-21 01:00:00" id="21911" opendate="2019-6-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pluggable LlapMetricsListener on Tez side to disable / resize Daemons</summary>
      <description>We need to have a way to plug in different listeners which act upon the LlapDaemon statistics.This listener should be able to disable / resize the LlapDaemons based on health data.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.metrics.TestLlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.metrics.LlapMetricsCollector.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-7-10 01:00:00" id="21979" opendate="2019-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestReplication tests time out regularily</summary>
      <description>I think we should add TestTableLevelReplicationScenarios and friends to be executed in isolationfrom a recent ptest execution:[INFO] Running org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 150.413 s - in org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.084 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.062 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Running org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 57.568 s - in org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 348.769 s - in org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Running org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.089 s - in org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Running org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,044.666 s - in org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 225.734 s - in org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplAcrossInstancesWithJsonMessageFormat</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-18 01:00:00" id="22659" opendate="2019-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JClouds needs to be updated to 2.1.3 in ptest</summary>
      <description>Since a couple of days ptest responded 404 to test queries coming in from jenkins side.I took a look into the issue and saw this exception on hiveptest-server-upstream side:Caused by: java.lang.IllegalStateException: zone https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-d not present in [https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-fhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-a] Debugging into the issue, it seems like that when we we put our cloud context in place for test execution, some default values originating from default templates are matched with actual GCP capabilities - and I guess zone us-central-1d was decommissioned in real-life, hence the exception.I upgraded jclouds version from 2.1.0 to 2.1.3 on server side and retried running Tomcat with this new installation. It seems to have fixed the issue. NO PRECOMMIT TESTS </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>