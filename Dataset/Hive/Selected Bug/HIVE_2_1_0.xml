<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  <bug fixdate="2015-2-13 01:00:00" id="12165" opendate="2015-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong result when hive.optimize.sampling.orderby=true with some aggregate functions</summary>
      <description>This simple query give wrong result , when , i use the parallel order .select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;Current wrong result :c0 c1 c2 c332740 32740 0 163695113172 113172 163700 72955554088 54088 729560 999995Right result :c0 c1 c2 c31000000 1000000 0 999999The sql script for my test drop table foobar_1 ;create table foobar_1 ( dummyint int , dummystr string ) ;insert into table foobar_1 select count(*),'dummy 0' from foobar_1 ;drop table foobar_1M ;create table foobar_1M ( dummyint bigint , dummystr string ) ;insert overwrite table foobar_1M select val_int , concat('dummy ',val_int) from ( select ((((((d_1*10)+d_2)*10+d_3)*10+d_4)*10+d_5)*10+d_6) as val_int from foobar_1 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_1 as d_1 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_2 as d_2 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_3 as d_3 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_4 as d_4 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_5 as d_5 lateral view outer explode(split("0,1,2,3,4,5,6,7,8,9",",")) tbl_6 as d_6 ) as f ;set hive.optimize.sampling.orderby.number=10000;set hive.optimize.sampling.orderby.percent=0.1f;set mapreduce.job.reduces=3 ;set hive.optimize.sampling.orderby=false;select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;set hive.optimize.sampling.orderby=true;select count(*) , count(distinct dummyint ) , min(dummyint),max(dummyint) from foobar_1M ;</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.SamplingOptimizer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-8 01:00:00" id="12614" opendate="2015-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RESET command does not close spark session</summary>
      <description/>
      <version>1.3.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ResetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-9 01:00:00" id="12628" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate flakiness in TestMetrics</summary>
      <description>TestMetrics relies on timing of json file dumps. Rewrite these tests to eliminate flakiness.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.session.TestSessionManagerMetrics.java</file>
      <file type="M">service.pom.xml</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.zookeeper.TestZookeeperLockManager.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStorePartitionSpecs.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.TestHs2Metrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMetrics.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreMetrics.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.metrics.MetricsTestUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.metrics.metrics2.CodahaleMetrics.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-9 01:00:00" id="12632" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: don&amp;#39;t use IO elevator for ACID tables</summary>
      <description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-9 01:00:00" id="12633" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: package included serde jars</summary>
      <description>Some SerDes like JSONSerde are not packaged with LLAP. One cannot localize jars on the daemon (due to security consideration if nothing else), so we should package them.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapOptionsProcessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-10 01:00:00" id="12648" opendate="2015-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP IO was disabled in CliDriver by accident (and tests are broken)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HdfsUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseQTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.accumulo.AccumuloQTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestLocationQueries.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-16 01:00:00" id="12693" opendate="2015-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Use Slider Anti-Affinity scheduling mode for daemon distribution</summary>
      <description>Slider has SLIDER-82 which adds anti-affinity placement policies for containers, to avoid colliding on to the same machine when deploying LLAP instances.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-16 01:00:00" id="12694" opendate="2015-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Slider destroy semantics require force</summary>
      <description>2015-12-16 20:10:55,118 [main] ERROR main.ServiceLauncher - Destroy will permanently delete directories and registries. Reissue this command with the --force option if you want to proceed.NO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-1-30 01:00:00" id="12765" opendate="2015-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Intersect (distinct/all) Except (distinct/all) Minus (distinct/all)</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBExpr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSortLimitPullUpConstantsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-1-7 01:00:00" id="12805" opendate="2016-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver skewjoin.q failure</summary>
      <description>Set hive.cbo.returnpath.hiveop=trueFROM T1 a FULL OUTER JOIN T2 c ON c.key+1=a.key SELECT /*+ STREAMTABLE(a) */ sum(hash(a.key)), sum(hash(a.val)), sum(hash(c.key))The stack trace:java.lang.IndexOutOfBoundsException: Index: 1, Size: 1 at java.util.ArrayList.rangeCheck(ArrayList.java:635) at java.util.ArrayList.get(ArrayList.java:411) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate$JoinSynthetic.process(SyntheticJoinPredicate.java:183) at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:105) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:89) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:43) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.PreOrderOnceWalker.walk(PreOrderOnceWalker.java:54) at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:120) at org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.transform(SyntheticJoinPredicate.java:100) at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:236) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10170) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:231) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:237) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:471)Same error happens in auto_sortmerge_join_6.q.out for select count(*) FROM tbl1 a JOIN tbl2 b ON a.key = b.key join src h on h.value = a.value</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinToMultiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-8 01:00:00" id="12809" opendate="2016-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fast-path for coalesce if input.noNulls = true</summary>
      <description>Coalesce can skip processing other columns, if all the input columns are non-null.Possibly retaining, isRepeating=true.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorCoalesce.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-8 01:00:00" id="12820" opendate="2016-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the check if carriage return and new line are used for separator or escape character</summary>
      <description>The change in HIVE-11785 doesn't allow \r or \n to be used as separator or escape character which may break some existing tables which uses \r as separator or escape character e.g..This case actually can be supported regardless of SERIALIZATION_ESCAPE_CRLF set or not.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-9 01:00:00" id="12826" opendate="2016-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: fix VectorUDAF* suspect isNull checks</summary>
      <description>for isRepeating=true, checking isNull[selected&amp;#91;i&amp;#93;] might return incorrect results (without a heavy array fill of isNull).VectorUDAFSum/Min/Max/Avg and SumDecimal impls need to be reviewed for this pattern. private void iterateHasNullsRepeatingSelectionWithAggregationSelection( VectorAggregationBufferRow[] aggregationBufferSets, int aggregateIndex, &lt;ValueType&gt; value, int batchSize, int[] selection, boolean[] isNull) { for (int i=0; i &lt; batchSize; ++i) { if (!isNull[selection[i]]) { Aggregation myagg = getCurrentAggregationBuffer( aggregationBufferSets, aggregateIndex, i); myagg.sumValue(value); } } }</description>
      <version>1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-9 01:00:00" id="12827" opendate="2016-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: VectorCopyRow/VectorAssignRow/VectorDeserializeRow assign needs explicit isNull[offset] modification</summary>
      <description>Some scenarios do set Double.NaN instead of isNull=true, but all types aren't consistent.Examples of un-set isNull for the valid values are private class FloatReader extends AbstractDoubleReader { FloatReader(int columnIndex) { super(columnIndex); } @Override void apply(VectorizedRowBatch batch, int batchIndex) throws IOException { DoubleColumnVector colVector = (DoubleColumnVector) batch.cols[columnIndex]; if (deserializeRead.readCheckNull()) { VectorizedBatchUtil.setNullColIsNullValue(colVector, batchIndex); } else { float value = deserializeRead.readFloat(); colVector.vector[batchIndex] = (double) value; } } } private class DoubleCopyRow extends CopyRow { DoubleCopyRow(int inColumnIndex, int outColumnIndex) { super(inColumnIndex, outColumnIndex); } @Override void copy(VectorizedRowBatch inBatch, int inBatchIndex, VectorizedRowBatch outBatch, int outBatchIndex) { DoubleColumnVector inColVector = (DoubleColumnVector) inBatch.cols[inColumnIndex]; DoubleColumnVector outColVector = (DoubleColumnVector) outBatch.cols[outColumnIndex]; if (inColVector.isRepeating) { if (inColVector.noNulls || !inColVector.isNull[0]) { outColVector.vector[outBatchIndex] = inColVector.vector[0]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } else { if (inColVector.noNulls || !inColVector.isNull[inBatchIndex]) { outColVector.vector[outBatchIndex] = inColVector.vector[inBatchIndex]; } else { VectorizedBatchUtil.setNullColIsNullValue(outColVector, outBatchIndex); } } } } private static abstract class VectorDoubleColumnAssign extends VectorColumnAssignVectorBase&lt;DoubleColumnVector&gt; { protected void assignDouble(double value, int destIndex) { outCol.vector[destIndex] = value; } }The pattern to imitate would be the earlier code from VectorBatchUtil case DOUBLE: { DoubleColumnVector dcv = (DoubleColumnVector) batch.cols[offset + colIndex]; if (writableCol != null) { dcv.vector[rowIndex] = ((DoubleWritable) writableCol).get(); dcv.isNull[rowIndex] = false; } else { dcv.vector[rowIndex] = Double.NaN; setNullColIsNullValue(dcv, rowIndex); } } break;</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-9 01:00:00" id="12828" opendate="2016-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Spark version to 1.6</summary>
      <description/>
      <version>None</version>
      <fixedVersion>spark-branch,2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">pom.xml</file>
      <file type="M">data.conf.spark.yarn-client.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-11 01:00:00" id="12836" opendate="2016-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Install wget &amp; curl packages on LXC containers for HMS upgrade tests</summary>
      <description>Install wget &amp; curl packages on LXC containers for HMS upgrade tests.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-15 01:00:00" id="12880" opendate="2016-1-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>spark-assembly causes Hive class version problems</summary>
      <description>It looks like spark-assembly contains versions of Hive classes (e.g. HiveConf), and these sometimes (always?) come from older versions of Hive.We've seen problems where depending on classpath perturbations, NoSuchField errors may be thrown for recently added ConfVars because the HiveConf class comes from spark-assembly.Would making sure spark-assembly comes last in the classpath solve the problem?Otherwise, can we depend on something that does not package older Hive classes?Currently, HIVE-12179 provides a workaround (in non-Spark use case, at least; I am assuming this issue can also affect Hive-on-Spark).</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-19 01:00:00" id="12889" opendate="2016-1-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support COUNT(DISTINCT) for partitioning query.</summary>
      <description>We need to support avg(distinct), count(distinct), sum(distinct) for the parent jira HIVE-9534. Separate the work for count(distinct) in this subtask.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PTFInvocationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.PlanModifierForASTConv.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlSumAggFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.functions.HiveSqlCountAggFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-6 01:00:00" id="129" opendate="2008-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix aux.jar packaging to work properly with 0.17 and 0.18 versions of hadoop</summary>
      <description>ant -lib testlibs -Dhadoop.version="0.17" clean-test testleads to failures ininput16.qinput16_cc.qinput3.qas TestSerDe cannot be located.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.gen-php.ThriftHive.php</file>
      <file type="M">service.src.gen-javabean.org.apache.hadoop.hive.service.ThriftHive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-22 01:00:00" id="12911" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PPD might get exercised even when flag is false if CBO is on</summary>
      <description>Introduced in HIVE-11865.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-2-28 01:00:00" id="12958" opendate="2016-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make embedded Jetty server more configurable</summary>
      <description>Currently you can't configure embedded jetty within HCatalog. Propose to support an xml configuration which Jetty already supports. A new Web-hcat property will be added to specify the configure file location. If the file doesn't exist, falls back to old behavior. If it exists, such configuration will be loaded to configure embedded Jetty server. Some default parameters for Jetty may not be sufficient for some cases such as request/response buffer size. This improvement allows to make such change.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Main.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-29 01:00:00" id="12964" opendate="2016-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestOperationLoggingAPIWithMr,TestOperationLoggingAPIWithTez fail on branch-2.0 (with Java 7, at least)</summary>
      <description/>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.LogDivertAppender.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-29 01:00:00" id="12965" opendate="2016-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite local directory should perserve the overwritten directory permission</summary>
      <description>In Hive, "insert overwrite local directory" first deletes the overwritten directory if exists, recreate a new one, then copy the files from src directory to the new local directory. This process sometimes changes the permissions of the to-be-overwritten local directory, therefore causing some applications no more to be able to access its content.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-2-5 01:00:00" id="13015" opendate="2016-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bundle Log4j2 jars with hive-exec</summary>
      <description>In some of the recent test runs, we are seeing multiple bindings for SLF4j that causes issues with LOG4j2 logger. SLF4J: Found binding in [jar:file:/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1454694331819_0001/container_e06_1454694331819_0001_01_000002/app/install/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]We have added explicit exclusions for slf4j-log4j12 but some library is pulling it transitively and it's getting packaged with hive libs. Also hive currently uses version 1.7.5 for slf4j. We should add dependency convergence for sl4fj and also remove packaging of slf4j-log4j12.*.jar</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-2-9 01:00:00" id="13032" opendate="2016-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive services need HADOOP_CLIENT_OPTS for proper log4j2 initialization</summary>
      <description>HIVE-12497 removed HADOOP_CLIENT_OPTS as it slowed down cli launch time. But it leads to log4j2 not being initialized when using services other than CLI. Other services like metastore, schematool etc. rely on log4j to initialize the logging based on the presence of log4j2.properties file in the classpath. If we use the standard name for log4j configuration file (log4j2.properties) then automatic initialization will happen. If not, we have to tell log4j to look for specific properties file. This is done via -Dlog4j.configurationFile system property. If we pass this system property via HADOOP_CLIENT_OPTS then all hive services will have logging initialized properly. In HIVE-12497, the problem was we had HADOOP_CLIENT_OPTS at the top of the script. As a result, hadoop and hbase commands tries to initialize logging which took long time slowing down the startup time.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.schemaTool.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-10 01:00:00" id="13034" opendate="2016-2-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add jdeb plugin to build debian</summary>
      <description>It would be nice to also generate a debian as a part of build. This can be done by adding jdeb plugin to dist profile.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-11 01:00:00" id="13043" opendate="2016-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reload function has no impact to function registry</summary>
      <description>With HIVE-2573, users should run "reload function" to refresh cached function registry. However, "reload function" has no impact at all. We need to fix this. Otherwise, HS2 needs to be restarted to see new global functions.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.HiveCommand.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-2-12 01:00:00" id="13054" opendate="2016-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: disable permanent fns by default (for now)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-17 01:00:00" id="13069" opendate="2016-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable cartesian product merging</summary>
      <description>Currently we can merge 2-way joins into n-way joins when the joins are executed over the same column.In turn, CBO might produce plans containing cartesian products if the join columns are constant values; after HIVE-12543 went in, this is rather common, as those constant columns are correctly pruned. However, currently we do not merge a cartesian product with two inputs into a cartesian product with multiple inputs, which could result in performance loss.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.coltype.literals.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketizedhiveinputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-17 01:00:00" id="13070" opendate="2016-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Precommit HMS tests should run in addition to precommit normal tests, not instead of</summary>
      <description>When a certain patch makes changes in the metastore upgrade scripts folder, precommit HMS tests are triggered. The problem is that precommit HMS marks the patch as tested, thus normal precommit tests are never triggered.I hit the issue while testing HIVE-12994.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-17 01:00:00" id="13077" opendate="2016-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Scrub daemon-site.xml from client configs</summary>
      <description>if (llapMode) { // add configs for llap-daemon-site.xml + localize llap jars // they cannot be referred to directly as it would be a circular depedency conf.addResource("llap-daemon-site.xml");</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-18 01:00:00" id="13079" opendate="2016-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Allow reading log4j properties from default JAR resources</summary>
      <description>If the log4j2 configuration is not overriden by the user, the Slider pkg creation fails since the config is generated from a URL.Allow for the .properties file to be created from default JAR resources if user provides no overrides.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-18 01:00:00" id="13087" opendate="2016-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Print STW pause time and useful application time</summary>
      <description>The daemons currently prints GC details. It will be useful to print the total useful time application spent and the total time for which application threads are stopped.Need to add-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTimeto get something likeApplication time: 0.3440086 secondsTotal time for which application threads were stopped: 0.0620105 secondsApplication time: 0.2100691 secondsTotal time for which application threads were stopped: 0.0890223 secondsReference: https://plumbr.eu/blog/performance-blog/logging-stop-the-world-pauses-in-jvmNO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-19 01:00:00" id="13095" opendate="2016-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support view column authorization</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-19 01:00:00" id="13096" opendate="2016-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cost to choose side table in MapJoin conversion based on cumulative cardinality</summary>
      <description>HIVE-11954 changed the logic to choose the side table in the MapJoin conversion algorithm. Initial heuristic for the cost was based on number of heavyweight operators.This extends that work so the heuristic is based on accumulate cardinality. In the future, we should choose the side based on total latency for the input.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-19 01:00:00" id="13097" opendate="2016-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Umbrella] Changes dependent on Tez 0.8.3</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-11-14 01:00:00" id="1310" opendate="2010-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Partitioning columns should be of primitive types only</summary>
      <description>If the user specify the partitioning column as complex type (e.g., array, map) we should throw an error in semantic analyzer.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-19 01:00:00" id="13100" opendate="2016-2-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-13015 that bundles log4j2 jars in hive-exec.jar</summary>
      <description>HIVE-13015 bundles log4j2 jars in hive-exec. This is causing other systems like tez and pig that uses log4j1.x to fail initialization due to the incompatibility in properties based configuration.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-20 01:00:00" id="13107" opendate="2016-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Rotate GC logs periodically to prevent full disks</summary>
      <description>STDOUT cannot be rotated easily, so log GC logs to a different file and rotate periodically with -XX:+UseGCLogFileRotationNO PRECOMMIT TESTS</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-20 01:00:00" id="13110" opendate="2016-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Package log4j2 jars into Slider pkg</summary>
      <description>This forms the alternative path for HIVE-13015 (reverted), so that HIVE-13027 can pick up the right logger impl always.</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-22 01:00:00" id="13111" opendate="2016-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix timestamp / interval_day_time wrong results with HIVE-9862</summary>
      <description>Fix timestamp / interval_day_time issues discovered when testing the Vectorized Text patch.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareLongDoubleScalar.txt</file>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.common.type.TestPisaTimestamp.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.ColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.RandomTypeUtil.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.PisaTimestamp.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.io.TimestampWritable.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestInputOutputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorRowBatchFromObjectIterables.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorExpressionWriters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.DateTimeMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSerializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupKeyHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExtractRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorExpressionDescriptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorDeserializeRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorCopyRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnSetInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnAssignFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAssignRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.udf.VectorUDFAdaptor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.NullUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalarBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprTimestampColumnColumnBase.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeScalarColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.IfExprIntervalDayTimeColumnColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateScalarSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DateColSubtractDateColumn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToIntervalDayTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastMillisecondsLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDoubleToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFVarPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdSampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFStdPopTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgTimestamp.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxTimestamp.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.DateUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateColumnArithmeticTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DateScalarArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeColumnCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeColumnCompareIntervalDayTimeScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterIntervalDayTimeScalarCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterLongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnBetween.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampColumnCompareTimestampScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareLongDoubleColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterTimestampScalarCompareTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeColumnCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeColumnCompareIntervalDayTimeScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalDayTimeScalarCompareIntervalDayTimeColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.IntervalYearMonthScalarArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleColumnCompareTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.LongDoubleScalarCompareTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticDateScalarBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticIntervalYearMonthScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumn.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampColumnBase.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalar.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.TimestampColumnArithmeticTimestampScalarBase.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-23 01:00:00" id="13122" opendate="2016-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: simple Model/View separation for UI</summary>
      <description>The current LLAP UI in master uses a fixed loop to both extract data and to display it in the same loop.Split this up into a model-view, for modularity.NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.js.metrics.js</file>
      <file type="M">llap-server.src.main.resources.hive-webapps.llap.index.html</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-7-1 01:00:00" id="13191" opendate="2016-3-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DummyTable map joins mix up columns between tables</summary>
      <description>SELECT a.key, a.a_one, b.b_one, a.a_zero, b.b_zeroFROM( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero) aLEFT JOIN( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero) bON a.key = b.key;11 1 0 0 1This should be 11, 1, 1, 0, 0 instead. Disabling map-joins &amp; using shuffle-joins returns the right result.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mapjoin2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-3-9 01:00:00" id="13246" opendate="2016-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add log line to ORC writer to print out the file path</summary>
      <description>Currently ORC writer does not log anything making it difficult find where the destination path is. NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">orc.src.java.org.apache.orc.impl.WriterImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-9 01:00:00" id="13248" opendate="2016-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change date_add/date_sub/to_date functions to return Date type rather than String</summary>
      <description>Some of the original "date" related functions return string values rather than Date values, because they were created before the Date type existed in Hive. We can try to change these to return Date in the 2.x line.Date values should be implicitly convertible to String.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.offcbo.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDateAdd.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFDate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateSub.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDateAdd.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToDate.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2016-4-21 01:00:00" id="13320" opendate="2016-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Apply HIVE-11544 to explicit conversions as well as implicit ones</summary>
      <description>Parsing 1 million blank values through cast(x as int) is 3x slower than parsing a valid single digit.</description>
      <version>1.2.1,1.3.0,2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToShort.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-21 01:00:00" id="13324" opendate="2016-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: history log for FRAGMENT_START doesn&amp;#39;t log DagId correctly</summary>
      <description>$ grep -B 1 "TaskId=212" history.log Event=FRAGMENT_START, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000213, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=0, VertexName=Map 1, TaskId=212, TaskAttemptId=0, SubmitTime=1457493007357--Event=FRAGMENT_END, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000213, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=2, VertexName=Map 1, TaskId=212, TaskAttemptId=0, ThreadName=Task-Executor-1, Succeeded=true, StartTime=1457493007358, EndTime=1457493011916--Event=FRAGMENT_START, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000434, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=0, VertexName=Map 1, TaskId=212, TaskAttemptId=0, SubmitTime=1457493023131--Event=FRAGMENT_END, HostName=..., ApplicationId=application_1455662455106_2695, ContainerId=container_222212222_2695_01_000434, DagName=selectsum(l_extendedprice * l_discount...25(Stage-1), DagId=3, VertexName=Map 1, TaskId=212, TaskAttemptId=0, ThreadName=Task-Executor-2, Succeeded=true, StartTime=1457493023132, EndTime=1457493024695etc. It's always 0.</description>
      <version>None</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-22 01:00:00" id="13327" opendate="2016-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SessionID added to HS2 threadname does not trim spaces</summary>
      <description>HIVE-13153 introduced off-by-one in appending spaces to thread names. NO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-23 01:00:00" id="13340" opendate="2016-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: from_unixtime UDF shim</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-23 01:00:00" id="13342" opendate="2016-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging in llap decider and throw exception in case llap mode is all but we cannot run in llap.</summary>
      <description>Currently we do not log our decisions with respect to llap. Are we running everything in llap mode or only parts of the plan. We need more logging. Also, if llap mode is all but for some reason, we cannot run the work in llap mode, fail and throw an exception advise the user to change the mode to auto.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.udf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.llap.udf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-23 01:00:00" id="13343" opendate="2016-3-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need to disable hybrid grace hash join in llap mode except for dynamically partitioned hash join</summary>
      <description>Due to performance reasons, we should disable use of hybrid grace hash join in llap when dynamic partition hash join is not used. With dynamic partition hash join, we need hybrid grace hash join due to the possibility of skews.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.result.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.hybridgrace.hashjoin.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-4-26 01:00:00" id="13365" opendate="2016-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change the MiniLLAPCluster to work with a MiniZKCluster</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.MiniLlapCluster.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.llap.LlapItUtils.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-3-28 01:00:00" id="13367" opendate="2016-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extending HPLSQL parser</summary>
      <description>Extending HPL/SQL parser to support more procedural constructs.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.local.interval.out.txt</file>
      <file type="M">hplsql.src.test.results.local.if.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.procedure.no.params.out.txt</file>
      <file type="M">hplsql.src.test.results.local.create.package.out.txt</file>
      <file type="M">hplsql.src.test.results.db.rowtype.attribute.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.return.cursor2.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.return.cursor.out.txt</file>
      <file type="M">hplsql.src.test.results.db.create.procedure.mssql.out.txt</file>
      <file type="M">hplsql.src.test.queries.local.interval.sql</file>
      <file type="M">hplsql.src.test.queries.local.if.sql</file>
      <file type="M">hplsql.src.test.queries.db.schema.sql</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Row.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Package.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionString.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.File.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Converter.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conn.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Conf.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-4-31 01:00:00" id="13400" opendate="2016-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Following up HIVE-12481, add retry for Zookeeper service discovery</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-31 01:00:00" id="13402" opendate="2016-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Temporarily disable failing spark tests</summary>
      <description>There's a bunch of tests which end up hanging causing the precommit builds to run forever.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-4-6 01:00:00" id="13438" opendate="2016-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a service check script for llap</summary>
      <description>We want to have a test script that can be run by an installer such as ambari that makes sure that the service is up and running.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-9 01:00:00" id="13469" opendate="2016-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Support delayed scheduling for locality</summary>
      <description>LLAP currently supports forcing locality. Change this to support a time based delay for locality as well.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-tez.src.test.org.apache.hadoop.hive.llap.tezplugins.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.ContainerFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-11 01:00:00" id="13475" opendate="2016-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow aggregate functions in over clause</summary>
      <description>Support to reference aggregate functions within the over clause needs to be added. For instance, currently the following query will fail:select rank() over (order by sum(ws.c_int)) as return_rankfrom cbo_t3 wsgroup by ws.key;</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-11 01:00:00" id="13485" opendate="2016-4-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Session id appended to thread name multiple times.</summary>
      <description>HIVE-13153 addressed a portion of this issue. Follow up from there.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-4-12 01:00:00" id="13487" opendate="2016-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Finish time is wrong when perflog is missing SUBMIT_TO_RUNNING</summary>
      <description>Sometimes PerfLog misses SUBMIT_TO_RUNNING end time which makes finish time to be wrong. Like belowCompile Query 0.60sPrepare Plan 0.44sSubmit Plan 0.83sStart 0.00sFinish 1460423234.44sNO PRECOMMIT TESTS</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-5-14 01:00:00" id="13516" opendate="2016-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding BTEQ .IF, .QUIT, ERRORCODE to HPL/SQL</summary>
      <description>Adding Teradata BTEQ features to HPL/SQL such as .IF, .QUIT, ERRORCODE.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.db2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.pg.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.ora2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.ora.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mysql.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mssql2.out.txt</file>
      <file type="M">hplsql.src.test.results.offline.create.table.mssql.out.txt</file>
      <file type="M">hplsql.src.test.results.local.lang.out.txt</file>
      <file type="M">hplsql.src.test.results.db.select.into2.out.txt</file>
      <file type="M">hplsql.src.test.results.db.select.into.out.txt</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlOffline.java</file>
      <file type="M">hplsql.src.test.java.org.apache.hive.hplsql.TestHplsqlLocal.java</file>
      <file type="M">hplsql.src.main.resources.hplsql-site.xml</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Stmt.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Signal.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Select.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Exec.java</file>
      <file type="M">hplsql.src.main.antlr4.org.apache.hive.hplsql.Hplsql.g4</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-18 01:00:00" id="13537" opendate="2016-4-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update slf4j version to 1.7.10</summary>
      <description>Both Hadoop and Tez are on 1.7.10, and have been for a while. We should update hive to use this version as well. Should get rid of some of the noise in the logs related to multiple version.s</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-1-20 01:00:00" id="13554" opendate="2016-4-20 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>[Umbrella] SQL:2011 compliance</summary>
      <description>There are various gaps in language which needs to be addressed to bring Hive under SQL:2011 compliance</description>
      <version>2.1.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.subquery.subquery.chain.exists.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.subquery.in.lhs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.select.expression.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestSQL11ReservedKeyWordsNegative.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeSubQueryDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBSubQuery.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-20 01:00:00" id="13555" opendate="2016-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add nullif udf</summary>
      <description>nullif(exp1, exp2) is shorthand for: case when exp1 = exp2 then null else exp1</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-20 01:00:00" id="13556" opendate="2016-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for double precision data type</summary>
      <description>Add support for DOUBLE PRECISION data type as an alias for DOUBLE datatype</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-20 01:00:00" id="13557" opendate="2016-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make interval keyword optional while specifying DAY in interval arithmetic</summary>
      <description>Currently we support expressions like: WHERE SOLD_DATE BETWEEN ((DATE('2000-01-31')) - INTERVAL '30' DAY) AND DATE('2000-01-31')We should support:WHERE SOLD_DATE BETWEEN ((DATE('2000-01-31')) + (-30) DAY) AND DATE('2000-01-31')</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-4-22 01:00:00" id="13591" opendate="2016-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestSchemaTool is failing on master</summary>
      <description>Not sure at what point this started to fail.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.1.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-4-25 01:00:00" id="13603" opendate="2016-4-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ptest unit tests broken by HIVE13505</summary>
      <description>HIVE-13505 broke some unit tests in the ptest2 framework, which need to be fixed.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-26 01:00:00" id="13616" opendate="2016-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate renaming a table without invalidating the column stats</summary>
      <description>Right now when we rename a table, we clear the column stats rather than updating it (HIVE-9720) since ObjectStore uses DN to talk to DB. Investigate the possibility that if we can achieve updating the stats without rescanning the whole table.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-26 01:00:00" id="13619" opendate="2016-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucket map join plan is incorrect</summary>
      <description>Same as HIVE-12992. Missed a single line check. TPCDS query 4 with bucketing can produce this issue.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-21 01:00:00" id="1362" opendate="2010-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimizer statistics on columns in tables and partitions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ql.if.queryplan.thrift</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.cpp</file>
      <file type="M">ql.src.gen.thrift.gen-cpp.queryplan.types.h</file>
      <file type="M">ql.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.ql.plan.api.StageType.java</file>
      <file type="M">ql.src.gen.thrift.gen-php.queryplan.queryplan.types.php</file>
      <file type="M">ql.src.gen.thrift.gen-py.queryplan.ttypes.py</file>
      <file type="M">ql.src.gen.thrift.gen-rb.queryplan.types.rb</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.index.RewriteParseContextGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-28 01:00:00" id="13637" opendate="2016-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fold CASE into NVL when CBO optimized the plan</summary>
      <description>After HIVE-13068 goes in, folding CASE into NVL got disabled when CBO has optimized the plan, as it was done by ConstantPropagate in Hive. We need to enable it back.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.constantPropWhen.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.JoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-5-2 01:00:00" id="13671" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add PerfLogger to log4j2.properties logger</summary>
      <description>To enable perflogging, root logging has to be set to DEBUG. Provide a way to to independently configure perflogger and root logger levels.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.main.resources.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-2 01:00:00" id="13672" opendate="2016-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use loginUser from UGI to get llap user when generating LLAP splits.</summary>
      <description>HIVE-13389 used RegistryUtils.currentUser() to get the llap user name when generating LLAP splits. However it looks like this will return the client username, while we really want to get the hive/llap user.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-3 01:00:00" id="13675" opendate="2016-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add HMAC signatures to LLAPIF splits</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.security.LlapSignerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.tez.TestConverters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SigningSecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapSigner.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.SubmitWorkInfo.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.security.LlapTokenLocalClient.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapProxy.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-3 01:00:00" id="13683" opendate="2016-5-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove erroneously included patch file</summary>
      <description>The commit for HIVE-13509 accidentally included a patch file in the source tree.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">HIVE-13509.2.patch</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-6 01:00:00" id="13705" opendate="2016-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert into table removes existing data</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.META-INF.services.org.apache.hadoop.fs.FileSystem</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-10 01:00:00" id="13730" opendate="2016-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid double spilling the same partition when memory threshold is set very low</summary>
      <description>I am seeing hybridgrace_hashjoin_1.q getting stuck on master.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-10 01:00:00" id="13731" opendate="2016-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: return LLAP token with the splits</summary>
      <description>Need to return the token with the splits, then take it in LLAPIF and make sure it's used when talking to LLAP</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapInputSplit.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.llap.ext.TestLlapInputSplit.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-9-1 01:00:00" id="1378" opendate="2010-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return value for map, array, and struct needs to return a string</summary>
      <description>In order to be able to select/display any data from JDBC Hive driver, return value for map, array, and struct needs to return a string</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchTask.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.JdbcColumn.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveBaseResultSet.java</file>
      <file type="M">data.scripts.input20.script</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-23 01:00:00" id="13818" opendate="2016-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fast Vector MapJoin Long hashtable has to handle all integral types</summary>
      <description>Changes for HIVE-13682 did fix a bug in Fast Hash Tables, but evidently not this issue according to Gopal/Rajesh/Nita.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorMapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-23 01:00:00" id="13823" opendate="2016-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary log line in common join operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-24 01:00:00" id="13827" opendate="2016-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAPIF: authentication on the output channel</summary>
      <description>The current thinking is that we'd send the token. There's no protocol on the channel right now.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFGetSplits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormatService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapOutputFormat.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.tez.Converters.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.SecretManager.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-24 01:00:00" id="13832" opendate="2016-5-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add missing license header to files</summary>
      <description>Preparing to cut the branch for 2.1.0.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hadoop.hive.ql.util.JavaDataModelTest.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.thrift.ThriftFormatter.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestTezWorkConcurrency.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.AcidWriteSetService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SparkJobUtils.java</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">orc.src.test.org.apache.orc.impl.TestDataReaderProperties.java</file>
      <file type="M">orc.src.java.org.apache.orc.impl.DataReaderProperties.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MConstraint.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.hbase.stats.IExtrapolatePartStatus.java</file>
      <file type="M">llap-server.sql.serviceCheckScript.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-5-26 01:00:00" id="13860" opendate="2016-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix more json related JDK8 test failures</summary>
      <description/>
      <version>None</version>
      <fixedVersion>java8,2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.sort.array.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.collect.set.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.int.type.promotion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.create.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.json.serde1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-26 01:00:00" id="13868" opendate="2016-5-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include derby.log file in the Hive ptest logs</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2016-6-1 01:00:00" id="13911" opendate="2016-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load inpath fails throwing org.apache.hadoop.security.AccessControlException</summary>
      <description>Similar to HIVE-13857</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-2 01:00:00" id="13917" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate and fix tests which are timing out in the precommit build</summary>
      <description>Three tests seem to timeout consistently.TestJdbcWithMiniHATestJdbcWithMiniMrTestOperationLoggingAPIWithTez</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-2 01:00:00" id="13927" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding missing header to Java files</summary>
      <description/>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.ThriftCliServiceMessageSizeTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.miniHS2.StartMiniHS2Cluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-2 01:00:00" id="13933" opendate="2016-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add an option to turn off parallel file moves</summary>
      <description>Since this is a new feature, it make sense to have an ability to turn it off.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-6-12 01:00:00" id="13997" opendate="2016-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Insert overwrite directory doesn&amp;#39;t overwrite existing files</summary>
      <description>Can be easily reproduced by running INSERT OVERWRITE DIRECTORY to the same dir twice.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-6-14 01:00:00" id="14012" opendate="2016-6-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>some ColumnVector-s are missing ensureSize</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.2,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.IntervalDayTimeColumnVector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-15 01:00:00" id="14018" opendate="2016-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make IN clause row selectivity estimation customizable</summary>
      <description>After HIVE-13287 went in, we calculate IN clause estimates natively (instead of just dividing incoming number of rows by 2). However, as the distribution of values of the columns is considered uniform, we might end up heavily underestimating/overestimating the resulting number of rows.This issue is to add a factor that multiplies the IN clause estimation so we can alleviate this problem. The solution is not very elegant, but it is the best we can do until we have histograms to improve our estimate.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-20 01:00:00" id="14060" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive: Remove bogus "localhost" from Hive splits</summary>
      <description>On remote filesystems like Azure, GCP and S3, the splits contain a filler location of "localhost".This is worse than having no location information at all - on large clusters yarn waits upto 200&amp;#91;1&amp;#93; seconds for heartbeat from "localhost" before allocating a container.To speed up this process, the split affinity provider should scrub the bogus "localhost" from the locations and allow for the allocation of "*" containers instead on each heartbeat.&amp;#91;1&amp;#93; - yarn.scheduler.capacity.node-locality-delay=40 x heartbeat of 5s</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-6-20 01:00:00" id="14062" opendate="2016-6-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Changes from HIVE-13502 overwritten by HIVE-13566</summary>
      <description>Appears that changes from HIVE-13566 overwrote the changes from HIVE-13502. I will confirm with the author that it was inadvertent before I re-add it. Thanks</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-6-24 01:00:00" id="14091" opendate="2016-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>some errors are not propagated to LLAP external clients</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.llap.TestLlapOutputFormat.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.LlapBaseRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-28 01:00:00" id="14115" opendate="2016-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Custom FetchFormatter is not supported</summary>
      <description>The following code is supported only FetchFormatter of ThriftFormatter and DefaultFetchFormatter. It can not be used Custom FetchFormatter. if (SessionState.get().isHiveServerQuery()) { conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER,ThriftFormatter.class.getName()); } else { conf.set(SerDeUtils.LIST_SINK_OUTPUT_FORMATTER, DefaultFetchFormatter.class.getName()); }</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-29 01:00:00" id="14126" opendate="2016-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>With ranger enabled, partitioned columns is returned first when you execute select star</summary>
      <description>Click to add description</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-17 01:00:00" id="1413" opendate="2010-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bring a table/partition offline</summary>
      <description>There should be a way to bring a table/partition offline.At that time, no read/write operations should be supported on that table.It would be very useful for housekeeping operations</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-29 01:00:00" id="14132" opendate="2016-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t fail config validation for removed configs</summary>
      <description>Users may have set config in their scripts. If we remove said config in later version then config validation code will throw exception for scripts containing said config. This unnecessary incompatibility can be avoided.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.set.metaconf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-1 01:00:00" id="14144" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permanent functions are showing up in show functions, but describe says it doesn&amp;#39;t exist</summary>
      <description>Click to add description</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.ResourceDownloader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="14149" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Joda Time causes an AmazonS3Exception on Hadoop3.0.0</summary>
      <description>Java1.8u60 and higher cause Joda Time 2.5 to incorrectly format timezones, which leads to the aws server rejecting requests with the aws sdk hadoop3.0 uses. This means any queries involving the s3a connector will return the following AmazonS3Exception:com.amazonaws.services.s3.model.AmazonS3Exception: AWS authentication requires a valid Date or x-amz-date headerThe fix for this is to update Joda Time from 2.5 to 2.8.1. See here for details:https://github.com/aws/aws-sdk-java/issues/444</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="14151" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use of USE_DEPRECATED_CLI environment variable does not work</summary>
      <description>According to https://cwiki.apache.org/confluence/display/Hive/Replacing+the+Implementation+of+Hive+CLI+Using+Beeline if we set USE_DEPRECATED_CLI=false it should use beeline for hiveCli. But it doesn't seem to work.In order to reproduce this issue:$ echo $USE_DEPRECATED_CLI$ ./hiveHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt;$$ export USE_DEPRECATED_CLI=false$ echo $USE_DEPRECATED_CLIfalse$ ./hiveHive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. tez, spark) or using Hive 1.X releases.hive&gt;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.ext.cli.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-1 01:00:00" id="14153" opendate="2016-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline: beeline history doesn&amp;#39;t work on Hive2</summary>
      <description>The up arrow on console is supposed to display history, which is broken currently. Changes in HIVE-6758 broke it.</description>
      <version>1.2.1,2.0.0,2.0.1,2.1.0</version>
      <fixedVersion>1.3.0,2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.beeline</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-6 01:00:00" id="14167" opendate="2016-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use work directories provided by Tez instead of directly using YARN local dirs</summary>
      <description>HIVE-13303 fixed things to use multiple directories instead of a single tmp directory. However it's using yarn-local-dirs directly.I'm not sure how well using the yarn-local-dir will work on a secure cluster.Would be better to use Tez*Context.getWorkDirs. This provides an app specific directory - writable by the user.cc sershe</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.LlapUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-6 01:00:00" id="14175" opendate="2016-7-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix creating buckets without scheme information</summary>
      <description>If a table is created on a non-default filesystem (i.e. non-hdfs), the empty files will be created with incorrect scheme information. This patch extracts the scheme and authority information for the new paths.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-6 01:00:00" id="14177" opendate="2016-7-6 00:00:00" resolution="Not A Bug">
    <buginformation>
      <summary>AddPartitionEvent contains the table location, but not the partition location</summary>
      <description>AddPartitionEvent contains the table location, but not the partition location</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.OperationManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.session.TestQueryDisplay.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-7-7 01:00:00" id="14191" opendate="2016-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bump a new api version for ThriftJDBCBinarySerde changes</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service-rpc.src.gen.thrift.gen-rb.t.c.l.i.service.types.rb</file>
      <file type="M">service-rpc.src.gen.thrift.gen-py.TCLIService.ttypes.py</file>
      <file type="M">service-rpc.src.gen.thrift.gen-php.Types.php</file>
      <file type="M">service-rpc.src.gen.thrift.gen-javabean.org.apache.hive.service.rpc.thrift.TProtocolVersion.java</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.h</file>
      <file type="M">service-rpc.src.gen.thrift.gen-cpp.TCLIService.types.cpp</file>
      <file type="M">service-rpc.if.TCLIService.thrift</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-7-8 01:00:00" id="14197" opendate="2016-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP service driver precondition failure should include the values</summary>
      <description>LLAP service driver's precondition failure message are like belowWorking memory + cache has to be smaller than the container sizingIt will be better to include the actual values for the sizes in the precondition failure message.NO PRECOMMIT TESTS</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-7-11 01:00:00" id="14207" opendate="2016-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Strip HiveConf hidden params in webui conf</summary>
      <description>HIVE-12338 introduced a new web ui, which has a page that displays the current HiveConf being used by HS2. However, before it displays that config, it does not strip entries from it which are considered "hidden" conf parameters, thus exposing those values from a web-ui for HS2. We need to add stripping to this.</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.server.TestHS2HttpServer.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2010-10-22 01:00:00" id="1427" opendate="2010-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide metastore schema migration scripts (0.5 -&gt; 0.6)</summary>
      <description>At a minimum this ticket covers packaging up example MySQL migration scripts (cumulative across all schema changes from 0.5 to 0.6) and explaining what to do with them in the release notes.This is also probably a good point at which to decide and clearly state which Metastore DBs we officially support in production, e.g. do we need to provide migration scripts for Derby?</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-8-18 01:00:00" id="14270" opendate="2016-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Write temporary data to HDFS when doing inserts on tables located on S3</summary>
      <description>Currently, when doing INSERT statements on tables located at S3, Hive writes and reads temporary (or intermediate) files to S3 as well. If HDFS is still the default filesystem on Hive, then we can keep such temporary files on HDFS to keep things run faster.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  <bug fixdate="2016-7-26 01:00:00" id="14338" opendate="2016-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete/Alter table calls failing with HiveAccessControlException</summary>
      <description>Many Hcatalog/Webhcat tests are failing with below error, when tests try to alter/delete/describe tables. Error is thrown when the same user or a different user (same group) who created the table is trying to run the delete/alter table call.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestPermsGrp.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-8-28 01:00:00" id="14367" opendate="2016-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Estimated size for constant nulls is 0</summary>
      <description>since type is incorrectly assumed as void.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.nvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.stack.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.least.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.greatest.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduceSinkDeDuplication.pRS.key.empty.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.results.clientnegative.udf.assert.true.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantfolding.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.literal.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-31 01:00:00" id="14394" opendate="2016-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce excessive INFO level logging</summary>
      <description>We need to cull down on the number of logs we generate in HMS and HS2 that are not needed.</description>
      <version>2.1.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-2 01:00:00" id="14405" opendate="2016-8-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Have tests log to the console along with hive.log</summary>
      <description>When running tests from the IDE (not itests), logs end up going to hive.log - making it difficult to debug tests.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-log4j2.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-10 01:00:00" id="14511" opendate="2016-8-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve MSCK for partitioned table to deal with special cases</summary>
      <description>Some users will have a folder rather than a file under the last partition folder. However, msck is going to search for the leaf folder rather than the last partition folder. We need to improve that.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-16 01:00:00" id="14552" opendate="2016-8-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestParseNegative fix</summary>
      <description>1300s runtime.Straggler towards the end of the build.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.parse.CoreParseNegative.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-18 01:00:00" id="14576" opendate="2016-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Testing: Fixes to TestHBaseMinimrCliDriver</summary>
      <description>1. Runtime over 1000s.2. Runs as an isolated test.Need to fix both.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-19 01:00:00" id="14580" opendate="2016-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Introduce || operator</summary>
      <description>Functionally equivalent to concat() udf. But standard allows usage of || for string concatenations.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-9-19 01:00:00" id="14590" opendate="2016-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path) Incorrect result set when limit is present in one of the union branches</summary>
      <description>Limit gets propagated outside union.</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.null.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.null.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-8-26 01:00:00" id="14652" opendate="2016-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect results for not in on partition columns</summary>
      <description>create table foo (i int) partitioned by (s string);insert overwrite table foo partition(s='foo') select cint from alltypesorc limit 10;insert overwrite table foo partition(s='bar') select cint from alltypesorc limit 10;select * from foo where s not in ('bar');No results. IN ... works correctly</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-1-6 01:00:00" id="14706" opendate="2016-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lineage information not set properly</summary>
      <description>I am trying to fetch column level lineage after a CTAS query in a Post Execution hook in Hive. Below are the queries:create table t1(id int, name string);create table t2 as select * from t1;The lineage information is retrieved using the following sample piece of code:lInfo = hookContext.getLinfo()for(Map.Entry&lt;LineageInfo.DependencyKey, LineageInfo.Dependency&gt; e : lInfo.entrySet()) { System.out.println("Col Lineage Key : " + e.getKey()); System.out.println("Col Lineage Value: " + e.getValue());}The Dependency field(i.e Col Lineage Value) is coming in as null.</description>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-9-8 01:00:00" id="14726" opendate="2016-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>delete statement fails when spdo is on</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.optimization.acid.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-14 01:00:00" id="14762" opendate="2016-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add logging while removing scratch space</summary>
      <description>Useful for debugging</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-15 01:00:00" id="14768" opendate="2016-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a new UDTF Replicate_Rows</summary>
      <description>For intersect all and except all implementation purpose.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-9-19 01:00:00" id="14793" opendate="2016-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow ptest branch to be specified, PROFILE override</summary>
      <description>Post HIVE-14734 - the profile is automatically determined. Add an option to override this via Jenkins. Also add an option to specify the branch from which ptest is built (This is hardcoded to github.com/apache/hive)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-10 01:00:00" id="148" opendate="2008-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>extend bin/hive to include the lineage tool</summary>
      <description>biin/hive currently used only to execute the query. Add options to bin/hive to output the lineage info given the query as the input.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-21 01:00:00" id="14806" opendate="2016-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support UDTF in CBO (AST return path)</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.inline.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udtf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-29 01:00:00" id="14861" opendate="2016-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support precedence for set operator using parentheses</summary>
      <description>We should support precedence for set operator by using parentheses. For exampleselect * from src union all (select * from src union select * from src);</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionSortBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionOrderBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionLimit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionDistributeBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unionClusterBy.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.ptf.negative.NoWindowDefn.q</file>
      <file type="M">ql.src.test.queries.clientnegative.ptf.negative.AmbiguousWindowDefn.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-4 01:00:00" id="14889" opendate="2016-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline leaks sensitive environment variables of HiveServer2 when you type set;</summary>
      <description>When you type set; beeline prints all the environment variables including passwords which could be major security risk. Eg: HADOOP_CREDENTIAL_PASSWORD below is leaked.| env:HADOOP_CREDSTORE_PASSWORD=password || env:HADOOP_DATANODE_OPTS=-Dhadoop.security.logger=ERROR,RFAS || env:HADOOP_HOME_WARN_SUPPRESS=true || env:HADOOP_IDENT_STRING=vihang || env:HADOOP_PID_DIR= |</description>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.processors.TestSetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-14 01:00:00" id="14958" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve the &amp;#39;TestClass&amp;#39; did not produce a TEST-*.xml file message to include list of all qfiles in a batch, batch id</summary>
      <description>Should make it easier to hunt down the logs.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.ExecutionPhase.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.QFileTestBatch.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-14 01:00:00" id="14966" opendate="2016-10-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JDBC: Make cookie-auth work in HTTP mode</summary>
      <description>HiveServer2 cookie-auth is non-functional and forces authentication to be repeated for the status check loop, row fetch loop and the get logs loop.The repeated auth in the fetch-loop is a performance issue, but is also causing occasional DoS responses from the remote auth-backend if this is not using local /etc/passwd.The HTTP-Cookie auth once made functional will behave similarly to the binary protocol, authenticating exactly once per JDBC session and not causing further load on the authentication backend irrespective how many rows are returned from the JDBC request.This due to the fact that the cookies are not sent out with matching flags for SSL usage.</description>
      <version>1.2.1,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCliServiceTestWithCookie.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">itests.hive-minikdc.src.test.java.org.apache.hive.minikdc.TestJdbcWithMiniKdcCookie.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-16 01:00:00" id="14982" opendate="2016-10-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove some reserved keywords in Hive 2.2</summary>
      <description>It seems that CACHE, DAYOFWEEK, VIEWS are reserved keywords in master. This conflicts with SQL2011 standard.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-19 01:00:00" id="15018" opendate="2016-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ALTER rewriting flag in materialized view</summary>
      <description>We should extend the ALTER statement in case we want to change the rewriting behavior of the materialized view after we have created it.ALTER MATERIALIZED VIEW [db_name.]materialized_view_name DISABLE REWRITE;ALTER MATERIALIZED VIEW [db_name.]materialized_view_name ENABLE REWRITE;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.materialized.view.create.rewrite.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.materialized.view.create.rewrite.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-19 01:00:00" id="15019" opendate="2016-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle import for MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DependencyCollectionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CopyTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-21 01:00:00" id="15031" opendate="2016-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix flaky LLAP unit test (TestSlotZNode)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.llap.registry.impl.TestSlotZnode.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-24 01:00:00" id="15042" opendate="2016-10-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support intersect/except without distinct keyword</summary>
      <description>basically, intersect = intersect distinct.</description>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-26 01:00:00" id="15072" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should recognize missing tables in metastore</summary>
      <description>When Install a new database failed half way(for some other reasons), not all of the metastore tables are installed. This caused HMS server failed to start up due to missing tables. Re-run the Schematool, It ran successfully, and in the stdout log said: "Database already has tables. Skipping table creation".However, restarting HMS getting the same error reporting missing tables.Schematool should detect missing tables and provide options to go ahead and recreate missing tables in the case of new installation</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-26 01:00:00" id="15073" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool should detect malformed URIs</summary>
      <description>For some causes(most unknown), HMS DB tables sometimes has invalid entries, for example URI missing scheme for SDS table's LOCATION column or DBS's DB_LOCATION_URI column. These malformed URIs lead to hard to analyze errors in HIVE and SENTRY. Schematool need to provide a command to detect these malformed URI, give a warning and provide an option to fix the URIs</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-26 01:00:00" id="15074" opendate="2016-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schematool provides a way to detect invalid entries in VERSION table</summary>
      <description>For some unknown reason, we see customer's HMS can not start because there are multiple entries in their HMS VERSION table. Schematool should provide a way to validate the HMS db and provide warning and fix options for this kind of issues.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-27 01:00:00" id="15085" opendate="2016-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce the memory used by unit tests, MiniCliDriver, MiniLlapLocal, MiniSpark</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-11-4 01:00:00" id="15125" opendate="2016-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Parallelize slider package generator</summary>
      <description>The metastore init + download of functions takes approx 4 seconds.This is enough time to complete all the other operations in parallel.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-8 01:00:00" id="15155" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Hive version shortname to 2.2.0</summary>
      <description>Pointing to 2.1.0.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-8 01:00:00" id="15160" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t order by an unselected column</summary>
      <description>If a grouping key hasn't been selected, Hive complains. For comparison, Postgres does not.Example. Notice i_item_id is not selected:select i_item_desc ,i_category ,i_class ,i_current_price ,sum(cs_ext_sales_price) as itemrevenue ,sum(cs_ext_sales_price)*100/sum(sum(cs_ext_sales_price)) over (partition by i_class) as revenueratio from catalog_sales ,item ,date_dim where cs_item_sk = i_item_sk and i_category in ('Jewelry', 'Sports', 'Books') and cs_sold_date_sk = d_date_sk and d_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30 days) group by i_item_id ,i_item_desc ,i_category ,i_class ,i_current_price order by i_category ,i_class ,i_item_id ,i_item_desc ,revenueratiolimit 100;</description>
      <version>2.0.0,2.1.0,2.2.0,2.3.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectSortTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.basic2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.1.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-8 01:00:00" id="15161" opendate="2016-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>migrate ColumnStats to use jackson</summary>
      <description>json.org has license issues jackson can provide a fully compatible alternative to it there are a few flakiness issues caused by the order of the map entries of the columns...this cat be addressed, org.json api was unfriendly in this manner</description>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.TestStatsSetupConst.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-17 01:00:00" id="15227" opendate="2016-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize join + gby into semijoin</summary>
      <description>Calcite has a rule which can do this transformation. Lets take advantage of this since Hive has native Left semi join operator.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-18 01:00:00" id="15247" opendate="2016-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass the purge option for drop table to storage handlers</summary>
      <description>This gives storage handler more control on how to handle drop table.</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-11-28 01:00:00" id="15295" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix HCatalog javadoc generation with Java 8</summary>
      <description>Realized while generating artifacts for Hive 2.1.1 release.</description>
      <version>2.1.0,2.2.0</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.ZooKeeperCleanup.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonStorage.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-28 01:00:00" id="15298" opendate="2016-11-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Unit test failures in TestCliDriver sample[2,4,6,7,9]</summary>
      <description>Failing for the past 5 builds:https://builds.apache.org/job/PreCommit-HIVE-Build/2301/testReport/</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.sample2.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-12 01:00:00" id="1531" opendate="2010-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive build work with Ivy versions &lt; 2.1.0</summary>
      <description>Many projects in the Hadoop ecosystem still use Ivy 2.0.0 (including Hadoop and Pig),yet Hive requires version 2.1.0. Ordinarily this would not be a problem, but many usershave a copy of an older version of Ivy in their $ANT_HOME directory, and this copy willalways get picked up in preference to what the Hive build downloads for itself.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-11-29 01:00:00" id="15311" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze column stats should skip non-primitive column types</summary>
      <description>after this patch, when you compute column stats, it will skip the non-primitive column types and give you warning on the console.</description>
      <version>1.2.0,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.columnstats.tbllvl.complex.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-30 01:00:00" id="15312" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce logging in certain places</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-27 01:00:00" id="15515" opendate="2016-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove the docs directory</summary>
      <description>Hive xdocs have not been used since 2012. The docs directory only holds six xml documents, and their contents are in the wiki.It's past time to remove the docs directory from the Hive code.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">docs.xdocs.udf.reflect.xml</file>
      <file type="M">docs.xdocs.language.manual.working.with.bucketed.tables.xml</file>
      <file type="M">docs.xdocs.language.manual.var.substitution.xml</file>
      <file type="M">docs.xdocs.language.manual.joins.xml</file>
      <file type="M">docs.xdocs.language.manual.data-manipulation-statements.xml</file>
      <file type="M">docs.xdocs.language.manual.cli.xml</file>
      <file type="M">docs.xdocs.index.xml</file>
      <file type="M">docs.velocity.properties</file>
      <file type="M">docs.stylesheets.site.vsl</file>
      <file type="M">docs.stylesheets.project.xml</file>
      <file type="M">docs.site.css</file>
      <file type="M">docs.changes.ChangesSimpleStyle.css</file>
      <file type="M">docs.changes.ChangesFancyStyle.css</file>
      <file type="M">docs.changes.changes2html.pl</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-27 01:00:00" id="15517" opendate="2016-12-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NOT (x &lt;=&gt; y) returns NULL if x or y is NULL</summary>
      <description>I created a table as following:create table test(x string, y string);insert into test values ('q', 'q'), ('q', 'w'), (NULL, 'q'), ('q', NULL), (NULL, NULL);Then I try to compare values taking NULLs into account:select *, x&lt;=&gt;y, not (x&lt;=&gt; y), (x &lt;=&gt; y) = false from test;OKq q true false falseq w false true trueq NULL false NULL trueNULL q false NULL trueNULL NULL true NULL falseI expected that 4th column will be the same as 5th one but actually got NULL as result of "not false" and "not true" expressions.Hive 1.2.1000.2.5.0.0-1245Subversion git://c66-slave-20176e25-3/grid/0/jenkins/workspace/HDP-parallel-centos6/SOURCES/hive -r da6c690d384d1666f5a5f450be5cbc54e2fe4bd6Compiled by jenkins on Fri Aug 26 01:39:52 UTC 2016From source with checksum c30648316a632f7a753f4359e5c8f4d6</description>
      <version>1.2.1,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualNS.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-1-28 01:00:00" id="15522" opendate="2016-12-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental ALTER_TABLE/ALTER_PTN including renames</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONCreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONAddPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.CreateTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterTableMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AlterPartitionMessage.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.AddPartitionMessage.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-1-3 01:00:00" id="15534" opendate="2017-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update db/table repl.last.id at the end of REPL LOAD of a batch of events</summary>
      <description>Tracking TODO task in ReplSemanticAnalyzer : // TODO : Over here, we need to track a Map&lt;dbName:String,evLast:Long&gt; for every db updated // and update repl.last.id for each, if this is a wh-level load, and if it is a db-level load, // then a single repl.last.id update, and if this is a tbl-lvl load which does not alter the // table itself, we'll need to update repl.last.id for that as well.</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-6 01:00:00" id="15556" opendate="2017-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replicate views</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateViewDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-1-17 01:00:00" id="15646" opendate="2017-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Column level lineage is not available for table Views</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.cli.createtab.noauthzapi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.authorization.sqlstd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.unbalancedppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionall.join.nullconstant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unicode.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.struct.in.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.field.garbage.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.single.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.column.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.subq.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.windowing.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.unionDistinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.onview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.escape.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.ddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cteViews.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.defaultformats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.subq.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.const.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.disable.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.join.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.viewjoins.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.LineageState.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.not.owner.drop.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.select.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.view.disable.cbo.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.view.delete.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.view.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.cli.createtab.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-3-14 01:00:00" id="15903" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compute table stats when user computes column stats</summary>
      <description/>
      <version>2.1.0,2.1.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.special.character.in.tabnames.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.extrapolate.part.stats.partial.ndv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partition.with.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.deleteAnalyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnstats.part.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.alter.table.invalidate.column.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="15955" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make explain formatted to include opId and etc</summary>
      <description/>
      <version>2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-17 01:00:00" id="15964" opendate="2017-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Llap IO codepath not getting invoked due to file column id mismatch</summary>
      <description>LLAP IO codepath is not getting invoked in certain cases when schema evolution checks are done. Though "int --&gt; long" (fileType to readerType) conversions are allowed, the file type columns are not matched correctly when such conversions need to happen.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapRecordReader.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-20 01:00:00" id="15983" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support the named columns join</summary>
      <description>The named columns join is a common shortcut allowing joins on identically named keys. Example: select * from t1 join t2 using c1 is equivalent to select * from t1 join t2 on t1.c1 = t2.c1. SQL standard reference: Section 7.7</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-20 01:00:00" id="15986" opendate="2017-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "is [not] distinct from"</summary>
      <description>Support standard "is &amp;#91;not&amp;#93; distinct from" syntax. For example this gives a standard way to do a comparison to null safe join: select * from t1 join t2 on t1.x is not distinct from t2.y. SQL standard reference Section 8.15</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-23 01:00:00" id="16019" opendate="2017-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query fails when group by/order by on same column with uppercase name</summary>
      <description>Query with group by/order by on same column KEY failed:SELECT T1.KEY AS MYKEY FROM SRC T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-16 01:00:00" id="16232" opendate="2017-3-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support stats computation for column in QuotedIdentifier</summary>
      <description>right now if a column contains double quotes ``, we can not compute its stats.</description>
      <version>2.1.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-3-22 01:00:00" id="16276" opendate="2017-3-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NoSuchMethodError: com.amazonaws.services.s3.transfer.TransferManagerConfiguration.setMultipartUploadThreshold(I)V</summary>
      <description>The druid-handler is pulling in some com.amazonaws dependencies that conflict with the version that Hadoop is using. This causes the above exception to be thrown when running Hive against S3. This patch fixes the dependency issue by shading the aws dependencies in the druid artifacts. Unfortunately, I can't find a great way to add a test for this so it doesn't happen in the future. We will need some more robust S3-integration tests for that.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-4-5 01:00:00" id="16386" opendate="2017-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add debug logging to describe why runtime filtering semijoins are removed</summary>
      <description>Add a few logging statements to detail the reason why semijoin optimizations are being removed, which can help during debugging.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16422" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Should kill running Spark Jobs when a query is cancelled.</summary>
      <description>Should kill running Spark Jobs when a query is cancelled. When a query is cancelled, Driver.releaseDriverContext will be called by Driver.close. releaseDriverContext will call DriverContext.shutdown which will call all the running tasks' shutdown. public synchronized void shutdown() { LOG.debug("Shutting down query " + ctx.getCmd()); shutdown = true; for (TaskRunner runner : running) { if (runner.isRunning()) { Task&lt;?&gt; task = runner.getTask(); LOG.warn("Shutting down task : " + task); try { task.shutdown(); } catch (Exception e) { console.printError("Exception on shutting down task " + task.getId() + ": " + e); } Thread thread = runner.getRunner(); if (thread != null) { thread.interrupt(); } } } running.clear(); }since SparkTask didn't implement shutdown method to kill the running spark job, the spark job may be still running after the query is cancelled. So it will be good to kill the spark job in SparkTask.shutdown to save cluster resource.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-12 01:00:00" id="16423" opendate="2017-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hint to enforce semi join optimization</summary>
      <description>Add hints in semijoin to enforce particular semi join optimization.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFBloomFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.SyntheticJoinPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDynamicListDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HintParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-13 01:00:00" id="16440" opendate="2017-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failing test columnstats_partlvl_invalid_values when autogather column stats is on</summary>
      <description/>
      <version>2.1.0,2.3.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-13 01:00:00" id="16441" opendate="2017-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>De-duplicate semijoin branches in n-way joins</summary>
      <description>Currently in n-way joins, semi join optimization creates n branches on same key. Instead it should reuse one branch for all the joins.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-20 01:00:00" id="16488" opendate="2017-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replicating into existing db if the db is empty</summary>
      <description>This is a potential usecase where a user may want to manually create a db on destination to make sure it goes to a certain dir root, or they may have cases where the db (default, for instance) was automatically created. We should still allow replicating into this without failing if the db is empty.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-21 01:00:00" id="16493" opendate="2017-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip column stats when colStats is empty</summary>
      <description>Otherwise it will throw NPE</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-21 01:00:00" id="16494" opendate="2017-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>udaf percentile_approx() may fail on CBO</summary>
      <description>select percentile_approx(key, array(0.50, 0.70, 0.90, 0.95, 0.99)) from t; fails with error : The second argument must be a constant.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udaf.percentile.approx.23.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-9 01:00:00" id="16628" opendate="2017-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix query25 when it uses a mix of MergeJoin and MapJoin</summary>
      <description/>
      <version>2.0.0,2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.Op.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-16 01:00:00" id="16684" opendate="2017-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL DUMP shouldn&amp;#39;t fail when table is dropped after fetching the table names.</summary>
      <description>Currently, bootstrap dump will fail if the table does't exist when try to dump. This shall occur when table is dropped after REPL DUMP fetched the table names.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-18 01:00:00" id="16706" opendate="2017-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bootstrap REPL DUMP shouldn&amp;#39;t fail when a partition is dropped/renamed when dump in progress.</summary>
      <description>Currently, bootstrap REPL DUMP gets the partitions in a batch and then iterate through it. If any partition is dropped/renamed during iteration, it may lead to failure/exception. In this case, the partition should be skipped from dump and also need to ensure no failure of REPL DUMP and the subsequent incremental dump should ensure the consistent state of the table.This bug is related to HIVE-16684.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-5-23 01:00:00" id="16742" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cap the number of reducers for LLAP at the configured value</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-6-2 01:00:00" id="16813" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incremental REPL LOAD should load the events in the same sequence as it is dumped.</summary>
      <description>Currently, incremental REPL DUMP use $dumpdir/&lt;eventID&gt; to dump the metadata and data files corresponding to the event. The event is dumped in the same sequence in which it was generated.Now, REPL LOAD, lists the directories inside $dumpdir using listStatus and sort it using compareTo algorithm of FileStatus class which doesn't check the length before sorting it alphabetically.Due to this, the event-100 is processed before event-99 and hence making the replica database non-sync with source.Need to use a customized compareTo algorithm to sort the FileStatus.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-7-4 01:00:00" id="17021" opendate="2017-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support replication of concatenate operation.</summary>
      <description>We need to handle cases like ALTER TABLE ... CONCATENATE that also change the files on disk, and potentially treat them similar to INSERT OVERWRITE, as it does something equivalent to a compaction.Note that a ConditionalTask might also be fired at the end of inserts at the end of a tez task (or other exec engine) if appropriate HiveConf settings are set, to automatically do this operation - these also need to be taken care of for replication.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2017-9-28 01:00:00" id="17196" opendate="2017-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CM: ReplCopyTask should retain the original file names even if copied from CM path.</summary>
      <description>Consider the below scenario,1. Insert into table T1 with value(X).2. Insert into table T1 with value(X).3. Truncate the table T1. – This step backs up 2 files with same content to cmroot which ends up with one file in cmroot as checksum matches.4. Incremental repl with above 3 operations.– In this step, both the insert event files will be read from cmroot where copy of one leads to overwrite the other one as the file name is same in cm path (checksum as file name).So, this leads to data loss and hence it is necessary to retain the original file names even if we copy from cm path.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestReplChangeManager.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-31 01:00:00" id="17212" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic add partition by insert shouldn&amp;#39;t generate INSERT event.</summary>
      <description>A partition is dynamically added if INSERT INTO is invoked on a non-existing partition.Generally, insert operation generated INSERT event to notify the operation with new data files.In this case, Hive should generate only ADD_PARTITION events with the new files added. It shouldn't create INSERT event.Need to test and verify this behaviour.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-8-31 01:00:00" id="17213" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: file merging doesn&amp;#39;t work for union all</summary>
      <description>HoS file merging doesn't work properly since it doesn't set linked file sinks properly which is used to generate move tasks.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-31 01:00:00" id="17214" opendate="2017-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>check/fix conversion of unbucketed non-acid to acid</summary>
      <description>bucketed tables have stricter rules for file layout on disk - bucket files are direct children of a partition directory.for un-bucketed tables I'm not sure there are any rulesfor example, CTAS with Tez + Union operator creates 1 directory for each leg of the unionSupposedly Hive can read table by picking all files recursively. Can it also write (other than CTAS example above) arbitrarily?Does it mean Acid write can also write anywhere?Figure out what can be supported and how can existing layout can be checked? Examining a full "ls -l -R" for a large table could be expensive.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-1 01:00:00" id="17428" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD of ALTER_PARTITION event doesn&amp;#39;t create import tasks if the partition doesn&amp;#39;t exist during analyze phase.</summary>
      <description>If the incremental dump event sequence have ADD_PARTITION followed by ALTER_PARTITION doesn't create any task for ALTER_PARTITION event as the partition doesn't exist during analyze phase. Due to this REPL STATUS returns wrong last repl ID.Scenario:1. Create DB2. Create partitioned table.3. Bootstrap dump and load4. Insert into table to a dynamically created partition. - This insert generate ADD_PARTITION and ALTER_PARTITION events.5. Incremental dump and load. Load will be successful. But the last repl ID set was incorrect as ALTER_PARTITION event was never applied.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-1 01:00:00" id="17429" opendate="2017-9-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC doesn&amp;#39;t return rows when querying Impala</summary>
      <description>The Hive JDBC driver used to return a result set when querying Impala. Now, instead, it gets data back but interprets the data as query logs instead of a resultSet. This causes many issues (we see complaints about beeline as well as test failures).This appears to be a regression introduced with asynchronous operation against Hive.Ideally, we could make both behaviors work. I have a simple patch that should fix the problem.</description>
      <version>2.1.0,2.2.0,2.3.0,2.3.1,2.3.2</version>
      <fixedVersion>2.1.0,2.1.1,2.2.1,2.3.4,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-22 01:00:00" id="1743" opendate="2010-10-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group-by to determine equals of Keys in reverse order</summary>
      <description>When processing group-by, in reduce side, keys are ordered. Comparing equality of two keys can be more efficient in reverse order.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ListObjectsEqualComparer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPOr.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPNotEqual.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseCompare.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-2 01:00:00" id="17664" opendate="2017-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor and add new tests</summary>
      <description/>
      <version>2.1.0,2.1.1,2.2.0,2.3.0</version>
      <fixedVersion>2.1.2,2.2.1,2.3.1,2.4.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TableMask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizationValidatorForTest.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-10-18 01:00:00" id="17830" opendate="2017-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>dbnotification fails to work with rdbms other than postgres</summary>
      <description>as part of HIVE-17721 we had changed the direct sql to acquire the lock for postgres asselect "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update;however this breaks other databases and we have to use different sql statements for different databases for postgres useselect "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" for update;for SQLServer select "NEXT_EVENT_ID" from "NOTIFICATION_SEQUENCE" with (updlock);for other databases select NEXT_EVENT_ID from NOTIFICATION_SEQUENCE for update;</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-3 01:00:00" id="17976" opendate="2017-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: don&amp;#39;t set output collector if there&amp;#39;s no data to process</summary>
      <description>MR doesn't set an output collector if no row is processed, i.e. ExecMapper::map is never called. Let's investigate whether Spark should do the same.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkReduceRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-12-8 01:00:00" id="18251" opendate="2017-12-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Loosen restriction for some checks</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-9 01:00:00" id="18414" opendate="2018-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>upgrade to tez-0.9.1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-16 01:00:00" id="18459" opendate="2018-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec.jar leaks contents fb303.jar into classpath</summary>
      <description>thrift classes are now in the hive classpath in the hive-exec.jar (HIVE-11553). This makes it hard to test with other versions of this library. This library is already a declared dependency and is not required to be included in the hive-exec.jar.I am proposing that we not include these classes like we have done in the past releases.</description>
      <version>2.1.0</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-4-14 01:00:00" id="18955" opendate="2018-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: Unable to create Channel from class NioServerSocketChannel</summary>
      <description>Hit the issue when trying launch spark job. Stack trace:Caused by: java.lang.NoSuchMethodError: io.netty.channel.DefaultChannelId.newInstance()Lio/netty/channel/DefaultChannelId; at io.netty.channel.AbstractChannel.newId(AbstractChannel.java:111) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.AbstractChannel.&lt;init&gt;(AbstractChannel.java:83) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.nio.AbstractNioChannel.&lt;init&gt;(AbstractNioChannel.java:84) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.nio.AbstractNioMessageChannel.&lt;init&gt;(AbstractNioMessageChannel.java:42) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.socket.nio.NioServerSocketChannel.&lt;init&gt;(NioServerSocketChannel.java:86) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at io.netty.channel.socket.nio.NioServerSocketChannel.&lt;init&gt;(NioServerSocketChannel.java:72) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_151] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_151] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_151] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_151] at io.netty.channel.ReflectiveChannelFactory.newChannel(ReflectiveChannelFactory.java:38) ~[netty-all-4.1.17.Final.jar:4.1.17.Final] ... 32 moreIt seems we have conflicts versions of class io.netty.channel.DefaultChannelId from async-http-client.jar and netty-all.jar</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-4-30 01:00:00" id="19083" opendate="2018-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make partition clause optional for INSERT</summary>
      <description>Partition clause should be optional for INSERT INTO VALUES INSERT OVERWRITE INSERT SELECT</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.insert.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.insert.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-17 01:00:00" id="19231" opendate="2018-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline generates garbled output when using UnsupportedTerminal</summary>
      <description>We had a customer that was using some sort of front end that would invoke beeline commands with some query files on a node that that remote to the HS2 node.So beeline runs locally on this edge but connects to a remote HS2. Since the fix made in HIVE-14342, the beeline started producing garbled line in the output. Something like^Mnull ^Mnull^Mnull ^Mnull00-0000 All Occupations 135185230 4227011-0000 Management occupations 6152650 100310 I havent been able to reproduce the issue locally as I do not have their system, but with some additional instrumentation I have been able to get some info regarding the beeline process.Essentially, such invocation causes beeline process to run with -Djline.terminal=jline.UnsupportedTerminal all the time and thus causes the issue. They can run the same beeline command directly in the shell on the same host and it does not cause this issue.PID            S   TTY          TIME COMMAND44107  S    S  ?        00:00:00 bash beeline -u ...PID              S     TTY          TIME COMMAND48453  S+   S     pts/4    00:00:00 bash beeline -u ...Somehow that process wasnt attached to any local terminals. So the check made for /dev/stdin wouldnt work. Instead an additional check to check the TTY session of the process before using the UnsupportedTerminal (which really should only be used for backgrounded beeline sessions) seems to resolve the issue.</description>
      <version>2.1.0</version>
      <fixedVersion>3.1.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-19 01:00:00" id="19250" opendate="2018-4-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema column definitions inconsistencies in MySQL</summary>
      <description>There are some inconsistencies in column definitions in MySQL between a schema that was upgraded to 2.1 (from an older release) vs installing the 2.1.0 schema directly.&gt; `CQ_TBLPROPERTIES` varchar(2048) DEFAULT NULL,117d117&lt; `CQ_TBLPROPERTIES` varchar(2048) DEFAULT NULL,135a136&gt; `CC_TBLPROPERTIES` varchar(2048) DEFAULT NULL,143d143&lt; `CC_TBLPROPERTIES` varchar(2048) DEFAULT NULL,156c156&lt; `CTC_TXNID` bigint(20) DEFAULT NULL,&amp;#8212;&gt; `CTC_TXNID` bigint(20) NOT NULL,158c158&lt; `CTC_TABLE` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,&amp;#8212;&gt; `CTC_TABLE` varchar(256) DEFAULT NULL,476c476&lt; `TBL_NAME` varchar(256) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL,&amp;#8212;&gt; `TBL_NAME` varchar(256) DEFAULT NULL,664c664&lt; KEY `PCS_STATS_IDX` (`DB_NAME`,`TABLE_NAME`,`COLUMN_NAME`,`PARTITION_NAME`),&amp;#8212;&gt; KEY `PCS_STATS_IDX` (`DB_NAME`,`TABLE_NAME`,`COLUMN_NAME`,`PARTITION_NAME`) USING BTREE,768c768&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,814c814&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,934c934&lt; `PARAM_VALUE` mediumtext,&amp;#8212;&gt; `PARAM_VALUE` mediumtext CHARACTER SET latin1 COLLATE latin1_bin,1066d1065&lt; `TXN_HEARTBEAT_COUNT` int(11) DEFAULT NULL,1067a1067&gt; `TXN_HEARTBEAT_COUNT` int(11) DEFAULT NULL,1080c1080&lt; `TC_TXNID` bigint(20) DEFAULT NULL,&amp;#8212;&gt; `TC_TXNID` bigint(20) NOT NULL,1082c1082&lt; `TC_TABLE` varchar(128) DEFAULT NULL,&amp;#8212;&gt; `TC_TABLE` varchar(128) NOT NULL,1084c1084&lt; `TC_OPERATION_TYPE` char(1) DEFAULT NULL,&amp;#8212;&gt; `TC_OPERATION_TYPE` char(1) NOT NULL,</description>
      <version>2.1.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mysql.039-HIVE-12274.mysql.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-5-9 01:00:00" id="19471" opendate="2018-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucket_map_join_tez1 and bucket_map_join_tez2 are failing</summary>
      <description>https://builds.apache.org/job/PreCommit-HIVE-Build/10766/testReport/TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;bucket_map_join_tez1&amp;#93;TestMiniLlapLocalCliDriver.testCliDriver&amp;#91;bucket_map_join_tez2&amp;#93;Both are failing. Probably need golden file update.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez1.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-7-19 01:00:00" id="19944" opendate="2018-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Investigate and fix version mismatch of GCP</summary>
      <description>We've observed that adding a new image to the ptest GCP project breaks our currently working infrastructure when we try to restart the hive ptest server.This is because upon initialization the project's images are queried and we immediately get an exception for newly added images - they don't have a field that our client thinks should be mandatory to have. I believe there's an upgrade needed on our side for the GCP libs we depend on.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-4 01:00:00" id="20082" opendate="2018-7-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveDecimal to string conversion doesn&amp;#39;t format the decimal correctly - master</summary>
      <description>Example: LPAD on a decimal(7,1) values of 0 returns "0" (plus padding) but it should be "0.0" (plus padding)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.primitive.TestPrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.string.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.complex.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.predicate.TestAccumuloRangeGenerator.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIColumnArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.DTIScalarArithmeticDTIColumnNoConvert.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexPredicateAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.pad.convert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.pruner.multiple.children.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-5 01:00:00" id="20100" opendate="2018-7-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>OpTraits : Select Optraits should stop when a mismatch is detected</summary>
      <description>The select operator's optraits logic as stated in the comment is,// For bucket columns// If all the columns match to the parent, put them in the bucket cols// else, add empty list.// For sort columns// Keep the subset of all the columns as long as order is maintained. However, this is not happening due to a bug. The bool found is never reset, so if a single match is found, the value remains true and allows the optraits get populated with partial list of columns for bucket col which is incorrect.This may lead to creation of SMB join which should not happen.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-7-18 01:00:00" id="20201" opendate="2018-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive shouldn&amp;#39;t use HBase&amp;#39;s Base64 implementation</summary>
      <description>HBase is removing their Base64 implementation because it never should have been public, so Hive should switch to a different provider. Hive already uses Commons-Codec Base64 in other places, so that would be a natural replacement.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableSnapshotInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2014-6-16 01:00:00" id="7240" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add classifier for avro-mapred jar</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>