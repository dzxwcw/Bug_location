<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-3-18 01:00:00" id="10003" opendate="2015-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniTez ut fail with missing configs</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.llapdecider.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-20 01:00:00" id="10042" opendate="2015-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>clean up TreeReaders - ORC refactoring for LLAP on trunk</summary>
      <description>See gopalv's comments on HIVE-9555</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.InStream.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-21 01:00:00" id="10044" opendate="2015-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow interval params for year/month/day/hour/minute/second functions</summary>
      <description>Update the year/month/day/hour/minute/second functions to retrieve the various fields of the year-month and day-time interval types.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.month.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.dayofmonth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.day.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFYear.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSecond.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMonth.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMinute.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFHour.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDayOfMonth.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-21 01:00:00" id="10046" opendate="2015-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP : disable ORC trace logging for now</summary>
      <description>For perf testing. I just need a JIRA to commit the change</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.DebugUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-24 01:00:00" id="10071" opendate="2015-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join to MultiJoin rule</summary>
      <description>CBO return path: auto_join3.q can be used to reproduce the problem.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-26 01:00:00" id="10106" opendate="2015-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression : Dynamic partition pruning not working after HIVE-9976</summary>
      <description>After HIVE-9976 got checked in dynamic partition pruning doesn't work.Partitions are pruned and later show up in splits.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-30 01:00:00" id="10140" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-1 01:00:00" id="10187" opendate="2015-4-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avro backed tables don&amp;#39;t handle cyclical or recursive records</summary>
      <description>HIVE-7653 changed the Avro SerDe to make it generate TypeInfos even for recursive/cyclical schemas. However, any attempt to serialize data which exploits that ability results in silently dropped fields.</description>
      <version>1.2.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestAvroSerializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.SchemaToTypeInfo.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-3-30 01:00:00" id="1022" opendate="2009-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>desc Table should work</summary>
      <description>desc is supported as a shortcut to describe in almost all the databases - it would be convenient to support that.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.input2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-6 01:00:00" id="10222" opendate="2015-4-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite dependency to newest version</summary>
      <description>Upgrade Calcite version to 1.2.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-3-5 01:00:00" id="1027" opendate="2010-1-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create UDFs for XPath expression evaluation</summary>
      <description>Create UDFs for evaluating XPath expressions against XML documents.Examples:&gt; SELECT xpath_double ('&lt;a&gt;&lt;b class="odd"&gt;1&lt;/b&gt;&lt;b class="even"&gt;2&lt;/b&gt;&lt;b class="odd"&gt;4&lt;/b&gt;&lt;c&gt;8&lt;/c&gt;&lt;/a&gt;', 'sum(a/b&amp;#91;@class="odd"&amp;#93;)') FROM src LIMIT 1 ;5.0&gt; SELECT xpath_string ('&lt;a&gt;&lt;b&gt;b1&lt;/b&gt;&lt;b&gt;b2&lt;/b&gt;&lt;/a&gt;', 'a/b&amp;#91;2&amp;#93;') FROM src LIMIT 1 ;b2&gt; SELECT xpath ('&lt;a&gt;&lt;b&gt;b1&lt;/b&gt;&lt;b&gt;b2&lt;/b&gt;&lt;b&gt;b3&lt;/b&gt;&lt;c&gt;c1&lt;/c&gt;&lt;c&gt;c2&lt;/c&gt;&lt;/a&gt;', 'a/c/text()') FROM src LIMIT 1 ;&amp;#91;"c1","c2"&amp;#93;Included functions are: xpath_short, xpath_int, xpath_long, xpath_float, xpath_double/xpath_number, xpath_string, xpath</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-4-9 01:00:00" id="10276" opendate="2015-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement date_format(timestamp, fmt) UDF</summary>
      <description>date_format(date/timestamp/string, fmt) converts a date/timestamp/string to a value of String in the format specified by the java date format fmt.Supported formats listed here:https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-10 01:00:00" id="10295" opendate="2015-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fix timeouts for tasks which stay in the pending queue</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.configuration.LlapConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-12 01:00:00" id="10312" opendate="2015-4-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SASL.QOP in JDBC URL is ignored for Delegation token Authentication</summary>
      <description>When HS2 is configured for QOP other than auth (auth-int or auth-conf), Kerberos client connection works fine when the JDBC URL specifies the matching QOP, however when this HS2 is accessed through Oozie (Delegation token / Digest authentication), connections fails because the JDBC driver ignores the SASL.QOP parameters in the JDBC URL. SASL.QOP setting should be valid for DIGEST Auth mech.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-13 01:00:00" id="10323" opendate="2015-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez merge join operator does not honor hive.join.emit.interval</summary>
      <description>This affects efficiency in case of skews.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-17 01:00:00" id="10386" opendate="2015-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Disable Trivial Project Removal on ret path</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-20 01:00:00" id="10400" opendate="2015-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Exception when column name contains dot or colon characters</summary>
      <description>If return path is on, this query produces the problem:select cbo_t3.c_int, c, count(*)from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1where (cbo_t1.c_int + 1 &gt;= 0) and (cbo_t1.c_int &gt; 0 or cbo_t1.c_float &gt;= 0)group by c_float, cbo_t1.c_int, key order by a) cbo_t1join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2where (cbo_t2.c_int + 1 &gt;= 0) and (cbo_t2.c_int &gt; 0 or cbo_t2.c_float &gt;= 0)group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=pjoin cbo_t3 on cbo_t1.a=keywhere (b + cbo_t2.q &gt;= 0) and (b &gt; 0 or c_int &gt;= 0)group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c;</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-20 01:00:00" id="10403" opendate="2015-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add n-way join support for Hybrid Grace Hash Join</summary>
      <description>Currently Hybrid Grace Hash Join only supports 2-way join (one big table and one small table). This task will enable n-way join (one big table and multiple small tables).</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridhashjoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.join.hash.q</file>
      <file type="M">ql.src.test.queries.clientpositive.hybridhashjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.13.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastMultiKeyHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastStringHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastMultiKeyHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.ObjectContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.KeyValueContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableLoader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-20 01:00:00" id="10407" opendate="2015-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>separate out the timestamp ranges for testing purposes</summary>
      <description>Some platforms have limits for date ranges, so separate out the test cases that are outside of the range 1970 to 2038.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcTimezone1.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestOrcFile.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-21 01:00:00" id="10413" opendate="2015-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Return path assumes distinct column cant be same as grouping column</summary>
      <description>Found in cbo_udf_udaf.q tests.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-21 01:00:00" id="10424" opendate="2015-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Factor known capacity into scheduling decisions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.configuration.LlapConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-21 01:00:00" id="10425" opendate="2015-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Limit number of threads used to communicate with a single LLAP instance to 1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-21 01:00:00" id="10427" opendate="2015-4-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>collect_list() and collect_set() should accept struct types as argument</summary>
      <description>The collect_list() and collect_set() functions currently only accept scalar argument types. It would be very useful if these functions could also accept struct argument types for creating nested data from flat data.For example, suppose I wanted to create a nested customers/orders table from two flat tables, customers and orders. Then it'd be very convenient to write something like this:insert into table nested_customers_ordersselect c.*, collect_list(named_struct("oid", o.oid, "order_date": o.date...))from customers c inner join orders o on (c.cid = o.oid)group by c.cidThanks you for your consideration.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMkCollectionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectList.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-22 01:00:00" id="10444" opendate="2015-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-10223 breaks hadoop-1 build</summary>
      <description>FileStatus.isFile() and FileStatus.isDirectory() methods added in HIVE-10223 are not present in hadoop 1.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-22 01:00:00" id="10447" opendate="2015-4-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline JDBC Driver to support 2 way SSL</summary>
      <description>This jira should cover 2-way SSL authentication between the JDBC Client and server which requires the driver to support it.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-4-26 01:00:00" id="10490" opendate="2015-4-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase Snapshot IF fails at run time with missing dependency of MetricsRegistry</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-28 01:00:00" id="10513" opendate="2015-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] return path : Fix create_func1.q for return path</summary>
      <description>throws class cast exception.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTestGetJavaBoolean.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-4-28 01:00:00" id="10522" opendate="2015-4-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): fix the wrong needed column names when TS is created</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-29 01:00:00" id="10529" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove references to tez task context before storing operator plan in object cache</summary>
      <description/>
      <version>1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-29 01:00:00" id="10530" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregate stats cache: bug fixes for RDBMS path</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.extrapolate.part.stats.partial.ndv.q</file>
      <file type="M">ql.src.test.queries.clientpositive.extrapolate.part.stats.partial.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.2.q</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.AggregateStatsCache.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-29 01:00:00" id="10542" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Full outer joins in tez produce incorrect results in certain cases</summary>
      <description>If there is no records for one of the tables in the full outer join, we do not read the other input and end up not producing rows which we should be.</description>
      <version>1.0.0,1.1.0,1.2.0,1.3.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mergejoin.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-30 01:00:00" id="10549" opendate="2015-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Enable NonBlockingOpDeDupProc</summary>
      <description>According to our discussion, enable NonBlockingOpDeDupProc for return path.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-30 01:00:00" id="10563" opendate="2015-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniTezCliDriver tests ordering issues</summary>
      <description>There are a bunch of tests related to TestMiniTezCliDriver which gives ordering issues when run on Centos/Windows/OSX</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.multi.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.udf.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.env.var1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.date2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.nonacid.from.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.udf.udaf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.merge.2.orc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join0.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.join1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.auto.sortmerge.join.5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.gby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.udf.udaf.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.windowing.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cross.product.check.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.cross.product.check.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ctas.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.optimization2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.filter.join.breaktask2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.into1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.into2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.insert.nonacid.from.acid.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.merge.multi.expressions.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.nullsafe.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.decimal.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.partition.date2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.script.env.var1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.selectDistinctStar.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.exists.q</file>
      <file type="M">ql.src.test.queries.clientpositive.temp.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.dml.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.joins.explain.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.join.hash.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.multi.union.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.schema.evolution.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.all.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.update.where.partitioned.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorization.part.project.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.distinct.gby.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.nested.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.rcfile.columnar.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.shufflejoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.timestamp.funcs.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.data.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.null.projection.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.outer.join3.q</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.2.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-30 01:00:00" id="10564" opendate="2015-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat should use webhcat-site.xml properties for controller job submission</summary>
      <description>webhcat should use webhcat-site.xml in configuration for the TempletonController map-only job that it launches. This will allow users to set any MR/hdfs properties that want to see used for the controller job.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-1 01:00:00" id="10565" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Native Vector Map Join doesn&amp;#39;t handle filtering and matching on LEFT OUTER JOIN repeated key correctly</summary>
      <description>Filtering can knock out some of the rows for a repeated key, but those knocked out rows need to be included in the LEFT OUTER JOIN result and are currently not when only some rows are filtered out.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.partition.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.left.outer.join2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinOuterGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinLeftSemiGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyStringOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyMultiKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyLongOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinInnerBigOnlyGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.optimized.VectorMapJoinOptimizedLongCommon.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastValueStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastKeyStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-6 01:00:00" id="10628" opendate="2015-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result when vectorized native mapjoin is enabled using null safe operators &lt;=&gt;</summary>
      <description>Incorrect results for this query:select count(*) from store_sales ss join store_returns sr on (sr.sr_item_sk &lt;=&gt; ss.ss_item_sk and sr.sr_customer_sk &lt;=&gt; ss.ss_customer_sk and sr.sr_item_sk &lt;=&gt; ss.ss_item_sk) where ss.ss_net_paid &gt; 1000;</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-7 01:00:00" id="10639" opendate="2015-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create SHA1 UDF</summary>
      <description>Calculates an SHA-1 160-bit checksum for the string and binary, as described in RFC 3174 (Secure Hash Algorithm). The value is returned as a string of 40 hex digits, or NULL if the argument was NULL.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-11 01:00:00" id="10675" opendate="2015-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide option to skip Accumulo related Hive tests in itests directory</summary>
      <description>We shouldn’t be running Accumulo+Hive tests in Windows as Accumulo is currently unsupported on Windows. Hence we should provide an option to disable these tests; that being mentioned, the default behavior would be to enable these tests.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-12 01:00:00" id="10682" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-5-15 01:00:00" id="10715" opendate="2015-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>RAT failures - many files do not have ASF licenses</summary>
      <description>Lots of files do not have proper ASF headers included in. We should add them in.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.auth.TestLdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.CookieSigner.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySerDeParameters.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.TestUDFJson.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestAddResource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestMapStructures.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestArrayCompatibility.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.AbstractTestParquetDirect.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseDTI.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSortExchange.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveGroupingID.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelCollation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.Repeated.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveCollectionConverter.java</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpRequestInterceptorBase.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.inpdir.xmlreducer.py</file>
      <file type="M">hcatalog.src.test.e2e.templeton.inpdir.xmlmapper.py</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestStructSerializer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-15 01:00:00" id="10724" opendate="2015-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e test TestStreaming_5 fails on Windows</summary>
      <description>The test TestStreaming_5 fails with the following error on Windows:Passed in parameter is incorrectly quoted: \"\"StreamXmlRecordReader,begin=xml,end=/xml\"\"The problem is the extra quotes in the post_options in the test 'inputreader="StreamXmlRecordReader,begin=xml,end=/xml"'Removing the double quotes 'inputreader=StreamXmlRecordReader,begin=xml,end=/xml' makes the test happy on both Linux and Windows.NO PRECOMMIT TESTS</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.streaming.conf</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-18 01:00:00" id="10741" opendate="2015-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>count distinct rewrite is not firing</summary>
      <description>Rewrite introduced in HIVE-10568 is not effective outside of test environment</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-18 01:00:00" id="10744" opendate="2015-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: dags get stuck in yet another way</summary>
      <description>DAG gets stuck when number of tasks that is multiple of number of containers on machine (6, 12, ... in my case) fails to finish at the end of the stage (I am running a job with 500-1000 maps). Status just hangs forever (beyond 5 min timeout) with some tasks shown as running. Happened twice on 3rd DAG with 1000-map job (TPCH Q1), then when I reduced to 500 happened on 7th DAG so far. Siddharth Seth has the details.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-18 01:00:00" id="10745" opendate="2015-5-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better null handling by Vectorizer</summary>
      <description>Minor refactoring around null handling in Vectorization.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-19 01:00:00" id="10748" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace StringBuffer with StringBuilder where possible</summary>
      <description>I found 40 places in Hive where "new StringBuffer(" is used."Where possible, it is recommended that StringBuilder be used in preference to StringBuffer as it will be faster under most implementations"https://docs.oracle.com/javase/7/docs/api/java/lang/StringBuilder.html</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableScanDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FilterDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRTableScan1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HivePreparedStatement.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.data.TestJsonSerDe.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatException.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveVarchar.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.jsonexplain.tez.TezJsonParser.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-10-19 01:00:00" id="10755" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rework on HIVE-5193 to enhance the column oriented table access</summary>
      <description>Add the support of column pruning for column oriented table access which was done in HIVE-5193 but was reverted due to the join issue in HIVE-10720.In 1.3.0, the patch posted by Viray didn't work, probably due to some jar reference. That seems to get fixed and that patch works in 2.0.0 now.</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatLoader.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatLoader.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-19 01:00:00" id="10757" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain query plan should have operation name EXPLAIN</summary>
      <description>In the plan of an Explain query, the operation name is not set to EXPLAIN. Instead, it is set to the operation name of the query itself.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-19 01:00:00" id="10758" opendate="2015-5-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Modify running / wait queues on on fragment finishable state changes</summary>
      <description>Click to add description</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryTracker.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.QueryFragmentInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.EvictingPriorityBlockingQueue.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-21 01:00:00" id="10789" opendate="2015-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>union distinct query with NULL constant on both the sides throws "Unsuported vector output type: void" error</summary>
      <description>A NULL expression in the SELECT projection list causes exception to be thrown instead of not vectorizing.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-3 01:00:00" id="108" opendate="2008-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sampling does not use internal column name</summary>
      <description>sampling does not use internal column name, it uses column table name instead - which will not work in case there is any operator between tablescan and sampling, which is possible with column pruning.The following test breaks: (assuming srcbucket has more than 1 column: in this case 2 columns: key and value)&amp;#8211; no input pruning, sample filterEXPLAINSELECT s.keyFROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s;SELECT s.keyFROM srcbucket TABLESAMPLE (BUCKET 1 OUT OF 5 on key) s;</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-22 01:00:00" id="10804" opendate="2015-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): optimizer for limit 0 does not work</summary>
      <description>explainselect key,value from src order by key limit 0POSTHOOK: type: QUERYSTAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 depends on stages: Stage-1STAGE PLANS: Stage: Stage-1 Map Reduce Map Operator Tree: TableScan alias: src Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE Select Operator expressions: key (type: string), value (type: string) outputColumnNames: key, value Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE Reduce Output Operator key expressions: key (type: string) sort order: + Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE value expressions: value (type: string) Reduce Operator Tree: Select Operator expressions: KEY.reducesinkkey0 (type: string), VALUE.value (type: string) outputColumnNames: key, value Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE Limit Number of rows: 0 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE File Output Operator compressed: false Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-27 01:00:00" id="10835" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-27 01:00:00" id="10840" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NumberFormatException while running analyze table partition compute statics query</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.stats.only.null.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-7-29 01:00:00" id="10863" opendate="2015-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge master to Spark branch 7/29/2015 [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>spark-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-29 01:00:00" id="10864" opendate="2015-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): auto_join2.q returning wrong results</summary>
      <description>auto_join2.q returns wrong results when return path is on. The problem is that we create the same join expression once per input reference when we are translating. Thus, we incorrectly end up with a key composed by multiple expressions in those cases.</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveInsertExchange4JoinRule.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-29 01:00:00" id="10868" opendate="2015-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update release note for 1.2.0 and 1.1.0</summary>
      <description>It's recently found that Hive's release notes don't contain all JIRAs fixed. This happened due to a lack of correct or missing fix version in a JIRA. A large chunk of such JIRAs are due to the fact that their fix versions didn't get updated when a merge from feature branch to trunk (master). This JIRA is to fix such JIRAs related to Hive on Spark work.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-1 01:00:00" id="10886" opendate="2015-6-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Fixes to TaskReporter after recent Tez changes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="10889" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: HIVE-10778 has NPE</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GlobalWorkMapFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="10894" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: make sure the branch builds on hadoop-1: part 1 (non-llap)</summary>
      <description>for HIVE-10872</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="10896" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-3 01:00:00" id="10907" opendate="2015-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Tez: Classcast exception in some cases with SMB joins</summary>
      <description>In cases where there is a mix of Map side work and reduce side work, we get a classcast exception because we assume homogeneity in the code. We need to fix this correctly. For now this is a workaround.</description>
      <version>1.0.0,1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-1-25 01:00:00" id="1093" opendate="2010-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a "skew join map join size" variable to control the input size of skew join&amp;#39;s following map join job.</summary>
      <description>In a test, many skew join key itself &gt;250M size. And the following mapjoin will take several hours to do a mapjoin for those big skew keys. This can be better by using a small map input size for the following map join job.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-4 01:00:00" id="10934" opendate="2015-6-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Restore support for DROP PARTITION PURGE</summary>
      <description>HIVE-9086 added support for PURGE in ALTER TABLE my_doomed_table DROP IF EXISTS PARTITION (part_key = "sayonara") IGNORE PROTECTION PURGE;looks like this was accidentally lost in HIVE-10228</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-5 01:00:00" id="10941" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide option to disable spark tests outside itests</summary>
      <description>HIVE-10477 provided an option to disable spark module, however we missed the following files that are outside itests directory. i.e we need to club the option with disabling the following tests as well :org.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testMultiSessionMultipleUseorg.apache.hadoop.hive.ql.exec.spark.session.TestSparkSessionManagerImpl.testSingleSessionMultipleUseorg.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testTempTableorg.apache.hive.jdbc.TestJdbcWithLocalClusterSpark.testSparkQueryorg.apache.hive.jdbc.TestMultiSessionsHS2WithLocalClusterSpark.testSparkQuery</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-5 01:00:00" id="10943" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline-cli: Enable precommit for beelie-cli branch</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-submit-build.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-5 01:00:00" id="10949" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable hive-minikdc tests in Windows</summary>
      <description>hive-minikdc needs to be disabled for Windows OS since we dont have kerberos support yet for Hadoop Cluster running under Windows OS.</description>
      <version>None</version>
      <fixedVersion>1.2.1,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-minikdc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-5 01:00:00" id="10955" opendate="2015-6-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CliDriver leaves tables behind at end of test run</summary>
      <description>When run serially with other drivers, this causes problems</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.scripts.q.test.cleanup.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-6 01:00:00" id="10957" opendate="2015-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryPlan&amp;#39;s start time is incorrect in certain cases</summary>
      <description>In some cases the start time of the previous query is used mistakenly.</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-11 01:00:00" id="10979" opendate="2015-6-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix failed tests in TestSchemaTool after the version number change in HIVE-10921</summary>
      <description>Some version variables in sql are not updated in HIVE-10921 which caused unit test failed. See http://ec2-174-129-184-35.compute-1.amazonaws.com/jenkins/job/PreCommit-HIVE-TRUNK-Build/4241/testReport/org.apache.hive.beeline/TestSchemaTool/testSchemaUpgrade/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-1.2.0-to-2.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-2.0.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-1.2.0-to-2.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-2.0.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-1.2.0-to-2.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-2.0.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-2.0.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-1.2.0-to-2.0.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-2.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-13 01:00:00" id="10999" opendate="2015-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Spark dependency to 1.4 [Spark Branch]</summary>
      <description>Spark 1.4.0 is release. Let's update the dependency version from 1.3.1 to 1.4.0.</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.TestSparkClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-15 01:00:00" id="11007" opendate="2015-6-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): dpCtx&amp;#39;s mapInputToDP should depends on the last SEL</summary>
      <description>In dynamic partitioning case, for example, we are going to have TS0-SEL1-SEL2-FS3. The dpCtx's mapInputToDP is populated by SEL1 rather than SEL2, which causes error in return path.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-16 01:00:00" id="11014" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: some MiniTez tests have result changes compared to master</summary>
      <description>vector_binary_join_groupby, vector_outer_join1, vector_outer_join2 and cbo_windowing</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.binary.join.groupby.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-16 01:00:00" id="11029" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop.proxyuser.mapr.groups does not work to restrict the groups that can be impersonated</summary>
      <description>In the core-site.xml, the hadoop.proxyuser.&lt;user&gt;.groups specifies the user groups which can be impersonated by the HS2 &lt;user&gt;. However, this does not work properly in Hive. In my core-site.xml, I have the following configs:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.groups&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;I would expect with this configuration that 'mapr' can impersonate only members of the Unix group 'root'. However if I submit a query as user 'jon' the query is running as user 'jon' even though 'mapr' should not be able to impersonate this user.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-26 01:00:00" id="1103" opendate="2010-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add .gitignore file</summary>
      <description>Add a .gitignore file (equivalent to svn:ignore) for those using git-svn.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-7-19 01:00:00" id="11055" opendate="2015-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HPL/SQL - Implementing Procedural SQL in Hive (PL/HQL Contribution)</summary>
      <description>There is PL/HQL tool (www.plhql.org) that implements procedural SQL for Hive (actually any SQL-on-Hadoop implementation and any JDBC source).Alan Gates offered to contribute it to Hive under HPL/SQL name (org.apache.hive.hplsql package). This JIRA is to create a patch to contribute the PL/HQL code.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-22 01:00:00" id="11076" opendate="2015-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explicitly set hive.cbo.enable=true for some tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.2,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.join.merge.multi.expressions.q</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2015-9-26 01:00:00" id="11132" opendate="2015-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Queries using join and group by produce incorrect output when hive.auto.convert.join=false and hive.optimize.reducededuplication=true</summary>
      <description>Queries using join and group by produce multiple output rows with the same key when hive.auto.convert.join=false and hive.optimize.reducededuplication=true. This interaction between configuration parameters is unexpected and should be well documented at the very least and should likely be considered a bug.e.g. hive&gt; set hive.auto.convert.join = false;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 79XYZ 74XYZ 297XYZ 66hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.reducededuplication = true;hive&gt; SELECT foo.id, count as factor &gt; FROM foo &gt; JOIN bar ON (foo.id = bar.id and foo.line_id = bar.line_id) &gt; JOIN split ON (foo.id = split.id and foo.line_id = split.line_id) &gt; JOIN forecast ON (foo.id = forecast.id AND foo.line_id = forecast.line_id) &gt; WHERE foo.order != ‘blah’ AND foo.id = ‘XYZ' &gt; GROUP BY foo.id;XYZ 516</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-7-2 01:00:00" id="11177" opendate="2015-7-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: spark out file changes compared to master</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-8 01:00:00" id="11201" opendate="2015-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatalog is ignoring user specified avro schema in the table definition</summary>
      <description>HCatalog is ignoring user specified avro schema in the table definition , instead generating its own avro based from hive meta store. By generating its own schema will result in mismatch names. For exmple Avro fields name are Case Sensitive. By generating it's own schema will result in incorrect schema written to the avro file , and result select fail on read. And also Even if user specified schema does not allow null , when data is written using Hcatalog , it will write a schema that will allow null. For example in the table , user specified , all CAPITAL letters in the schema , and record name as LINEITEM. The schema should be written as it is. Instead Hcatalog ignores it and generated its own avro schema from the hive table case.</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.SpecialCases.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-8 01:00:00" id="11206" opendate="2015-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Join translation should update all ExprNode recursively</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-9 01:00:00" id="11215" opendate="2015-7-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized grace hash-join throws FileUtil warnings</summary>
      <description>TPC-DS query13 warnings about a null-file deletion.2015-07-09 03:14:18,880 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows restored from KeyValueContainer: 311842015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Deserializing spilled hash partition...2015-07-09 03:14:18,881 INFO [TezChild] exec.MapJoinOperator: Hybrid Grace Hash Join: Number of rows in hashmap: 311842015-07-09 03:14:18,897 INFO [TezChild] exec.MapJoinOperator: spilled: true abort: false. Clearing spilled partitions.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.2015-07-09 03:14:18,898 WARN [TezChild] fs.FileUtil: null file argument.</description>
      <version>1.2.0,1.3.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinRowBytesContainer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-7-16 01:00:00" id="11273" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Register for finishable state change notifications when adding a task instead of when scheduling it</summary>
      <description>Registering when trying to execute is far too late. The task won't be considered for execution (queue may not be re-oredered) without the notification coming in.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-7-16 01:00:00" id="11282" opendate="2015-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO (Calcite Return Path): Inferring Hive type char/varchar of length zero which is not allowed</summary>
      <description>When RT is on, we try to infer the Hive type from the Calcite type for the value '’ e.g. in udf3.q, and we end up with char (length=0) as a result. The min length of char/varchar in Hive is 1, thus an Exception is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-12-19 01:00:00" id="11312" opendate="2015-7-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC format: where clause with CHAR data type not returning any rows</summary>
      <description>Test case:Setup: create table orc_test( col1 string, col2 char(10)) stored as orc tblproperties ("orc.compress"="NONE");insert into orc_test values ('val1', '1');Query:select * from orc_test where col2='1'; Query returns no row.Problem is introduced with HIVE-10286, class RecordReaderImpl.java, method evaluatePredicateRange.Old code: Object baseObj = predicate.getLiteral(PredicateLeaf.FileFormat.ORC); Object minValue = getConvertedStatsObj(min, baseObj); Object maxValue = getConvertedStatsObj(max, baseObj); Object predObj = getBaseObjectForComparison(baseObj, minValue);New code:+ Object baseObj = predicate.getLiteral();+ Object minValue = getBaseObjectForComparison(predicate.getType(), min);+ Object maxValue = getBaseObjectForComparison(predicate.getType(), max);+ Object predObj = getBaseObjectForComparison(predicate.getType(), baseObj);The values for min and max are of type String which contain as many characters as the CHAR column indicated. For example if the type is CHAR(10), and the row has value 1, the value of String min is "1 ";Before Hive 1.2, the method getConvertedStatsObj would call StringUtils.stripEnd(statsObj.toString(), null); which would remove the trailing spaces from min and max. Later in the compareToRange method, it was able to compare "1" with "1".In Hive 1.2 with the use getBaseObjectForComparison method, it simply returns obj.String if the data type is String, which means minValue and maxValue are still "1 ".As a result, the compareToRange method will return a wrong value ("1".compareTo("1 ") -9 instead of 0.</description>
      <version>1.2.0,1.2.1,1.3.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.parquet.ppd.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.char.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-20 01:00:00" id="11319" opendate="2015-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTAS with location qualifier overwrites directories</summary>
      <description>CTAS with location clause acts as an insert overwrite. This can cause problems when there sub directories with in a directory.This cause some users accidentally wipe out directories with very important data. We should ban CTAS with location to a non-empty directory. Reproduce:create table ctas1 location '/Users/ychen/tmp' as select * from jsmall limit 10;create table ctas2 location '/Users/ychen/tmp' as select * from jsmall limit 5;Both creates will succeed. But value in table ctas1 will be replaced by ctas2 accidentally.</description>
      <version>0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-12-25 01:00:00" id="11372" opendate="2015-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>join with between predicate comparing integer types returns no rows when ORC format used</summary>
      <description>getDatabaseProductName Apache HivegetDatabaseProductVersion 1.2.1.2.3.0.0-2557getDriverName Hive JDBCgetDriverVersion 1.2.1.2.3.0.0-2557getDriverMajorVersion 1getDriverMinorVersion 2select tint.rnum, tsint.rnum from tint , tsint where tint.cint between tsint.csint and tsint.csintwhen ORC used no rows returned versus TEXTcreate table if not exists TSINT ( RNUM int , CSINT smallint )&amp;#8211; ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' STORED AS orc ;create table if not exists TINT ( RNUM int , CINT int )&amp;#8211; ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' LINES TERMINATED BY '\n' STORED AS orc ;</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-28 01:00:00" id="11387" opendate="2015-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : fix reduce_deduplicate optimization</summary>
      <description>The main problem is that, due to return path, now we may have (RS1-GBY2)&amp;#45;(RS3-GBY4) when map.aggr=false, i.e., no map aggr. However, in the non-return path, it will be treated as (RS1)-(GBY2-RS3-GBY4). The main problem is that it does not take into account of the setting.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.AbstractCorrelationProcCtx.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-3-31 01:00:00" id="11424" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rule to transform OR clauses into IN clauses in CBO</summary>
      <description>We create a rule that will transform OR clauses into IN clauses (when possible).</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.semijoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrExprProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-31 01:00:00" id="11429" opendate="2015-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase default JDBC result set fetch size (# rows it fetches in one RPC call) to 1000 from 50</summary>
      <description>This is in addition to HIVE-10982 which plans to make the fetch size customizable. This just bumps the default to 1000.</description>
      <version>0.14.0,1.0.0,1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-4-6 01:00:00" id="11484" opendate="2015-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ObjectInspector for Char and VarChar</summary>
      <description>The creation of HiveChar and Varchar is not happening through ObjectInspector.Here is fix we pushed internally : https://github.com/InMobi/hive/commit/fe95c7850e7130448209141155f28b25d3504216</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestStandardObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestObjectInspectorConverters.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorConverter.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveVarcharObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaHiveCharObjectInspector.java</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveVarchar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-8-8 01:00:00" id="11502" opendate="2015-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Map side aggregation is extremely slow</summary>
      <description>For the query as following:create table tbl2 as select col1, max(col2) as col2 from tbl1 group by col1;If the column for group by has many different values (for example 400000) and it is in type double, the map side aggregation is very slow. I ran the query which took more than 3 hours , after 3 hours, I have to kill the query.The same query can finish in 7 seconds, if I turn off map side aggregation by:set hive.map.aggr = false;</description>
      <version>1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-11 01:00:00" id="11526" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-9-19 01:00:00" id="11605" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results with bucket map join in tez.</summary>
      <description>In some cases, we aggressively try to convert to a bucket map join and this ends up producing incorrect results.</description>
      <version>1.0.0,1.0.1,1.2.0</version>
      <fixedVersion>1.0.2,1.2.2,1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-8-19 01:00:00" id="11607" opendate="2015-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Export tables broken for data &gt; 32 MB</summary>
      <description>Broken for both hadoop-1 as well as hadoop-2 line</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.0.20S.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-31 01:00:00" id="11694" opendate="2015-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Exclude hbase-metastore for hadoop-1</summary>
      <description>hbase-metastore doesn't compile for hadoop-1 and we don't have development plan to make it work with hadoop-1. Exclude hbase-metastore related file so hadoop-1 still compiles.</description>
      <version>None</version>
      <fixedVersion>hbase-metastore-branch,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-3 01:00:00" id="11723" opendate="2015-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect string literal escaping</summary>
      <description>When I execute the following queriesCREATE TABLE t_hive (f1 STRING);INSERT INTO t_hive VALUES ('Cooper\'s');SELECT * FROM t_hive;via the Hive shell or through HiveServer2 directly (via impyla), I would expect that the result to beCooper'sbut instead I actually getCooper\'sActually, I'm not sure how that INSERT query is not even a syntax error.</description>
      <version>1.1.1,1.2.0,2.0.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-9-8 01:00:00" id="11761" opendate="2015-9-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DoubleWritable hashcode for GroupBy is not properly generated</summary>
      <description>HIVE-11502 fixed the hashcode for LazyDouble. Additionally we should fix for DoubleWritable as well due to HADOOP-12217 issue. In some cases such as select avg(t) from (select * from over1k cross join src) t group by d; where d is double type, the data is actually in DoubleWritable, not LazyDouble. Thus, before HADOOP-12217 gets fixed, we need to fix hashcode for LazyDouble as well as DoubleWritable.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.KeyWrapperFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-9 01:00:00" id="11766" opendate="2015-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Remove MiniLlapCluster from shim layer after hadoop-1 removal</summary>
      <description>Remove HIVE-11732 changes after HIVE-11378 goes in.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.configuration.LlapDaemonConfiguration.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">data.conf.llap.llap-daemon-site.xml</file>
      <file type="M">data.conf.llap.hive-site.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-14 01:00:00" id="11810" opendate="2015-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Exception is ignored if MiniLlap cluster fails to start</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-14 01:00:00" id="11816" opendate="2015-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade groovy to 2.4.4</summary>
      <description>Groovy 2.4.4 is the latest release and the first done under ASF.Also there are some issues with old Groovy like CVE-2015-3253, which doesn't seem to affect Hive itself but might affect applications depending on Hive that get leaked classpath artifacts.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-10-30 01:00:00" id="12004" opendate="2015-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SDPO doesnt set colExprMap correctly on new RS</summary>
      <description>As a result plan gets into a bad state.</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.opt.vectorization.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.optimization2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynpart.sort.optimization.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.CorrelationUtilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-5 01:00:00" id="12032" opendate="2015-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add unit test for HIVE-9855</summary>
      <description/>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-6 01:00:00" id="12042" opendate="2015-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update some out files</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-26 01:00:00" id="12265" opendate="2015-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate lineage info only if requested</summary>
      <description>If lineage related hook is not configured, we should not generate lineage info.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-10-27 01:00:00" id="12278" opendate="2015-10-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip logging lineage for explain queries</summary>
      <description>For explain queries, we don't generate the lineage info. So we should not try to log it at all.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-10-29 01:00:00" id="12294" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>log line "Duplicate ID &lt;number&gt; in column ID list" appears in the logs</summary>
      <description>As far as I could tell, this should not happen because some code in Hive iterates this list in parallel with names, assuming each ID will correspond to a name; however, I've seen it in some logs recently. Either names also contain duplicates and we should remove the line, or something could be broken. Could also be different for different operators? No idea.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ColumnProjectionUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-29 01:00:00" id="12297" opendate="2015-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path) : dealing with &amp;#39;$&amp;#39; in typeInfo</summary>
      <description>To repo, run udf_max.q with return path turned on.</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-11-30 01:00:00" id="12305" opendate="2015-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): UDAF can not pull up constant expressions</summary>
      <description>to repro, run annotate_stats_groupby.q with return path turned on.</description>
      <version>1.2.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveCalciteUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-12-9 01:00:00" id="12632" opendate="2015-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: don&amp;#39;t use IO elevator for ACID tables</summary>
      <description>Until HIVE-12631 is fixed, we need to avoid ACID tables in IO elevator. Right now, a FileNotFound error is thrown.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.ColumnVectorProducer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapInputFormat.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-1-22 01:00:00" id="12907" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading - II</summary>
      <description>Remove unnecessary calls to metastore.</description>
      <version>0.14.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-2-22 01:00:00" id="12908" opendate="2016-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading III</summary>
      <description>Remove unnecessary Namenode calls.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-2-8 01:00:00" id="13020" opendate="2016-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metastore and HiveServer2 to Zookeeper fails with IBM JDK</summary>
      <description>HiveServer2 and Hive Metastore Zookeeper component is hardcoded to only support the Oracle/Open JDK. I was performing testing of Hadoop running on the IBM JDK and discovered this issue and have since drawn up the attached patch. This looks to resolve the issue in a similar manner as how the Hadoop core folks handle the IBM JDK.</description>
      <version>1.2.0,1.2.1,1.3.0</version>
      <fixedVersion>1.2.2,1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.Utils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-3-24 01:00:00" id="13141" opendate="2016-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on Spark over HBase should accept parameters starting with "zookeeper.znode"</summary>
      <description>HBase related paramters has been added by HIVE-12708.Following the same way,parameters starting with "zookeeper.znode" should be add too,which are also HBase related paramters .Refering to http://blog.cloudera.com/blog/2013/10/what-are-hbase-znodes/I have seen a failure with Hive on Spark over HBase due to customize zookeeper.znode.parent.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveSparkClientFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-29 01:00:00" id="13373" opendate="2016-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use most specific type for numerical constants</summary>
      <description>tinyint &amp; shortint are currently inferred as ints, if they are without postfix.</description>
      <version>1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.type.widening.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-4-30 01:00:00" id="13385" opendate="2016-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Cleanup] Streamline Beeline instantiation</summary>
      <description>Janitorial. Remove circular dependencies in BeelineCommandLineCompleter. Stream line code readability.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLineCommandCompleter.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2016-5-20 01:00:00" id="13561" opendate="2016-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 is leaking ClassLoaders when add jar / temporary functions are used</summary>
      <description>I can repo this on branch-1.2 and branch-2.0.It looks to be the same issues as: HIVE-11408The patch from HIVE-11408 looks to fix the issue as well.I've updated the patch from HIVE-11408 to be aligned with branch-1.2 and master</description>
      <version>1.2.0,1.2.1,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Registry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-5-29 01:00:00" id="13652" opendate="2016-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Import table change order of dynamic partitions</summary>
      <description>Table with multiple dynamic partitions like year,month, day exported using "export table" command is imported (using "import table") such a way that order of partitions is changed to day, month, year.Export DB: Hive 0.14Import DB: Hive 1.2.1000.2.4.0.0-169Tables created as:create table T1( ... ) PARTITIONED BY (period_year string, period_month string, period_day string) STORED AS ORC TBLPROPERTIES ("orc.compress"="SNAPPY");export command:export table t1 to 'path'import command:import table t1 from 'path'HDFS file structure on both original table location and export path keeps the original partition order ../year/month/dayHDFS file structure after import is .../day/month/year</description>
      <version>1.2.0,1.2.1</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-5-25 01:00:00" id="1366" opendate="2010-5-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>inputFileFormat error if the merge job takes a different input file format than the default output file format</summary>
      <description>If the input file format is say SequenceFileFormat and the default fileformat is RCFile. the merge job after the MR job assumes the input format is SequenceFile format rather than RCFile. This is probably introduced in HIVE-1357.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-5-30 01:00:00" id="13660" opendate="2016-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorizing IN expression with list of columns throws java.lang.ClassCastException ExprNodeColumnDesc cannot be cast to ExprNodeConstantDesc</summary>
      <description>Example:SELECT * FROM alltypesorc WHERE cint in (ctinyint, cbigint);</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2016-5-10 01:00:00" id="13726" opendate="2016-5-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve dynamic partition loading VI</summary>
      <description>Parallelize deletes and other refactoring.</description>
      <version>1.2.0,2.0.0</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-8-17 01:00:00" id="1413" opendate="2010-6-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bring a table/partition offline</summary>
      <description>There should be a way to bring a table/partition offline.At that time, no read/write operations should be supported on that table.It would be very useful for housekeeping operations</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.AlterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2016-11-29 01:00:00" id="15311" opendate="2016-11-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze column stats should skip non-primitive column types</summary>
      <description>after this patch, when you compute column stats, it will skip the non-primitive column types and give you warning on the console.</description>
      <version>1.2.0,2.0.0,2.1.0</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.columnstats.tbllvl.complex.type.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ColumnStatsSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-30 01:00:00" id="15312" opendate="2016-11-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce logging in certain places</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="15322" opendate="2016-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skipping "hbase mapredcp" in hive script for certain services</summary>
      <description>"hbase mapredcp" is intended to append hbase classpath to hive. However, the command can take some time when the system is heavy loaded. In some extreme cases, we saw ~20s delay due to it. For certain commands, such as "schemaTool", hbase classpath is certainly useless, and we can safely skip invoking it.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-1 01:00:00" id="15323" opendate="2016-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow the user to turn off reduce-side SMB join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.OpTraits.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SparkMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.metainfo.annotation.OpTraitsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2010-9-17 01:00:00" id="1650" opendate="2010-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestContribNegativeCliDriver fails</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-5-11 01:00:00" id="16652" opendate="2017-5-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LlapInputFormat: Seeing "output error" WARN message</summary>
      <description>Another warning message I'm seeing in the logs for TestJdbcWithMiniLlap after adding the line to close the RecordReader in the test:2017-05-11T11:08:34,511 WARN [IPC Server handler 0 on 54847] ipc.Server: IPC Server handler 0 on 54847, call Call#341 Retry#0 heartbeat({ containerId=container_6830411502416918223_0003_00_000000, requestId=2, startIndex=0, preRoutedStartIndex=0, maxEventsToGet=500, taskAttemptId=attempt_6830411502416918223_0003_0_00_000000_0, eventCount=2 }), rpc version=2, client version=1, methodsFingerPrint=996603002 from 10.22.8.180:54849: output error</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.tezplugins.helpers.LlapTaskUmbilicalServer.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.ext.LlapTaskUmbilicalExternalClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-5-23 01:00:00" id="16742" opendate="2017-5-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>cap the number of reducers for LLAP at the configured value</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-2 01:00:00" id="16815" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up javadoc from error for the rest of modules</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
      <file type="M">testutils.src.java.org.apache.hive.testutils.jdbc.HiveBurnInClient.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.StreamingTransaction.java</file>
      <file type="M">streaming.src.java.org.apache.hive.streaming.HiveStreamingConnection.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.FileMetadataCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PartitionIterable.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MaterializationsRebuildLockHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaHook.java</file>
      <file type="M">service.src.java.org.apache.hive.service.ServiceOperations.java</file>
      <file type="M">service.src.java.org.apache.hive.service.Service.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.predicate.AccumuloPredicateHandler.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloCompositeRowId.java</file>
      <file type="M">common.src.java.org.apache.hive.http.ProfileServlet.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.mr.GenericMR.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.RegexSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.udaf.example.UDAFExampleAvg.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseCompositeKey.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseStructValue.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.struct.HBaseValueFactory.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.transfer.HCatWriter.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.NotificationListener.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.AbstractRecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.DelimitedInputWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.HiveEndPoint.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.mutate.worker.MutatorCoordinator.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.RecordWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StreamingConnection.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictJsonWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.TransactionBatch.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationTask.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Meta.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Utils.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Var.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapDump.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCache.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.ConsumerFeedback.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.PriorityBlockingDeque.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonJvmInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.CustomQueryFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.GroupFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.SearchResultHandler.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.UserFilterFactory.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.PasswdAuthenticationProvider.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.TSetIpAddressProcessor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.CLIServiceUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.ClassicTableTypeMapping.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.TableTypeMapping.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-6-2 01:00:00" id="16819" opendate="2017-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add MM test for temporary table</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-9-21 01:00:00" id="17152" opendate="2017-7-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve security of random generator for HS2 cookies</summary>
      <description>The random number generated is used as a secret to append to a sequence and SHA to implement a CookieSigner. If this is attackable, then it's possible for an attacker to sign a cookie as if we had. We should fix this and use SecureRandom as a stronger random function .HTTPAuthUtils has a similar issue. If that is attackable, an attacker might be able to create a similar cookie. Paired with the above issue with the CookieSigner, it could reasonably spoof a HS2 cookie.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.HttpAuthUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-5 01:00:00" id="17455" opendate="2017-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>External LLAP client: connection to HS2 should be kept open until explicitly closed</summary>
      <description>In the case that a complex query (aggregation/join) is passed to external LLAP client, the query result is first saved as a Hive temp table before being read by LLAP to client. Currently the HS2 connection used to fetch the LLAP splits is closed right after the splits are fetched, which means the temp table is gone by the time LLAP tries to read it.Try to keep the connection open so that the table is still around when LLAP tries to read it. Add close methods which can be used to close the connection when the client is done with the query.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-9-5 01:00:00" id="17456" opendate="2017-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set current database for external LLAP interface</summary>
      <description>Currently the query passed in to external LLAP client has the default DB as the current database.Allow user to specify a different current database.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-ext-client.src.java.org.apache.hadoop.hive.llap.LlapBaseInputFormat.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniLlap.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2019-5-16 01:00:00" id="21739" opendate="2019-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make metastore DB backward compatible with pre-catalog versions of hive.</summary>
      <description>Since the addition of foreign key constraint between Database ('DBS') table and catalogs ('CTLGS') table in HIVE-18755 we are unable to run a simple create database command with an older version of Metastore Server. This is due to older versions having JDO schema as per older schema of 'DBS' which did not have an additional 'CTLG_NAME' column.The error is as follows: org.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Exception thrown flushing changes to datastore)....java.sql.BatchUpdateException: Cannot add or update a child row: a foreign key constraint fails ("metastore_1238"."DBS", CONSTRAINT "CTLG_FK1" FOREIGN KEY ("CTLG_NAME") REFERENCES "CTLGS" ("NAME"))</description>
      <version>1.2.0,2.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-2.3.0-to-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.2.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.1.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-3.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-2.3.0-to-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.2.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.1.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-3.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-2.3.0-to-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.2.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.1.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-3.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.2.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.1.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-3.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-2.3.0-to-3.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.2.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.1.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-3.0.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-2-7 01:00:00" id="3995" opendate="2013-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PostgreSQL upgrade scripts are not valid</summary>
      <description>I've noticed that scripts for upgrading metastore backed up on PostgreSQL are not valid.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.012-HIVE-1362.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.011-HIVE-3649.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.010-HIVE-3072.postgres.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-4-17 01:00:00" id="8164" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adding in a ReplicationTask that converts a Notification Event to actionable tasks</summary>
      <description/>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.test.java.org.apache.hive.hcatalog.api.TestHCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatNotificationEvent.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClient.java</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-17 01:00:00" id="8166" opendate="2014-9-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: 1) Bailout in strict mode 2) OB,LIMIT RR table alias is same as that of sub query 3) If RowCount Not found then fall back to non cbo 4)Fix NPE in unique col name check</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveProjectRel.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-9-23 01:00:00" id="853" opendate="2009-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a hint to select which tables to stream in a join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.uniquejoin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.union2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.rc.seq.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.wrong.fileformat.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.uniquejoin.q</file>
      <file type="M">ql.src.test.queries.clientnegative.union2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.load.wrong.fileformat.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBJoinTree.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8530" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Preserve types of literals</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-10-15 01:00:00" id="877" opendate="2009-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>error with cast(null as short)</summary>
      <description>also cast(null as long)</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.negative.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.negative.q</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-28 01:00:00" id="9485" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update trunk to 1.2.0-SNAPSHOT</summary>
      <description>As discussed on list, 0.14.1 will be 1.0 and 0.15 will be 1.1. As such we should change trunk to 1.2.0.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.pom.xml</file>
      <file type="M">spark-client.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.aggregator.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">shims.0.20S.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">odbc.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.test-serde.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hive-unit-hadoop2.pom.xml</file>
      <file type="M">itests.hive-minikdc.pom.xml</file>
      <file type="M">itests.hive-jmh.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">itests.custom-serde.pom.xml</file>
      <file type="M">hwi.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">ant.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-28 01:00:00" id="9487" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Remote Spark Context secure [Spark Branch]</summary>
      <description>The RSC currently uses an ad-hoc, insecure authentication mechanism. We should instead use a proper auth mechanism and add encryption to the mix.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestRpc.java</file>
      <file type="M">spark-client.src.test.java.org.apache.hive.spark.client.rpc.TestKryoMessageCodec.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientFactory.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcServer.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.Rpc.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.README.md</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.RemoteDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-28 01:00:00" id="9499" opendate="2015-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.limit.query.max.table.partition makes queries fail on non-partitioned tables</summary>
      <description>If you use hive.limit.query.max.table.partition to limit the amount of partitions that can be queried it makes queries on non-partitioned tables fail.Example:CREATE TABLE tmp(test INT);SELECT COUNT(*) FROM TMP; -- works fineSET hive.limit.query.max.table.partition=20;SELECT COUNT(*) FROM TMP; -- generates NPE (FAILED: NullPointerException null)SET hive.limit.query.max.table.partition=-1;SELECT COUNT(*) FROM TMP; -- works fine again</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0,2.0.0</version>
      <fixedVersion>2.0.1,2.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-2 01:00:00" id="95" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>improve cli error messages by lowering backtracking to 1</summary>
      <description>Stop antlr from backtracking so much should (and does) improve error messages since antlr will report the error closer to where it happened.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-30 01:00:00" id="9525" opendate="2015-1-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable constant propagation optimization in few existing tests where it was disabled.</summary>
      <description>We have disabled it previously because of issues. But testing again those issues looks like have gone away. We should reenable optimization for these tests.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.vector.coalesce.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.vectorization.ppd.q</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-2 01:00:00" id="9554" opendate="2015-2-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rename 0.15 upgrade scripts to 1.1</summary>
      <description/>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade.order.postgres</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.14.0-to-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.15.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade.order.oracle</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.14.0-to-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.15.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade.order.mysql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.14.0-to-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.15.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade.order.mssql</file>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.14.0-to-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.15.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade.order.derby</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.14.0-to-0.15.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.15.0.derby.sql</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-5 01:00:00" id="9586" opendate="2015-2-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Too verbose log can hurt performance, we should always check log level first</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyBinary.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-2-7 01:00:00" id="9608" opendate="2015-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Define SPARK_HOME if not defined automagically</summary>
      <description>many hadoop installs are in dir/{spark,hive,hadoop,..}. We can infer SPARK_HOME in these cases.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-1 01:00:00" id="961" opendate="2009-12-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug in Groupby causing type cast exception</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-9 01:00:00" id="9621" opendate="2015-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 http mode - embedded jetty should use SynchronousQueue</summary>
      <description>Noticed an unreproducible bug (customer reported), where HiveServer2 in http mode would accept the incoming tcp connection but would not allocate a new thread to process the request. Switching from LinkedBlockingQueue to SynchronousQueue fixes the issue and also simplifies the threading model.</description>
      <version>1.1.0,1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-11 01:00:00" id="9652" opendate="2015-2-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez in place updates should detect redirection of STDERR</summary>
      <description>Tez in place updates detects STDOUT redirection and logs using old logging method. Similarly it should detect STDERR redirection as well. This will make sure following will log using old methodhive -e '&lt;some_query&gt;' 2&gt; err.log</description>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-17 01:00:00" id="9705" opendate="2015-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>All curator deps should be listed in dependency management section</summary>
      <description>HADOOP-11492 brings in a new version of curator which doesn't work for us.</description>
      <version>1.2.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2015-3-24 01:00:00" id="9775" opendate="2015-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add a MiniLLAPCluster for tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-24 01:00:00" id="9777" opendate="2015-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Add an option to disable uberization</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-26 01:00:00" id="9793" opendate="2015-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove hard coded paths from cli driver tests</summary>
      <description>At some point a change which generates a hard coded path into the test files snuck in. Insert we should use the HIVE_ROOT directory as this is better for ptest environments.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-12-2 01:00:00" id="98" opendate="2008-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dependency management with hadoop core using either maven or ivy</summary>
      <description>We need to move from pre packaging jars to managing external dependencies with hadoop core (and later other packaged jars) using maven or ivy.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subq.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapreduce2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input3.limit.q</file>
      <file type="M">ql.src.test.queries.clientpositive.input13.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">hadoopcore.lib.slf4j-LICENSE.txt</file>
      <file type="M">hadoopcore.lib.kfs-0.2.LICENSE.txt</file>
      <file type="M">hadoopcore.lib.junit-3.8.1.LICENSE.txt</file>
      <file type="M">hadoopcore.lib.jetty-5.1.4.LICENSE.txt</file>
      <file type="M">hadoopcore.lib.hsqldb-1.8.0.10.LICENSE.txt</file>
      <file type="M">hadoopcore.conf.ssl-server.xml.example</file>
      <file type="M">hadoopcore.conf.ssl-client.xml.example</file>
      <file type="M">hadoopcore.conf.slaves.template</file>
      <file type="M">hadoopcore.conf.slaves</file>
      <file type="M">hadoopcore.conf.masters.template</file>
      <file type="M">hadoopcore.conf.masters</file>
      <file type="M">hadoopcore.conf.log4j.properties</file>
      <file type="M">hadoopcore.conf.hadoop-site.xml.template</file>
      <file type="M">hadoopcore.conf.hadoop-site.xml</file>
      <file type="M">hadoopcore.conf.hadoop-metrics.properties</file>
      <file type="M">hadoopcore.conf.hadoop-env.sh.template</file>
      <file type="M">hadoopcore.conf.hadoop-env.sh</file>
      <file type="M">hadoopcore.conf.hadoop-default.xml</file>
      <file type="M">hadoopcore.conf.configuration.xsl</file>
      <file type="M">hadoopcore.conf.capacity-scheduler.xml.template</file>
      <file type="M">hadoopcore.conf.capacity-scheduler.xml</file>
      <file type="M">hadoopcore.bin.stop-mapred.sh</file>
      <file type="M">hadoopcore.bin.stop-dfs.sh</file>
      <file type="M">hadoopcore.bin.stop-balancer.sh</file>
      <file type="M">hadoopcore.bin.stop-all.sh</file>
      <file type="M">hadoopcore.bin.start-mapred.sh</file>
      <file type="M">hadoopcore.bin.start-dfs.sh</file>
      <file type="M">hadoopcore.bin.start-balancer.sh</file>
      <file type="M">hadoopcore.bin.start-all.sh</file>
      <file type="M">hadoopcore.bin.slaves.sh</file>
      <file type="M">hadoopcore.bin.rcc</file>
      <file type="M">hadoopcore.bin.hadoop-daemons.sh</file>
      <file type="M">hadoopcore.bin.hadoop-daemon.sh</file>
      <file type="M">hadoopcore.bin.hadoop-config.sh</file>
      <file type="M">hadoopcore.bin.hadoop</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-26 01:00:00" id="9800" opendate="2015-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create scripts to do metastore upgrade tests on Jenkins</summary>
      <description>NO PRECOMMIT TESTSIn order to have a better quality code for Hive Metastore, we need to create some upgrade scripts that can run on Jenkins nightly or everytime a patch is added to the ticket that makes structural changes on the database.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-2 01:00:00" id="9831" opendate="2015-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 should use ConcurrentHashMap in ThreadFactory</summary>
      <description/>
      <version>0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.server.ThreadFactoryWithGarbageCleanup.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-3 01:00:00" id="9836" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: fails when virtual columns are present in the join conditions (for e.g. partition columns)</summary>
      <description>explainselect a.key, a.value, b.valuefrom tab a join tab_part b on a.key = b.key and a.ds = b.ds;fails.</description>
      <version>1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-3 01:00:00" id="9842" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable session/operation timeout by default in HiveServer2</summary>
      <description>HIVE-5799 introduced a session/operation timeout which cleans up abandoned session and op handles. Currently, the default is set to no-op. We should set it to some reasonable value.</description>
      <version>1.2.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-3 01:00:00" id="9843" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: instrument logging for fragments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.hive-exec-log4j.properties</file>
      <file type="M">llap-server.src.test.resources.llap-daemon-log4j.properties</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.decode.OrcEncodedDataConsumer.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">common.src.main.resources.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-4 01:00:00" id="9849" opendate="2015-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: add multi threaded object registry</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.llapdecider.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ObjectCacheFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
</bugrepository>