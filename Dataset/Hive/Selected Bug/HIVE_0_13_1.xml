<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  
  
  <bug fixdate="2015-5-29 01:00:00" id="10541" opendate="2015-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline requires newline at the end of each query in a file</summary>
      <description>Beeline requires newline at the end of each query in a file.</description>
      <version>0.13.1</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.BeeLine.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-9-21 01:00:00" id="10785" opendate="2015-5-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support aggregate push down through joins</summary>
      <description>Enable AggregateJoinTransposeRule in CBO that pushes Aggregate through Join operators. The rule has been extended in Calcite 1.4 to cover complex cases e.g. Aggregate operators comprising UDAF. The decision on whether to push the Aggregate through Join or not should be cost-driven.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-27 01:00:00" id="10835" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-11-22 01:00:00" id="11072" opendate="2015-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add data validation between Hive metastore upgrades tests</summary>
      <description>An existing Hive metastore upgrade test is running on Hive jenkins. However, these scripts do test only database schema upgrade, not data validation between upgrades.We should validate data between metastore version upgrades. Using data validation, we may ensure that data won't be damaged, or corrupted when upgrading the Hive metastore.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-30 01:00:00" id="11409" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): add SEL before UNION</summary>
      <description>Two purpose: (1) to ensure that the data type of non-primary branch (the 1st branch is the primary branch) of union can be casted to that of the primary branch; (2) to make UnionProcessor optimizer work; (3) if the SEL is redundant, it will be removed by IdentidyProjectRemover optimizer.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-2-11 01:00:00" id="11526" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-1-4 01:00:00" id="119" opendate="2008-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add more informative error messages to grammar parsing</summary>
      <description>Some error messages give the user no context or help really to know what part of their stmt is wrong.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.wrong.distinct2.q.out</file>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.tbl.name.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl2.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.create.tbl2.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-9-24 01:00:00" id="11952" opendate="2015-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>disable q tests that are both slow and less relevant</summary>
      <description>We will disable several tests that test obscure and old features and take inordinate amount of time, and file JIRAs to look at their perf if someone still cares about them.</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-3 01:00:00" id="12327" opendate="2015-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat e2e tests TestJob_1 and TestJob_2 fail</summary>
      <description>The tests are added in HIVE-7035. Both are negative tests and check if the http status code is 400. The original patch capture the exception containing specific message. However, in latter version of Hadoop, the message change so the exception is not contained.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.mapred.WebHCatJTShim23.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-2-24 01:00:00" id="13133" opendate="2016-2-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create initial InputFormat + record readers/writers</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HiveSplitGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SerializationUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-2-20 01:00:00" id="5859" opendate="2013-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create view does not captures inputs</summary>
      <description>For example, CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;should capture "default.tbl_j5jbymsx8e" as input entity for authorization process but currently it's not.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-5 01:00:00" id="6551" opendate="2014-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>group by after join with skew join optimization references invalid task sometimes</summary>
      <description>For example,hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.skewjoin = true;hive&gt; set hive.skewjoin.key = 3;hive&gt; &gt; EXPLAIN FROM &gt; (SELECT src.* FROM src) x &gt; JOIN &gt; (SELECT src.* FROM src) Y &gt; ON (x.key = Y.key) &gt; SELECT sum(hash(Y.key)), sum(hash(Y.value));OKSTAGE DEPENDENCIES: Stage-8 is a root stage Stage-6 depends on stages: Stage-8 Stage-5 depends on stages: Stage-6 , consists of Stage-4, Stage-2 Stage-4 Stage-2 depends on stages: Stage-4, Stage-1 Stage-0 is a root stage...Stage-2 references not-existing Stage-1</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-11-18 01:00:00" id="68" opendate="2008-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Casting to short not working</summary>
      <description>Casting to and from short does not work.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cast1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToInteger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToDouble.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToByte.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFToBoolean.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.exprNodeFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-6 01:00:00" id="7016" opendate="2014-5-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive returns wrong results when execute UDF on top of DISTINCT column</summary>
      <description>The following query returns wrong result:select hash(distinct value) from table;This kind of query should be identified as syntax error. However, Hive ignores DISTINCT and returns the result.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-14 01:00:00" id="7066" opendate="2014-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar is missing avro core</summary>
      <description>Running a simple query that reads an Avro table caused the following exception to be thrown on the cluster side:java.lang.RuntimeException: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:365) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:276) at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:254) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:445) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:438) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587) at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:191) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:412) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:394) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190) at org.apache.hadoop.mapred.Child.main(Child.java:249)Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:942) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:850) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:864) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:334) ... 13 moreCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:26) at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:343) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:336) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:56) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:476) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:148) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113) ... 25 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:32) ... 37 moreCaused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumWriter at java.lang.Class.getDeclaredFields0(Native Method) at java.lang.Class.privateGetDeclaredFields(Class.java:2348) at java.lang.Class.getDeclaredFields(Class.java:1779) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.&lt;init&gt;(FieldSerializer.java:109) ... 42 moreCaused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumWriter at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ... 47 moreI took a look at the hive-exec jar and found that the Avro core jar was not included, though avro-mapred is included.I confirmed that Avro core was included in the Hive 0.12 hive-exec jar. Was there a reason why this was removed in trunk? It seems that this would break the AvroSerDe.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-15 01:00:00" id="7068" opendate="2014-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate AccumuloStorageHandler</summary>
      <description>Accumulo is a BigTable-clone which is similar to HBase. Some initial work has been done to support querying an Accumulo table using Hive already. It is not a complete solution as, most notably, the current implementation presently lacks support for INSERTs.I would like to polish up the AccumuloStorageHandler (presently based on 0.10), implement missing basic functionality and compare it to the HBaseStorageHandler (to ensure that we follow the same general usage patterns).I've also been in communication with bfem (the initial author) who expressed interest in working on this again. I hope to coordinate efforts with him.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-6-28 01:00:00" id="7138" opendate="2014-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add row index dump capability to ORC file dump</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.FileDump.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-11-18 01:00:00" id="72" opendate="2008-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>wrong results if partition pruning not strict and no mep-reduce job needed</summary>
      <description>Suppose T is a partitioned table on ds, where ds is a string column, the following queries: SELECT a.* FROM T a WHERE a.ds=2008-09-08 LIMIT 1; SELECT a.* FROM T a WHERE a.ds=2008-11-10 LIMIT 1;return the first row from the first partition.This is because of the typecast to double.for a.ds=2008-01-01 or anything (a.ds=1), evaluate (Double, Double) is invoked at partition pruning.Since '2008-11-01' is not a valid double, it is converted to a null, and therefore the result of pruning returns null (unknown) - not FALSE.All unknowns are also accepted, therefore all partitions are accepted which explains this behavior.filter is not invoked since it is a select * query, so map-reduce job is started.We just turn off this optimization if pruning indicates that there can be unknown partitions.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-16 01:00:00" id="7234" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select on decimal column throws NPE</summary>
      <description>Select on decimal column throws NPE for values greater than maximum permissible value (9999999999)Steps to repro:DROP TABLE IF EXISTS DECIMAL;CREATE TABLE DECIMAL (dec decimal);// Content of decimal_10_0.txt =&gt; "9999999999.999"LOAD DATA LOCAL INPATH '../../data/files/decimal_10_0.txt' OVERWRITE INTO TABLE DECIMAL;SELECT dec FROM DECIMAL; =&gt; throws NPEDROP TABLE DECIMAL;</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyHiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-16 01:00:00" id="7235" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TABLESAMPLE on join table is regarded as alias</summary>
      <description>SELECT c_custkey, o_custkeyFROM customer tablesample (1000 ROWS) join orders tablesample (1000 ROWS) on c_custkey = o_custkey;Fails with NPE</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sample8.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-12-22 01:00:00" id="7270" opendate="2014-6-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SerDe Properties are not considered by show create table Command</summary>
      <description>The HIVE table DDl generated by show create table &lt;target_table&gt; command does not contain SerDe properties of the target table even though it contain specific SerDe properties.</description>
      <version>0.13.1</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.serde.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.show.create.table.serde.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-27 01:00:00" id="7308" opendate="2014-6-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show auto-parallelism in extended explain</summary>
      <description>Add auto-parallelism flag to explain so that we can write tests verifying that we don't break bmj, etc...</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceSinkDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-30 01:00:00" id="7314" opendate="2014-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results of UDF when hive.cache.expr.evaluation is set</summary>
      <description>It seems that the expression caching doesn't work when using UDF inside another UDF or a hive function.For example :tbl has one row : 'a','b'The following query : select concat(custUDF(a),' ', custUDF(b)) from tbl; returns 'a a'seems to cache custUDF(a) and use it for custUDF(b).Same query without the concat works fine.Replacing the concat with another custom UDF also returns 'a a'</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-6 01:00:00" id="732" opendate="2009-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store intermediate data in binary using LazyBinarySerDe</summary>
      <description>Follow-up on HIVE-640. We should use LazyBinarySerDe in several places in the code to improve the efficiency: value between map-reduce boundary, temporary tables.We should also allow users to create tables stored as binary format.CREATE TABLE xxx (...)ROW FORMAT BINARYSTORED AS SEQUENCEFILE;</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-1 01:00:00" id="7323" opendate="2014-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type stats in ORC sometimes go stale</summary>
      <description>I cannot make proper test case but sometimes min/max value in date type stats is changed in runtime. Stats for other type contains non-mutable values in it but date type stats contains DateWritable, which of inner value can be changed anytime.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-1 01:00:00" id="7326" opendate="2014-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive complains invalid column reference with &amp;#39;having&amp;#39; aggregate predicates</summary>
      <description>CREATE TABLE TestV1_Staples ( Item_Count INT, Ship_Priority STRING, Order_Priority STRING, Order_Status STRING, Order_Quantity DOUBLE, Sales_Total DOUBLE, Discount DOUBLE, Tax_Rate DOUBLE, Ship_Mode STRING, Fill_Time DOUBLE, Gross_Profit DOUBLE, Price DOUBLE, Ship_Handle_Cost DOUBLE, Employee_Name STRING, Employee_Dept STRING, Manager_Name STRING, Employee_Yrs_Exp DOUBLE, Employee_Salary DOUBLE, Customer_Name STRING, Customer_State STRING, Call_Center_Region STRING, Customer_Balance DOUBLE, Customer_Segment STRING, Prod_Type1 STRING, Prod_Type2 STRING, Prod_Type3 STRING, Prod_Type4 STRING, Product_Name STRING, Product_Container STRING, Ship_Promo STRING, Supplier_Name STRING, Supplier_Balance DOUBLE, Supplier_Region STRING, Supplier_State STRING, Order_ID STRING, Order_Year INT, Order_Month INT, Order_Day INT, Order_Date_ STRING, Order_Quarter STRING, Product_Base_Margin DOUBLE, Product_ID STRING, Receive_Time DOUBLE, Received_Date_ STRING, Ship_Date_ STRING, Ship_Charge DOUBLE, Total_Cycle_Time DOUBLE, Product_In_Stock STRING, PID INT, Market_Segment STRING );Query that works:SELECT customer_name, SUM(customer_balance), SUM(order_quantity) FROM default.testv1_staples s1 GROUP BY customer_name HAVING ((COUNT(s1.discount) &lt;= 822) AND(SUM(customer_balance) &lt;= 4074689.000000041));Query that fails:SELECT customer_name, SUM(customer_balance), SUM(order_quantity) FROM default.testv1_staples s1 GROUP BY customer_name HAVING ((SUM(customer_balance) &lt;= 4074689.000000041)AND (COUNT(s1.discount) &lt;= 822));</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-3 01:00:00" id="7344" opendate="2014-7-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add streaming support in Windowing mode for FirstVal, LastVal</summary>
      <description>Continuation of HIVE-7062, HIVE-7143</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.navfn.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFFirstValue.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2014-7-8 01:00:00" id="7364" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk cannot be built on -Phadoop1 after HIVE-7144</summary>
      <description>Text.copyBytes() is introduced in hadoop-2</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-8 01:00:00" id="7365" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain authorize for auth2 throws exception</summary>
      <description>throws NPE in auth v2.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.view.sqlstd.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-10 01:00:00" id="7385" opendate="2014-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize for empty relation scans</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-11 01:00:00" id="7388" opendate="2014-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Remove non-ascii char from comments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-11 01:00:00" id="7392" opendate="2014-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Support Columns Stats for Partition Columns</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-15 01:00:00" id="7407" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Handle UDFs generically</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-15 01:00:00" id="7412" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>column stats collection throws exception if all values for a column is null</summary>
      <description/>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-15 01:00:00" id="7413" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fall back to Non-CBO optimizer if CBO fails</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-7-16 01:00:00" id="7423" opendate="2014-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>produce hive-exec-core.jar from ql module</summary>
      <description>currently ql module produces hive-exec-$version.jar which is an uber jar. It's also useful to have a thin jar, let's call it hive-exec-$version-core.jar, that only has classes from ql.</description>
      <version>0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-16 01:00:00" id="7429" opendate="2014-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set replication for archive called before file exists</summary>
      <description>The call to set replication is called prior to uploading the archive file to hdfs, which does not throw an error, but the replication never gets set.This has a significant impact on large jobs (especially hash joins) due to too many tasks hitting the data nodes.</description>
      <version>0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-8 01:00:00" id="743" opendate="2009-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>let user specify serde for custom scripts</summary>
      <description>Splitting up https://issues.apache.org/jira/browse/HIVE-708 into this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-18 01:00:00" id="7446" opendate="2014-7-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support to ALTER TABLE .. ADD COLUMN to Avro backed tables</summary>
      <description>HIVE-6806 adds native support for creating hive table stored as Avro. It would be good to add support to ALTER TABLE .. ADD COLUMN to Avro backed tables.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.resources.avro-struct.avsc</file>
      <file type="M">serde.src.test.resources.avro-nested-struct.avsc</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.avro.TestTypeInfoToSchema.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.TypeInfoToSchema.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-22 01:00:00" id="7474" opendate="2014-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Expression Converter needs to handle Flattened expressions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-23 01:00:00" id="7486" opendate="2014-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Delete jar should close current classloader</summary>
      <description>Deleting a jar, hive creates a new classloader discarding old one. The old loader should be properly removed by JavaUtils.closeClassLoader.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-7-25 01:00:00" id="7515" opendate="2014-7-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix NPE in CBO</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-28 01:00:00" id="7531" opendate="2014-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>auxpath parameter does not handle paths relative to current working directory.</summary>
      <description>NO PRECOMMIT TESTSIf I were to specify the auxpath value as a relative pathhive --auxpath libI get the following errorjava.lang.IllegalArgumentException: Wrong FS: file://lib/Test.jar, expected: file:/// at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:625) at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:69) at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:464) at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:380) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:231) at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:183) at org.apache.hadoop.mapred.JobClient.copyRemoteFiles(JobClient.java:715) at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:818) at org.apache.hadoop.mapred.JobClient.copyAndConfigureFiles(JobClient.java:743) at org.apache.hadoop.mapred.JobClient.access$400(JobClient.java:174) at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:960) at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:945) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:945) at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:919) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:420)</description>
      <version>0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-31 01:00:00" id="7563" opendate="2014-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader should be released from LogFactory</summary>
      <description>NO PRECOMMIT TESTSLogFactory uses ClassLoader as a key in map, which makes the classloader impossible to be unloaded. LogFactory.release() should be called explicitly.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-31 01:00:00" id="7565" opendate="2014-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Fix exception in Greedy Join reordering Algo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-31 01:00:00" id="7577" opendate="2014-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Introduce Bushy Join Re order Algo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-1 01:00:00" id="7595" opendate="2014-8-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>isKerberosMode() does a case sensitive comparison</summary>
      <description/>
      <version>0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-3 01:00:00" id="7597" opendate="2014-8-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support analyze table [Spark Branch]</summary>
      <description>Both MR and Tez has a visitor processing "analyze table ..." command. We cloned the code from Tez, but may need to make it fit for Spark, verify, and test.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-17 01:00:00" id="760" opendate="2009-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add version info to META-INF/MANIFEST.MF</summary>
      <description>Add Implementation-Title, Implementation-Version and Implementation-Vendor data to the manifest files included in Hive jars. This will be useful to the JDBC client and Hive Sever in particular, which need to return version info as part of their APIs.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-5 01:00:00" id="7612" opendate="2014-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Add link to parent vertex to mapjoin in explain</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="7615" opendate="2014-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should have an option for user to see the query progress</summary>
      <description>When executing query in Beeline, user should have a option to see the progress through the outputs.Beeline could use the API introduced in HIVE-4629 to get and display the logs to the client.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-6 01:00:00" id="7625" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: stats for Partitioned tables are not read correctly.</summary>
      <description>The wrong call is being made to read stats for Partitioned tables.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-6 01:00:00" id="7628" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: log Plan coming out of each phase in Optiq Planning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-7 01:00:00" id="7642" opendate="2014-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set hive input format by configuration.[Spark Branch]</summary>
      <description>Currently hive input format is hard coded as HiveInputFormat, we should set this parameter from configuration.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-8-7 01:00:00" id="7654" opendate="2014-8-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A method to extrapolate columnStats for partitions of a table</summary>
      <description>In a PARTITIONED table, there are many partitions. For example, create table if not exists loc_orc ( state string, locid int, zip bigint) partitioned by(year string) stored as orc;We assume there are 4 partitions, partition(year='2000'), partition(year='2001'), partition(year='2002') and partition(year='2003').We can use the following command to compute statistics for columns state,locid of partition(year='2001')analyze table loc_orc partition(year='2001') compute statistics for columns state,locid;We need to know the “aggregated” column status for the whole table loc_orc. However, we may not have the column status for some partitions, e.g., partition(year='2002') and also we may not have the column status for some columns, e.g., zip bigint for partition(year='2001')We propose a method to extrapolate the missing column status for the partitions.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-8 01:00:00" id="7665" opendate="2014-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create TestSparkCliDriver to run test in spark local mode [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-12 01:00:00" id="7685" opendate="2014-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet memory manager</summary>
      <description>Similar to HIVE-4248, Parquet tries to write large very large "row groups". This causes Hive to run out of memory during dynamic partitions when a reducer may have many Parquet files open at a given time.As such, we should implement a memory manager which ensures that we don't run out of memory due to writing too many row groups within a single JVM.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-12 01:00:00" id="7687" opendate="2014-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Invoke cbo only when needed</summary>
      <description>also use greedy algo for all cases.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-14 01:00:00" id="7728" opendate="2014-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable q-tests for TABLESAMPLE feature [Spark Branch]</summary>
      <description>Enable q-tests for TABLESAMPLE feature since automatic test environment is ready.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-14 01:00:00" id="7729" opendate="2014-8-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable q-tests for ANALYZE TABLE feature [Spark Branch]</summary>
      <description>Enable q-tests for ANALYZE TABLE feature since automatic test environment is ready.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-15 01:00:00" id="7742" opendate="2014-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Predicate Push Down to Honor Hive Join Condition restrictions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="776" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make div as infix operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.divider.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.divider.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-18 01:00:00" id="7767" opendate="2014-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.optimize.union.remove does not work properly [Spark Branch]</summary>
      <description>Turing on the hive.optimize.union.remove property generates wrong union all result. For Example:create table inputTbl1(key string, val string) stored as textfile;load data local inpath '../../data/files/T1.txt' into table inputTbl1;SELECT *FROM ( SELECT key, count(1) as values from inputTbl1 group by key UNION ALL SELECT key, count(1) as values from inputTbl1 group by key) a; when the hive.optimize.union.remove is turned on, the query result is like: 1 12 13 17 18 2when the hive.optimize.union.remove is turned off, the query result is like: 7 12 18 23 11 17 12 18 23 11 1The expected query result is:7 12 18 23 11 17 12 18 23 11 1</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GraphTran.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="777" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CREATE TABLE with STRUCT type</summary>
      <description>Currently we only support map/array in type definition.We should support STRUCT as well.CREATE TABLE abc ( pageid INT, ads STRUCT&lt;adid:INT,location:STRING&gt;, userid INT);</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="778" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add describe div</summary>
      <description>Can you add a test which describe div ?We should do it for all the existing tests also - but definitely for all the new tests.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.describe.function.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.describe.function.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFOPLongDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-19 01:00:00" id="7781" opendate="2014-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable windowing and analytic function qtests [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-20 01:00:00" id="7791" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (1) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. alter_merge_orc.q,\ alter_merge_stats_orc.q,\ auto_join0.q,\ auto_join1.q,\ bucket2.q,\ bucket3.q,\ bucket4.q,\ count.q,\ create_merge_compressed.q,\ cross_join.q,\ cross_product_check_1.q,\ cross_product_check_2.q,\ ctas.q,\custom_input_output_format.q,\ disable_merge_for_bucketing.q,\ dynpart_sort_opt_vectorization.q,\ dynpart_sort_optimization.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7792" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (2) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on.limit_pushdown.q,\ load_dyn_part1.q,\ load_dyn_part2.q,\ load_dyn_part3.q,\ mapjoin_mapjoin.q,\ mapreduce1.q,\ mapreduce2.q,\ merge1.q,\ merge2.q,\ metadata_only_queries.q,\ optimize_nullscan.q,\ orc_analyze.q,\ orc_merge1.q,\ orc_merge2.q,\ orc_merge3.q,\ orc_merge4.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7793" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (3) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. ptf.q,\ sample1.q,\ script_env_var1.q,\ script_env_var2.q,\ script_pipe.q,\ scriptfile1.q,\ stats_counter.q,\ stats_counter_partitioned.q,\ stats_noscan_1.q,\ subquery_exists.q,\ subquery_in.q,\ temp_table.q,\ transform1.q,\ transform2.q,\ transform_ppr1.q,\ transform_ppr2.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-20 01:00:00" id="7794" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (4) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. vector_cast_constant.q,\ vector_data_types.q,\ vector_decimal_aggregate.q,\ vector_left_outer_join.q,\ vector_string_concat.q,\ vectorization_12.q,\ vectorization_13.q,\ vectorization_14.q,\ vectorization_15.q,\ vectorization_9.q,\ vectorization_part_project.q,\ vectorization_short_regress.q,\ vectorized_mapjoin.q,\ vectorized_nested_mapjoin.q,\ vectorized_ptf.q,\ vectorized_shufflejoin.q,\ vectorized_timestamp_funcs.q</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="7795" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ptf.q and ptf_streaming.q.[Spark Branch]</summary>
      <description>ptf.q and ptf_streaming.q contains join queries, we should enable these qtests in milestone2.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7804" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Support SemiJoins</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-21 01:00:00" id="7835" opendate="2014-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle a case where FieldTimmer trims all fields from input</summary>
      <description>eg in queries like select 1 from t1 where nothing needs to be projected we generate empty project list which results in incorrect ast getting generated. Currently, we fix while generating ast by eliminating such AST node, but better is to generate correct project list to begin with.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-21 01:00:00" id="7839" opendate="2014-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update union_null results now that it&amp;#39;s deterministic [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.null.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-24 01:00:00" id="784" opendate="2009-8-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support uncorrelated subqueries in the WHERE clause</summary>
      <description>Hive currently only support views in the FROM-clause, some Facebook use cases suggest that Hive should support subqueries such as those connected by IN/EXISTS in the WHERE-clause.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-21 01:00:00" id="7841" opendate="2014-8-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Case, When, Lead, Lag UDF is missing annotation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFWhen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCase.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-30 01:00:00" id="7918" opendate="2014-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: String Type Conversion, Stats error Propagation, CBO Patch check</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-30 01:00:00" id="7922" opendate="2014-8-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge from trunk (2) [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-3 01:00:00" id="7949" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table LIKE command doesn&amp;#39;t set new owner</summary>
      <description>'Create table like' command doesn't set the current user as owner of new table, instead new table owner is same as source table owner.This is a regression from 0.12</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-3 01:00:00" id="7951" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>InputFormats implementing (Job)Configurable should not be cached</summary>
      <description>Currently, initial configuration instance is shared to all following input formats, which should not be like that.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-3 01:00:00" id="7960" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 2.5</summary>
      <description>Tracking JIRA for upgrading to 2.5</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-4 01:00:00" id="7984" opendate="2014-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>AccumuloOutputFormat Configuration items from StorageHandler not re-set in Configuration in Tez</summary>
      <description>Ran AccumuloStorageHandler queries with Tez and found that configuration elements that are pulled from the -hiveconf and passed to the inputJobProperties or outputJobProperties by the AccumuloStorageHandler aren't available inside of the Tez container.I'm guessing that there is a disconnect from the configuration that the StorageHandler creates and what the Tez container sees.The HBaseStorageHandler likely doesn't run into this because it expects to have hbase-site.xml available via tmpjars (and can extrapolate connection information from that file). Accumulo's site configuration file is not meant to be shared with consumers which means that this exact approach is not sufficient.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.serde.AccumuloRowSerializer.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.mr.HiveAccumuloTableOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-5 01:00:00" id="7998" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance JDBC Driver to not require class specification</summary>
      <description>The hotspot VM offers a way to avoid having to specify the driver class explicitly when using the JDBC driver. The DriverManager methods getConnection and getDrivers have been enhanced to support the Java Standard Edition Service Provider mechanism. JDBC 4.0 Drivers must include the file META-INF/services/java.sql.Driver. This file contains the name of the JDBC drivers implementation of java.sql.Driver. For example, to load the my.sql.Driver class, the META-INF/services/java.sql.Driver file would contain the entry: `my.sql.Driver`Applications no longer need to explicitly load JDBC drivers using Class.forName(). Existing programs which currently load JDBC drivers using Class.forName() will continue to work without modification.via http://docs.oracle.com/javase/7/docs/api/java/sql/DriverManager.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="8008" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE while reading null decimal value</summary>
      <description>Say you have this table dec_test:dec decimal(10,0) If the table has a row that is 9999999999.5, and if we doselect * from dec_test;it will crash with NPE:2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerExceptionjava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137) ... 12 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39) at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87) ... 19 more</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-6 01:00:00" id="8014" opendate="2014-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.in.test not set in MiniTezCli tests</summary>
      <description>When the TestCli driver is run, the configuration value hive.in.test is set to true. However, when TestMiniTezCli driver is run, this value is not set, which results in some classes not realizing they are running in a test.</description>
      <version>0.13.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.tez.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-6 01:00:00" id="8015" opendate="2014-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge from trunk (3) [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-6 01:00:00" id="8016" opendate="2014-9-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: PPD to honor hive Join Cond, Casting fixes, Add annotations for IF, Code cleanup</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFIf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.PreCBOOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.TraitsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.CBOTableStatsValidator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveSwapJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushJoinThroughJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePullUpProjectsAboveJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveMergeProjectRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveUnionRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveTableScanRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveSortRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveProjectRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveLimitRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveFilterRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveAggregateRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.Pair.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.JoinUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveCostUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveCost.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-7 01:00:00" id="8017" opendate="2014-9-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use HiveKey instead of BytesWritable as key type of the pair RDD [Spark Branch]</summary>
      <description>HiveKey should be used as the key type because it holds the hash code for partitioning. While BytesWritable serves partitioning well for simple cases, we have to use HiveKey.hashCode for more complicated ones, e.g. join, bucketed table, etc.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestHiveKVResultCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.UnionTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SortByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.ReduceTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapTran.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.KryoSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveVoidFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GroupByShuffler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.GraphTran.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-9 01:00:00" id="8034" opendate="2014-9-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Don&amp;#39;t add colon when no port is specified</summary>
      <description>In HIVE-4910 we added a : even if there was no port due to HADOOP-9776. Now that this is fixed I think we should fix ours as well.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-10 01:00:00" id="8037" opendate="2014-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Refactor Join condn gen code, loosen restrictions on Join Conditions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.JoinTypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.JoinCondnTypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-10 01:00:00" id="8039" opendate="2014-9-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle repeated alias</summary>
      <description>Relax condition in CBO of not allowing repeated alias.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-11-27 01:00:00" id="804" opendate="2009-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support deletion of partitions based on a prefix partition spefication</summary>
      <description>Sometimes users create partitions like (date='...', time='...'). It is useful if user can delete all the partitions of the same day (and different time) with a single command:ALTER TABLE test DROP PARTITION (date='2009-08-26');</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-11 01:00:00" id="8060" opendate="2014-9-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle tables with serde supplied schema</summary>
      <description>Disable cbo in such cases.</description>
      <version>None</version>
      <fixedVersion>cbo-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-12 01:00:00" id="8066" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle charset</summary>
      <description>Disable cbo in such a case.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.HiveOptiqUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-12 01:00:00" id="8083" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Authorization DDLs should not enforce hive identifier syntax for user or group</summary>
      <description>The compiler expects principals (user, group and role) as hive identifiers for authorization DDLs. The user and group are entities that belong to external namespace and we can't expect those to follow hive identifier syntax rules. For example, a userid or group can contain '-' which is not allowed by compiler.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-12 01:00:00" id="8084" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle casting for parameterized type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-12 01:00:00" id="8086" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Disable Trivial Project Removal Rule, Fix Result Schema</summary>
      <description>Disable trivial project rule till Optiq-407 gets fixed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-13 01:00:00" id="8087" opendate="2014-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Bug in constant conversion for Date type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-14 01:00:00" id="8094" opendate="2014-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add LIKE keyword support for SHOW FUNCTIONS</summary>
      <description>It would be nice to add LIKE keyword support for SHOW FUNCTIONS as below, and keep the patterns consistent to the way as SHOW DATABASES, SHOW TABLES.SHOW FUNCTIONS LIKE 'foo*';</description>
      <version>0.13.1,0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.show.functions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ShowFunctionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-15 01:00:00" id="8097" opendate="2014-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Reduce-Side [SMB] MapJoin operator fails</summary>
      <description>Fails attempting to getScratchColumnVectorTypes since mapWork is null on reduce-side.Fix by calling that method using reduceWork object.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatchCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-9-31 01:00:00" id="810" opendate="2009-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>annotation for function for JDBC</summary>
      <description>After committing 645, I realized a problem - the function does not have annotation, and therefore the describe will fail.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.queries.clientpositive.dboutput.q</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-9-19 01:00:00" id="8189" opendate="2014-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>A select statement with a subquery is failing with HBaseSerde</summary>
      <description>Hive tables in the query are hbase tables, and the subquery is a join statement.Whenset hive.optimize.ppd=true; andset hive.auto.convert.join=false;The query does not return data. While hive.optimize.ppd=true and hive.auto.convert.join=true return values back. See attached query file.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-24 01:00:00" id="8245" opendate="2014-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect table read entities at same time as view read entities</summary>
      <description/>
      <version>0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-25 01:00:00" id="8256" opendate="2014-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add SORT_QUERY_RESULTS for test that doesn&amp;#39;t guarantee order #2</summary>
      <description>Following HIVE-8533, we need to further add SORT_QUERY_RESULTS to a few more tests that doesn't guarantee output order.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.table.access.keys.stats.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby.complex.types.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby10.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-25 01:00:00" id="8257" opendate="2014-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Accumulo introduces old hadoop-client dependency</summary>
      <description>It was brought to my attention that Accumulo is transitively bringing in some artifacts with the wrong version of Hadoop.Accumulo-1.6.0 sets the Hadoop version at 2.2.0 and uses hadoop-client to get its necessary dependencies. Because there is no dependency with the correct version in Hive, this introduces hadoop-2.2.0 dependencies.A solution is to make sure that hadoop-client is set with the correct hadoop-20S.version or hadoop-23.version.Snippet from mvn dependency:tree -Phadoop-2[INFO] --- maven-dependency-plugin:2.8:tree (default-cli) @ hive-accumulo-handler ---[INFO] org.apache.hive:hive-accumulo-handler:jar:0.14.0-SNAPSHOT[INFO] +- commons-lang:commons-lang:jar:2.6:compile[INFO] +- commons-logging:commons-logging:jar:1.1.3:compile[INFO] +- org.apache.accumulo:accumulo-core:jar:1.6.0:compile...[INFO] | +- org.apache.hadoop:hadoop-client:jar:2.2.0:compile[INFO] | | +- org.apache.hadoop:hadoop-hdfs:jar:2.4.0:compile...</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-25 01:00:00" id="8261" opendate="2014-9-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO : Predicate pushdown is removed by Optiq</summary>
      <description>Plan for TPC-DS Q64 wasn't optimal upon looking at the logical plan I realized that predicate pushdown is not applied on date_dim d1.Interestingly before optiq we have the predicate pushed :HiveFilterRel(condition=[&lt;=($5, $1)]) HiveJoinRel(condition=[=($3, $6)], joinType=[inner]) HiveProjectRel(_o__col0=[$0], _o__col1=[$2], _o__col2=[$3], _o__col3=[$1]) HiveFilterRel(condition=[=($0, 2000)]) HiveAggregateRel(group=[{0, 1}], agg#0=[count()], agg#1=[sum($2)]) HiveProjectRel($f0=[$4], $f1=[$5], $f2=[$2]) HiveJoinRel(condition=[=($1, $8)], joinType=[inner]) HiveJoinRel(condition=[=($1, $5)], joinType=[inner]) HiveJoinRel(condition=[=($0, $3)], joinType=[inner]) HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_wholesale_cost=[$11]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.store_sales]]) HiveProjectRel(d_date_sk=[$0], d_year=[$6]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.date_dim]]) HiveFilterRel(condition=[AND(in($2, 'maroon', 'burnished', 'dim', 'steel', 'navajo', 'chocolate'), between(false, $1, 35, +(35, 10)), between(false, $1, +(35, 1), +(35, 15)))]) HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.item]]) HiveProjectRel(_o__col0=[$0]) HiveAggregateRel(group=[{0}]) HiveProjectRel($f0=[$0]) HiveJoinRel(condition=[AND(=($0, $2), =($1, $3))], joinType=[inner]) HiveProjectRel(cs_item_sk=[$15], cs_order_number=[$17]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_sales]]) HiveProjectRel(cr_item_sk=[$2], cr_order_number=[$16]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_returns]]) HiveProjectRel(_o__col0=[$0], _o__col1=[$2], _o__col3=[$1]) HiveFilterRel(condition=[=($0, +(2000, 1))]) HiveAggregateRel(group=[{0, 1}], agg#0=[count()]) HiveProjectRel($f0=[$4], $f1=[$5], $f2=[$2]) HiveJoinRel(condition=[=($1, $8)], joinType=[inner]) HiveJoinRel(condition=[=($1, $5)], joinType=[inner]) HiveJoinRel(condition=[=($0, $3)], joinType=[inner]) HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_wholesale_cost=[$11]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.store_sales]]) HiveProjectRel(d_date_sk=[$0], d_year=[$6]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.date_dim]]) HiveFilterRel(condition=[AND(in($2, 'maroon', 'burnished', 'dim', 'steel', 'navajo', 'chocolate'), between(false, $1, 35, +(35, 10)), between(false, $1, +(35, 1), +(35, 15)))]) HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.item]]) HiveProjectRel(_o__col0=[$0]) HiveAggregateRel(group=[{0}]) HiveProjectRel($f0=[$0]) HiveJoinRel(condition=[AND(=($0, $2), =($1, $3))], joinType=[inner]) HiveProjectRel(cs_item_sk=[$15], cs_order_number=[$17]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_sales]]) HiveProjectRel(cr_item_sk=[$2], cr_order_number=[$16]) HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_returns]])While after Optiq the filter on date_dim gets pulled up the plan HiveFilterRel(condition=[&lt;=($5, $1)]): rowcount = 1.0, cumulative cost = {5.50188454E8 rows, 0.0 cpu, 0.0 io}, id = 6895 HiveProjectRel(_o__col0=[$0], _o__col1=[$1], _o__col2=[$2], _o__col3=[$3], _o__col00=[$4], _o__col10=[$5], _o__col30=[$6]): rowcount = 1.0, cumulative cost = {5.50188454E8 rows, 0.0 cpu, 0.0 io}, id = 7046 HiveJoinRel(condition=[=($3, $6)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50188454E8 rows, 0.0 cpu, 0.0 io}, id = 7041 HiveProjectRel(_o__col0=[$0], _o__col1=[$2], _o__col2=[$3], _o__col3=[$1]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6857 HiveFilterRel(condition=[=($0, 2000)]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6855 HiveAggregateRel(group=[{0, 1}], agg#0=[count()], agg#1=[sum($2)]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6853 HiveProjectRel($f0=[$4], $f1=[$5], $f2=[$2]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6851 HiveProjectRel(ss_sold_date_sk=[$3], ss_item_sk=[$4], ss_wholesale_cost=[$5], d_date_sk=[$0], d_year=[$1], i_item_sk=[$6], i_current_price=[$7], i_color=[$8], _o__col0=[$2]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 7039 HiveJoinRel(condition=[=($3, $0)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 7037 HiveProjectRel(d_date_sk=[$0], d_year=[$6]): rowcount = 73049.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6861 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 6537 HiveJoinRel(condition=[=($2, $0)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50115402E8 rows, 0.0 cpu, 0.0 io}, id = 7035 HiveProjectRel(_o__col0=[$0]): rowcount = 38846.0, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6847 HiveAggregateRel(group=[{0}]): rowcount = 38846.0, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6845 HiveProjectRel($f0=[$0]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6843 HiveProjectRel(cs_item_sk=[$0], cs_order_number=[$1], cr_item_sk=[$2], cr_order_number=[$3]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6945 HiveJoinRel(condition=[AND(=($0, $2), =($1, $3))], joinType=[inner]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6940 HiveProjectRel(cs_item_sk=[$15], cs_order_number=[$17]): rowcount = 2.86549727E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6871 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_sales]]): rowcount = 2.86549727E8, cumulative cost = {0}, id = 6531 HiveProjectRel(cr_item_sk=[$2], cr_order_number=[$16]): rowcount = 2.8798881E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6873 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_returns]]): rowcount = 2.8798881E7, cumulative cost = {0}, id = 6532 HiveJoinRel(condition=[=($1, $3)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50076555E8 rows, 0.0 cpu, 0.0 io}, id = 6996 HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_wholesale_cost=[$11]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6859 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 6538 HiveFilterRel(condition=[AND(in($2, 'maroon', 'burnished', 'dim', 'steel', 'navajo', 'chocolate'), between(false, $1, 35, +(35, 10)), between(false, $1, +(35, 1), +(35, 15)))]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6833 HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17]): rowcount = 48000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6831 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.item]]): rowcount = 48000.0, cumulative cost = {0}, id = 6539 HiveProjectRel(_o__col0=[$0], _o__col1=[$2], _o__col3=[$1]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6891 HiveFilterRel(condition=[=($0, +(2000, 1))]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6889 HiveAggregateRel(group=[{0, 1}], agg#0=[count()]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6887 HiveProjectRel($f0=[$4], $f1=[$5], $f2=[$2]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6885 HiveProjectRel(ss_sold_date_sk=[$3], ss_item_sk=[$4], ss_wholesale_cost=[$5], d_date_sk=[$0], d_year=[$1], i_item_sk=[$6], i_current_price=[$7], i_color=[$8], _o__col0=[$2]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6992 HiveJoinRel(condition=[=($3, $0)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50188452E8 rows, 0.0 cpu, 0.0 io}, id = 6990 HiveProjectRel(d_date_sk=[$0], d_year=[$6]): rowcount = 73049.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6861 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.date_dim]]): rowcount = 73049.0, cumulative cost = {0}, id = 6537 HiveJoinRel(condition=[=($2, $0)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50115402E8 rows, 0.0 cpu, 0.0 io}, id = 6988 HiveProjectRel(_o__col0=[$0]): rowcount = 38846.0, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6881 HiveAggregateRel(group=[{0}]): rowcount = 38846.0, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6879 HiveProjectRel($f0=[$0]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6877 HiveProjectRel(cs_item_sk=[$0], cs_order_number=[$1], cr_item_sk=[$2], cr_order_number=[$3]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6938 HiveJoinRel(condition=[AND(=($0, $2), =($1, $3))], joinType=[inner]): rowcount = 6.692553251460564E8, cumulative cost = {3.15348608E8 rows, 0.0 cpu, 0.0 io}, id = 6933 HiveProjectRel(cs_item_sk=[$15], cs_order_number=[$17]): rowcount = 2.86549727E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6871 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_sales]]): rowcount = 2.86549727E8, cumulative cost = {0}, id = 6531 HiveProjectRel(cr_item_sk=[$2], cr_order_number=[$16]): rowcount = 2.8798881E7, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6873 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.catalog_returns]]): rowcount = 2.8798881E7, cumulative cost = {0}, id = 6532 HiveJoinRel(condition=[=($1, $3)], joinType=[inner]): rowcount = 1.0, cumulative cost = {5.50076555E8 rows, 0.0 cpu, 0.0 io}, id = 6949 HiveProjectRel(ss_sold_date_sk=[$0], ss_item_sk=[$2], ss_wholesale_cost=[$11]): rowcount = 5.50076554E8, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6859 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.store_sales]]): rowcount = 5.50076554E8, cumulative cost = {0}, id = 6538 HiveFilterRel(condition=[AND(in($2, 'maroon', 'burnished', 'dim', 'steel', 'navajo', 'chocolate'), between(false, $1, 35, +(35, 10)), between(false, $1, +(35, 1), +(35, 15)))]): rowcount = 1.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6867 HiveProjectRel(i_item_sk=[$0], i_current_price=[$5], i_color=[$17]): rowcount = 48000.0, cumulative cost = {0.0 rows, 0.0 cpu, 0.0 io}, id = 6865 HiveTableScanRel(table=[[tpcds_bin_partitioned_orc_200.item]]): rowcount = 48000.0, cumulative cost = {0}, id = 6539I simplified the query a little bit while still maintaining the query structure The query : Note that the final join between cs1 and cs2 has a predicates "cs1.syear = 2000 and cs2.syear = 2000 + 1"select cs1.syear ,cs1.cnt ,cs1.s1 ,cs2.syear ,cs2.cntfrom(select d1.d_year as syear ,count(*) as cnt,sum(ss_wholesale_cost) as s1 ,i_item_sk as item_sk FROM store_sales JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk JOIN item ON store_sales.ss_item_sk = item.i_item_sk JOIN (select cs_item_sk from catalog_sales JOIN catalog_returns ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk and catalog_sales.cs_order_number = catalog_returns.cr_order_number group by cs_item_sk) cs_uiON store_sales.ss_item_sk = cs_ui.cs_item_sk WHERE i_color in ('maroon','burnished','dim','steel','navajo','chocolate') and i_current_price between 35 and 35 + 10 and i_current_price between 35 + 1 and 35 + 15group by d1.d_year,i_item_sk) cs1JOIN(select d1.d_year as syear ,count(*) as cnt,sum(ss_wholesale_cost) as s1 , i_item_sk as item_sk FROM store_sales JOIN date_dim d1 ON store_sales.ss_sold_date_sk = d1.d_date_sk JOIN item ON store_sales.ss_item_sk = item.i_item_sk JOIN (select cs_item_sk from catalog_sales JOIN catalog_returns ON catalog_sales.cs_item_sk = catalog_returns.cr_item_sk and catalog_sales.cs_order_number = catalog_returns.cr_order_number group by cs_item_sk) cs_uiON store_sales.ss_item_sk = cs_ui.cs_item_sk WHERE i_color in ('maroon','burnished','dim','steel','navajo','chocolate') and i_current_price between 35 and 35 + 10 and i_current_price between 35 + 1 and 35 + 15group by d1.d_year,i_item_sk) cs2ON cs1.item_sk=cs2.item_skwhere cs1.syear = 2000 and cs2.syear = 2000 + 1 and cs2.cnt &lt;= cs1.cnt;</description>
      <version>0.13.1,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-26 01:00:00" id="8266" opendate="2014-9-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>create function using &lt;resource&gt; statement compilation should include resource URI entity</summary>
      <description>The compiler add function name and db name as write entities for "create function using &lt;resource&gt;" statement. We should also include the resource URI path in the write entity.</description>
      <version>0.13.1</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.using.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.udf.local.resource.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-29 01:00:00" id="8298" opendate="2014-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results for n-way join when join expressions are not in same order across joins</summary>
      <description>select * from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;is minimal query which reproduces it</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-10-6 01:00:00" id="8360" opendate="2014-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add cross cluster support for webhcat E2E tests</summary>
      <description>In current Webhcat E2E test setup, cross domain secure cluster runs will fail since the realm name for user principles are not included in the kinit command. This patch concatenates the realm name to the user principal there by resulting in a successful kinit.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-9-16 01:00:00" id="838" opendate="2009-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>in strict mode, no partition selected error</summary>
      <description>set hive.mapred.mode=strict;select * from (select count(1) from src union all select count(1) from srcpart where ds = '2009-08-09' )x;Is it a blocker for 0.4 ?</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.input3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.input3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-8 01:00:00" id="8395" opendate="2014-10-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: enable by default</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.date.trim.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.second.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.minute.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hour.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.between.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.set.processor.namespaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.not.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.unquote.and.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.print.header.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.wise.fileformat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.num.op.type.conv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.partition.metadataonly.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.timestamp.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">ql.src.test.queries.clientnegative.join.nonexistent.part.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ambiguous.col.q</file>
      <file type="M">ql.src.test.queries.clientpositive.annotate.stats.groupby2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.constantPropagateForSubQuery.q</file>
      <file type="M">ql.src.test.queries.clientpositive.filter.join.breaktask2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.vc.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.optimize.nullscan.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ppd.gby.join.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.test.queries.clientpositive.query.properties.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.exists.explain.rewrite.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.in.explain.rewrite.q</file>
      <file type="M">ql.src.test.results.clientnegative.join.nonexistent.part.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.ptf.negative.InvalidValueBoundary.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ambiguous.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ansi.sql.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autogen.colalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-18 01:00:00" id="842" opendate="2009-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Authentication Infrastructure for Hive</summary>
      <description>This issue deals with the authentication (user name,password) infrastructure. Not the authorization components that specify what a user should be able to do.</description>
      <version>None</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.build.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-10-17 01:00:00" id="8500" opendate="2014-10-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>beeline does not need to set hive.aux.jars.path</summary>
      <description>Since beeline may be run from a host not running HS2, there is no reason for it to set hive.aux.jars.path property. Beeline doesn't need these jars, HS2 does and should use hive.aux.jars.path property populated with jars on HS2 host (not the ones sent over by beeline connection.)</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-21 01:00:00" id="8533" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable all q-tests for multi-insertion [Spark Branch]</summary>
      <description>As HIVE-8436 is done, we should be able to enable all multi-insertion related tests. This JIRA is created to track this and record any potential issue encountered.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8535" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable compile time skew join optimization for spark [Spark Branch]</summary>
      <description>Sub-task of HIVE-8406</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.union.remove.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoin.union.remove.1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-10-30 01:00:00" id="861" opendate="2009-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NumberFormatException in sum and avg</summary>
      <description>SELECT sum('a'), avg('a') FROM zshao_tt;It should return NULL for both columns, instead of throwing out exceptions.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-11-29 01:00:00" id="8649" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase level of parallelism in reduce phase [Spark Branch]</summary>
      <description>We calculate the number of reducers based on the same code for MapReduce. However, reducers are vastly cheaper in Spark and it's generally recommended we have many more reducers than in MR.Sandy Ryza who works on Spark has some ideas about a heuristic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-1 01:00:00" id="865" opendate="2009-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin: memory leak for same key with very large number of values</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-4 01:00:00" id="868" opendate="2009-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add last ddl time and dml time for table/partition</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.src.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.src.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen-javabean.org.apache.hadoop.hive.metastore.api.Constants.java</file>
      <file type="M">metastore.src.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen-php.hive.metastore.constants.php</file>
      <file type="M">metastore.src.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.cast1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.part1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-31 01:00:00" id="8682" opendate="2014-10-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable table statistic collection on counter for CTAS query[Spark Branch]</summary>
      <description>CREATE TABLE AS SELECT query would load data into new created table, we should enable table statistic collection on counter either.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-2 01:00:00" id="8696" opendate="2014-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HCatClientHMSImpl doesn&amp;#39;t use a Retrying-HiveMetastoreClient.</summary>
      <description>The HCatClientHMSImpl doesn't use a RetryingHiveMetastoreClient. Users of the HCatClient API that log in through keytabs will fail without retry, when their TGTs expire.The fix is inbound.</description>
      <version>0.12.0,0.13.1</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestPassProperties.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-3 01:00:00" id="8707" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ordering differences due to Java 8 HashMap function</summary>
      <description>Java 8 uses a different hash function for HashMap, which is leading to iteration order differences in several cases. (See Java8 vs Java7)For example: In Explain output: Stages are numbered differently In PostExecutePrinter hook, table order is different in lineage.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.context.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.repeated.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optional.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.test.results.clientnegative.bucket.mapjoin.mismatch1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.join.nonexistent.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-5 01:00:00" id="8739" opendate="2014-11-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle Derby and Oracle errors with joins and filters in Direct SQL in a invalid-DB-specific path</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-11 01:00:00" id="8825" opendate="2014-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQLCompletor catches Throwable and ignores it</summary>
      <description>We should be catching the specific exception which is thrown.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SQLCompleter.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-11 01:00:00" id="8826" opendate="2014-11-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove jdbm from top level license file</summary>
      <description>HIVE-1754 removed jdbm but we did not remove it from the top level license file.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-12 01:00:00" id="8847" opendate="2014-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bugs in jenkins scripts</summary>
      <description>1) Incorrect help message in process_jira function2) Spark builds do not work3) Build "profiles" (which map to a properties file) are hard coded4) A JIRA is required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-14 01:00:00" id="8879" opendate="2014-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade derby version to address race candition</summary>
      <description>DERBY-4160 describes the race condition that i sometimes notice.. particularly on windows. Below is the stack traceError Messagejava.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientStacktracejava.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:444) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1449) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) ... 29 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) ... 34 moreCaused by: javax.jdo.JDODataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP""NestedThrowables:java.sql.SQLNonTransientConnectionException: No current connection. at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) ... 39 moreCaused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045) at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365) at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827) at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571) at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513) at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232) at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414) at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218) at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065) at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913) at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727) ... 57 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 80 moreStandard Output2014-11-14 12:56:43,043 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,047 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,048 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,089 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,128 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,162 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,165 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,203 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,240 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,242 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,278 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,281 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing2014-11-14 12:56:43,295 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,298 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.02014-11-14 12:56:43,311 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,328 (main) [WARN - org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:589)] Retrying creating default database after error: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP""javax.jdo.JDODataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)NestedThrowablesStackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045) at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365) at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827) at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571) at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513) at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232) at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414) at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218) at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065) at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913) at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217) at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727) at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6664) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:587) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 80 more2014-11-14 12:56:43,382 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MDatabase and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:118) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,384 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,423 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,424 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,459 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,500 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MTableColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:119) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,501 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,502 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,541 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,542 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,589 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.2014-11-14 12:56:43,633 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 65 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:120) at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:270) at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:233) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133) at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:56) at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:65) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:538) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 73 more2014-11-14 12:56:43,637 (main) [INFO - org.datanucleus.util.Log4JLogger.info(Log4JLogger.java:77)] Reading in results for query "org.datanucleus.store.rdbms.query.SQLQuery@0" since the connection used is closing2014-11-14 12:56:43,647 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.getMetaStoreSchemaVersion(ObjectStore.java:6605) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6564) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more2014-11-14 12:56:43,649 (main) [WARN - org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6570)] Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.14.02014-11-14 12:56:43,662 (main) [WARN - org.datanucleus.util.Log4JLogger.warn(Log4JLogger.java:106)] Query for candidates of org.apache.hadoop.hive.metastore.model.MVersionTable and subclasses resulted in no possible candidatesSchema Transaction threw exception "Add classes to Catalog "", Schema "APP""org.datanucleus.exceptions.NucleusDataStoreException: Schema Transaction threw exception "Add classes to Catalog "", Schema "APP"" at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:176) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) ... 67 moreCaused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 moreNested Throwables StackTrace:java.sql.SQLNonTransientConnectionException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.newEmbedSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.noCurrentConnection(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.checkIfClosed(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.getAutoCommit(Unknown Source) at com.jolbox.bonecp.ConnectionHandle.getAutoCommit(ConnectionHandle.java:778) at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:131) at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605) at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954) at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679) at org.datanucleus.store.rdbms.query.RDBMSQueryUtils.getStatementForCandidates(RDBMSQueryUtils.java:408) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileQueryFull(JDOQLQuery.java:947) at org.datanucleus.store.rdbms.query.JDOQLQuery.compileInternal(JDOQLQuery.java:370) at org.datanucleus.store.query.Query.executeQuery(Query.java:1744) at org.datanucleus.store.query.Query.executeWithArray(Query.java:1672) at org.datanucleus.store.query.Query.execute(Query.java:1654) at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:221) at org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion(ObjectStore.java:6623) at org.apache.hadoop.hive.metastore.ObjectStore.setMetaStoreSchemaVersion(ObjectStore.java:6654) at org.apache.hadoop.hive.metastore.ObjectStore.checkSchema(ObjectStore.java:6574) at org.apache.hadoop.hive.metastore.ObjectStore.verifySchema(ObjectStore.java:6552) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:98) at $Proxy12.verifySchema(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:539) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:591) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:429) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:66) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72) at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5554) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:178) at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:73) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1447) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:63) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:73) at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2689) at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2708) at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:425) at org.apache.flume.sink.hive.TestHiveWriter.&lt;init&gt;(TestHiveWriter.java:96) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:525) at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:187) at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:236) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:233) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:231) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:60) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:229) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:50) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:222) at org.junit.runners.ParentRunner.run(ParentRunner.java:300) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:601) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75)Caused by: java.sql.SQLException: No current connection. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source) ... 75 more</description>
      <version>0.13.1,0.14.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-17 01:00:00" id="8898" opendate="2014-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HIVE-8874 once HBASE-12493 is fixed</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-11-19 01:00:00" id="8910" opendate="2014-11-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactoring of PassThroughOutputFormat</summary>
      <description>It's overly complicated just for doing simple wrapping of output format. Before things get more worse, we should refactor this codes.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HivePassThroughOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-21 01:00:00" id="8931" opendate="2014-11-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Test TestAccumuloCliDriver is not completing</summary>
      <description>Tests are taking 3 hours due to TestAccumuloCliDriver not finishing.Logs:http://ec2-174-129-184-35.compute-1.amazonaws.com/logs/PreCommit-HIVE-TRUNK-Build-1848/failed/TestAccumuloCliDriver/</description>
      <version>None</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-25 01:00:00" id="8956" opendate="2014-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive hangs while some error/exception happens beyond job execution [Spark Branch]</summary>
      <description>Remote spark client communicate with remote spark context asynchronously, if error/exception is throw out during job execution in remote spark context, it would be wrapped and send back to remote spark client, but if error/exception is throw out beyond job execution, such as job serialized failed, remote spark client would never know what's going on in remote spark context, and it would hangs there.Set a timeout in remote spark client side may not a great idea, as we are not sure how long the query executed in spark cluster. we need find a way to check whether job has failed(whole life cycle) in remote spark context.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-11-4 01:00:00" id="911" opendate="2009-11-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add weekofyear UDF</summary>
      <description>Provide a way for users to get the week number for a date, similar to weekofyear() in MySQL.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-12-18 01:00:00" id="9161" opendate="2014-12-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ordering differences on UDF functions due to Java8</summary>
      <description>Java 8 uses a different hash function for HashMap, which is leading to iteration order differences in several cases. (See Java8 vs Java7)This part is related to UDF functions.</description>
      <version>0.13.1</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.stddev.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.map.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.str.to.map.q</file>
      <file type="M">ql.src.test.queries.clientpositive.char.udf1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-23 01:00:00" id="9202" opendate="2014-12-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Choose Kryo as the serializer for pTest [Spark Branch]</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.spark.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-30 01:00:00" id="9234" opendate="2014-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks FileSystem objects in FileSystem.CACHE</summary>
      <description>Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true), but creating new FileSystem objects is expensive.</description>
      <version>0.12.0,0.12.1,0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-8 01:00:00" id="9303" opendate="2015-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parquet files are written with incorrect definition levels</summary>
      <description>The definition level, which determines which level of nesting is NULL, appears to always be n or n-1, where n is the maximum definition level. This means that only the innermost level of nesting can be NULL. This is only relevant for Parquet files. For example:CREATE TABLE text_tbl (a STRUCT&lt;b:STRUCT&lt;c:INT&gt;&gt;)STORED AS TEXTFILE;INSERT OVERWRITE TABLE text_tblSELECT IF(false, named_struct("b", named_struct("c", 1)), NULL)FROM tbl LIMIT 1;CREATE TABLE parq_tblSTORED AS PARQUETAS SELECT * FROM text_tbl;SELECT * FROM text_tbl;=&gt; NULL # rightSELECT * FROM parq_tbl;=&gt; {"b":{"c":null}} # wrong</description>
      <version>0.13.1</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-1-8 01:00:00" id="9309" opendate="2015-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>schematool fails on Postgres 8.1</summary>
      <description>Postgres upgrade scripts set standard_conforming_strings which is not allowed in 8.1:ERROR: parameter "standard_conforming_strings" cannot be changed (state=55P02,code=0)Postgres 8.1 Release notes say that "standard_conforming_strings value is read-only"Postgres 8.2 notes say that it can be set at runtime.It'd be nice to address this for those still using Postgres 8.1This patch provides a schemaTool db option "postgres.filter.81" which, if set, filters out the "standard_conforming_strings" statement from upgrade scripts.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-13 01:00:00" id="931" opendate="2009-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize GROUP BY aggregations where key is a sorted/bucketed column</summary>
      <description>If the table is sorted by a given key, we don't use that for group by. That can be very useful.For eg: if T is sorted by column c1,For select c1, aggr() from T group by c1we always use a single map-reduce job. No hash table is needed on the mapper, since the data is sorted by c1 anyway.This will reduce the memory pressure on the mapper and also remove overhead of maintaining the hash table.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.alterTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.groupByDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-21 01:00:00" id="9438" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The standalone-jdbc jar missing some jars</summary>
      <description>The standalone-jdbc jar does not contain all the jars required for secure connections.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-1-21 01:00:00" id="9440" opendate="2015-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Folders may not be pruned for Hadoop 2</summary>
      <description>HIVE-9367 is not a complete fix. It fixed for Hadoop 1. For Hadoop2, this method is not invoked.protected FileStatus[] listStatus(JobConf job) throws IOException;</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-27 01:00:00" id="9480" opendate="2015-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Build UDF TRUNC to implement FIRST_DAY as compared with LAST_DAY</summary>
      <description>Hive already supports LAST_DAY UDF, in some cases, FIRST_DAY is necessary to do date/timestamp related computation. This JIRA is to track such an implementation. Choose to impl TRUNC, a more standard way to get the first day of a a month, e.g., SELECT TRUNC('2009-12-12', 'MM'); will return 2009-12-01, SELECT TRUNC('2009-12-12', 'YEAR'); will return 2009-01-01.BTW, this TRUNC is not as feature complete as aligned with Oracle one. only 'MM' and 'YEAR' are supported as format, however, it's a base to add on other formats.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-29 01:00:00" id="9513" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2015-2-23 01:00:00" id="9750" opendate="2015-2-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>avoid log locks in operators</summary>
      <description>Basically wrap all LOG.xx calls in isLogXXXEnabled to avoid unnecessary locks on these calls.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-3 01:00:00" id="9839" opendate="2015-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks OperationHandle on async queries which fail at compile phase</summary>
      <description>Using beeline to connect to HiveServer2.And type the following:drop table if exists table_not_exists;select * from table_not_exists;There will be an OperationHandle object staying in HiveServer2's memory for ever even after quit from beeline .</description>
      <version>0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-4 01:00:00" id="9855" opendate="2015-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Runtime skew join doesn&amp;#39;t work when skewed data only exists in big table</summary>
      <description>To reproduce, enable runtime skew join and then join two tables that skewed data only exists in one of them. The task will fail with the following exception:Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: java.io.IOException: Unable to rename output to: hdfs://......</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-10 01:00:00" id="9909" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Specify hive branch to use on jenkins hms tests</summary>
      <description>The HMS metastore upgrade scripts work with 'trunk' branch only. We should allow to checkout any branch specified on Jenkins job in order to allow branch users test their changes.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.metastore.execute-test-on-lxc.sh</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-12-14 01:00:00" id="991" opendate="2009-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>union with 200 kids fail</summary>
      <description>It throws an array out of bound exception</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-10 01:00:00" id="9911" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Clean up structures and intermediate data when a query completes</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.DirWatcher.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.LlapNodeId.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.ContainerRunner.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.configuration.LlapConfiguration.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="9912" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Improvements to the Shuffle handler to avoid unnecessary disk scans</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.shufflehandler.ShuffleHandler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.LlapDaemonConfiguration.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="9914" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Post success comments on Jira from Jenkins metastore upgrades scripts</summary>
      <description>Currently, the HMS upgrade testing post failure comments on Jira only. We need to post success comments as well so that users know that their upgrade changes are working.</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.metastore.metastore-upgrade-test.sh</file>
      <file type="M">dev-support.jenkins-execute-hms-test.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-3-10 01:00:00" id="9915" opendate="2015-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow specifying file format for managed tables</summary>
      <description>We already allow setting a system wide default format. In some cases it's useful though to specify this only for managed tables, or distinguish external and managed via two variables. You might want to set a more efficient (than text) format for managed tables, but leave external to text (as they often are log files etc.)</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
</bugrepository>