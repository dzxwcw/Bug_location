<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2015-5-1 01:00:00" id="10568" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-15 01:00:00" id="12181" opendate="2015-10-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change hive.stats.fetch.column.stats value to true for MiniTezCliDriver</summary>
      <description>There was a performance concern earlier, but HIVE-7587 has fixed that. We can change the default to true now.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.non.string.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.if.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.vecrow.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.text.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.vec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.all.complex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.mapwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.schema.evol.orc.nonvec.fetchwork.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.nonmr.fetch.threshold.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">data.conf.tez.hive-site.xml</file>
      <file type="M">ql.src.test.queries.clientpositive.bucket.map.join.tez1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.pruning.q</file>
      <file type="M">ql.src.test.queries.clientpositive.dynamic.partition.pruning.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.explainuser.4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mapjoin.mapjoin.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mrr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.dynpart.hashjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.smb.main.q</file>
      <file type="M">ql.src.test.queries.clientpositive.tez.vector.dynpart.hashjoin.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.unionDistinct.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.dynamic.partition.pruning.q</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cte.mat.5.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-21 01:00:00" id="1264" opendate="2010-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive work with Hadoop security</summary>
      <description/>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.src.0.19.java.org.apache.hadoop.hive.shims.Hadoop19Shims.java</file>
      <file type="M">shims.src.0.18.java.org.apache.hadoop.hive.shims.Hadoop18Shims.java</file>
      <file type="M">shims.src.0.17.java.org.apache.hadoop.hive.shims.Hadoop17Shims.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">shims.build.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">ql.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-6-13 01:00:00" id="1304" opendate="2010-4-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add row_sequence UDF</summary>
      <description>This is a poor man's answer to the standard analytic function row_number(); it assigns a sequence of numbers to rows, starting from 1.I'm calling it row_sequence() to distinguish it from the real analytic function, so that once we add support for those, there won't be any conflict with the existing UDF.The problem with this UDF approach is that there are no guarantees about ordering in SQL processing internals, so use with caution.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-5 01:00:00" id="14444" opendate="2016-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade qtest execution framework to junit4 - migrate most of them</summary>
      <description>this is the second step..migrating all exiting qtestgen generated tests to junit4it might be possible that not all will get migrated in this ticket...I will leave out the problematic ones...</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestPerfCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCompareCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestBeeLineDriver.vm</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
      <file type="M">itests.qtest-accumulo.pom.xml</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseNegativeCliDriver.vm</file>
      <file type="M">hbase-handler.src.test.templates.TestHBaseCliDriver.vm</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.antlib.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-8-9 01:00:00" id="14480" opendate="2016-8-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ORC ETLSplitStrategy should use thread pool when computing splits</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.1.1,2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-7-7 01:00:00" id="1453" opendate="2010-7-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Eclipse launch templates auto-adjust to Hive version number changes</summary>
      <description>The changes to prepare for branching out 0.6.0 required changes to build configuration which caused the launch configurations to break as the jars they referred to were renamed automatically. As a result, none of the launch configurations are working at this point.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">eclipse-templates.TestTruncate.launchtemplate</file>
      <file type="M">eclipse-templates.TestMTQueries.launchtemplate</file>
      <file type="M">eclipse-templates.TestJdbc.launchtemplate</file>
      <file type="M">eclipse-templates.TestHive.launchtemplate</file>
      <file type="M">eclipse-templates.TestCliDriver.launchtemplate</file>
      <file type="M">eclipse-templates.HiveCLI.launchtemplate</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2016-10-23 01:00:00" id="14830" opendate="2016-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move a majority of the MiniLlapCliDriver tests to use an inline AM</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-7-30 01:00:00" id="1494" opendate="2010-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Index followup: remove sort by clause and fix a bug in collect_set udaf</summary>
      <description/>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-10-13 01:00:00" id="14954" opendate="2016-10-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>put FSOP manifests for the instances of the same vertex into a directory</summary>
      <description>Deleting 100s of manifests can be expensive.</description>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TaskCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.unionproc.UnionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MmCleanerThread.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.ValidWriteIds.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-31 01:00:00" id="1497" opendate="2010-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support COMMENT clause on CREATE INDEX, and add new command for SHOW INDEXES</summary>
      <description>We need to work out the syntax for SHOW/DESCRIBE, taking partitioning into account.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.HiveIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-10-31 01:00:00" id="1498" opendate="2010-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support IDXPROPERTIES on CREATE INDEX</summary>
      <description>It's partially there in the grammar but not hooked in; should work pretty much the same as TBLPROPERTIES.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.index.creation.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateIndexDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-18 01:00:00" id="15000" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove addlocaldriverjar, and addlocaldrivername from command line help</summary>
      <description>As discussed with Ferd, the following commands are not working, and never were intended as a command line parameters, so they should be removed from the command line help.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.main.resources.BeeLine.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2016-11-18 01:00:00" id="15003" opendate="2016-10-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update &amp;#39;ALTER TABLE...UPDATE STATISTICS FOR COLUMN..&amp;#39; statement to support more data types</summary>
      <description>Currently ALTER TABLE...UDPATE STATISTICS FOR COLUMN... only support updating statistics for following data types: STRING DOUBLE BOOLEAN BINARY DECIMAL DATEWe are missing the following data types TINYINT SMALLINT INT BIGINT FLOAT VARCHAR CHAR TIMESTAMP</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.alter.table.update.status.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-19 01:00:00" id="15013" opendate="2016-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Config dir generated for tests should not be under the test tmp directory</summary>
      <description>mvn is used to clean up tmp directories created for tests, and to setup the config directory. The current structure is target/tmptarget/tmp/configAll of this is setup when mvn test is executed.Tests generate data under tmp - warehouse, metastore, etc. Having the conf dir there (generated by mvn) makes it complicate to add per test cleanup - since the entire tmp directory cannot be removed.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-19 01:00:00" id="15020" opendate="2016-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle truncate for MM tables (not atomic yet)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-10-19 01:00:00" id="15021" opendate="2016-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>handle (or add a test for) multi-insert into MM tables</summary>
      <description/>
      <version>None</version>
      <fixedVersion>hive-14535</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.current.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.current.q</file>
      <file type="M">ql.src.test.queries.clientpositive.mm.all.q</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2010-8-11 01:00:00" id="1529" opendate="2010-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ANSI SQL covariance aggregate functions: covar_pop and covar_samp.</summary>
      <description>Create new built-in aggregate functions covar_pop and covar_samp, functions commonly used in statistical data analyses.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-16 01:00:00" id="15460" opendate="2016-12-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix ptest2 test failures</summary>
      <description>I see these failures when I try to run tests on ptest2Failed tests: testBatch(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testAlternativeTestJVM(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepNone(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepGit(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepHadoop1(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..) testPrepSvn(org.apache.hive.ptest.execution.TestScripts): expected:&lt;...yPort=3128"(..)</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepSvn.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepNone.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepHadoop1.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testPrepGit.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2016-12-19 01:00:00" id="15466" opendate="2016-12-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD &amp; DUMP support for incremental DROP_TABLE/DROP_PTN</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.messaging.json.JSONMessageFactory.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-8-17 01:00:00" id="1549" opendate="2010-8-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ANSI SQL correlation aggregate function CORR(X,Y).</summary>
      <description>Aggregate function that computes the Pearson's coefficient of correlation between a set of number pairs.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-18 01:00:00" id="1556" opendate="2010-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix TestContribCliDriver test</summary>
      <description>Due to https://issues.apache.org/jira/browse/HIVE-1548, TestContribCliDriver is broken. Some test results need to be updated</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.null.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes6.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes4.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-8-19 01:00:00" id="1563" opendate="2010-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HBase tests broken</summary>
      <description>Broken by HIVE-1548, which did not update all log files.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.joins.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.bulk.m.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2017-2-26 01:00:00" id="15735" opendate="2017-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In some cases, view objects inside a view do not have parents</summary>
      <description>This cause Sentry throws "No valid privileges" error:Error: Error while compiling statement: FAILED: SemanticException No valid privileges.To reproduce:Enable sentry:create table t1( i int);create view v1 as select * from t1;create view v2 as select * from v1 union all select * from v1;If the user does not have read permission on t1 and v1, the queryselect * from v2; This will fail with:Error: Error while compiling statement: FAILED: SemanticException No valid privileges User foo does not have privileges for QUERY The required privileges: Server=server1-&gt;Db=database2-&gt;Table=v1-&gt;action=select; (state=42000,code=40000)Sentry should not check v1's permission, for v1 has at least one parent(v2).</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.plan.TestViewEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-14 01:00:00" id="15910" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improvements in Hive Unit Test by using In-memory Derby DB</summary>
      <description>Hive UT currently uses Derby DB with storage on disk which have some practical problems.1. The run-time of Hive unit tests are high as need to operate on the disk quite often.2. It can cause conflict if multiple test cases operates on the same table name (such as table being created already exist).To solve these problems, we shall use an in-memory storage option of Derby DB which can be even persisted if the test case demands that.https://db.apache.org/derby/docs/10.8/devguide/cdevdvlpinmemdb.html</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestCleaner.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-14 01:00:00" id="15921" opendate="2017-2-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Re-order the slider stop command to avoid a force if possible</summary>
      <description>A graceful stop is required for slider --service llapstatus to work properly</description>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-2-22 01:00:00" id="16012" opendate="2017-2-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>BytesBytes hash table - better capacity exhaustion handling</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-9-3 01:00:00" id="1614" opendate="2010-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDTF json_tuple should return null row when input is not a valid JSON string</summary>
      <description>If the input column is not a valid JSON string, json_tuple will not return anything but this will prevent the downstream operators to access the left-hand side table. We should output a NULL row instead, similar to when the input column is a NULL value.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDTFJSONTuple.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-7 01:00:00" id="16140" opendate="2017-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stabilize few randomly failing tests</summary>
      <description>Golden file update for vector_between_in test and sort_before_diff for couple of Perf tests.</description>
      <version>None</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query23.q</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.query14.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-30 01:00:00" id="16340" opendate="2017-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Kerberos + SSL connections to HMS</summary>
      <description>It should be possible to connect to HMS with Kerberos authentication and SSL enabled, at the same time.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-4-31 01:00:00" id="16341" opendate="2017-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez Task Execution Summary has incorrect input record counts on some operators</summary>
      <description>Task Execution Summary-------------------------------------------------------------------------------------------------------------------------------- VERTICES TOTAL_TASKS FAILED_ATTEMPTS KILLED_TASKS DURATION(ms) CPU_TIME(ms) GC_TIME(ms) INPUT_RECORDS OUTPUT_RECORDS-------------------------------------------------------------------------------------------------------------------------------- Map 1 167 0 0 17640.00 2,109,200 23,068 150,000,004 11,995,136 Map 11 5 0 0 10559.00 71,960 633 4,023,690 799,900 Map 13 1 0 0 2244.00 6,090 29 25 3 Map 3 1 0 0 2849.00 7,080 99 25 3 Map 5 271 0 0 55834.00 12,934,890 358,376 1,500,000,001 1,500,000,161 Map 7 241 0 0 91243.00 5,020,860 71,182 1,827,250,341 652,413,443Reducer 10 1 0 0 1010.00 1,900 0 4 0Reducer 12 1 0 0 3854.00 1,320 0 799,900 1Reducer 14 1 0 0 1420.00 3,790 45 3 1 Reducer 2 1 0 0 9720.00 6,220 122 11,995,136 1 Reducer 4 1 0 0 810.00 2,100 105 3 1 Reducer 6 1 0 0 24863.00 3,260 5 1,500,000,161 1 Reducer 8 412 0 0 88215.00 17,106,440 184,524 2,165,208,640 1,864 Reducer 9 2 0 0 29752.00 3,980 0 1,864 4--------------------------------------------------------------------------------------------------------------------Seeing this on queries using runtime filtering. Noticed the INPUT_RECORDS look incorrect for the reducers that are responsible for aggregating the min/max/bloomfilter (Reducers 12, 14, 2, 6). For example Reducer 2 shows 12M input records. However looking at the task logs for Reducer 2, there were only 167 input records.It looks like Map 1 has 2 different output vertices (Reducer 2 and Reducer 8), but the total output rows for Map 1 (rather than just the rows going to each specific vertex) is being counted in the input rows for both Reducer 2 and Reducer 8.</description>
      <version>None</version>
      <fixedVersion>2.3.0,3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.monitoring.DAGSummary.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-10-21 01:00:00" id="1658" opendate="2010-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix describe [extended] column formatting</summary>
      <description>When displaying the column schema, the formatting should follow should be name&lt;TAB&gt;type&lt;TAB&gt;comment&lt;NEWLINE&gt;to be inline with the previous formatting style for backward compatibility.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablename.with.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rename.column.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.default.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.bigdata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.protectmode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out.0.17</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.s3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">hbase-handler.src.test.queries.hbase.stats.q</file>
      <file type="M">hbase-handler.src.test.results.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.hbase.stats.q.out</file>
      <file type="M">hwi.src.test.org.apache.hadoop.hive.hwi.TestHWISessionManager.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescTableDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.create.view.q</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.part.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.protectmode.tbl.no.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.format.loc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.default.prop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.escape.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.insert.outputformat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.nested.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ddltime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.xpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inoutdriver.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-8-15 01:00:00" id="17321" opendate="2017-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS: analyze ORC table doesn&amp;#39;t compute raw data size when noscan/partialscan is not specified</summary>
      <description>Need to implement HIVE-9560 for Spark.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-26 01:00:00" id="1753" opendate="2010-10-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE 1633 hit for Stage2 jobs with CombineHiveInputFormat</summary>
      <description>Errors are the same as HIVE-1633 but I see them for Stage-2 jobs.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2017-10-10 01:00:00" id="17750" opendate="2017-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a flag to automatically create most tables as MM</summary>
      <description>After merge we are going to do another round of gap identification... similar to HIVE-14990.However the approach used there is a huge PITA. It'd be much better to make tables MM by default at create time, not pretend they are MM at check time, from the perspective of spurious error elimination.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-7-19 01:00:00" id="17840" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetaStore eats exception if transactionalListeners.notifyEvent fail</summary>
      <description>For example, in add_partitions_core, if there's exception in MetaStoreListenerNotifier.notifyEvent(transactionalListeners,....), transaction rollback but no exception thrown. Client will assume add partition is successful and take a positive path.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.TestObjectStore.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-11-19 01:00:00" id="17841" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement applying the resource plan</summary>
      <description/>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestWorkloadManager.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.SampleTezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapClusterStateForCompile.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WmTezSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.UserPoolMapping.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SessionExpirationTracker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.QueryAllocationManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.LlapPluginEndpointClientImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.GuaranteedTasksAllocator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.AmPluginNode.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestTriggersWorkloadManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2017-3-19 01:00:00" id="17843" opendate="2017-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UINT32 Parquet columns are handled as signed INT32-s, silently reading incorrect data</summary>
      <description>An unsigned 32 bit Parquet column, such asoptional int32 uint_32_col (UINT_32)is read by Hive as if it were signed, leading to incorrect results.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.ParquetDataColumnReaderFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.ETypeConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-12 01:00:00" id="1786" opendate="2010-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve documentation for str_to_map() UDF</summary>
      <description>Currently, desc and desc extended return the same info.There is no mention of defaults anywhere</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.str.to.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFStringToMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-14 01:00:00" id="1792" opendate="2010-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>track the joins which are being converted to map-join automatically</summary>
      <description>We should be able to track how many queries (join) got converted tomap-join</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.CommonJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2010-11-22 01:00:00" id="1804" opendate="2010-11-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Mapjoin will fail if there are no files associating with the join tables</summary>
      <description>If there are some empty tables without any file associated, the map join will fail.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverCommonJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-11-24 01:00:00" id="1811" opendate="2010-11-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Show the time the local task takes</summary>
      <description>After the local tasks finished, show the how much time it takes</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2010-12-21 01:00:00" id="1858" opendate="2010-12-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement DROP {PARTITION, INDEX, TEMPORARY FUNCTION} IF EXISTS</summary>
      <description>Extend HIVE-1856 to support IF EXISTS for {DROP TABLE, VIEW} and ALTER TABLE DROP PARTITION signal an error if the to-be-dropped entity doesn't exist and IF EXISTS isn't specified this behavior can be disabled by setting hive.exec.drop.ignorenonexistent to true</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.drop.multi.partitions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.view.q</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.table.q</file>
      <file type="M">ql.src.test.queries.clientpositive.drop.multi.partitions.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-1-3 01:00:00" id="1874" opendate="2011-1-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fix HBase filter pushdown broken by HIVE-1638</summary>
      <description>See comments at end of HIVE-1660 for what happened.</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.test.results.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-2-4 01:00:00" id="1956" opendate="2011-2-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>"Provide DFS initialization script for Hive</summary>
      <description>This script automates the creation of the Hive warehouse and scratch directories on DFS</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-8 01:00:00" id="1969" opendate="2011-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestMinimrCliDriver merge_dynamic_partition2 and 3 are failing on trunk</summary>
      <description>I haven't looked into it yet but saw this at the end of the .q.out:+Ended Job = job_201102071402_0020 with errors+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask</description>
      <version>0.7.0</version>
      <fixedVersion>0.7.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-2-8 01:00:00" id="1973" opendate="2011-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Getting error when join on tables where name of table has uppercase letters</summary>
      <description>When execute a join query on tables containing Uppercase letters in the table names hit an exception Ex: create table a(b int); create table tabForJoin(b int,c int); select * from a join tabForJoin on(a.b=tabForJoin.b); Got an exception like this FAILED: Error in semantic analysis: Invalid Table Alias tabForJoinBut if i give without capital letters ,It is working</description>
      <version>0.5.0,0.7.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2018-8-6 01:00:00" id="20322" opendate="2018-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FlakyTest: TestMiniDruidCliDriver</summary>
      <description>TestMiniDruidCliDriver is failing intermittently but I'm seeing it fail a significant percentage of the time.druid_timestamptzdruidmini_joinsdruidmini_maskingdruidmini_test1</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-18 01:00:00" id="20420" opendate="2018-8-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a fallback authorizer when no other authorizer is in use</summary>
      <description/>
      <version>None</version>
      <fixedVersion>2.3.4,3.1.1,4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.SettableConfigUpdater.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-20 01:00:00" id="20423" opendate="2018-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set NULLS LAST as the default null ordering</summary>
      <description>HIVE-20150 TopNKeyOperator pushdown can be more efficient if NULLS LAST becomes the default null ordering.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.tmp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.vs.table.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.localtimezone.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.part.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.text.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.llap.io.data.conversion.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.like.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.trailing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.round.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.10.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.where.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.tmp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.update.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.fixed.bucket.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedworkext.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.matchpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.schema.evol.3a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.counters.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PTFDeserializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.TableFunctionResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.ValueBoundaryScanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDAFEvaluator.java</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.serde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.delete.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.values.non.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.rp.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.names.with.leading.and.trailing.spaces.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.delete.all.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.with.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.non.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.acid.fast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-8-20 01:00:00" id="20425" opendate="2018-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use a custom range of port for embedded Derby used by Druid.</summary>
      <description>Seems like good amount of the flakiness of Druid Tests is due to port collision between Derby used by Hive and the one used by Druid. The goal of this Patch is to use a custom range 60_000 to 65535 and find the first available to be used by Druid Derby process.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest-druid.src.main.java.org.apache.hive.druid.MiniDruidCluster.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-8-23 01:00:00" id="20450" opendate="2018-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add replication test for LOAD command on ACID table.</summary>
      <description>Add replication test for LOAD command on ACID/MM table.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-10-19 01:00:00" id="20590" opendate="2018-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow merge statement to have column schema</summary>
      <description>Currently MERGE statement doesn't let user specify column schema with INSERT statements, therefore DEFAULT constraint are not applicable with it.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.stats.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.sqlmerge.stats.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFCardinalityViolation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-9-21 01:00:00" id="20620" opendate="2018-9-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>manifest collisions when inserting into bucketed sorted MM tables with dynamic partitioning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2018-12-2 01:00:00" id="20860" opendate="2018-11-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix or disable TestMiniLlapLocalCliDriver.testCliDriver[cbo_limit]</summary>
      <description>Test failed in one of the precommit job. Looks like there is some case where there is additonal space in the diffError MessageClient Execution succeeded but contained differences (error code = 1) after executing cbo_limit.q 11c11&lt; 1 4 2---&gt; 1 4 2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-11 01:00:00" id="21030" opendate="2018-12-11 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Add credential store env properties redaction in JobConf</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestHiveCredentialProviders.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-1-13 01:00:00" id="21040" opendate="2018-12-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>msck does unnecessary file listing at last level of directory tree</summary>
      <description>Here is the code snippet which is run by msck to list directoriesfinal Path currentPath = pd.p; final int currentDepth = pd.depth; FileStatus[] fileStatuses = fs.listStatus(currentPath, FileUtils.HIDDEN_FILES_PATH_FILTER); // found no files under a sub-directory under table base path; it is possible that the table // is empty and hence there are no partition sub-directories created under base path if (fileStatuses.length == 0 &amp;&amp; currentDepth &gt; 0 &amp;&amp; currentDepth &lt; partColNames.size()) { // since maxDepth is not yet reached, we are missing partition // columns in currentPath logOrThrowExceptionWithMsg( "MSCK is missing partition columns under " + currentPath.toString()); } else { // found files under currentPath add them to the queue if it is a directory for (FileStatus fileStatus : fileStatuses) { if (!fileStatus.isDirectory() &amp;&amp; currentDepth &lt; partColNames.size()) { // found a file at depth which is less than number of partition keys logOrThrowExceptionWithMsg( "MSCK finds a file rather than a directory when it searches for " + fileStatus.getPath().toString()); } else if (fileStatus.isDirectory() &amp;&amp; currentDepth &lt; partColNames.size()) { // found a sub-directory at a depth less than number of partition keys // validate if the partition directory name matches with the corresponding // partition colName at currentDepth Path nextPath = fileStatus.getPath(); String[] parts = nextPath.getName().split("="); if (parts.length != 2) { logOrThrowExceptionWithMsg("Invalid partition name " + nextPath); } else if (!parts[0].equalsIgnoreCase(partColNames.get(currentDepth))) { logOrThrowExceptionWithMsg( "Unexpected partition key " + parts[0] + " found at " + nextPath); } else { // add sub-directory to the work queue if maxDepth is not yet reached pendingPaths.add(new PathDepthInfo(nextPath, currentDepth + 1)); } } } if (currentDepth == partColNames.size()) { return currentPath; } }You can see that when the currentDepth at the maxDepth it still does a unnecessary listing of the files. We can improve this call by checking the currentDepth and bailing out early.This can improve the performance of msck command significantly especially when there are lot of files in each partitions on remote filesystems like S3 or ADLS</description>
      <version>None</version>
      <fixedVersion>2.4.0,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreChecker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-12-14 01:00:00" id="21041" opendate="2018-12-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE, ParseException in getting schema from logical plan</summary>
      <description>HIVE-20552 makes getting schema from logical plan faster. But it throws ParseException when it has column alias, and NullPointerException when it has subqueries.</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-6-2 01:00:00" id="2140" opendate="2011-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return correct Major / Minor version numbers for Hive Driver</summary>
      <description>Click to add description</description>
      <version>0.6.0,0.7.0</version>
      <fixedVersion>0.7.1,0.8.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2019-10-2 01:00:00" id="22281" opendate="2019-10-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table statement fails with "not supported NULLS LAST for ORDER BY in ASC order"</summary>
      <description>CREATE TABLE table_core2c4ywq7yjx ( k1 STRING, f1 STRING, sequence_num BIGINT, create_bsk BIGINT, change_bsk BIGINT, op_code STRING ) PARTITIONED BY (run_id BIGINT) CLUSTERED BY (k1) SORTED BY (k1, change_bsk, sequence_num) INTO 4 BUCKETS STORED AS ORCError while compiling statement: FAILED: SemanticException create/alter table: not supported NULLS LAST for ORDER BY in ASC order</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2019-6-29 01:00:00" id="22681" opendate="2019-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2011-7-26 01:00:00" id="2307" opendate="2011-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="23073" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-23 01:00:00" id="23283" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate random temp ID for lock enqueue and commitTxn</summary>
      <description>In order to optimize the S4U scope of enqueue lock and commitTxn, currently a hardcoded constant (-1) is used to first insert all the lock and ws entries with a temporary lockID/commitID. However, in a concurrent environment this seems to cause some performance degradation (and deadlock issues with some rdbms) as multiple concurrent transactions are trying to insert rows with the same primary key (e.g. (-1, 1), (-1, 2), (-1, 3), .. etc. for (extID/intID) in HIVE_LOCKS). The proposed solution is to replace the constant with a random generated negative number, which seems to resolve this issue.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-23 01:00:00" id="23284" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on mariadb-java-client</summary>
      <description>It has GNU Lesser General Public License which is Category X.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Mysql.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.DEV-README</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-23 01:00:00" id="23287" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on icu4j</summary>
      <description>Brought in transitively via druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-24 01:00:00" id="23293" opendate="2020-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locks: Implement zero-wait readers</summary>
      <description>With a new lock type (EXCL_WRITE) for INSERT_OVERWRITE, SHARED_READ does not have to wait for any lock - it can fails fast for a pending EXCLUSIVE, because even if there is an EXCL_WRITE or SHARED_WRITE pending, there's no semantic reason to wait for them.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-1-22 01:00:00" id="2735" opendate="2012-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-25 01:00:00" id="2748" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2012-2-25 01:00:00" id="2825" opendate="2012-2-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concatenating a partition does not inherit location from table</summary>
      <description>When a table is created in one dfs, a partition is added to that table, the table's dfs is changed, and the partitioned is concatenated, the partitions location remains exactly the same. Instead, it should inherit its location from the table, and be updated accordingly.See https://issues.apache.org/jira/browse/HIVE-1707 for an analogous change to insert overwrite.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-7-26 01:00:00" id="2905" opendate="2012-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Desc table can&amp;#39;t show non-ascii comments</summary>
      <description>When desc a table with command line or hive jdbc way, the table's comment can't be read.1. I have updated javax.jdo.option.ConnectionURL parameter in hive-site.xml file. jdbc:mysql://...:3306/hive?characterEncoding=UTF-82. In mysql database, the comment field of COLUMNS table can be read normally.</description>
      <version>0.7.0,0.10.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
    </fixedFiles>
  </bug>
  
  
</bugrepository>