<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  
  <bug fixdate="2015-5-30 01:00:00" id="10140" opendate="2015-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Window boundary is not compared correctly</summary>
      <description>“ROWS between 10 preceding and 2 preceding” is not handled correctly.Underlying error: Window range invalid, start boundary is greater than end boundary: window(start=range(10 PRECEDING), end=range(2 PRECEDING))If I change it to “2 preceding and 10 preceding”, the syntax works but the results are 0 of course.Reason for the function: during analysis, it is sometimes desired to design the window to filter the most recent events, in the case of the events' responses are not available yet. There is a workaround for this, but it is better/more proper to fix the bug.</description>
      <version>0.13.0,0.14.0,1.0.0,1.1.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.windowspec.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.WindowingSpec.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-4-9 01:00:00" id="10284" opendate="2015-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>enable container reuse for grace hash join</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.lvj.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridhashjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.HybridHashTableContainer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.test.queries.clientpositive.vectorized.context.q</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-8 01:00:00" id="1038" opendate="2010-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin dies if the join prunes all the columns</summary>
      <description>The query:select /*+ mapjoin(a) */ count(1) from src a join src b on a.key = b.keydies.It is a blocker for 0.5</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-11 01:00:00" id="1042" opendate="2010-1-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>function in a transform with more than 1 argument fails</summary>
      <description>select transform(substr(key, 1, 3)) USING '/bin/cat' FROM srcthrows an error:FAILED: Error in semantic analysis: AS clause has an invalid number of aliases</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-5-1 01:00:00" id="10568" opendate="2015-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Select count(distinct()) can have more optimal execution plan</summary>
      <description>select count(distinct ss_ticket_number) from store_sales;can be rewritten asselect count(1) from (select distinct ss_ticket_number from store_sales) a;which may run upto 3x faster</description>
      <version>0.6.0,0.7.0,0.8.0,0.9.0,0.10.0,0.11.0,0.12.0,0.13.0,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-5-12 01:00:00" id="10682" opendate="2015-5-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Make use of the task runner which allows killing tasks</summary>
      <description>TEZ-2434 adds a runner which allows tasks to be killed. Jira to integrate with that without the actual kill functionality. That will follow.</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-5-27 01:00:00" id="10835" opendate="2015-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Concurrency issues in JDBC driver</summary>
      <description>Though JDBC specification specifies that "Each Connection object can create multiple Statement objects that may be used concurrently by the program", but that does not work in current Hive JDBC driver. In addition, there also exist race conditions between DatabaseMetaData, Statement and ResultSet as long as they make RPC calls to HS2 using same Thrift transport, which happens within a connection.So we need a connection level lock to serialize all these RPC calls in a connection.</description>
      <version>0.13.0,0.13.1,0.14.0,0.14.1,0.15.0,1.0.0,1.0.1,1.1.0,1.1.1,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-5-28 01:00:00" id="10846" opendate="2015-5-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: preemption in AM due to failures / out of order scheduling</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.tez.dag.app.rm.TestLlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestTaskExecutorService.java</file>
      <file type="M">llap-server.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-server.src.java.org.apache.tez.dag.app.rm.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.TaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskCommunicator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.tezplugins.Converters.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.Scheduler.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.AMReporter.java</file>
      <file type="M">llap-server.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-2 01:00:00" id="10896" opendate="2015-6-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: the return of the stuck DAG</summary>
      <description>Mapjoin issue again - preempted task that is loading the hashtable</description>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeVectorDataSourceOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.util.FakeCaptureOutputOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOuterFilteredOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinBaseOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.UDTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SparkHashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.PTFOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ListSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LimitOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.LateralViewForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableDummyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ForwardOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DemuxOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CollectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AppMasterEventOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-6-16 01:00:00" id="11027" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive on tez: Bucket map joins fail when hashcode goes negative</summary>
      <description>Seeing an issue when dynamic sort optimization is enabled while doing an insert into bucketed table. We seem to be flipping the negative sign on the hashcode instead of taking the complement of it for routing the data correctly. This results in correctness issues in bucket map joins in hive on tez when the hash code goes negative.</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-6-16 01:00:00" id="11029" opendate="2015-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hadoop.proxyuser.mapr.groups does not work to restrict the groups that can be impersonated</summary>
      <description>In the core-site.xml, the hadoop.proxyuser.&lt;user&gt;.groups specifies the user groups which can be impersonated by the HS2 &lt;user&gt;. However, this does not work properly in Hive. In my core-site.xml, I have the following configs:&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.mapr.groups&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;I would expect with this configuration that 'mapr' can impersonate only members of the Unix group 'root'. However if I submit a query as user 'jon' the query is running as user 'jon' even though 'mapr' should not be able to impersonate this user.</description>
      <version>0.13.0,0.14.0,1.0.0,1.2.0</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-2-26 01:00:00" id="1103" opendate="2010-1-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add .gitignore file</summary>
      <description>Add a .gitignore file (equivalent to svn:ignore) for those using git-svn.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-6-24 01:00:00" id="11099" opendate="2015-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for running negative q-tests [Spark Branch]</summary>
      <description>Add support for TestSparkNegativeCliDriver TestMiniSparkOnYarnNegativeCliDriver to negative q-tests</description>
      <version>None</version>
      <fixedVersion>spark-branch,1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.qtest-spark.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2008-12-3 01:00:00" id="111" opendate="2008-12-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>join without a ON clause dies</summary>
      <description>join without a ON clause diesFor eg: the following query:select x.* from x JOIN yresults in a null pointer exception.It should be treated as a cartesian product</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2010-1-27 01:00:00" id="1110" opendate="2010-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add counters to show that skew join triggered</summary>
      <description>It would be very useful to debug, and quickly find out if the skew join was triggered.</description>
      <version>None</version>
      <fixedVersion>0.6.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-8-30 01:00:00" id="11409" opendate="2015-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Calcite Operator To Hive Operator (Calcite Return Path): add SEL before UNION</summary>
      <description>Two purpose: (1) to ensure that the data type of non-primary branch (the 1st branch is the primary branch) of union can be casted to that of the primary branch; (2) to make UnionProcessor optimizer work; (3) if the SEL is redundant, it will be removed by IdentidyProjectRemover optimizer.</description>
      <version>None</version>
      <fixedVersion>1.3.0,2.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-2-11 01:00:00" id="11526" opendate="2015-8-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: implement LLAP UI as a separate service - part 1</summary>
      <description>The specifics are vague at this point. Hadoop metrics can be output, as well as metrics we collect and output in jmx, as well as those we collect per fragment and log right now. This service can do LLAP-specific views, and per-query aggregation.gopalv may have some information on how to reuse existing solutions for part of the work.</description>
      <version>None</version>
      <fixedVersion>2.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.js.jquery.min.js</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.woff</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.ttf</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.svg</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.fonts.glyphicons-halflings-regular.eot</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.hive.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap.min.css</file>
      <file type="M">llap-server.src.main.resources.webapps.llap.css.bootstrap-theme.min.css</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2015-9-4 01:00:00" id="11729" opendate="2015-9-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: update to use Tez 0.8</summary>
      <description/>
      <version>None</version>
      <fixedVersion>llap</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2015-12-2 01:00:00" id="12566" opendate="2015-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect result returns when using COALESCE in WHERE condition with LEFT JOIN</summary>
      <description>The left join query with on/where clause returns incorrect result (more rows are returned). See the reproducible sample below.Left table with data:CREATE TABLE ltable (i int, la int, lk1 string, lk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---1,\N,CD5415192314304,000712,\N,CD5415192225530,00071Right table with data:CREATE TABLE rtable (ra int, rk1 string, rk2 string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';---1,CD5415192314304,0007145,CD5415192314304,00072Query:SELECT * FROM ltable l LEFT OUTER JOIN rtable r on (l.lk1 = r.rk1 AND l.lk2 = r.rk2) WHERE COALESCE(l.la,'EMPTY')=COALESCE(r.ra,'EMPTY');Result returns:1 NULL CD5415192314304 00071 NULL NULL NULL2 NULL CD5415192225530 00071 NULL NULL NULLThe correct result should be2 NULL CD5415192225530 00071 NULL NULL NULL</description>
      <version>0.13.0</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2017-3-8 01:00:00" id="15848" opendate="2017-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>count or sum distinct incorrect when hive.optimize.reducededuplication set to true</summary>
      <description>Test Table:create table test(id int,key int,name int);Data：idkeyname1 1 21 2 31 3 21 4 21 5 3Test SQL1:select id,count(Distinct key),count(Distinct name)from (select id,key,name from count_distinct_test group by id,key,name)mgroup by id;result：154expect:152Test SQL2:select id,count(Distinct name),count(Distinct key)from (select id,key,name from count_distinct_test group by id,name,key)mgroup by id;result:125</description>
      <version>0.13.0</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.correlation.ReduceSinkDeDuplication.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2018-1-22 01:00:00" id="18510" opendate="2018-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable running checkstyle on test sources as well</summary>
      <description>Currently only source files are in the scope of checkstyle testing. We should expand the scope to include our testing code as well.</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2018-2-22 01:00:00" id="18511" opendate="2018-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix generated checkstyle errors</summary>
      <description>HIVE-18510 identified, that checkstyle was not running for test sources.After running checkstyle several errors are identified</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.RemoteMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.MiniHMS.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.EmbeddedMetaStoreForTests.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.TestDatabases.java</file>
      <file type="M">standalone-metastore.src.test.java.org.apache.hadoop.hive.metastore.client.MetaStoreFactoryForTests.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-1-23 01:00:00" id="3405" opendate="2012-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-6-19 01:00:00" id="4756" opendate="2013-6-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hadoop 0.23 profile to 2.0.5-alpha</summary>
      <description>The minimr tests fail at present with the 0.23 profile. In my tests upgrading to 2.0.5-alpha fixes this.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2013-9-12 01:00:00" id="5278" opendate="2013-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move some string UDFs to GenericUDFs, for better varchar support</summary>
      <description>To better support varchar/char types in string UDFs, select UDFs should be converted to GenericUDFs. This allows the UDF to return the resulting char/varchar length in the type metadata.This work is being split off as a separate task from HIVE-4844. The initial UDFs as part of this work are concat/lower/upper.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFUpper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLower.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConcat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-9-19 01:00:00" id="5322" opendate="2013-9-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>FsPermission is initialized incorrectly in HIVE 5513</summary>
      <description>The change in HIVE-5313 converts the octal string into short using Short.parseShort(scratchDirPermission) but Short.parseShort function expects decimal. So "700" gets converted to 700 instead of 448.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-9-20 01:00:00" id="5327" opendate="2013-9-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Potential leak and cleanup in utilities.java</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-9-24 01:00:00" id="5349" opendate="2013-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>QTestutil does not properly set UTF-8</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-6-3 01:00:00" id="538" opendate="2009-6-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make hive_jdbc.jar self-containing</summary>
      <description>Currently, most jars in hive/build/dist/lib and the hadoop-*-core.jar are required in the classpath to run jdbc applications on hive. We need to do atleast the following to get rid of most unnecessary dependencies:1. get rid of dynamic serde and use a standard serialization format, maybe tab separated, json or avro2. dont use hadoop configuration parameters3. repackage thrift and fb303 classes into hive_jdbc.jar</description>
      <version>0.3.0,0.4.0,0.6.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-8-27 01:00:00" id="5382" opendate="2013-9-27 00:00:00" resolution="Cannot Reproduce">
    <buginformation>
      <summary>Allow strings represented as exponential notation to be typecasted to int/smallint/bigint/tinyint</summary>
      <description>Follow up jira for HIVE-5352</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testBatch.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.testAlternativeTestJVM.approved.txt</file>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.TestScripts.java</file>
      <file type="M">testutils.ptest2.src.main.resources.batch-exec.vm</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.PTest.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-1-4 01:00:00" id="5446" opendate="2013-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive can CREATE an external table but not SELECT from it when file path have spaces</summary>
      <description>Create external table table1 (age int, gender string, totBil float, dirBill float, alkphos int,sgpt int, sgot int, totProt float, aLB float, aG float, sel int) ROW FORMAT DELIMITEDFIELDS TERMINATED BY ','STORED AS TEXTFILELOCATION 'hdfs://namenodehost:9000/hive newtable';select * from table1;return nothing even there is file in the target folder</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Partition.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-10-4 01:00:00" id="5449" opendate="2013-10-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive schematool info option incorrectly reports error for Postgres metastore</summary>
      <description>The schema tool has an option to verify the schema version stored in the metastore. This is implemented as a simple select query executed via JDBC. The problem is that Postgres requires object names to be quoted due to the way tables are created. It's a similar issues hit by metastore direct SQL (HIVE-5264, HIVE-5265 etc).</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-11-10 01:00:00" id="554" opendate="2009-6-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add GenericUDF to create arrays, maps</summary>
      <description>Here is an example:SELECT array(1,2,3)[3], map("a":1,"b":2,"c":3)["a"], struct(user_id:3, revenue: sum(rev))FROM tableGROUP BY user_id;This is relatively easy to do with the GenericUDF framework, and will greatly increase the flexibility of the language.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  
  
  
  
  
  <bug fixdate="2013-10-25 01:00:00" id="5653" opendate="2013-10-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Shuffle Join produces incorrect results</summary>
      <description>Vectorized shuffle join should work out-of-the-box, but it produces empty result set. Investigating.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-11-1 01:00:00" id="5729" opendate="2013-11-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline displays version as ???? after mavenization</summary>
      <description>NO PRECOMMIT TESTSIn Beeline.java, method getApplicationTitle(), it looks to the Beeline class's package to find version information. However, MANIFESTs are not included in Beeline jar after mavenization.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2013-11-8 01:00:00" id="5782" opendate="2013-11-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PTest2 should be able to ride out price spikes</summary>
      <description>Price spikes for spot instances cause PTest2 major issues. We should be able to ride them out while providing some minimum level of service.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.test.java.org.apache.hive.ptest.execution.context.TestCloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudExecutionContextProvider.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.context.CloudComputeService.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-11-13 01:00:00" id="5813" opendate="2013-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multi-way Left outer join fails in vectorized mode</summary>
      <description>with hive.auto.convert.join=true</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-11-14 01:00:00" id="5827" opendate="2013-11-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect location of logs for failed tests.</summary>
      <description>Extending HIVE-5790 to fix other tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.templates.TestParseNegative.vm</file>
      <file type="M">ql.src.test.templates.TestParse.vm</file>
      <file type="M">ql.src.test.templates.TestNegativeCliDriver.vm</file>
      <file type="M">ql.src.test.templates.TestCliDriver.vm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-1-15 01:00:00" id="5829" opendate="2013-11-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rewrite Trim and Pad UDFs based on GenericUDF</summary>
      <description>This JIRA includes following UDFs:1. trim()2. ltrim()3. rtrim()4. lpad()5. rpad()</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLpad.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-11-18 01:00:00" id="5846" opendate="2013-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze command fails with vectorization on</summary>
      <description>analyze table alltypesorc compute statistics; fails</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-4-18 01:00:00" id="5847" opendate="2013-11-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DatabaseMetadata.getColumns() doesn&amp;#39;t show correct column size for char/varchar/decimal</summary>
      <description>column_size, decimal_digits, num_prec_radix should be set appropriately based on the type qualifiers.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.TypeDescriptor.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.Type.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetTypeInfoOperation.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.GetColumnsOperation.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.JdbcColumn.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-2-20 01:00:00" id="5859" opendate="2013-11-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create view does not captures inputs</summary>
      <description>For example, CREATE VIEW view_j5jbymsx8e_1 as SELECT * FROM tbl_j5jbymsx8e;should capture "default.tbl_j5jbymsx8e" as input entity for authorization process but currently it's not.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.noalias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.inputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.unset.table.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.create.table.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.as.select.with.partition.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.view.failure9.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.analyze.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view6.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.or.replace.view8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.view.failure4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalidate.view1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.load.view.failure.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.recursive.view.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.unset.view.property.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.as.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.view.rename.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.big.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.tbl.props.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.like.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.or.replace.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.translate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.drop.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.describe.formatted.view.partitioned.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.view.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2013-12-26 01:00:00" id="5897" opendate="2013-11-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix hadoop2 execution environment Milestone 2</summary>
      <description>Follow on to HIVE-5755.List of known issues:hcatalog-pig-adapter and ql need &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-common&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;hcatalog core and hbase storage handler needs &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;classifier&gt;tests&lt;/classifier&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-hs&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-server-tests&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;classifier&gt;tests&lt;/classifier&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;hcatalog core needs: &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;beeline needs &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;${hadoop-23.version}&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
      <file type="M">itests.hcatalog-unit.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.storage-handlers.hbase.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-1-4 01:00:00" id="5946" opendate="2013-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL authorization task factory should be better tested</summary>
      <description>Thejas is working on various authorization issues and one element that might be useful in that effort and increase test coverage and testability would be perform authorization task creation in a factory.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TaskFactory.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-12-4 01:00:00" id="5948" opendate="2013-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Output file name is random when using Tez with "insert overwrite local directory"</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-2-4 01:00:00" id="5955" opendate="2013-12-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - metastore api support for get_privilege_set api that checks specific role</summary>
      <description>If the user has a set a specific role using 'SET ROLE role', then the authorization check should be done for specific role.The authorization check should not check with all the roles the user belongs to.This would new/different method in metastore api .</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-12-5 01:00:00" id="5966" opendate="2013-12-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix eclipse:eclipse post shim aggregation changes</summary>
      <description>The shim bundle module marks it's deps provided so users of the bundle won't pull in the child dependencies. This causes the eclipse workspace generated by eclipse:eclipse to fail because it only includes the source from the bundle source directory, which is empty.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.pom.xml</file>
      <file type="M">shims.assembly.src.assemble.uberjar.xml</file>
      <file type="M">shims.assembly.pom.xml</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-12-6 01:00:00" id="5973" opendate="2013-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SMB joins produce incorrect results with multiple partitions and buckets</summary>
      <description>It looks like there is an issue with re-using the output object array in the select operator. When we read rows of the non-big tables, we hold on to the output object in the priority queue. This causes hive to produce incorrect results because all the elements in the priority queue refer to the same object and the join happens on only one of the buckets.output[i] = eval[i].evaluate(row);</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DummyStoreOperator.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2013-12-6 01:00:00" id="5978" opendate="2013-12-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollups not supported in vector mode.</summary>
      <description>Rollups are not supported in vector mode, the query should fail to vectorize. A separate jira will be filed to implement rollups in vector mode.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2013-12-11 01:00:00" id="6011" opendate="2013-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>correlation optimizer unit tests are failing on tez</summary>
      <description>Some extra clean-ups in tez branch made this to fail.</description>
      <version>0.13.0</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MuxOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-3-12 01:00:00" id="6012" opendate="2013-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>restore backward compatibility of arithmetic operations</summary>
      <description>HIVE-5356 changed the behavior of some of the arithmetic operations, and the change is not backward compatible, as pointed out in this jira commentint / int =&gt; decimalfloat / float =&gt; doublefloat * float =&gt; doublefloat + float =&gt; double</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.pmod.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFPosMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPPlus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMultiply.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMod.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPMinus.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFOPDivide.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExpressionEvaluator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPDivide.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseNumeric.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDF.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2013-12-22 01:00:00" id="6095" opendate="2013-12-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently II</summary>
      <description>This is follow-up of HIVE-3616.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadMultiFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadFileDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MapReduceCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.AlterTablePartMergeFilesDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.MergeWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2008-4-13 01:00:00" id="61" opendate="2008-11-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implment ORDER BY</summary>
      <description>ORDER BY is in the query language reference but currently is a no-op. We should make it an op.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-1-6 01:00:00" id="6152" opendate="2014-1-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert query fails on hdfs federation + viewfs</summary>
      <description>This is because Hive first writes data to /tmp/ and than moves from /tmp to final destination. In federated HDFS recommendation is to mount /tmp on a separate nameservice, which is usually different than /user. Since renames across different mount points are not supported, this fails.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-7 01:00:00" id="6156" opendate="2014-1-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement vectorized reader for Date datatype for ORC format.</summary>
      <description>We need to implement vectorized reader for Date datatype for ORC format.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.orc.TestVectorizedORCReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-8 01:00:00" id="6164" opendate="2014-1-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build on Windows failed with datanucleus enhancer error "command line is too long"</summary>
      <description>Build hive 0.13 against hadoop 2.0 on Windows always fail:mvn install -Phadoop-2...&amp;#91;ERROR&amp;#93; --------------------&amp;#91;ERROR&amp;#93; Standard error from the DataNucleus tool + org.datanucleus.enhancer.DataNucleusEnhancer :&amp;#91;ERROR&amp;#93; --------------------&amp;#91;ERROR&amp;#93; The command line is too long.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-9 01:00:00" id="6181" opendate="2014-1-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>support grant/revoke on views - parser changes</summary>
      <description>Support grant/revoke statements on views. Includes parser changes.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-1-14 01:00:00" id="6196" opendate="2014-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect package name for few tests.</summary>
      <description>These are tests which were moved from one dir to another recently.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFRpad.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLTrim.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFLpad.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-14 01:00:00" id="6197" opendate="2014-1-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently - VI</summary>
      <description>Next in series.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestSymlinkTextInputFormat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.TestHiveBinarySearchRecordReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredLocalWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRUnion1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-16 01:00:00" id="6211" opendate="2014-1-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat job status E2E tests fail in presence of other jobs</summary>
      <description>Some job status related system tests in the WebHCat E2E testsuite fail intermittently when other MR jobs are run in the cluster running the tests.The testsuite during verification should improve to handle the above situation.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-17 01:00:00" id="6222" opendate="2014-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Vector Group By operator abandon grouping if too many distinct keys</summary>
      <description>Row mode GBY is becoming a pass-through if not enough aggregation occurs on the map side, relying on the shuffle+reduce side to do the work. Have VGBY do the same.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorAggregationBufferRow.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFSumDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCountStar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorUDAFAvgDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.aggregates.VectorAggregateExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVarDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFVar.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFSum.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxString.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMaxDecimal.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFMinMax.txt</file>
      <file type="M">ql.src.gen.vectorization.UDAFTemplates.VectorUDAFAvg.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-17 01:00:00" id="6224" opendate="2014-1-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unneeded tez dependencies from hive</summary>
      <description>After re-organization of some of the classes in tez, we no longer need to depend on certain packages. Removing these from the shims and from the tests dependencies.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-1-18 01:00:00" id="6227" opendate="2014-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test JOBS_7 fails</summary>
      <description>WebHCat E2E test JOBS_7 fails while verifying the job status of a TempletonControllerJob and its child pig job. The filter currently is such that only pig jobs are looked at, it should also include TempletonControllerJob.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-18 01:00:00" id="6228" opendate="2014-1-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use paths consistently - VII</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.JoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverSkewJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MapJoinResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileMergeMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SkewJoinHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-20 01:00:00" id="6233" opendate="2014-1-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>JOBS testsuite in WebHCat E2E tests does not work correctly in secure mode</summary>
      <description>JOBS testsuite performs operations with two users test.user.name and test.other.user.name. In Kerberos secure mode it should kinit as the respective user.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
      <file type="M">hcatalog.src.test.e2e.templeton.README.txt</file>
      <file type="M">hcatalog.src.test.e2e.templeton.build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-2-21 01:00:00" id="6255" opendate="2014-1-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change Hive to not pass MRSplitsProto in MRHelpers.createMRInputPayloadWithGrouping</summary>
      <description>TEZ-650 removed this superfluous parameter since splits dont need to be passed to the AM when doing split calculation on the AM. This is needed after Hive builds against TEZ 0.3.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-22 01:00:00" id="6257" opendate="2014-1-22 00:00:00" resolution="Implemented">
    <buginformation>
      <summary>Add more unit tests for high-precision Decimal128 arithmetic</summary>
      <description>Add more unit tests for high-precision Decimal128 arithmetic, with arguments close to or at 38 digit limit. Consider some random stress tests for broader coverage. Coverage is pretty good now (after HIVE-6243) for precision up to about 18. This is to go beyond that.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-22 01:00:00" id="6258" opendate="2014-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - disallow cycles between roles</summary>
      <description>It should not be possible to have cycles in role relationships.If a grant role statement would end up adding such a cycle, it should result in an error.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-10 01:00:00" id="626" opendate="2009-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Typecast bug in Join operator</summary>
      <description>There is a type cast error in Join operator. Produced by the following steps:create table zshao_foo (foo_id int, foo_name string, foo_a string, foo_b string,foo_c string, foo_d string) row format delimited fields terminated by ','stored as textfile;create table zshao_bar (bar_id int, bar_0 int, foo_id int, bar_1 int, bar_namestring, bar_a string, bar_b string, bar_c string, bar_d string) row formatdelimited fields terminated by ',' stored as textfile;create table zshao_count (bar_id int, n int) row format delimited fieldsterminated by ',' stored as textfile;Each table has a single row as follows:zshao_foo:1,foo1,a,b,c,dzshao_bar:10,0,1,1,bar10,a,b,c,dzshao_count:10,2load data local inpath 'zshao_foo' overwrite into table zshao_foo;load data local inpath 'zshao_bar' overwrite into table zshao_bar;load data local inpath 'zshao_count' overwrite into table zshao_count;explain extendedselect zshao_foo.foo_name, zshao_bar.bar_name, n from zshao_foo join zshao_bar on zshao_foo.foo_id =zshao_bar.foo_id join zshao_count on zshao_count.bar_id = zshao_bar.bar_id;The case is from David Lerman.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-22 01:00:00" id="6260" opendate="2014-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compress plan when sending via RPC (Tez)</summary>
      <description>When trying to send plan via RPC it's helpful to compress the payload. That way more potential plans can be sent (size limit).</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-22 01:00:00" id="6263" opendate="2014-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid sending input files multiple times on Tez</summary>
      <description>Input paths can be recontructed from the plan. No need to send them in the job conf as well.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-23 01:00:00" id="6288" opendate="2014-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSCK can be slow when adding partitions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-27 01:00:00" id="6313" opendate="2014-1-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Minimr tests in hadoop-1 hangs on shutdown</summary>
      <description>It takes minutes after all tests run waiting for all task trackers shutdown. Just shutting down JobTracker after killing pending jobs seemed enough.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  
  <bug fixdate="2014-3-7 01:00:00" id="6392" opendate="2014-2-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive (and HCatalog) don&amp;#39;t allow super-users to add partitions to tables.</summary>
      <description>HDFS allows for users to be added to a "supergroup" (identified by the "dfs.permissions.superusergroup" key in hdfs-site.xml). Users in this group are allowed to modify HDFS contents regardless of the path's ogw permissions.However, Hive's StorageBasedAuthProvider disallows such a superuser from adding partitions to any table that doesn't explicitly grant write permissions to said superuser. This causes the odd scenario where the superuser writes data to a partition-directory (under the table's path), but can't register the appropriate partition.I have a patch that brings the Metastore's behaviour in line with what the HDFS allows.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.StorageBasedAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hcatalog.security.HdfsAuthorizationProvider.java</file>
      <file type="M">hcatalog.core.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-8 01:00:00" id="6393" opendate="2014-2-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support unqualified column references in Joining conditions</summary>
      <description>Support queries of the form:create table r1(a int);create table r2(b);select a, bfrom r1 join r2 on a = bThis becomes more useful in old style syntax:select a, bfrom r1, r2where a = b</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.parse.TestQBJoinTreeApplyPredicate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-15 01:00:00" id="640" opendate="2009-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add LazyBinarySerDe to Hive</summary>
      <description>LazyBinarySerDe will serialize the data in binary format while supporting LazyDeserialization.This will be used as the SerDe for value between map and reduce, and also between different map-reduce jobs.This will help improve the performance of Hive a lot.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestInnerStruct.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.MyTestClass.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardMapObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyObjectInspectorFactory.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  <bug fixdate="2009-7-16 01:00:00" id="642" opendate="2009-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>udf equivalent to string split</summary>
      <description>It would be very useful to have a function equivalent to string split in java</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-12 01:00:00" id="6421" opendate="2014-2-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>abs() should preserve precision/scale of decimal input</summary>
      <description>hive&gt; describe dec1;OKc1 decimal(10,2) None hive&gt; explain select c1, abs(c1) from dec1; ... Select Operator expressions: c1 (type: decimal(10,2)), abs(c1) (type: decimal(38,18))Given that abs() is a GenericUDF it should be possible for the return type precision/scale to match the input precision/scale.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFAbs.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFAbs.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-16 01:00:00" id="644" opendate="2009-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>change default size for merging files at the end of the job</summary>
      <description>Currently, the size is 1G and the reducers end up taking a really long time.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-17 01:00:00" id="6442" opendate="2014-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>load_dyn_part1 is flaky on Tez because it doesn&amp;#39;t have the stage re-arranger</summary>
      <description>Need to use the stage re-arranger on tez as well. That will give predictable order of the stages.</description>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.insert1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-17 01:00:00" id="6446" opendate="2014-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to specify hadoop.bin.path from command line -D</summary>
      <description>the surefire plugin configures hadoop.bin.path as a system property:&lt;hadoop.bin.path&gt;${basedir}/${hive.path.to.root}/testutils/hadoop&lt;/hadoop.bin.path&gt;On Windows testing, this should be: &lt;hadoop.bin.path&gt;${basedir}/${hive.path.to.root}/testutils/hadoop.cmd&lt;/hadoop.bin.path&gt;Additionally, it would be useful to be able to specify the Hadoop CLI location from -D mvn command line.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-17 01:00:00" id="6453" opendate="2014-2-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update TezProrcessors to work with Tez API changes (TEZ-668, TEZ-837)</summary>
      <description/>
      <version>None</version>
      <fixedVersion>tez-branch</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HivePreWarmProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-18 01:00:00" id="6457" opendate="2014-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ensure Parquet integration has good error messages for data types not supported</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-18 01:00:00" id="6458" opendate="2014-2-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schema upgrade scripts for metastore changes related to permanent functions</summary>
      <description>Since HIVE-6330 has metastore changes, there need to be schema upgrade scripts.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.12.0-to-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.upgrade-0.12.0-to-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.12.0-to-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.12.0-to-0.13.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-16 01:00:00" id="646" opendate="2009-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDFs for conversion between different number bases (conv, hex, bin)</summary>
      <description>Add conv, hex and bin UDFs</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-2-20 01:00:00" id="6474" opendate="2014-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>SQL std auth - only db owner should be allowed to create table within a db</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-20 01:00:00" id="6475" opendate="2014-2-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement support for appending to mutable tables in HCatalog</summary>
      <description>Part of HIVE-6405, this is the implementation of the append feature on the HCatalog side. If a table is mutable, we must support being able to append to existing data instead of erroring out as a duplicate publish.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.test.java.org.apache.hive.hcatalog.pig.TestHCatStorerWrapper.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatExternalHCatNonPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.TestHCatDynamicPartitioned.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.mapreduce.HCatMapReduceTest.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FosterStorageHandler.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputFormatContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatConstants.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-3-26 01:00:00" id="6510" opendate="2014-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up math based UDFs</summary>
      <description>HIVE-6327, HIVE-6246 and HIVE-6385 touched a lot of the math based UDFs. There are some code inconsistencies and warnings left. This cleans up all the problems I could find.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFTan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSqrt.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSign.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFRadians.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFMath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog10.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFLn.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFExp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFDegrees.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFCos.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFBaseBitOP.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAtan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAsin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFAcos.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-26 01:00:00" id="6511" opendate="2014-2-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>casting from decimal to tinyint,smallint, int and bigint generates different result when vectorization is on</summary>
      <description>select dc,cast(dc as int), cast(dc as smallint),cast(dc as tinyint) from vectortab10korc limit 20 generates following result when vectorization is enabled:4619756289662.078125 -1628520834 -16770 1261553532646710.316406 -1245514442 -2762 543367942487288.360352 688127224 -776 -84386447830839.337891 1286221623 12087 55-3234165331139.458008 -54957251 27453 61-488378613475.326172 1247658269 -16099 29-493942492598.691406 -21253559 -19895 733101852523586.039062 886135874 23618 662544105595941.381836 1484956709 -23515 37-3997512403067.0625 1102149509 30597 -123-1183754978977.589355 1655994718 31070 941408783849655.676758 34576568 -26440 -72-2993175106993.426758 417098319 27215 793004723551798.100586 -1753555402 -8650 541103792083527.786133 -14511544 -28088 72469767055288.485352 1615620024 26552 -72-1263700791098.294434 -980406074 12486 -58-4244889766496.484375 -1462078048 30112 -96-3962729491139.782715 1525323068 -27332 60NULL NULL NULL NULLWhen vectorization is disabled, result looks like this:4619756289662.078125 -1628520834 -16770 1261553532646710.316406 -1245514442 -2762 543367942487288.360352 688127224 -776 -84386447830839.337891 1286221623 12087 55-3234165331139.458008 -54957251 27453 61-488378613475.326172 1247658269 -16099 29-493942492598.691406 -21253558 -19894 743101852523586.039062 886135874 23618 662544105595941.381836 1484956709 -23515 37-3997512403067.0625 1102149509 30597 -123-1183754978977.589355 1655994719 31071 951408783849655.676758 34576567 -26441 -73-2993175106993.426758 417098319 27215 793004723551798.100586 -1753555402 -8650 541103792083527.786133 -14511545 -28089 71469767055288.485352 1615620024 26552 -72-1263700791098.294434 -980406074 12486 -58-4244889766496.484375 -1462078048 30112 -96-3962729491139.782715 1525323069 -27331 61NULL NULL NULL NULLThis issue is visible only for certain decimal values. In above example, row 7,11,12, and 15 generates different results.vectortab10korc table schema:t tinyint from deserializer si smallint from deserializer i int from deserializer b bigint from deserializer f float from deserializer d double from deserializer dc decimal(38,18) from deserializer bo boolean from deserializer s string from deserializer s2 string from deserializer ts timestamp from deserializer # Detailed Table Information Database: default Owner: xyz CreateTime: Tue Feb 25 21:54:28 UTC 2014 LastAccessTime: UNKNOWN Protect Mode: None Retention: 0 Location: hdfs://host1.domain.com:8020/apps/hive/warehouse/vectortab10korc Table Type: MANAGED_TABLE Table Parameters: COLUMN_STATS_ACCURATE true numFiles 1 numRows 10000 rawDataSize 0 totalSize 344748 transient_lastDdlTime 1393365281 # Storage Information SerDe Library: org.apache.hadoop.hive.ql.io.orc.OrcSerde InputFormat: org.apache.hadoop.hive.ql.io.orc.OrcInputFormat OutputFormat: org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat Compressed: No Num Buckets: -1 Bucket Columns: [] Sort Columns: [] Storage Desc Params: serialization.format 1 Time taken: 0.196 seconds, Fetched: 41 row(s</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestDecimal128.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.Decimal128.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-27 01:00:00" id="6514" opendate="2014-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestExecDriver/HCat Pig tests fails with -Phadoop-2</summary>
      <description>Running TestExecDriver with -Phadoop-2 results in the error below. Looks like the test isn't able to access LocalClientProtocolProvider.java.io.IOException: Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses. at org.apache.hadoop.mapreduce.Cluster.initialize(Cluster.java:120) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:82) at org.apache.hadoop.mapreduce.Cluster.&lt;init&gt;(Cluster.java:75) at org.apache.hadoop.mapred.JobClient.init(JobClient.java:470) at org.apache.hadoop.mapred.JobClient.&lt;init&gt;(JobClient.java:449) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:396) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:739) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Job Submission failed with exception 'java.io.IOException(Cannot initialize Cluster. Please check your configuration for mapreduce.framework.name and the correspond server addresses.)'Execution failed with exit status: 1</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-27 01:00:00" id="6519" opendate="2014-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow optional "as" in subquery definition</summary>
      <description>Allow both:select * from (select * from foo) bar select * from (select * from foo) as bar</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FromClauseParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-7-17 01:00:00" id="652" opendate="2009-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>turn off auto-merge for insert local</summary>
      <description>auto-merge should be turned off for insert local</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMRFileSink1.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-28 01:00:00" id="6521" opendate="2014-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat cannot fetch correct percentComplete for Hive jobs</summary>
      <description>WebHCat E2E test TestHive_7 failed because percentComplete wasn't returned as expected.check_job_percent_complete failed. got percentComplete "map 0% reduce 0%", expected "map 100% reduce 100%"So, there are two problems here. The log parsing is broken for status of percentComplete. In the stderr of the job we see:Launching Job 1 out of 1Number of reduce tasks is set to 0 since there's no reduce operatorStarting Job = job_1393486488858_0691, Tracking URL = http://ambari-sec-1393480847-others-2-4.cs1cloud.internal:8088/proxy/application_1393486488858_0691/Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1393486488858_0691Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 02014-02-27 18:40:50,166 Stage-1 map = 0%, reduce = 0%2014-02-27 18:40:56,599 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 sec2014-02-27 18:40:57,656 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 sec2014-02-27 18:40:58,706 Stage-1 map = 100%, reduce = 0%, Cumulative CPU 0.87 secMapReduce Total cumulative CPU time: 870 msecEnded Job = job_1393486488858_0691MapReduce Jobs Launched: Job 0: Map: 1 Cumulative CPU: 0.87 sec HDFS Read: 305 HDFS Write: 0 SUCCESSTotal MapReduce CPU Time Spent: 870 msecThe assumption in the code is that the line containing the percent status will end after "reduce = \d+%" but that fails with the above. The last status from Hive job is "map = 100%, reduce = 0%" instead of expected "map = 100%, reduce = 100%".</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-17 01:00:00" id="654" opendate="2009-7-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming the thrift SerDe</summary>
      <description>We recently moved ThriftSerDe from facebook thrift to open source thrift.However the name of the ThriftSerDe class didn't change, which makes it really hard to keep data-code compatibility.We should change the name of the ThriftSerDe as soon as possible, before our users start to use open source thrift with it.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.TReflectionUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ThriftDeserializer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ThriftByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.SerDeUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testxpath.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl8.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.inputddl8.q</file>
      <file type="M">ql.src.test.queries.clientnegative.invalid.create.tbl1.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-3-4 01:00:00" id="6548" opendate="2014-3-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing owner name and type fields in schema script for DBS table</summary>
      <description>HIVE-6386 introduced new columns in DBS table, but those are missing from schema scripts.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-5 01:00:00" id="6551" opendate="2014-3-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>group by after join with skew join optimization references invalid task sometimes</summary>
      <description>For example,hive&gt; set hive.auto.convert.join = true;hive&gt; set hive.optimize.skewjoin = true;hive&gt; set hive.skewjoin.key = 3;hive&gt; &gt; EXPLAIN FROM &gt; (SELECT src.* FROM src) x &gt; JOIN &gt; (SELECT src.* FROM src) Y &gt; ON (x.key = Y.key) &gt; SELECT sum(hash(Y.key)), sum(hash(Y.value));OKSTAGE DEPENDENCIES: Stage-8 is a root stage Stage-6 depends on stages: Stage-8 Stage-5 depends on stages: Stage-6 , consists of Stage-4, Stage-2 Stage-4 Stage-2 depends on stages: Stage-4, Stage-1 Stage-0 is a root stage...Stage-2 references not-existing Stage-1</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-20 01:00:00" id="656" opendate="2009-7-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a PMOD (POSITIVE_MOD) function</summary>
      <description>There are a lot of cases people want to get a positive modulo result.For example, people want to bucket the data into 10 buckets. They use the hash code in Hive (based on Java) which can return a negative number. Then they need this POSITIVE_MOD(a, b) to return the results.Otherwise they can still do it but it will be very verbose:((hash(xxx) % 10) + 10) % 10</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-6 01:00:00" id="6563" opendate="2014-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hdfs jar being pulled in when creating a hadoop-2 based hive tar ball</summary>
      <description>Looks like some dependency issue is causing hadoop-hdfs jar to be packaged in the hive tar ball.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-6 01:00:00" id="6564" opendate="2014-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E tests that launch MR jobs fail on check job completion timeout</summary>
      <description>WebHCat E2E tests that fire off an MR job are not correctly being detected as complete so those tests are timing out.The problem is happening because of JSON module available through cpan which returns 1 or 0 instead of true or false.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-6 01:00:00" id="6566" opendate="2014-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect union-all plan with map-joins on Tez</summary>
      <description>The tez dag is hooked up incorrectly for some union all queries involving map joins. That's quite common and results in either NPE or invalid results.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-6 01:00:00" id="6571" opendate="2014-3-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>query id should be available for logging during query compilation</summary>
      <description>Would be nice to have the query id set during compilation to tie logs together etc.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-3-7 01:00:00" id="6580" opendate="2014-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.</summary>
      <description>Refactor ThriftBinaryCLIService and ThriftHttpCLIService tests.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hive.service.cli.thrift.ThriftCLIServiceTest.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftHttpCLIService.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.thrift.TestThriftBinaryCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-7 01:00:00" id="6585" opendate="2014-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucket map join fails in presence of _SUCCESS file</summary>
      <description>Reason is missing path filters.</description>
      <version>0.12.0,0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-7 01:00:00" id="6587" opendate="2014-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>allow specifying additional Hive classpath for Hadoop</summary>
      <description>Allow users to add jars to hive's Hadoop classpath without explicitly modifying their Hadoop classpath</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-8 01:00:00" id="6592" opendate="2014-3-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test abort when pointing to https url of webhdfs</summary>
      <description>WebHCat E2E tests when running against a ssl enabled webhdfs url fails.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.drivers.TestDriverCurl.pm</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-9 01:00:00" id="6597" opendate="2014-3-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E tests doAsTests_6 and doAsTests_7 need to be updated</summary>
      <description>Currently the following WebHCat doAsTests need to be fixed:In doAsTests_6 REST request url needs to be updated and corresponding expected output to reflect the correct intent.doAsTests_7 fails because of the strict error message checking.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.doas.conf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-10 01:00:00" id="6601" opendate="2014-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter database commands should support schema synonym keyword</summary>
      <description>It should be possible to use "alter schema" as an alternative to "alter database". But the syntax is not currently supported.alter schema db1 set owner user x; NoViableAltException(215@[])FAILED: ParseException line 1:6 cannot recognize input near 'schema' 'db1' 'set' in alter statement</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-10 01:00:00" id="6605" opendate="2014-3-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive does not set the environment correctly when running in Tez mode</summary>
      <description>When running in Tez mode, Hive does not correctly set the java.library.path.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2014-3-12 01:00:00" id="6635" opendate="2014-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Heartbeats are not being sent when DbLockMgr is used and an operation holds locks</summary>
      <description>The new DbLockManager depends on heartbeats from the client in order to determine that a lock has not timed out. The client is not currently sending those heartbeats.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.stats.PartialScanTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HadoopJobExecHelper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-3-12 01:00:00" id="6641" opendate="2014-3-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>optimized HashMap keys won&amp;#39;t work correctly with decimals</summary>
      <description>Decimal values with can be equal while having different byte representations (different precision/scale), so comparing bytes is not enough. For a quick fix, we can disable this for decimals</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinKey.java</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-3-13 01:00:00" id="6653" opendate="2014-3-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test JOBS_7 and JOBS_9 fail as profile.url in job details is being returned as null</summary>
      <description>the 2 tests should not be checking profile.url property in the returned JSON since 'url' comes from org.apache.hadoop.mapred.JobProfile class which is marked LimitedPrivate</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobstatus.conf</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-3-14 01:00:00" id="6661" opendate="2014-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat E2E test TestPig_10 fails (Hadoop 2)</summary>
      <description>enablelog=true is only supported with Hadoop 1. Need to add a flag to skip the test with Hadoop 2</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.jobsubmission.conf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-14 01:00:00" id="6666" opendate="2014-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore init scripts should always populate the version information at the end</summary>
      <description>The metastore schema create scripts for 0.13 and 0.14 (current trunk) has multiple other operations after setting the schema version. This is problematic as any failure in those later operations would leave metastore in inconsistent state, and yet with valid version information. The schemaTool depends on the schema version details.Recording the schema version should be the last step in schema initialization script.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.14.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.13.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.14.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.13.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.14.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.13.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.14.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.13.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-14 01:00:00" id="6676" opendate="2014-3-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hcat cli fails to run when running with hive on tez</summary>
      <description>HIVE_CLASSPATH should be added to HADOOP_CLASSPATH before launching hcat CLI</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.bin.hcat</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-17 01:00:00" id="6686" opendate="2014-3-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>webhcat does not honour -Dlog4j.configuration=$WEBHCAT_LOG4J of log4j.properties file on local filesystem.</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.bin.webhcat.server.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-22 01:00:00" id="670" opendate="2009-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain should show output column names</summary>
      <description>Explain currently only shows the expressions (in which it references the output column names of the last operator).However, it does not show the output column names of the last operator, which makes it hard to debug.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.split.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.space.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.size.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.repeat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.parse.url.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lpad.rpad.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.lower.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.locate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.like.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isnull.isnotnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.instr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.if.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.elt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.ascii.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.abs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.cast.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.random.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullscript.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.no.hooks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.noalias.subq1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.lazyserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.dynamicserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.columnarserde.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.groupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.joinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.selectDesc.java</file>
      <file type="M">ql.src.test.results.clientnegative.script.error.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnarserde.create.shortcut.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.implicit.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.limit.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-19 01:00:00" id="6701" opendate="2014-3-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Analyze table compute statistics for decimal columns.</summary>
      <description>Analyze table should compute statistics for decimal columns as well.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsTask.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-20 01:00:00" id="6703" opendate="2014-3-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tez should store SHA of the jar when uploading to cache</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-3-21 01:00:00" id="6728" opendate="2014-3-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Missing file override-container-log4j.properties in Hcatalog</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.bin.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-24 01:00:00" id="6732" opendate="2014-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Notes for Hive 0.13</summary>
      <description>NO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-24 01:00:00" id="6733" opendate="2014-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Driver context logs every query in the "warn" level</summary>
      <description>Trivial, just lower the log level on one statement.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.DriverContext.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-24 01:00:00" id="6734" opendate="2014-3-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DDL locking too course grained in new db txn manager</summary>
      <description>All DDL operations currently acquire an exclusive lock. This is too course grained, as some operations like alter table add partition shouldn't get an exclusive lock on the entire table.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MacroSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-7-22 01:00:00" id="674" opendate="2009-7-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>add UDFs sin/cos</summary>
      <description>Add UDFs for sin/cos</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-25 01:00:00" id="6744" opendate="2014-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Permanent UDF lookup fails when current DB has uppercase letters</summary>
      <description>If defaulting to current DB/schema name for resolving UDF name, the DB name should be lowercased before doing the UDF lookup.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniMr.java</file>
      <file type="M">itests.hive-unit.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-26 01:00:00" id="6752" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorized Between and IN expressions don&amp;#39;t work with decimal, date types.</summary>
      <description>Vectorized Between and IN expressions don't work with decimal, date types.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.TestVectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorHashKeyWrapper.java</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.GenVectorCode.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-26 01:00:00" id="6760" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scalable dynamic partitioning should bail out properly for list bucketing</summary>
      <description>In case of list bucketing HIVE-6455 looks only at this config "hive.optimize.listbucketing" to bail out. There are cases when this config ("hive.optimize.listbucketing") is not set but list bucketing is enabled using SKEWED BY in CREATE TABLE statement.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-26 01:00:00" id="6761" opendate="2014-3-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hashcode computation does not use maximum parallelism for scalable dynamic partitioning</summary>
      <description>Hashcode computation for HIVE-6455 should consider all the partitioning columns and bucket number to distribute the rows. The following code for (int i = 0; i &lt; partitionEval.length - 1; i++) {ignores the last partition column thereby generating lesser hashcodes.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.0,0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-3-27 01:00:00" id="6763" opendate="2014-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 in http mode might send same kerberos client ticket in case of concurrent requests resulting in server throwing a replay exception</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HttpKerberosRequestInterceptor.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-27 01:00:00" id="6767" opendate="2014-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for hadoop-2</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-28 01:00:00" id="6771" opendate="2014-3-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update WebHCat E2E tests now that comments is reported correctly in "describe table" output</summary>
      <description>HIVE-6681 corrected the comments in the describe table output, earlier it would show "from deserializer" in comments.Some WebHCat E2E tests are checking for the string "from deserializer" even overshadowing the actual comments.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.src.test.e2e.templeton.tests.ddl.conf</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-29 01:00:00" id="6780" opendate="2014-3-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set tez credential file property along with MR conf property for Tez jobs</summary>
      <description>webhcat should set the additional property - "tez.credentials.path" to the same value as the MapReduce property.WebHCat should always proactively set this tez.credentials.path property to the same value and in the same cases where it is setting the MR equivalent property.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.LaunchMapper.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.JobSubmissionConstants.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2009-8-23 01:00:00" id="679" opendate="2009-7-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate JDBC driver with SQuirrelSQL for querying</summary>
      <description>Implement the JDBC methods required to support querying and other basic commands using the SQuirrelSQL tool.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSetMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveResultSet.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveConnection.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-31 01:00:00" id="6796" opendate="2014-3-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create/drop roles is case-sensitive whereas &amp;#39;set role&amp;#39; is case insensitive</summary>
      <description>Create/drop role operations should be case insensitive.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.roles.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.set.show.current.role.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.admin.almighty1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.revoke.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorize.grant.public.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.role.grant.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.rolehierarchy.privs.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.drop.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.public.create.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.priv.current.role.neg.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.fail.7.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.role.no.admin.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.empty.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.authorization.drop.db.cascade.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAccessController.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.RoleDDLDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PrincipalDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GrantRevokeRoleDDL.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.authorization.HiveAuthorizationTaskFactoryImpl.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-4-2 01:00:00" id="6817" opendate="2014-4-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some hadoop2-only tests need diffs to be updated</summary>
      <description>expected output needs updating due to pre/post hook messages from the authorization changes</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.recursive.dir.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">hbase-handler.src.test.results.negative.cascade.dbdrop.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-4-3 01:00:00" id="6823" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - database authorization does not check for role ownership</summary>
      <description>A role can own the database, but when the authorization checks are determining the privileges for a user, they are not checking if one of the roles the user belongs to is an owner of the database.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLAuthorizationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-3 01:00:00" id="6825" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>custom jars for Hive query should be uploaded to scratch dir per query; and/or versioned</summary>
      <description>Currently the jars are uploaded to either user directory or global, whatever is configured, which is a mess and can cause collisions. We can upload to scratch directory, and/or version. There's a tradeoff between having to upload files every time (for example, for commonly used things like HBase input format) (which is what is done now, into global/user path), and having a mess of one-off custom jars and files, versioned, sitting in .hiveJars.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-3 01:00:00" id="6828" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive tez bucket map join conversion interferes with map join conversion</summary>
      <description>The issue is that bucket count is used for checking the scaled down size of the hash tables but is used later on to convert to the map join as well which may be incorrect in cases where the entire hash table does not fit in the specified size.</description>
      <version>0.13.0,0.14.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-4-3 01:00:00" id="6834" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition optimization bails out after removing file sink operator</summary>
      <description>HIVE-6455 introduced scalable dynamic partitioning optimization that bails out after removing the file sink operator. This causes union_remove_16.q test to fail by removing all the stages in the plan.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-3 01:00:00" id="6836" opendate="2014-4-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade parquet to 1.4.0</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-4-4 01:00:00" id="6840" opendate="2014-4-4 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Unordered Output for Bucket Map Joins on Tez</summary>
      <description>Tez 0.4 adds a placeholder UnorderedOutput. Once Hive is changed to use 0.4, it should be possible to make use of this.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-4-5 01:00:00" id="6849" opendate="2014-4-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden files update for hadoop-2</summary>
      <description>More hadoop-2 related golden files update.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-4-8 01:00:00" id="6861" opendate="2014-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>more hadoop2 only golden files to fix</summary>
      <description>More hadoop2 golden files to fix due to HIVE-6643, HIVE-6642, HIVE-6808, HIVE-6144.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lb.fs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table.h23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.numbuckets.partitioned.table2.h23.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.union.remove.18.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-8 01:00:00" id="6863" opendate="2014-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 binary mode throws exception with PAM</summary>
      <description>Works fine in http mode</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.auth.HiveAuthFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-8 01:00:00" id="6864" opendate="2014-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 concurrency uses incorrect user information in unsecured mode</summary>
      <description>Concurrent queries create table with wrong ownership</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-8 01:00:00" id="6869" opendate="2014-4-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Golden file updates for tez tests.</summary>
      <description/>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.counter.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.stats.counter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-4-9 01:00:00" id="6876" opendate="2014-4-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Logging information should include thread id</summary>
      <description>The multi-threaded nature of hive server and remote metastore makes it difficult to debug issues without enabling thread information. It would be nice to have the thread id in the logs.</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.main.resources.hive-exec-log4j.properties</file>
      <file type="M">common.src.main.resources.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-10 01:00:00" id="6880" opendate="2014-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestHWISessionManager fails with -Phadoop-2</summary>
      <description>Looks like dependencies missing for -Phadoop-2Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec &lt;&lt;&lt; FAILURE! - in org.apache.hadoop.hive.hwi.TestHWISessionManagerwarning(junit.framework.TestSuite$1) Time elapsed: 0.009 sec &lt;&lt;&lt; FAILURE!junit.framework.AssertionFailedError: Exception in constructor: testHiveDriver (java.lang.NoClassDefFoundError: org/apache/hadoop/mapreduce/TaskAttemptContext at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:171) at org.apache.hadoop.hive.shims.ShimLoader.createShim(ShimLoader.java:120) at org.apache.hadoop.hive.shims.ShimLoader.loadShims(ShimLoader.java:115) at org.apache.hadoop.hive.shims.ShimLoader.getHadoopShims(ShimLoader.java:80) at org.apache.hadoop.hive.conf.HiveConf$ConfVars.&lt;clinit&gt;(HiveConf.java:248) at org.apache.hadoop.hive.conf.HiveConf.&lt;clinit&gt;(HiveConf.java:81) at org.apache.hadoop.hive.hwi.TestHWISessionManager.&lt;init&gt;(TestHWISessionManager.java:46) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at junit.framework.TestSuite.createTest(TestSuite.java:65) at junit.framework.TestSuite.addTestMethod(TestSuite.java:294) at junit.framework.TestSuite.addTestsFromTestCase(TestSuite.java:150) at junit.framework.TestSuite.&lt;init&gt;(TestSuite.java:129) at org.junit.internal.runners.JUnit38ClassRunner.&lt;init&gt;(JUnit38ClassRunner.java:71) at org.junit.internal.builders.JUnit3Builder.runnerForClass(JUnit3Builder.java:14) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.builders.AllDefaultPossibilitiesBuilder.runnerForClass(AllDefaultPossibilitiesBuilder.java:29) at org.junit.runners.model.RunnerBuilder.safeRunnerForClass(RunnerBuilder.java:57) at org.junit.internal.requests.ClassRequest.getRunner(ClassRequest.java:24) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:262) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapreduce.TaskAttemptContext at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ... 28 more) at junit.framework.Assert.fail(Assert.java:50) at junit.framework.TestSuite$1.runTest(TestSuite.java:97)</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-10 01:00:00" id="6881" opendate="2014-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Postgres Upgrade script for hive 0.13 is broken</summary>
      <description>The script added for HIVE-6757 didn't quote the identifiers</description>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.018-HIVE-6757.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-4-10 01:00:00" id="6882" opendate="2014-4-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make upgrade script schemaTool friendly</summary>
      <description>Current scripts work fine when invoked manually, but fails when invoked via schemaTool.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.015-HIVE-5700.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.015-HIVE-5700.oracle.sql</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-24 01:00:00" id="689" opendate="2009-7-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>dump more memory stat periodically</summary>
      <description>Usually we see out of memory errors for all kinds of reasons - it is very difficult to track what was the reason for that.We should dump more statistics at that point - currently, we dont even know whether we died because the current process was bad orwas it due to some other process on that machine</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.nullgroup5.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecMapper.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-4-15 01:00:00" id="6909" opendate="2014-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Note for Hive 0.13 RC1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-4-15 01:00:00" id="6917" opendate="2014-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update Release Notes for Hive 0.13 RC2</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">RELEASE.NOTES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-4-17 01:00:00" id="6927" opendate="2014-4-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for MSSQL in schematool</summary>
      <description>Schematool is the preferred way of initializing schema for Hive. Since HIVE-6862 provided the script for MSSQL it would be nice to add the support for it in schematool.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaHelper.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-27 01:00:00" id="693" opendate="2009-7-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a AWS S3 log format deserializer</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.build.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-5-23 01:00:00" id="6966" opendate="2014-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>More fixes for TestCliDriver on Windows</summary>
      <description>This prevents the Test*CliDriver tests from compiling properly.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">ant.src.org.apache.hadoop.hive.ant.QTestGenTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-29 01:00:00" id="6985" opendate="2014-4-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>sql std auth - privileges grants to public role not being honored</summary>
      <description>When a privilege is granted to public role, the privilege is supposed to be applicable for all users.However, the privilege check fails for users, even if the have public role in the list of current roles.Note that the issue is only with public role. Grant of privileges of other role are not affected.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-7-28 01:00:00" id="700" opendate="2009-7-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test error by adding "DROP FUNCTION"</summary>
      <description>Since we added "Show Functions" in HIVE-580, test results will depend on what temporary functions are added to the system.We should add the capability of "DROP FUNCTION", and do that at the end of those "create function" tests to make sure the "show functions" results are deterministic.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.testlength.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.udaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudf.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.testlength.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.udaf.q</file>
      <file type="M">ql.src.test.queries.clientpositive.create.genericudf.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FunctionWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.FunctionSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-1 01:00:00" id="7000" opendate="2014-5-1 00:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Several issues with javadoc generation</summary>
      <description>1.Ran 'mvn javadoc:javadoc -Phadoop-2'. Encountered several issues Generated classes are included in the javadoc generation fails in the top level hcatalog folder because its src folder contains no java files.Patch attached to fix these issues.2.Tried mvn javadoc:aggregate -Phadoop-2 cannot get an aggregated javadoc for all of hive tried setting 'aggregate' parameter to true. Didn't workThere are several questions in StackOverflow about multiple project javadoc. Seems like this is broken.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-1 01:00:00" id="7001" opendate="2014-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>fs.permissions.umask-mode is getting unset when Session is started</summary>
      <description>hive&gt; set fs.permissions.umask-mode;fs.permissions.umask-mode=022hive&gt; show tables;OKt1Time taken: 0.301 seconds, Fetched: 1 row(s)hive&gt; set fs.permissions.umask-mode;fs.permissions.umask-mode is undefined</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-1 01:00:00" id="7004" opendate="2014-5-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix more unit test failures on hadoop-2</summary>
      <description>Still a number of precommit failures with hadoop-2, will try to fix some of them.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.stats.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.stats.partialscan.autogether.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.dynamic.partitions.with.whitelist.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.groupby2.map.skew.q</file>
      <file type="M">ql.src.test.queries.clientnegative.stats.partialscan.autogether.q</file>
      <file type="M">ql.src.test.queries.clientnegative.dynamic.partitions.with.whitelist.q</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-2 01:00:00" id="7005" opendate="2014-5-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MiniTez tests have non-deterministic explain plans</summary>
      <description>TestMiniTezCliDriver has a few test failures where there is a diff in the explain plan generated. According to Vikram, the plan generated is correct, but the plan can be generated in a couple of different ways and so sometimes the plan will not diff against the expected output. We should probably come up with a way to validate this explain plan in a reproducible way.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2009-7-29 01:00:00" id="702" opendate="2009-7-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>DROP TEMPORARY FUNCTION should not drop builtin functions</summary>
      <description>Only temporary functions should be dropped. It should error out if the user tries to drop built-in functions.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionInfo.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-7 01:00:00" id="7024" opendate="2014-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Escape control characters for explain result</summary>
      <description>Comments for columns are now delimited by 0x00, which is binary and make git refuse to make proper diff file.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.ctas.hadoop20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.truncate.column.list.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.transform.ppr1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.serde.user.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regexp.extract.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.push.or.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.partial.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.full.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.disable.merge.for.bucketing.q.out</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.TableDesc.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropagateForSubQuery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-7 01:00:00" id="7033" opendate="2014-5-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>grant statements should check if the role exists</summary>
      <description>The following grant statement that grants to a role that does not exist succeeds, but it should result in an error.&gt; grant all on t1 to role nosuchrole;</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.role.grant1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.1.sql.std.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.role.grant2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.role.grant1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.1.sql.std.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrincipal.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-8 01:00:00" id="7037" opendate="2014-5-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add additional tests for transform clauses with Tez</summary>
      <description>Enabling some q tests for Tez wrt to ScriptOperator/Stream/Transform.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  
  <bug fixdate="2014-5-14 01:00:00" id="7066" opendate="2014-5-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar is missing avro core</summary>
      <description>Running a simple query that reads an Avro table caused the following exception to be thrown on the cluster side:java.lang.RuntimeException: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:365) at org.apache.hadoop.hive.ql.exec.Utilities.getMapWork(Utilities.java:276) at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:254) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:445) at org.apache.hadoop.hive.ql.io.HiveInputFormat.pushProjectionsAndFilters(HiveInputFormat.java:438) at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getRecordReader(CombineHiveInputFormat.java:587) at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.&lt;init&gt;(MapTask.java:191) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:412) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366) at org.apache.hadoop.mapred.Child$4.run(Child.java:255) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:394) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190) at org.apache.hadoop.mapred.Child.main(Child.java:249)Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormatSerialization trace:outputFileFormatClass (org.apache.hadoop.hive.ql.plan.PartitionDesc)aliasToPartnInfo (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:125) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:942) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:850) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:864) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:334) ... 13 moreCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.avro.AvroContainerOutputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:26) at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:343) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:336) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:56) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:476) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:148) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113) ... 25 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:513) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:32) ... 37 moreCaused by: java.lang.NoClassDefFoundError: org/apache/avro/io/DatumWriter at java.lang.Class.getDeclaredFields0(Native Method) at java.lang.Class.privateGetDeclaredFields(Class.java:2348) at java.lang.Class.getDeclaredFields(Class.java:1779) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:150) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.&lt;init&gt;(FieldSerializer.java:109) ... 42 moreCaused by: java.lang.ClassNotFoundException: org.apache.avro.io.DatumWriter at java.net.URLClassLoader$1.run(URLClassLoader.java:202) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:190) at java.lang.ClassLoader.loadClass(ClassLoader.java:306) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301) at java.lang.ClassLoader.loadClass(ClassLoader.java:247) ... 47 moreI took a look at the hive-exec jar and found that the Avro core jar was not included, though avro-mapred is included.I confirmed that Avro core was included in the Hive 0.12 hive-exec jar. Was there a reason why this was removed in trunk? It seems that this would break the AvroSerDe.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-15 01:00:00" id="7068" opendate="2014-5-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Integrate AccumuloStorageHandler</summary>
      <description>Accumulo is a BigTable-clone which is similar to HBase. Some initial work has been done to support querying an Accumulo table using Hive already. It is not a complete solution as, most notably, the current implementation presently lacks support for INSERTs.I would like to polish up the AccumuloStorageHandler (presently based on 0.10), implement missing basic functionality and compare it to the HBaseStorageHandler (to ensure that we follow the same general usage patterns).I've also been in communication with bfem (the initial author) who expressed interest in working on this again. I hope to coordinate efforts with him.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">packaging.pom.xml</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-16 01:00:00" id="7080" opendate="2014-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>In PTest framework, Add logs URL to the JIRA comment</summary>
      <description>Enhancement request to add the logs url to the JIRA report. Currently it contains URL to the console output and jenkins reports only.NO PRECOMMIT TESTS</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.JIRAService.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.TestConfiguration.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-5-20 01:00:00" id="7099" opendate="2014-5-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Decimal datatype support for Windowing</summary>
      <description>Decimal datatype is not handled by Windowing</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-30 01:00:00" id="711" opendate="2009-7-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Check that GROUP BY works for negative double values</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-22 01:00:00" id="7111" opendate="2014-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Extend join transitivity PPD to non-column expressions</summary>
      <description>Join transitive in PPD only supports column expressions, but it's possible to extend this to generic expressions.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicateTransitivePropagate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.PredicatePushDown.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.ExprWalkerInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-5-22 01:00:00" id="7116" opendate="2014-5-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HDFS FileSystem object cache causes permission issues in creating tmp directories</summary>
      <description>We change permissions of the directory creation to 777 for HiveServer 2 operation and it turns out that because of HDFS caching, it does not reflect once created. We need to use the non-cached version of the API to get a FileSystem object to fix this.</description>
      <version>0.13.0</version>
      <fixedVersion>0.13.1,0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.common-secure.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-27 01:00:00" id="7126" opendate="2014-5-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cleanup build warnings while building hive projects</summary>
      <description>Currently while doing a build of hive projects, warnings like the following show up:mac-swarnim:hive sk018283$ mvn clean install -pl serde -P hadoop-1[INFO] Scanning for projects...[WARNING] [WARNING] Some problems were encountered while building the effective model for org.apache.hive.shims:hive-shims-0.23:jar:0.14.0-SNAPSHOT[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.hadoop:hadoop-hdfs:jar -&gt; duplicate declaration of version ${hadoop-23.version} @ line 110, column 17[WARNING] [WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.[WARNING] [WARNING] For this reason, future Maven versions might no longer support building such malformed projects.[WARNING]These warnings should be cleaned up.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.0.23.pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-6-29 01:00:00" id="7143" opendate="2014-5-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Streaming support in Windowing mode for more UDAFs (min/max, lead/lag, fval/lval)</summary>
      <description>Provided implementations for Streaming for the above fns.Min/Max based on Alg by Daniel Lemire: http://www.archipel.uqam.ca/309/1/webmaximinalgo.pdf</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udaf.TestStreamingSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.ISupportStreamingModeForWindowing.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFSum.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStreamingEnhancer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFRank.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLeadLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLag.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-31 01:00:00" id="7155" opendate="2014-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>WebHCat controller job exceeds container memory limit</summary>
      <description>Submit a Hive query on a large table via WebHCat results in failure because the WebHCat controller job is killed by Yarn since it exceeds the memory limit (set by mapreduce.map.memory.mb, defaults to 1GB): INSERT OVERWRITE TABLE Temp_InjusticeEvents_2014_03_01_00_00 SELECT * from Stage_InjusticeEvents where LogTimestamp &gt; '2014-03-01 00:00:00' and LogTimestamp &lt;= '2014-03-01 01:00:00';We could increase mapreduce.map.memory.mb to solve this problem, but this way we are changing this setting system wise.We need to provide a WebHCat configuration to overwrite mapreduce.map.memory.mb when submitting the controller job.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.AppConfig.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.config.webhcat-default.xml</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-6-6 01:00:00" id="7192" opendate="2014-6-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Streaming - Some required settings are not mentioned in the documentation</summary>
      <description>Specifically: hive.support.concurrency on metastore hive.vectorized.execution.enabled for query client</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.package.html</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-6-12 01:00:00" id="7226" opendate="2014-6-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Windowing Streaming mode causes NPE for empty partitions</summary>
      <description>Change in HIVE-7062 doesn't handle empty partitions properly. StreamingState is not correctly initialized for empty partition</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.windowing.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-13 01:00:00" id="7230" opendate="2014-6-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Eclipse formatter file for Hive coding conventions</summary>
      <description>Eclipse's formatter is a convenient way to clean up formatting for Java code. Currently, there is no Eclipse formatter file checked into Hive's codebase.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-16 01:00:00" id="7241" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong lock acquired for alter table rename partition</summary>
      <description>Doing an "alter table foo partition (bar='x') rename to partition (bar='y')" acquires a read lock on table foo. It should instead acquire an exclusive lock on partition bar=x.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-6-16 01:00:00" id="7242" opendate="2014-6-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table drop partition is acquiring the wrong type of lock</summary>
      <description>Doing an "alter table foo drop partition ('bar=x')" acquired a shared-write lock on partition bar=x. It should be acquiring an exclusive lock in that case.</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-6-24 01:00:00" id="7280" opendate="2014-6-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO V1</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryProperties.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.RowResolver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.TypeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveSwapJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushJoinThroughJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveTableScanRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveSortRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.cost.HiveVolcanoPlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.CostBasedOptimizer.java</file>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-6-26 01:00:00" id="7298" opendate="2014-6-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>desc database extended does not show properties of the database</summary>
      <description>HIVE-6386 added owner information to desc, but not updated schema of it.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.database.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.owner.actions.db.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.db.owner.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DescDatabaseDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">hcatalog.core.src.test.java.org.apache.hive.hcatalog.cli.TestSemanticAnalysis.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-7-30 01:00:00" id="7314" opendate="2014-6-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong results of UDF when hive.cache.expr.evaluation is set</summary>
      <description>It seems that the expression caching doesn't work when using UDF inside another UDF or a hive function.For example :tbl has one row : 'a','b'The following query : select concat(custUDF(a),' ', custUDF(b)) from tbl; returns 'a a'seems to cache custUDF(a) and use it for custUDF(b).Same query without the concat works fine.Replacing the concat with another custom UDF also returns 'a a'</description>
      <version>0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeNullDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-6 01:00:00" id="732" opendate="2009-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Store intermediate data in binary using LazyBinarySerDe</summary>
      <description>Follow-up on HIVE-640. We should use LazyBinarySerDe in several places in the code to improve the efficiency: value between map-reduce boundary, temporary tables.We should also allow users to create tables stored as binary format.CREATE TABLE xxx (...)ROW FORMAT BINARYSTORED AS SEQUENCEFILE;</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join8.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input.testsequencefile.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input9.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input20.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.input1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.case.sensitivity.q.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-1 01:00:00" id="7323" opendate="2014-7-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Date type stats in ORC sometimes go stale</summary>
      <description>I cannot make proper test case but sometimes min/max value in date type stats is changed in runtime. Stats for other type contains non-mutable values in it but date type stats contains DateWritable, which of inner value can be changed anytime.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-6 01:00:00" id="734" opendate="2009-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>problem in dealing with null</summary>
      <description>This command fails with the following error:hive/bin/hive -e "INSERT OVERWRITE LOCAL DIRECTORY 'abc' select null from zshao_tt"FAILED: Error in semantic analysis:java.lang.RuntimeException: Internal error: Cannot find ObjectInspector for VOIDWhen 'null' is replaced by '' it works.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  
  
  
  
  <bug fixdate="2014-7-8 01:00:00" id="7364" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trunk cannot be built on -Phadoop1 after HIVE-7144</summary>
      <description>Text.copyBytes() is introduced in hadoop-2</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.ColumnStatisticsImpl.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-8 01:00:00" id="7365" opendate="2014-7-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Explain authorize for auth2 throws exception</summary>
      <description>throws NPE in auth v2.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.authorization.view.sqlstd.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.authorization.view.sqlstd.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.AuthorizationFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-10 01:00:00" id="7385" opendate="2014-7-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize for empty relation scans</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.PhysicalOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
      <file type="M">itests.qtest.testconfiguration.properties</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-11 01:00:00" id="7388" opendate="2014-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Remove non-ascii char from comments</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.stats.FilterSelectivityEstimator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-11 01:00:00" id="7392" opendate="2014-7-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Support Columns Stats for Partition Columns</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-15 01:00:00" id="7407" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Handle UDFs generically</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RexNodeConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.RelNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-7-15 01:00:00" id="7412" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>column stats collection throws exception if all values for a column is null</summary>
      <description/>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-15 01:00:00" id="7413" opendate="2014-7-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fall back to Non-CBO optimizer if CBO fails</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-7-16 01:00:00" id="7429" opendate="2014-7-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set replication for archive called before file exists</summary>
      <description>The call to set replication is called prior to uploading the archive file to hdfs, which does not throw an error, but the replication never gets set.This has a significant impact on large jobs (especially hash joins) due to too many tasks hitting the data nodes.</description>
      <version>0.11.0,0.12.0,0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-8 01:00:00" id="743" opendate="2009-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>let user specify serde for custom scripts</summary>
      <description>Splitting up https://issues.apache.org/jira/browse/HIVE-708 into this.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-9-12 01:00:00" id="750" opendate="2009-8-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>new partitionpruner does not work with test mode</summary>
      <description>set hive.test.mode=true;the new partition pruner does not work</description>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.when.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf.case.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.udf1.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.subq.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-31 01:00:00" id="7563" opendate="2014-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ClassLoader should be released from LogFactory</summary>
      <description>NO PRECOMMIT TESTSLogFactory uses ClassLoader as a key in map, which makes the classloader impossible to be unloaded. LogFactory.release() should be called explicitly.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.JavaUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-7-31 01:00:00" id="7565" opendate="2014-7-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Fix exception in Greedy Join reordering Algo</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveJoinRel.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-5 01:00:00" id="7612" opendate="2014-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Add link to parent vertex to mapjoin in explain</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="7615" opendate="2014-8-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline should have an option for user to see the query progress</summary>
      <description>When executing query in Beeline, user should have a option to see the progress through the outputs.Beeline could use the API introduced in HIVE-4629 to get and display the logs to the client.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-6 01:00:00" id="7625" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: stats for Partitioned tables are not read correctly.</summary>
      <description>The wrong call is being made to read stats for Partitioned tables.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-6 01:00:00" id="7628" opendate="2014-8-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: log Plan coming out of each phase in Optiq Planning</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-8 01:00:00" id="7658" opendate="2014-8-8 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive search order for hive-site.xml when using --config option</summary>
      <description>When using the hive cli, the tool appears to favour a hive-site.xml file in the current working directory even if the --config option is used with a valid directory containing a hive-site.xml file.I would have expected the directory specified with --config to take precedence in the CLASSPATH search order.Here's an example -/home/spurija/hive-site.xml =&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/example1&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;/tmp/hive/hive-site.xml =&lt;configuration&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/example2&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;-bash-4.1$ diff /home/spurija/hive-site.xml /tmp/hive/hive-site.xml23c23&lt; &lt;value&gt;/tmp/example1&lt;/value&gt;&amp;#8212;&gt; &lt;value&gt;/tmp/example2&lt;/value&gt;{ check the value of scratchdir, should be example 1 }-bash-4.1$ pwd/home/spurija-bash-4.1$ hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example1{ run with a specified config, check the value of scratchdir, should be example2 … still reported as example1 }-bash-4.1$ pwd/home/spurija-bash-4.1$ hive --config /tmp/hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example1{ remove the local config, check the value of scratchdir, should be example2 … now correct }-bash-4.1$ pwd/home/spurija-bash-4.1$ rm hive-site.xml-bash-4.1$ hive --config /tmp/hiveLogging initialized using configuration in jar:file:/opt/mapr/hive/hive-0.13/lib/hive-common-0.13.0-mapr-1405.jar!/hive-log4j.propertieshive&gt; set hive.exec.local.scratchdir;hive.exec.local.scratchdir=/tmp/example2Is this expected behavior or should it use the directory supplied with --config as the preferred configuration?</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">bin.hive</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-13 01:00:00" id="7715" opendate="2014-8-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO:Support Union All</summary>
      <description/>
      <version>None</version>
      <fixedVersion>cbo-branch</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.TraitsUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HiveRelFieldTrimmer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.rules.HivePushFilterPastJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.reloperators.HiveProjectRel.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="776" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>make div as infix operator</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.4.0,0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.divider.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.divider.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-8-20 01:00:00" id="7791" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (1) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. alter_merge_orc.q,\ alter_merge_stats_orc.q,\ auto_join0.q,\ auto_join1.q,\ bucket2.q,\ bucket3.q,\ bucket4.q,\ count.q,\ create_merge_compressed.q,\ cross_join.q,\ cross_product_check_1.q,\ cross_product_check_2.q,\ ctas.q,\custom_input_output_format.q,\ disable_merge_for_bucketing.q,\ dynpart_sort_opt_vectorization.q,\ dynpart_sort_optimization.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7792" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (2) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on.limit_pushdown.q,\ load_dyn_part1.q,\ load_dyn_part2.q,\ load_dyn_part3.q,\ mapjoin_mapjoin.q,\ mapreduce1.q,\ mapreduce2.q,\ merge1.q,\ merge2.q,\ metadata_only_queries.q,\ optimize_nullscan.q,\ orc_analyze.q,\ orc_merge1.q,\ orc_merge2.q,\ orc_merge3.q,\ orc_merge4.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-20 01:00:00" id="7793" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (3) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. ptf.q,\ sample1.q,\ script_env_var1.q,\ script_env_var2.q,\ script_pipe.q,\ scriptfile1.q,\ stats_counter.q,\ stats_counter_partitioned.q,\ stats_noscan_1.q,\ subquery_exists.q,\ subquery_in.q,\ temp_table.q,\ transform1.q,\ transform2.q,\ transform_ppr1.q,\ transform_ppr2.q,\</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-20 01:00:00" id="7794" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable tests on Spark branch (4) [Sparch Branch]</summary>
      <description>This jira is to enable most of the tests below. If tests don't pass because of some unsupported feature, ensure that a JIRA exists and move on. vector_cast_constant.q,\ vector_data_types.q,\ vector_decimal_aggregate.q,\ vector_left_outer_join.q,\ vector_string_concat.q,\ vectorization_12.q,\ vectorization_13.q,\ vectorization_14.q,\ vectorization_15.q,\ vectorization_9.q,\ vectorization_part_project.q,\ vectorization_short_regress.q,\ vectorized_mapjoin.q,\ vectorized_nested_mapjoin.q,\ vectorized_ptf.q,\ vectorized_shufflejoin.q,\ vectorized_timestamp_funcs.q</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-20 01:00:00" id="7795" opendate="2014-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable ptf.q and ptf_streaming.q.[Spark Branch]</summary>
      <description>ptf.q and ptf_streaming.q contains join queries, we should enable these qtests in milestone2.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-8-20 01:00:00" id="780" opendate="2009-8-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Better error messages for wrong argument length in terminatePartial()/merge() UDAF calls</summary>
      <description>This is a simple fix to make sure if the UDAF's argument length of terminatePartial()/merge() UDAF calls have a problem.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-8-23 01:00:00" id="7864" opendate="2014-8-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Query fails if it refers only partitioning column</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.cbo.correctness.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.cbo.correctness.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.RelOptHiveTable.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-8-26 01:00:00" id="7876" opendate="2014-8-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>further improve the columns stats update speed for all the partitions of a table</summary>
      <description>The previous solution HIVE-7736 is not enough for the case when there are too many columns/partitions.The user will encounter org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed outWe try to remove more of transaction overhead</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-3 01:00:00" id="7949" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create table LIKE command doesn&amp;#39;t set new owner</summary>
      <description>'Create table like' command doesn't set the current user as owner of new table, instead new table owner is same as source table owner.This is a regression from 0.12</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-3 01:00:00" id="7951" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>InputFormats implementing (Job)Configurable should not be cached</summary>
      <description>Currently, initial configuration instance is shared to all following input formats, which should not be like that.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-10-3 01:00:00" id="7960" opendate="2014-9-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Hadoop 2.5</summary>
      <description>Tracking JIRA for upgrading to 2.5</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-2-5 01:00:00" id="7998" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enhance JDBC Driver to not require class specification</summary>
      <description>The hotspot VM offers a way to avoid having to specify the driver class explicitly when using the JDBC driver. The DriverManager methods getConnection and getDrivers have been enhanced to support the Java Standard Edition Service Provider mechanism. JDBC 4.0 Drivers must include the file META-INF/services/java.sql.Driver. This file contains the name of the JDBC drivers implementation of java.sql.Driver. For example, to load the my.sql.Driver class, the META-INF/services/java.sql.Driver file would contain the entry: `my.sql.Driver`Applications no longer need to explicitly load JDBC drivers using Class.forName(). Existing programs which currently load JDBC drivers using Class.forName() will continue to work without modification.via http://docs.oracle.com/javase/7/docs/api/java/sql/DriverManager.html</description>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-5 01:00:00" id="8008" opendate="2014-9-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NPE while reading null decimal value</summary>
      <description>Say you have this table dec_test:dec decimal(10,0) If the table has a row that is 9999999999.5, and if we doselect * from dec_test;it will crash with NPE:2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerExceptionjava.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.hadoop.util.RunJar.main(RunJar.java:212)Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796) at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137) ... 12 moreCaused by: java.lang.NullPointerException at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439) at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70) at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39) at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87) ... 19 more</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.type.TestHiveDecimal.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-12 01:00:00" id="8083" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Authorization DDLs should not enforce hive identifier syntax for user or group</summary>
      <description>The compiler expects principals (user, group and role) as hive identifiers for authorization DDLs. The user and group are entities that belong to external namespace and we can't expect those to follow hive identifier syntax rules. For example, a userid or group can contain '-' which is not allowed by compiler.</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-12 01:00:00" id="8084" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Handle casting for parameterized type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.SqlFunctionConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-9-12 01:00:00" id="8086" opendate="2014-9-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Disable Trivial Project Removal Rule, Fix Result Schema</summary>
      <description>Disable trivial project rule till Optiq-407 gets fixed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.DerivedTableInjector.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-13 01:00:00" id="8087" opendate="2014-9-13 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Bug in constant conversion for Date type</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.ExprNodeConverter.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-9-31 01:00:00" id="810" opendate="2009-8-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>annotation for function for JDBC</summary>
      <description>After committing 645, I realized a problem - the function does not have annotation, and therefore the describe will fail.</description>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">contrib.src.test.results.clientpositive.dboutput.q.out</file>
      <file type="M">contrib.src.test.queries.clientpositive.dboutput.q</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.genericudf.example.GenericUDFDBOutput.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-16 01:00:00" id="8115" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive select query hang when fields contain map</summary>
      <description>Attached the repro of the issue. When creating an table loading the data attached, all hive query with hangs even just select * from the table.repro steps:1. run createTable.hql2. hadoop fs -put data /data3. LOAD DATA INPATH '/data' OVERWRITE INTO TABLE testtable;4. SELECT * FROM testtable;</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.lazy.TestLazyArrayMapStruct.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazyMap.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="8117" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[CBO] Let cbo handle field expression for join conditions</summary>
      <description>Was disabled, but can be enabled.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.optiq.translator.JoinTypeCheckCtx.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-1-16 01:00:00" id="8121" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create micro-benchmarks for ParquetSerde and evaluate performance</summary>
      <description>These benchmarks should not execute queries but test only the ParquetSerde code to ensure we are as efficient as possible. The output of this JIRA is:1) Benchmark tool exists2) We create new tasks under HIVE-8120 to track the improvements required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-16 01:00:00" id="8124" opendate="2014-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move local_mapred_error_cache.q to minimr</summary>
      <description>local_mapred_error_cache.q gets stuck sometimes. Looks like an issue with local runner. The test works fine in minimr.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-23 01:00:00" id="8239" opendate="2014-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>MSSQL upgrade schema scripts does not map Java long datatype columns correctly for transaction related tables</summary>
      <description>In Transaction related tables, Java long column fields are mapped to int which results in failure as shown:2014-09-23 18:08:00,030 DEBUG txn.TxnHandler (TxnHandler.java:lock(1243)) - Going to execute update &lt;insert into HIVE_LOCKS (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, hl_table, hl_partition, hl_lock_state, hl_lock_type, hl_last_heartbeat, hl_user, hl_host) values (28, 1,0, 'default', null, null, 'w', 'r', 1411495679547, 'hadoopqa', 'onprem-sqoop1')&gt;2014-09-23 18:08:00,033 DEBUG txn.TxnHandler (TxnHandler.java:lock(406)) - Going to rollback2014-09-23 18:08:00,045 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - org.apache.thrift.TException: MetaException(message:Unable to update transaction database com.microsoft.sqlserver.jdbc.SQLServerException: Arithmetic overflow error converting expression to data type int. at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError(SQLServerException.java:197) at com.microsoft.sqlserver.jdbc.TDSTokenHandler.onEOF(tdsparser.java:246) at com.microsoft.sqlserver.jdbc.TDSParser.parse(tdsparser.java:83) at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServerStatement.java:1488) at com.microsoft.sqlserver.jdbc.SQLServerStatement.doExecuteStatement(SQLServerStatement.java:775) at com.microsoft.sqlserver.jdbc.SQLServerStatement$StmtExecCmd.doExecute(SQLServerStatement.java:676) at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:4615) at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:1400) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLServerStatement.java:179) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLServerStatement.java:154) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeUpdate(SQLServerStatement.java:633) at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497) at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:1244) at org.apache.hadoop.hive.metastore.txn.TxnHandler.lock(TxnHandler.java:403) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.lock(HiveMetaStore.java:5255) ...In this query one of the column HL_LAST_HEARTBEAT defined as int datatype in HIVE_LOCKS is trying to take in a long value (1411495679547) and throws the error. We should use bigint as column type instead.NO PRECOMMIT TESTS</description>
      <version>0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.upgrade-0.13.0-to-0.14.0.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.hive-schema-0.14.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-9-24 01:00:00" id="8245" opendate="2014-9-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect table read entities at same time as view read entities</summary>
      <description/>
      <version>0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.query.properties.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.analyze.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.with.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.dependency.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.authorization.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.limit.partition.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-9-29 01:00:00" id="8298" opendate="2014-9-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Incorrect results for n-way join when join expressions are not in same order across joins</summary>
      <description>select * from srcpart a join srcpart b on a.key = b.key and a.hr = b.hr join srcpart c on a.hr = c.hr and a.key = c.key;is minimal query which reproduces it</description>
      <version>0.13.0,0.13.1</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-30 01:00:00" id="8313" opendate="2014-9-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Optimize evaluation for ExprNodeConstantEvaluator and ExprNodeNullEvaluator</summary>
      <description>Consider the following query:SELECT foo, bar, goo, idFROM myTableWHERE id IN ( 'A', 'B', 'C', 'D', ... , 'ZZZZZZ' );One finds that when the IN clause has several thousand elements (and the table has several million rows), the query above takes orders-of-magnitude longer to run on Hive 0.12 than say Hive 0.10.I have a possibly incomplete fix.</description>
      <version>0.12.0,0.13.0,0.14.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-10 01:00:00" id="8428" opendate="2014-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PCR doesnt remove filters involving casts</summary>
      <description>e.g.,select key,value from srcpart where hr = cast(11 as double);</description>
      <version>0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.pcr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.vectorization.ppd.q</file>
      <file type="M">ql.src.test.queries.clientpositive.orc.ppd.decimal.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-10 01:00:00" id="8429" opendate="2014-10-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add records in/out counters</summary>
      <description>We don't do counters for input/output records right now. That would help for debugging though (if it can be done with minimal overhead).</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestOperators.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.RecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MergeFileRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecReducer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecMapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-12 01:00:00" id="8435" opendate="2014-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add identity project remover optimization</summary>
      <description/>
      <version>0.9.0,0.10.0,0.11.0,0.12.0,0.13.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.type.widening.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.smb.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.tez.bmj.schema.evolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.select.transform.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.script.pipe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.clusterby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.reducers.power.two.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cluster.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-12 01:00:00" id="8436" opendate="2014-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Modify SparkWork to split works with multiple child works [Spark Branch]</summary>
      <description>Based on the design doc, we need to split the operator tree of a work in SparkWork if the work is connected to multiple child works. The way splitting the operator tree is performed by cloning the original work and removing unwanted branches in the operator tree. Please refer to the design doc for details.This process should be done right before we generate SparkPlan. We should have a utility method that takes the orignal SparkWork and return a modified SparkWork.This process should also keep the information about the original work and its clones. Such information will be needed during SparkPlan generation (HIVE-8437).</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.BaseWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkTableScanProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkProcessAnalyzeTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMultiInsertionProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkMergeTaskProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.MapInput.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveReduceFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-10-20 01:00:00" id="8528" opendate="2014-10-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add remote Spark client to Hive [Spark Branch]</summary>
      <description>For the time being, at least, we've decided to build the Spark client (see SPARK-3215) inside Hive. This task tracks merging the ongoing work into the Spark branch.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-10-21 01:00:00" id="8539" opendate="2014-10-21 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable collect table statistics based on SparkCounter[Spark Branch]</summary>
      <description>Hive support collect table statistics based on Counters/TezCounters in MR/Tez mode, we should enable this in Spark mode as well.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.varchar.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.char.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.rcfile.columnar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.project.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.decimal.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.transform1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.lazy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.table.access.keys.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.scriptfile1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.convert.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.enforce.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.custom.input.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.create.merge.compressed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin6.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.StatsSetupConst.java</file>
      <file type="M">data.conf.spark.hive-site.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.counter.SparkCounters.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSession.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkRecordHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.SimpleSparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.SparkJobStatus.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.SparkWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.CounterStatsPublisher.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.add.part.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.alter.merge.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.avro.decimal.native.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-11-29 01:00:00" id="8649" opendate="2014-10-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase level of parallelism in reduce phase [Spark Branch]</summary>
      <description>We calculate the number of reducers based on the same code for MapReduce. However, reducers are vastly cheaper in Spark and it's generally recommended we have many more reducers than in MR.Sandy Ryza who works on Spark has some ideas about a heuristic.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.spark.SetSparkReducerParallelism.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-10-1 01:00:00" id="865" opendate="2009-10-1 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin: memory leak for same key with very large number of values</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-11-3 01:00:00" id="8715" opendate="2014-11-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive 14 upgrade scripts can fail for statistics if database was created using auto-create</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.019-HIVE-7784.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.020-HIVE-7784.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.019-HIVE-7784.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.002-HIVE-7784.mssql.sql</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2014-11-12 01:00:00" id="8847" opendate="2014-11-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix bugs in jenkins scripts</summary>
      <description>1) Incorrect help message in process_jira function2) Spark builds do not work3) Build "profiles" (which map to a properties file) are hard coded4) A JIRA is required</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.jenkins-execute-build.sh</file>
      <file type="M">dev-support.jenkins-common.sh</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2014-3-17 01:00:00" id="8898" opendate="2014-11-17 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove HIVE-8874 once HBASE-12493 is fixed</summary>
      <description/>
      <version>None</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-11-25 01:00:00" id="8961" opendate="2014-11-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove unnecessary dependency collection task [Spark Branch]</summary>
      <description>Seems some dependency collection task we add for move task is unnecessary.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.varchar.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.char.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.tez.join.tests.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.tez.joins.explain.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.temp.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.partscan.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.router.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.reduce.deduplicate.exclude.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.transform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.join.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mergejoins.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.test.outer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.subquery2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.subquery.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.memcheck.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.hook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.filter.on.outerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.louter.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.vc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.thrift.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.nullsafe.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.hive.626.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.unqual1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join41.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.disable.merge.for.bucketing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.column.access.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkProcContext.java</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.reordering.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin5.q.out</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-9 01:00:00" id="9048" opendate="2014-12-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive build failed on hadoop-1 after HIVE-8828.</summary>
      <description>HIVE-8828 introduce org.apache.hadoop.tools.HadoopArchives which included in hadoop-tools(in hadoop-1)/hadoop-archives(in hadoop-2), while hadoop-tools is not added into hadoop-1 dependency. This lead to the following compile error:[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:compile (default-compile) on project hive-exec: Compilation failure: Compilation failure:[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[175,30] error: package org.apache.hadoop.tools does not exist[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,8] error: cannot find symbol[ERROR] class DDLTask[ERROR] /home/cxli/sources/github/chengxiangli/hive/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java:[1322,33] error: cannot find symbol[ERROR] -&gt; [Help 1]</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-12-10 01:00:00" id="9059" opendate="2014-12-10 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove wrappers for SparkJobInfo and SparkStageInfo [Spark Branch]</summary>
      <description>SPARK-4567 is resolved. We can remove the wrappers we added to solve the serailization issues.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkStageInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.status.HiveSparkJobInfo.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.KryoMessageCodec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.impl.RemoteSparkJobStatus.java</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2014-1-30 01:00:00" id="9226" opendate="2014-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Beeline interweaves the query result and query log sometimes</summary>
      <description>In most case, Beeline output the query log during execution and output the result at last. However, sometimes there are logs output after result, although the query has been done. This might make users a little confused.</description>
      <version>None</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">beeline.src.java.org.apache.hive.beeline.Commands.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2014-1-30 01:00:00" id="9234" opendate="2014-12-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer2 leaks FileSystem objects in FileSystem.CACHE</summary>
      <description>Running over extended period (48+ hrs), we've noticed HiveServer2 leaking FileSystem objects in FileSystem.CACHE. Linked jiras were previous attempts to fix it, but the issue still seems to be there. A workaround is to disable the caching (by setting fs.hdfs.impl.disable.cache and fs.file.impl.disable.cache to true), but creating new FileSystem objects is expensive.</description>
      <version>0.12.0,0.12.1,0.13.0,0.13.1,0.14.0</version>
      <fixedVersion>1.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.SessionManager.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionBase.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSession.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2015-2-29 01:00:00" id="9513" opendate="2015-1-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>NULL POINTER EXCEPTION</summary>
      <description>NPE duting parsing of :select * from ( select * from ( select 1 as id , "foo" as str_1 from staging.dual ) f union all select * from ( select 2 as id , "bar" as str_2 from staging.dual ) g) e ;</description>
      <version>0.12.0,0.13.0,0.13.1,0.14.0,1.0.0,1.1.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.union3.q.out</file>
      <file type="M">ql.src.test.queries.clientnegative.union3.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-11-6 01:00:00" id="9599" opendate="2015-2-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>remove derby, datanucleus and other not related to jdbc client classes from hive-jdbc-standalone.jar</summary>
      <description>Looks like the following packages (included to hive-jdbc-standalone.jar) are not used when jdbc client opens jdbc connection and runs queries:antlr/antlr/actions/cpp/antlr/actions/csharp/antlr/actions/java/antlr/actions/python/antlr/ASdebug/antlr/build/antlr/collections/antlr/collections/impl/antlr/debug/antlr/debug/misc/antlr/preprocessor/com/google/gson/com/google/gson/annotations/com/google/gson/internal/com/google/gson/internal/bind/com/google/gson/reflect/com/google/gson/stream/com/google/inject/com/google/inject/binder/com/google/inject/internal/com/google/inject/internal/asm/com/google/inject/internal/cglib/core/com/google/inject/internal/cglib/proxy/com/google/inject/internal/cglib/reflect/com/google/inject/internal/util/com/google/inject/matcher/com/google/inject/name/com/google/inject/servlet/com/google/inject/spi/com/google/inject/util/com/jamesmurty/utils/com/jcraft/jsch/com/jcraft/jsch/jce/com/jcraft/jsch/jcraft/com/jcraft/jsch/jgss/com/jolbox/bonecp/com/jolbox/bonecp/hooks/com/jolbox/bonecp/proxy/com/sun/activation/registries/com/sun/activation/viewers/com/sun/istack/com/sun/istack/localization/com/sun/istack/logging/com/sun/mail/handlers/com/sun/mail/iap/com/sun/mail/imap/com/sun/mail/imap/protocol/com/sun/mail/mbox/com/sun/mail/pop3/com/sun/mail/smtp/com/sun/mail/util/com/sun/xml/bind/com/sun/xml/bind/annotation/com/sun/xml/bind/api/com/sun/xml/bind/api/impl/com/sun/xml/bind/marshaller/com/sun/xml/bind/unmarshaller/com/sun/xml/bind/util/com/sun/xml/bind/v2/com/sun/xml/bind/v2/bytecode/com/sun/xml/bind/v2/model/annotation/com/sun/xml/bind/v2/model/core/com/sun/xml/bind/v2/model/impl/com/sun/xml/bind/v2/model/nav/com/sun/xml/bind/v2/model/runtime/com/sun/xml/bind/v2/runtime/com/sun/xml/bind/v2/runtime/output/com/sun/xml/bind/v2/runtime/property/com/sun/xml/bind/v2/runtime/reflect/com/sun/xml/bind/v2/runtime/reflect/opt/com/sun/xml/bind/v2/runtime/unmarshaller/com/sun/xml/bind/v2/schemagen/com/sun/xml/bind/v2/schemagen/episode/com/sun/xml/bind/v2/schemagen/xmlschema/com/sun/xml/bind/v2/util/com/sun/xml/txw2/com/sun/xml/txw2/annotation/com/sun/xml/txw2/output/com/thoughtworks/paranamer/contribs/mx/javax/activation/javax/annotation/javax/annotation/concurrent/javax/annotation/meta/javax/annotation/security/javax/el/javax/inject/javax/jdo/javax/jdo/annotations/javax/jdo/datastore/javax/jdo/identity/javax/jdo/listener/javax/jdo/metadata/javax/jdo/spi/javax/mail/javax/mail/event/javax/mail/internet/javax/mail/search/javax/mail/util/javax/security/auth/message/javax/security/auth/message/callback/javax/security/auth/message/config/javax/security/auth/message/module/javax/servlet/javax/servlet/http/javax/servlet/jsp/javax/servlet/jsp/el/javax/servlet/jsp/tagext/javax/transaction/javax/transaction/xa/javax/xml/bind/javax/xml/bind/annotation/javax/xml/bind/annotation/adapters/javax/xml/bind/attachment/javax/xml/bind/helpers/javax/xml/bind/util/javax/xml/stream/javax/xml/stream/events/javax/xml/stream/util/jline/jline/console/jline/console/completer/jline/console/history/jline/console/internal/jline/internal/net/iharder/base64/org/aopalliance/aop/org/aopalliance/intercept/org/apache/commons/beanutils/org/apache/commons/beanutils/converters/org/apache/commons/beanutils/expression/org/apache/commons/beanutils/locale/org/apache/commons/beanutils/locale/converters/org/apache/commons/cli/org/apache/commons/codec/org/apache/commons/codec/binary/org/apache/commons/codec/digest/org/apache/commons/codec/language/org/apache/commons/codec/net/org/apache/commons/collections/org/apache/commons/collections/bag/org/apache/commons/collections/bidimap/org/apache/commons/collections/buffer/org/apache/commons/collections/collection/org/apache/commons/collections/comparators/org/apache/commons/collections/functors/org/apache/commons/collections/iterators/org/apache/commons/collections/keyvalue/org/apache/commons/collections/list/org/apache/commons/collections/map/org/apache/commons/collections/set/org/apache/commons/configuration/org/apache/commons/configuration/beanutils/org/apache/commons/configuration/event/org/apache/commons/configuration/interpol/org/apache/commons/configuration/plist/org/apache/commons/configuration/reloading/org/apache/commons/configuration/tree/org/apache/commons/configuration/tree/xpath/org/apache/commons/configuration/web/org/apache/commons/dbcp/org/apache/commons/dbcp/cpdsadapter/org/apache/commons/dbcp/datasources/org/apache/commons/dbcp/managed/org/apache/commons/digester/org/apache/commons/digester/parser/org/apache/commons/digester/plugins/org/apache/commons/digester/plugins/strategies/org/apache/commons/digester/substitution/org/apache/commons/digester/xmlrules/org/apache/commons/el/org/apache/commons/el/parser/org/apache/commons/httpclient/org/apache/commons/httpclient/auth/org/apache/commons/httpclient/contrib/proxy/org/apache/commons/httpclient/cookie/org/apache/commons/httpclient/methods/org/apache/commons/httpclient/methods/multipart/org/apache/commons/httpclient/params/org/apache/commons/httpclient/protocol/org/apache/commons/httpclient/util/org/apache/commons/io/org/apache/commons/io/comparator/org/apache/commons/io/filefilter/org/apache/commons/io/input/org/apache/commons/io/monitor/org/apache/commons/io/output/org/apache/commons/jocl/org/apache/commons/math3/org/apache/commons/math3/analysis/org/apache/commons/math3/analysis/differentiation/org/apache/commons/math3/analysis/function/org/apache/commons/math3/analysis/integration/org/apache/commons/math3/analysis/integration/gauss/org/apache/commons/math3/analysis/interpolation/org/apache/commons/math3/analysis/polynomials/org/apache/commons/math3/analysis/solvers/org/apache/commons/math3/complex/org/apache/commons/math3/dfp/org/apache/commons/math3/distribution/org/apache/commons/math3/exception/org/apache/commons/math3/exception/util/org/apache/commons/math3/filter/org/apache/commons/math3/fitting/org/apache/commons/math3/fraction/org/apache/commons/math3/genetics/org/apache/commons/math3/geometry/org/apache/commons/math3/geometry/euclidean/oned/org/apache/commons/math3/geometry/euclidean/threed/org/apache/commons/math3/geometry/euclidean/twod/org/apache/commons/math3/geometry/partitioning/org/apache/commons/math3/geometry/partitioning/utilities/org/apache/commons/math3/linear/org/apache/commons/math3/ode/org/apache/commons/math3/ode/events/org/apache/commons/math3/ode/nonstiff/org/apache/commons/math3/ode/sampling/org/apache/commons/math3/optim/org/apache/commons/math3/optimization/org/apache/commons/math3/optimization/direct/org/apache/commons/math3/optimization/fitting/org/apache/commons/math3/optimization/general/org/apache/commons/math3/optimization/linear/org/apache/commons/math3/optimization/univariate/org/apache/commons/math3/optim/linear/org/apache/commons/math3/optim/nonlinear/scalar/org/apache/commons/math3/optim/nonlinear/scalar/gradient/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/org/apache/commons/math3/optim/nonlinear/vector/org/apache/commons/math3/optim/nonlinear/vector/jacobian/org/apache/commons/math3/optim/univariate/org/apache/commons/math3/random/org/apache/commons/math3/special/org/apache/commons/math3/stat/org/apache/commons/math3/stat/clustering/org/apache/commons/math3/stat/correlation/org/apache/commons/math3/stat/descriptive/org/apache/commons/math3/stat/descriptive/moment/org/apache/commons/math3/stat/descriptive/rank/org/apache/commons/math3/stat/descriptive/summary/org/apache/commons/math3/stat/inference/org/apache/commons/math3/stat/ranking/org/apache/commons/math3/stat/regression/org/apache/commons/math3/transform/org/apache/commons/math3/util/org/apache/commons/net/org/apache/commons/net/bsd/org/apache/commons/net/chargen/org/apache/commons/net/daytime/org/apache/commons/net/discard/org/apache/commons/net/echo/org/apache/commons/net/finger/org/apache/commons/net/ftp/org/apache/commons/net/ftp/parser/org/apache/commons/net/imap/org/apache/commons/net/io/org/apache/commons/net/nntp/org/apache/commons/net/ntp/org/apache/commons/net/pop3/org/apache/commons/net/smtp/org/apache/commons/net/telnet/org/apache/commons/net/tftp/org/apache/commons/net/time/org/apache/commons/net/util/org/apache/commons/net/whois/org/apache/commons/pool/org/apache/commons/pool/impl/org/apache/curator/org/apache/curator/drivers/org/apache/curator/ensemble/org/apache/curator/ensemble/exhibitor/org/apache/curator/ensemble/fixed/org/apache/curator/framework/org/apache/curator/framework/api/org/apache/curator/framework/api/transaction/org/apache/curator/framework/imps/org/apache/curator/framework/listen/org/apache/curator/framework/recipes/org/apache/curator/framework/recipes/atomic/org/apache/curator/framework/recipes/barriers/org/apache/curator/framework/recipes/cache/org/apache/curator/framework/recipes/leader/org/apache/curator/framework/recipes/locks/org/apache/curator/framework/recipes/nodes/org/apache/curator/framework/recipes/queue/org/apache/curator/framework/recipes/shared/org/apache/curator/framework/state/org/apache/curator/retry/org/apache/curator/utils/org/apache/derby/agg/org/apache/derby/authentication/org/apache/derby/catalog/org/apache/derby/catalog/types/org/apache/derby/database/org/apache/derby/diag/org/apache/derby/iapi/db/org/apache/derby/iapi/error/org/apache/derby/iapi/jdbc/org/apache/derby/iapi/reference/org/apache/derby/iapi/security/org/apache/derby/iapi/services/org/apache/derby/iapi/services/cache/org/apache/derby/iapi/services/classfile/org/apache/derby/iapi/services/compiler/org/apache/derby/iapi/services/context/org/apache/derby/iapi/services/crypto/org/apache/derby/iapi/services/daemon/org/apache/derby/iapi/services/diag/org/apache/derby/iapi/services/i18n/org/apache/derby/iapi/services/info/org/apache/derby/iapi/services/io/org/apache/derby/iapi/services/jmx/org/apache/derby/iapi/services/loader/org/apache/derby/iapi/services/locks/org/apache/derby/iapi/services/memory/org/apache/derby/iapi/services/monitor/org/apache/derby/iapi/services/property/org/apache/derby/iapi/services/stream/org/apache/derby/iapi/services/timer/org/apache/derby/iapi/services/uuid/org/apache/derby/iapi/sql/org/apache/derby/iapi/sql/compile/org/apache/derby/iapi/sql/conn/org/apache/derby/iapi/sql/depend/org/apache/derby/iapi/sql/dictionary/org/apache/derby/iapi/sql/execute/org/apache/derby/iapi/sql/execute/xplain/org/apache/derby/iapi/store/access/org/apache/derby/iapi/store/access/conglomerate/org/apache/derby/iapi/store/access/xa/org/apache/derby/iapi/store/raw/org/apache/derby/iapi/store/raw/data/org/apache/derby/iapi/store/raw/log/org/apache/derby/iapi/store/raw/xact/org/apache/derby/iapi/store/replication/master/org/apache/derby/iapi/store/replication/slave/org/apache/derby/iapi/tools/i18n/org/apache/derby/iapi/transaction/org/apache/derby/iapi/types/org/apache/derby/iapi/util/org/apache/derby/impl/db/org/apache/derby/impl/io/org/apache/derby/impl/io/vfmem/org/apache/derby/impl/jdbc/org/apache/derby/impl/jdbc/authentication/org/apache/derby/impl/load/org/apache/derby/impl/services/bytecode/org/apache/derby/impl/services/cache/org/apache/derby/impl/services/daemon/org/apache/derby/impl/services/jce/org/apache/derby/impl/services/jmx/org/apache/derby/impl/services/jmxnone/org/apache/derby/impl/services/locks/org/apache/derby/impl/services/monitor/org/apache/derby/impl/services/reflect/org/apache/derby/impl/services/stream/org/apache/derby/impl/services/timer/org/apache/derby/impl/services/uuid/org/apache/derby/impl/sql/org/apache/derby/impl/sql/catalog/org/apache/derby/impl/sql/compile/org/apache/derby/impl/sql/conn/org/apache/derby/impl/sql/depend/org/apache/derby/impl/sql/execute/org/apache/derby/impl/sql/execute/rts/org/apache/derby/impl/sql/execute/xplain/org/apache/derby/impl/store/access/org/apache/derby/impl/store/access/btree/org/apache/derby/impl/store/access/btree/index/org/apache/derby/impl/store/access/conglomerate/org/apache/derby/impl/store/access/heap/org/apache/derby/impl/store/access/sort/org/apache/derby/impl/store/raw/org/apache/derby/impl/store/raw/data/org/apache/derby/impl/store/raw/log/org/apache/derby/impl/store/raw/xact/org/apache/derby/impl/store/replication/org/apache/derby/impl/store/replication/buffer/org/apache/derby/impl/store/replication/master/org/apache/derby/impl/store/replication/net/org/apache/derby/impl/store/replication/slave/org/apache/derby/impl/tools/sysinfo/org/apache/derby/io/org/apache/derby/jdbc/org/apache/derby/mbeans/org/apache/derby/osgi/org/apache/derby/security/org/apache/derby/shared/common/error/org/apache/derby/shared/common/reference/org/apache/derby/tools/org/apache/derby/vti/org/apache/directory/api/asn1/org/apache/directory/api/asn1/util/org/apache/directory/api/util/org/apache/directory/api/util/exception/org/apache/directory/server/i18n/org/apache/directory/server/kerberos/changepwd/exceptions/org/apache/directory/server/kerberos/changepwd/io/org/apache/directory/server/kerberos/changepwd/messages/org/apache/directory/server/kerberos/protocol/codec/org/apache/directory/server/kerberos/shared/crypto/checksum/org/apache/directory/server/kerberos/shared/crypto/encryption/org/apache/directory/server/kerberos/shared/keytab/org/apache/directory/server/kerberos/shared/replay/org/apache/directory/server/kerberos/shared/store/org/apache/directory/shared/kerberos/org/apache/directory/shared/kerberos/codec/org/apache/directory/shared/kerberos/codec/actions/org/apache/directory/shared/kerberos/codec/adAndOr/org/apache/directory/shared/kerberos/codec/adAndOr/actions/org/apache/directory/shared/kerberos/codec/adIfRelevant/org/apache/directory/shared/kerberos/codec/adKdcIssued/org/apache/directory/shared/kerberos/codec/adKdcIssued/actions/org/apache/directory/shared/kerberos/codec/adMandatoryForKdc/org/apache/directory/shared/kerberos/codec/apRep/org/apache/directory/shared/kerberos/codec/apRep/actions/org/apache/directory/shared/kerberos/codec/apReq/org/apache/directory/shared/kerberos/codec/apReq/actions/org/apache/directory/shared/kerberos/codec/asRep/org/apache/directory/shared/kerberos/codec/asRep/actions/org/apache/directory/shared/kerberos/codec/asReq/org/apache/directory/shared/kerberos/codec/asReq/actions/org/apache/directory/shared/kerberos/codec/authenticator/org/apache/directory/shared/kerberos/codec/authenticator/actions/org/apache/directory/shared/kerberos/codec/authorizationData/org/apache/directory/shared/kerberos/codec/authorizationData/actions/org/apache/directory/shared/kerberos/codec/changePwdData/org/apache/directory/shared/kerberos/codec/changePwdData/actions/org/apache/directory/shared/kerberos/codec/checksum/org/apache/directory/shared/kerberos/codec/checksum/actions/org/apache/directory/shared/kerberos/codec/encApRepPart/org/apache/directory/shared/kerberos/codec/encApRepPart/actions/org/apache/directory/shared/kerberos/codec/encAsRepPart/org/apache/directory/shared/kerberos/codec/encAsRepPart/actions/org/apache/directory/shared/kerberos/codec/EncKdcRepPart/org/apache/directory/shared/kerberos/codec/EncKdcRepPart/actions/org/apache/directory/shared/kerberos/codec/encKrbCredPart/org/apache/directory/shared/kerberos/codec/encKrbCredPart/actions/org/apache/directory/shared/kerberos/codec/encKrbPrivPart/org/apache/directory/shared/kerberos/codec/encKrbPrivPart/actions/org/apache/directory/shared/kerberos/codec/encryptedData/org/apache/directory/shared/kerberos/codec/encryptedData/actions/org/apache/directory/shared/kerberos/codec/encryptionKey/org/apache/directory/shared/kerberos/codec/encryptionKey/actions/org/apache/directory/shared/kerberos/codec/encTgsRepPart/org/apache/directory/shared/kerberos/codec/encTgsRepPart/actions/org/apache/directory/shared/kerberos/codec/encTicketPart/org/apache/directory/shared/kerberos/codec/encTicketPart/actions/org/apache/directory/shared/kerberos/codec/etypeInfo/org/apache/directory/shared/kerberos/codec/etypeInfo2/org/apache/directory/shared/kerberos/codec/etypeInfo2/actions/org/apache/directory/shared/kerberos/codec/etypeInfo2Entry/org/apache/directory/shared/kerberos/codec/etypeInfo2Entry/actions/org/apache/directory/shared/kerberos/codec/etypeInfo/actions/org/apache/directory/shared/kerberos/codec/etypeInfoEntry/org/apache/directory/shared/kerberos/codec/etypeInfoEntry/actions/org/apache/directory/shared/kerberos/codec/hostAddress/org/apache/directory/shared/kerberos/codec/hostAddress/actions/org/apache/directory/shared/kerberos/codec/hostAddresses/org/apache/directory/shared/kerberos/codec/hostAddresses/actions/org/apache/directory/shared/kerberos/codec/kdcRep/org/apache/directory/shared/kerberos/codec/kdcRep/actions/org/apache/directory/shared/kerberos/codec/kdcReq/org/apache/directory/shared/kerberos/codec/kdcReq/actions/org/apache/directory/shared/kerberos/codec/kdcReqBody/org/apache/directory/shared/kerberos/codec/kdcReqBody/actions/org/apache/directory/shared/kerberos/codec/krbCred/org/apache/directory/shared/kerberos/codec/krbCred/actions/org/apache/directory/shared/kerberos/codec/krbCredInfo/org/apache/directory/shared/kerberos/codec/krbCredInfo/actions/org/apache/directory/shared/kerberos/codec/krbError/org/apache/directory/shared/kerberos/codec/krbError/actions/org/apache/directory/shared/kerberos/codec/krbPriv/org/apache/directory/shared/kerberos/codec/krbPriv/actions/org/apache/directory/shared/kerberos/codec/krbSafe/org/apache/directory/shared/kerberos/codec/krbSafe/actions/org/apache/directory/shared/kerberos/codec/krbSafeBody/org/apache/directory/shared/kerberos/codec/krbSafeBody/actions/org/apache/directory/shared/kerberos/codec/lastReq/org/apache/directory/shared/kerberos/codec/lastReq/actions/org/apache/directory/shared/kerberos/codec/methodData/org/apache/directory/shared/kerberos/codec/methodData/actions/org/apache/directory/shared/kerberos/codec/options/org/apache/directory/shared/kerberos/codec/padata/org/apache/directory/shared/kerberos/codec/padata/actions/org/apache/directory/shared/kerberos/codec/paEncTimestamp/org/apache/directory/shared/kerberos/codec/paEncTsEnc/org/apache/directory/shared/kerberos/codec/paEncTsEnc/actions/org/apache/directory/shared/kerberos/codec/principalName/org/apache/directory/shared/kerberos/codec/principalName/actions/org/apache/directory/shared/kerberos/codec/tgsRep/org/apache/directory/shared/kerberos/codec/tgsRep/actions/org/apache/directory/shared/kerberos/codec/tgsReq/org/apache/directory/shared/kerberos/codec/tgsReq/actions/org/apache/directory/shared/kerberos/codec/ticket/org/apache/directory/shared/kerberos/codec/ticket/actions/org/apache/directory/shared/kerberos/codec/transitedEncoding/org/apache/directory/shared/kerberos/codec/transitedEncoding/actions/org/apache/directory/shared/kerberos/codec/typedData/org/apache/directory/shared/kerberos/codec/typedData/actions/org/apache/directory/shared/kerberos/codec/types/org/apache/directory/shared/kerberos/components/org/apache/directory/shared/kerberos/crypto/checksum/org/apache/directory/shared/kerberos/exceptions/org/apache/directory/shared/kerberos/flags/org/apache/directory/shared/kerberos/messages/org/apache/hadoop/hive/metastore/org/apache/hadoop/hive/metastore/api/org/apache/hadoop/hive/metastore/events/org/apache/hadoop/hive/metastore/hooks/org/apache/hadoop/hive/metastore/model/org/apache/hadoop/hive/metastore/parser/org/apache/hadoop/hive/metastore/partition/spec/org/apache/hadoop/hive/metastore/tools/org/apache/hadoop/hive/metastore/txn/org/apache/hadoop/hive/schshim/org/apache/jasper/org/apache/jasper/compiler/org/apache/jasper/compiler/tagplugin/org/apache/jasper/runtime/org/apache/jasper/security/org/apache/jasper/servlet/org/apache/jasper/tagplugins/jstl/org/apache/jasper/tagplugins/jstl/core/org/apache/jasper/util/org/apache/jasper/xmlparser/org/apache/jute/org/apache/jute/compiler/org/apache/jute/compiler/generated/org/apache/zookeeper/org/apache/zookeeper/client/org/apache/zookeeper/common/org/apache/zookeeper/data/org/apache/zookeeper/jmx/org/apache/zookeeper/proto/org/apache/zookeeper/server/org/apache/zookeeper/server/auth/org/apache/zookeeper/server/persistence/org/apache/zookeeper/server/quorum/org/apache/zookeeper/server/quorum/flexible/org/apache/zookeeper/server/upgrade/org/apache/zookeeper/server/util/org/apache/zookeeper/txn/org/apache/zookeeper/version/org/apache/zookeeper/version/util/org/codehaus/jackson/jaxrs/org/codehaus/jackson/xc/org/codehaus/jettison/org/codehaus/jettison/badgerfish/org/codehaus/jettison/json/org/codehaus/jettison/mapped/org/codehaus/jettison/util/org/datanucleus/org/datanucleus/api/org/datanucleus/api/jdo/org/datanucleus/api/jdo/exceptions/org/datanucleus/api/jdo/metadata/org/datanucleus/api/jdo/query/org/datanucleus/api/jdo/state/org/datanucleus/asm/org/datanucleus/cache/org/datanucleus/enhancer/org/datanucleus/enhancer/jdo/org/datanucleus/enhancer/jdo/method/org/datanucleus/enhancer/spi/org/datanucleus/exceptions/org/datanucleus/flush/org/datanucleus/identity/org/datanucleus/management/org/datanucleus/management/jmx/org/datanucleus/metadata/org/datanucleus/metadata/annotations/org/datanucleus/metadata/xml/org/datanucleus/plugin/org/datanucleus/properties/org/datanucleus/query/org/datanucleus/query/cache/org/datanucleus/query/compiler/org/datanucleus/query/evaluator/org/datanucleus/query/evaluator/memory/org/datanucleus/query/expression/org/datanucleus/query/node/org/datanucleus/query/symbol/org/datanucleus/query/typesafe/org/datanucleus/state/org/datanucleus/store/org/datanucleus/store/autostart/org/datanucleus/store/connection/org/datanucleus/store/encryption/org/datanucleus/store/exceptions/org/datanucleus/store/federation/org/datanucleus/store/fieldmanager/org/datanucleus/store/objectvaluegenerator/org/datanucleus/store/query/org/datanucleus/store/query/cache/org/datanucleus/store/rdbms/org/datanucleus/store/rdbms/adapter/org/datanucleus/store/rdbms/autostart/org/datanucleus/store/rdbms/connectionpool/org/datanucleus/store/rdbms/datasource/org/datanucleus/store/rdbms/datasource/dbcp/org/datanucleus/store/rdbms/datasource/dbcp/cpdsadapter/org/datanucleus/store/rdbms/datasource/dbcp/datasources/org/datanucleus/store/rdbms/datasource/dbcp/jocl/org/datanucleus/store/rdbms/datasource/dbcp/managed/org/datanucleus/store/rdbms/datasource/dbcp/pool/org/datanucleus/store/rdbms/datasource/dbcp/pool/impl/org/datanucleus/store/rdbms/exceptions/org/datanucleus/store/rdbms/fieldmanager/org/datanucleus/store/rdbms/identifier/org/datanucleus/store/rdbms/key/org/datanucleus/store/rdbms/mapping/org/datanucleus/store/rdbms/mapping/datastore/org/datanucleus/store/rdbms/mapping/java/org/datanucleus/store/rdbms/mapping/oracle/org/datanucleus/store/rdbms/query/org/datanucleus/store/rdbms/request/org/datanucleus/store/rdbms/schema/org/datanucleus/store/rdbms/scostore/org/datanucleus/store/rdbms/sql/org/datanucleus/store/rdbms/sql/expression/org/datanucleus/store/rdbms/sql/method/org/datanucleus/store/rdbms/sql/operation/org/datanucleus/store/rdbms/table/org/datanucleus/store/rdbms/valuegenerator/org/datanucleus/store/schema/org/datanucleus/store/schema/naming/org/datanucleus/store/schema/table/org/datanucleus/store/scostore/org/datanucleus/store/types/org/datanucleus/store/types/backed/org/datanucleus/store/types/converters/org/datanucleus/store/types/simple/org/datanucleus/store/valuegenerator/org/datanucleus/transaction/org/datanucleus/transaction/jta/org/datanucleus/util/org/datanucleus/validation/org/fusesource/hawtjni/runtime/org/fusesource/jansi/org/fusesource/jansi/internal/org/fusesource/leveldbjni/org/fusesource/leveldbjni/internal/org/htrace/org/htrace/impl/org/htrace/wrappers/org/iq80/leveldb/org/jboss/netty/bootstrap/org/jboss/netty/buffer/org/jboss/netty/channel/org/jboss/netty/channel/group/org/jboss/netty/channel/local/org/jboss/netty/channel/socket/org/jboss/netty/channel/socket/http/org/jboss/netty/channel/socket/nio/org/jboss/netty/channel/socket/oio/org/jboss/netty/container/microcontainer/org/jboss/netty/container/osgi/org/jboss/netty/container/spring/org/jboss/netty/handler/codec/org/jboss/netty/handler/codec/base64/org/jboss/netty/handler/codec/compression/org/jboss/netty/handler/codec/embedder/org/jboss/netty/handler/codec/frame/org/jboss/netty/handler/codec/http/org/jboss/netty/handler/codec/http/multipart/org/jboss/netty/handler/codec/http/websocket/org/jboss/netty/handler/codec/http/websocketx/org/jboss/netty/handler/codec/marshalling/org/jboss/netty/handler/codec/oneone/org/jboss/netty/handler/codec/protobuf/org/jboss/netty/handler/codec/replay/org/jboss/netty/handler/codec/rtsp/org/jboss/netty/handler/codec/serialization/org/jboss/netty/handler/codec/socks/org/jboss/netty/handler/codec/spdy/org/jboss/netty/handler/codec/string/org/jboss/netty/handler/execution/org/jboss/netty/handler/ipfilter/org/jboss/netty/handler/logging/org/jboss/netty/handler/queue/org/jboss/netty/handler/ssl/org/jboss/netty/handler/stream/org/jboss/netty/handler/timeout/org/jboss/netty/handler/traffic/org/jboss/netty/logging/org/jboss/netty/util/org/jboss/netty/util/internal/org/jboss/netty/util/internal/jzlib/org/jets3t/service/org/jets3t/service/acl/org/jets3t/service/acl/gs/org/jets3t/service/impl/rest/org/jets3t/service/impl/rest/httpclient/org/jets3t/service/io/org/jets3t/service/model/org/jets3t/service/model/cloudfront/org/jets3t/service/model/container/org/jets3t/service/multi/org/jets3t/service/multi/event/org/jets3t/service/multi/s3/org/jets3t/service/multithread/org/jets3t/service/mx/org/jets3t/service/security/org/jets3t/service/utils/org/jets3t/service/utils/gatekeeper/org/jets3t/service/utils/oauth/org/jets3t/service/utils/signedurl/org/mortbay/component/org/mortbay/io/org/mortbay/io/bio/org/mortbay/io/nio/org/mortbay/jetty/org/mortbay/jetty/bio/org/mortbay/jetty/deployer/org/mortbay/jetty/handler/org/mortbay/jetty/nio/org/mortbay/jetty/security/org/mortbay/jetty/servlet/org/mortbay/jetty/webapp/org/mortbay/log/org/mortbay/resource/org/mortbay/servlet/org/mortbay/servlet/jetty/org/mortbay/thread/org/mortbay/util/org/mortbay/util/ajax/org/mortbay/xml/org/objectweb/asm/org/objectweb/asm/commons/org/objectweb/asm/signature/org/objectweb/asm/tree/org/xerial/snappy/org/znerd/xmlenc/org/znerd/xmlenc/sax/schema/</description>
      <version>None</version>
      <fixedVersion>2.0.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-12-2 01:00:00" id="966" opendate="2009-12-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive command line should output log messages in 24-hour format</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.5.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2015-3-7 01:00:00" id="9892" opendate="2015-3-7 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>various MSSQL upgrade scripts don&amp;#39;t work</summary>
      <description>Issue with GO statement when run through schematool - it results in syntax error. the create if not exists logic for PART_COL_STATS wasn't workingNO PRECOMMIT TESTS</description>
      <version>0.13.0,0.14.0,1.0.0</version>
      <fixedVersion>1.2.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.mssql.005-HIVE-9296.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.004-HIVE-8550.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.002-HIVE-7784.mssql.sql</file>
      <file type="M">metastore.scripts.upgrade.mssql.pre-0-upgrade-0.12.0-to-0.13.0.mssql.sql</file>
    </fixedFiles>
  </bug>
</bugrepository>