<?xml version="1.0" encoding="UTF-8" standalone="no"?><bugrepository name="HIVE">
  <bug fixdate="2011-10-28 01:00:00" id="2243" opendate="2011-6-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t publish maven release artifacts to apache repository</summary>
      <description>So far I haven't been able to push the maven artifacts to the Apache release repository. Here's the error I get:% ant maven-publish -Dmvn.publish.repo=releases...maven-publish-artifact:[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/releases[artifact:deploy] Uploading: org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jar to repository apache.releases.https at https://repository.apache.org/content/repositories/releases[artifact:deploy] Transferring 9K from apache.releases.https[artifact:deploy] An error has occurred while processing the Maven artifact tasks.[artifact:deploy] Diagnosis:[artifact:deploy] [artifact:deploy] Error deploying artifact 'org.apache.hive:hive-anttasks:jar': Error deploying artifact: Authorization failed: Access denied to: https://repository.apache.org/content/repositories/releases/org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jarI get the same error when I try to publish to the staging repository.I took another look at the Apache "Publishing Maven Artifacts" guide (http://www.apache.org/dev/publishing-maven-artifacts.html) and think that we're probably failing to include a couple fields that are required in the pom files. It also looks like we should be pushing this to the staging repository as opposed to the releases repository.</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.build.xml</file>
      <file type="M">service.build.xml</file>
      <file type="M">serde.build.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">pdk.build.xml</file>
      <file type="M">odbc.build.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.build.xml</file>
      <file type="M">cli.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
      <file type="M">ant.build.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-10-30 01:00:00" id="22436" opendate="2019-10-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more logging to the test.</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-29 01:00:00" id="2244" opendate="2011-6-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Plugin Developer Kit to Hive</summary>
      <description>See https://cwiki.apache.org/confluence/display/Hive/PluginDeveloperKit</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-11 01:00:00" id="22621" opendate="2019-12-11 00:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Disable unstable tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge9.q</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenariosACIDNoAutogather.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-12-11 01:00:00" id="22625" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syntax Error in findPotentialCompactions SQL query for MySql/Postgres</summary>
      <description>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '=&gt; current_timestamp - interval '254' second'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-11 01:00:00" id="22627" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schema changes introduced in HIVE-21443 to the schema upgrade scripts</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-11 01:00:00" id="22630" opendate="2019-12-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not retrieve Materialized View definitions for rebuild if query is test SQL</summary>
      <description>for the query like select 1, select current_timestamp, select current_datehive retrieve all the Materialized view from metastore, if the one of databases are too large then this call take lots of time, the situation becomes worse if there are too frequent if hive server receives frequent "select 1" query ( connection pool uses it to check if the connection is valid or not).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-1-12 01:00:00" id="22631" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid deep copying partition list in listPartitionsByExpr</summary>
      <description>This is an expensive call, I am not sure why deepCopy is required.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2019-12-12 01:00:00" id="22632" opendate="2019-12-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve estimateRowSizeFromSchema</summary>
      <description>estimateRowSizeFromSchema un-necessarily iterate and do look-up. This could be avoided.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2019-6-29 01:00:00" id="22681" opendate="2019-12-29 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-3-27 01:00:00" id="22940" opendate="2020-2-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the datasketches functions available as predefined functions</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2020-3-18 01:00:00" id="23042" opendate="2020-3-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge queries to a single one for updating MIN_OPEN_TXNS table</summary>
      <description>When opening a new transaction we issue 2 queries to update the MIN_OPEN_TXN table.&lt;SELECT MIN("TXN_ID") FROM "TXNS" WHERE "TXN_STATE" = 'o'&gt;&lt;insert into "MIN_HISTORY_LEVEL" ("MHL_TXNID", "MHL_MIN_OPEN_TXNID") values(763, 763)&gt;This could be archived with a single query faster, if we do not open transactions in batch, like:&lt;INSERT INTO "MIN_HISTORY_LEVEL" ("MHL_TXNID", "MHL_MIN_OPEN_TXNID") SELECT ?, MIN("TXN_ID") FROM "TXNS" WHERE "TXN_STATE" = 'o'&gt;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.src.main.java.org.apache.hadoop.hive.metastore.tools.HMSClient.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.src.main.java.org.apache.hadoop.hive.metastore.tools.HMSBenchmarks.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.src.main.java.org.apache.hadoop.hive.metastore.tools.BenchmarkTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-7-26 01:00:00" id="2307" opendate="2011-7-26 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-25 01:00:00" id="23073" opendate="2020-3-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-3-27 01:00:00" id="23097" opendate="2020-3-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: LLAPServiceDriver is wrongly using fixed llap- prefix for tarball name</summary>
      <description>LLAP: LLAPServiceDriver is wrongly using fixed llap- prefix for tarball nameMissed modifying this change too in the Jira https://issues.apache.org/jira/browse/HIVE-22937 </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.service.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-30 01:00:00" id="23103" opendate="2020-3-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Oracle statement batching</summary>
      <description>Examine how to really get better performance for oracle statement batches.Oracle JDBC doc describes:The Oracle implementation of standard update batching does not implement true batching for generic statements and callable statements. Even though Oracle JDBC supports the use of standard batching for Statement and CallableStatement objects, you are unlikely to see performance improvement.I would look for connection properties to set, so it is handled anyway, or if not, then use:begin query1; query2; query3;end;to we will have only a single roundtrip for the db.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  
  
  
  <bug fixdate="2020-4-15 01:00:00" id="23209" opendate="2020-4-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>ptest2 compilation failure after HIVE-21603 - upgrade mockito-core in testutils/ptest2</summary>
      <description>[ERROR] COMPILATION ERROR : [INFO] -------------------------------------------------------------[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/context/TestCloudExecutionContextProvider.java:[21,26] cannot find symbol symbol: class ArgumentMatchers location: package org.mockito[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/context/TestCloudExecutionContextProvider.java:[21,1] static import only from classes and interfacesreprocd testutils/ptest2mvn clean install -DskipTests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-20 01:00:00" id="23252" opendate="2020-4-20 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change spark related tests to be optional</summary>
      <description>HIVE-23137 have disabled the execution of some spark related tests; but they would be still considered by a plain maven command - and the spark artifacts are (unneccessarily) still downloaded</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2020-4-23 01:00:00" id="23283" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate random temp ID for lock enqueue and commitTxn</summary>
      <description>In order to optimize the S4U scope of enqueue lock and commitTxn, currently a hardcoded constant (-1) is used to first insert all the lock and ws entries with a temporary lockID/commitID. However, in a concurrent environment this seems to cause some performance degradation (and deadlock issues with some rdbms) as multiple concurrent transactions are trying to insert rows with the same primary key (e.g. (-1, 1), (-1, 2), (-1, 3), .. etc. for (extID/intID) in HIVE_LOCKS). The proposed solution is to replace the constant with a random generated negative number, which seems to resolve this issue.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-23 01:00:00" id="23284" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on mariadb-java-client</summary>
      <description>It has GNU Lesser General Public License which is Category X.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Mysql.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.DEV-README</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-23 01:00:00" id="23287" opendate="2020-4-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on icu4j</summary>
      <description>Brought in transitively via druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-4-24 01:00:00" id="23293" opendate="2020-4-24 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locks: Implement zero-wait readers</summary>
      <description>With a new lock type (EXCL_WRITE) for INSERT_OVERWRITE, SHARED_READ does not have to wait for any lock - it can fails fast for a pending EXCLUSIVE, because even if there is an EXCL_WRITE or SHARED_WRITE pending, there's no semantic reason to wait for them.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-5-30 01:00:00" id="23344" opendate="2020-4-30 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump scala version to 2.12.4, spark to 2.4.5</summary>
      <description>And bump up spark version, as 2.3.3 is not compatible with scala 2.12</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2020-6-16 01:00:00" id="23482" opendate="2020-5-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use junit5 to execute tests</summary>
      <description/>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-19 01:00:00" id="2398" opendate="2011-8-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive server doesn&amp;#39;t return schema for &amp;#39;set&amp;#39; command</summary>
      <description>The Hive server does process the CLI commands like 'set', 'set -v' sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.</description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-14 01:00:00" id="2445" opendate="2011-9-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>The PerfLogger should log the full name of hooks, not just the simple name.</summary>
      <description>Sometimes the simple name of a hook is not enough to identify it, so the PerfLogger should log the full name instead.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-15 01:00:00" id="2448" opendate="2011-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade JavaEWAH to 0.3</summary>
      <description>It contains performance improvements and should be a drop-in replacement.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.javaewah-0.2.jar</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-15 01:00:00" id="2451" opendate="2011-9-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TABLESAMBLE(BUCKET xxx) sometimes doesn&amp;#39;t trigger input pruning as regression of HIVE-1538</summary>
      <description>Example:select count(1) from &lt;bucket_table&gt; TABLESAMPLE(BUCKET xxx out of yyy) where &lt;partition_column&gt; = 'xxx'will not trigger input pruning.The reason is that we assume sample filtering operator only happens as the second filter after table scan, which is broken by HIVE-1538, even if the feature doesn't turn on.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-9-16 01:00:00" id="2453" opendate="2011-9-16 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a way to categorize queries in hooks for improved logging</summary>
      <description>We need a way to categorize queries, such as whether or not the include a join clause, a group by clause, etc., in the hooks. This will allow for better performance logging.Currently the only way I can find is to go through the operators in the tasks, but which operators are used for the different types of queries may change over time.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-18 01:00:00" id="2455" opendate="2011-9-18 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass correct remoteAddress in proxy user authentication</summary>
      <description/>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2009-1-23 01:00:00" id="246" opendate="2009-1-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Msck incorrectly finds tables not in ms</summary>
      <description>The msck command looks for tables on HDFS that are missing in the Metastore. It tries to find these tables in the same dir as already existing tables. Due to a bug it also does this for external tables, causing it to find unwanted results.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-11-23 01:00:00" id="2466" opendate="2011-9-23 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin_subquery dump small table (mapjoin table) to the same file</summary>
      <description>in mapjoin_subquery.q there is a query：SELECT /*+ MAPJOIN(z) */ subq.key1, z.valueFROM(SELECT /*+ MAPJOIN */ x.key as key1, x.value as value1, y.key as key2, y.value as value2 FROM src1 x JOIN src y ON (x.key = y.key)) subq JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);when dump x and z to a local file,there all dump to the same file, so we lost the data of x</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-5 01:00:00" id="2487" opendate="2011-10-5 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug from HIVE-2446, the code that calls client stats publishers run() methods is in wrong place, should be in the same method but inside of while (!rj.isComplete()) {} loop</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-6 01:00:00" id="2488" opendate="2011-10-6 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PDK tests failing on Hudson because HADOOP_HOME is not defined</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pdk.scripts.build-plugin.xml</file>
      <file type="M">pdk.build.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-12 01:00:00" id="2497" opendate="2011-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition pruning prune some right partition under specific conditions</summary>
      <description>create table src3(key string, value string) partitioned by (pt string)row format delimited fields terminated by ',';ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110911000000') ;ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110912000000') ;ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110913000000') ;explain extendedselect user_id from ( select cast(key as int) as user_id ,case when (value like 'aaa%' or value like 'vvv%') then 1 else 0 end as tag_student from src3 ) subwhere sub.tag_student &gt; 0;STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Map Reduce Alias -&gt; Map Operator Tree: sub:src3 TableScan alias: src3 Filter Operator isSamplingPred: false predicate: expr: (CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END &gt; 0) type: boolean Select Operator expressions: expr: UDFToInteger(key) type: int expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END type: int outputColumnNames: _col0, _col1 Filter Operator isSamplingPred: false predicate: expr: (_col1 &gt; 0) type: boolean Select Operator expressions: expr: _col0 type: int outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-26-12_894_9085644225727185586/-ext-10001 NumFilesPerFileSink: 1 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: columns _col0 columns.types int serialization.format 1 TotalFiles: 1 MultiFileSpray: false Needs Tagging: false Stage: Stage-0 Fetch Operator limit: -1if we set hive.optimize.ppd=false;STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Map Reduce Alias -&gt; Map Operator Tree: sub:src3 TableScan alias: src3 Select Operator expressions: expr: UDFToInteger(key) type: int expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END type: int outputColumnNames: _col0, _col1 Filter Operator isSamplingPred: false predicate: expr: (_col1 &gt; 0) type: boolean Select Operator expressions: expr: _col0 type: int outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-27-22_527_1729287213481398480/-ext-10001 NumFilesPerFileSink: 1 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: columns _col0 columns.types int serialization.format 1 TotalFiles: 1 MultiFileSpray: false Needs Tagging: false Path -&gt; Alias: hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 &amp;#91;sub:src3&amp;#93; hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110912000000 &amp;#91;sub:src3&amp;#93; hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110913000000 &amp;#91;sub:src3&amp;#93; Path -&gt; Partition: hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 Partition base file name: pt=20110911000000</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-4-25 01:00:00" id="250" opendate="2009-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>shared memory java dbm for map-side joins</summary>
      <description>can use either: sdbm: http://freshmeat.net/projects/solingerjavasdbm/ jdbm: http://sourceforge.net/projects/jdbm/both need modifications to use file mmaps instead of regular file io. will do some testing to see if there's a major difference between the two.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-12 01:00:00" id="2500" opendate="2011-10-12 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Hive to be debugged remotely</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">conf.hive-env.sh.template</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.help.sh</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2011-10-19 01:00:00" id="2515" opendate="2011-10-19 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Constant OIs work with UDTFs.</summary>
      <description>UDTFs are the last shoe to drop for constant OIs.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-4-28 01:00:00" id="254" opendate="2009-1-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tracker page should provide a link to the "next step"</summary>
      <description>This may belong in Hadoop Core, or maybe more about Hive integration with Hadoop, but many queries these days take multiple steps or mapreduce phases. It would be lovely if a "finished" mapreduce from a Hive query that has several steps could provide a link to the tracker page for the next mapreduce step.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.session.manage.jsp</file>
      <file type="M">hwi.web.session.history.jsp</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-1-22 01:00:00" id="2735" opendate="2012-1-22 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-25 01:00:00" id="2748" opendate="2012-1-25 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-2-9 01:00:00" id="283" opendate="2009-2-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition pruning not happening for uppercase table aliases</summary>
      <description>With upper case aliases partition pruning breaks.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-28 01:00:00" id="2831" opendate="2012-2-28 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestContribCliDriver.dboutput and TestCliDriver.input45 fail on 0.23</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-2 01:00:00" id="2833" opendate="2012-3-2 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test failures caused by HIVE-2716</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-3-3 01:00:00" id="2837" opendate="2012-3-3 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert into external tables should not be allowed</summary>
      <description>This is a very risky thing to allow. Since, the external tables can point to any user location, which can potentially corrupt some other tables.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-10-15 01:00:00" id="2874" opendate="2012-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming external partition changes location</summary>
      <description>Renaming an external partition will change the location of that partition to the default location of a managed partition with the same name.E.g. If ex_table is external and has partition part=1 with location /.../managed_table/part=1Calling ALTER TABLE ex_table PARTITION (part = '1') RENAME TO PARTITION (part = '2');Will change the location of the partition to /.../ex_table/part=2</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-3-15 01:00:00" id="2875" opendate="2012-3-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming partition changes partition location prefix</summary>
      <description>Renaming a partition changes the location of the partition to the default location of the table, followed by the partition specification. It should just change the partition specification of the path.If the path does not end with the old partition specification, we should probably throw an exception because renaming a partition should not change the path so dramatically, and not changing the path to reflect the new partition name could leave the partition in a very confusing state.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  
  <bug fixdate="2012-9-31 01:00:00" id="3068" opendate="2012-5-31 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to export table metadata as JSON on table drop</summary>
      <description>When a table is dropped, the contents go to the users trash but the metadata is lost. It would be super neat to be able to save the metadata as well so that tables could be trivially re-instantiated via thrift.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2012-9-15 01:00:00" id="3388" opendate="2012-8-15 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Performance of UDF PERCENTILE_APPROX()</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2009-3-11 01:00:00" id="339" opendate="2009-3-11 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hive] problem in count distinct in 1mapreduce job with map side aggregation</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.groupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2012-8-27 01:00:00" id="3409" opendate="2012-8-27 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase test.junit.timeout value</summary>
      <description/>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  
  
  <bug fixdate="2009-5-14 01:00:00" id="410" opendate="2009-4-14 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>Heartbeating for streaming jobs should not depend on stdout</summary>
      <description>jobs that require iterative processing may take longer than 10 mins to produce rows. This shouldn't be cause to kill the job. Producing keepalive dummy rows to stdout is bad if the data has to go into a Hive table or other Hive steps.If we adopt the solution of using stderr to indicate heartbeats, can that be combined with streaming counters (http://hadoop.apache.org/core/docs/current/streaming.html#How+do+I+update+counters+in+streaming+applications%3F )? Also, will limitations on size of stderr break this?</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug fixdate="2013-9-9 01:00:00" id="4531" opendate="2013-5-9 00:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Collecting task logs to hdfs</summary>
      <description>It would be nice we collect task logs after job finish. This is similar to what Amazon EMR does.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.pig.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.mapreducestreaming.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.mapreducejar.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.hive.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>