<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="2243" opendate="2011-6-28 00:00:00" fixdate="2011-10-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Can&amp;#39;t publish maven release artifacts to apache repository</summary>
      <description>So far I haven't been able to push the maven artifacts to the Apache release repository. Here's the error I get:% ant maven-publish -Dmvn.publish.repo=releases...maven-publish-artifact:[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/releases[artifact:deploy] Uploading: org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jar to repository apache.releases.https at https://repository.apache.org/content/repositories/releases[artifact:deploy] Transferring 9K from apache.releases.https[artifact:deploy] An error has occurred while processing the Maven artifact tasks.[artifact:deploy] Diagnosis:[artifact:deploy] [artifact:deploy] Error deploying artifact 'org.apache.hive:hive-anttasks:jar': Error deploying artifact: Authorization failed: Access denied to: https://repository.apache.org/content/repositories/releases/org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jarI get the same error when I try to publish to the staging repository.I took another look at the Apache "Publishing Maven Artifacts" guide (http://www.apache.org/dev/publishing-maven-artifacts.html) and think that we're probably failing to include a couple fields that are required in the pom files. It also looks like we should be pushing this to the staging repository as opposed to the releases repository.</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.build.xml</file>
      <file type="M">service.build.xml</file>
      <file type="M">serde.build.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">pdk.build.xml</file>
      <file type="M">odbc.build.xml</file>
      <file type="M">metastore.build.xml</file>
      <file type="M">jdbc.build.xml</file>
      <file type="M">hwi.build.xml</file>
      <file type="M">hbase-handler.build.xml</file>
      <file type="M">contrib.build.xml</file>
      <file type="M">common.build.xml</file>
      <file type="M">cli.build.xml</file>
      <file type="M">build.xml</file>
      <file type="M">build-common.xml</file>
      <file type="M">ant.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22431" opendate="2019-10-30 00:00:00" fixdate="2019-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive JDBC Storage Handler: java.lang.ClassCastException on accessing TINYINT, SMALLINT Data Type From JDBC Data Source</summary>
      <description>Steps to Reproduce://MySQL table:create table testtbl(a TINYINT, b SMALLINT);// Insert to table via mysql connector//Hive table:CREATE EXTERNAL TABLE `hive_table`( a TINYINT, b SMALLINT ) STORED BY 'org.apache.hive.storage.jdbc.JdbcStorageHandler' TBLPROPERTIES ( 'hive.sql.database.type'='MYSQL', 'hive.sql.dbcp.password'='', 'hive.sql.dbcp.username'='', 'hive.sql.jdbc.driver'='com.mysql.jdbc.Driver', 'hive.sql.jdbc.url'='', 'hive.sql.table'='testtbl');//Hive query:select * from hive_table;Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.ByteFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.ShortÂ </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc-handler.src.main.java.org.apache.hive.storage.jdbc.JdbcSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="22436" opendate="2019-10-30 00:00:00" fixdate="2019-10-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more logging to the test.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2244" opendate="2011-6-29 00:00:00" fixdate="2011-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add a Plugin Developer Kit to Hive</summary>
      <description>See https://cwiki.apache.org/confluence/display/Hive/PluginDeveloperKit</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22621" opendate="2019-12-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>Disable unstable tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.orc.merge9.q</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.security.TestLlapSignerImpl.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestSSL.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestStatsReplicationScenariosACIDNoAutogather.java</file>
    </fixedFiles>
  </bug>
  <bug id="22624" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix results_cache_invalidation and results_cache_lifetime</summary>
      <description>these tests are falling together they both run on hive-ptest-slaves-8e2.c.gcp-hive-upstream.internal I suspect that they are fighting for the ownership /tmp/hive/resultscacheexception#12019-12-11T00:16:02,564 INFO [22d96536-983a-417d-b10c-aa1766ba0994 main] results.QueryResultsCache: Initializing query results cache at /tmp/hive/_resultscache_2019-12-11T00:16:02,569 DEBUG [22d96536-983a-417d-b10c-aa1766ba0994 main] exec.Utilities: HDFS dir: /tmp/hive/_resultscache_ with schema null, permission: rwxr-xr-x2019-12-11T00:16:02,571 ERROR [22d96536-983a-417d-b10c-aa1766ba0994 main] ql.Driver: FAILED: IllegalStateException java.lang.RuntimeException: The dir: /tmp/hive/_resultscache_ on HDFS should be writable. Current permissions are: rwxr-xr-xjava.lang.IllegalStateException: java.lang.RuntimeException: The dir: /tmp/hive/_resultscache_ on HDFS should be writable. Current permissions are: rwxr-xr-x at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkResultsCache(SemanticAnalyzer.java:15230) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12467) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:359) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285) at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:171) at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:285)exception#22019-12-11T00:15:06,156 ERROR [5f435468-f02b-4587-ae48-517777bfdbed main] ql.Driver: FAILED: IllegalStateException java.lang.RuntimeException: The dir: /tmp/hive/_resultscache_ on HDFS should be writable. Current permissions are: rwxr-xr-xjava.lang.IllegalStateException: java.lang.RuntimeException: The dir: /tmp/hive/_resultscache_ on HDFS should be writable. Current permissions are: rwxr-xr-x at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.checkResultsCache(SemanticAnalyzer.java:15230) at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:12467) at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:359)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="22625" opendate="2019-12-11 00:00:00" fixdate="2019-12-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Syntax Error in findPotentialCompactions SQL query for MySql/Postgres</summary>
      <description>ERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '=&gt; current_timestamp - interval '254' second'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22627" opendate="2019-12-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add schema changes introduced in HIVE-21443 to the schema upgrade scripts</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="22630" opendate="2019-12-11 00:00:00" fixdate="2019-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not retrieve Materialized View definitions for rebuild if query is test SQL</summary>
      <description>for the query like select 1, select current_timestamp, select current_datehive retrieve all the Materialized view from metastore, if the one of databases are too large then this call take lots of time, the situation becomes worse if there are too frequent if hive server receives frequent "select 1" query ( connection pool uses it to check if the connection is valid or not).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22631" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid deep copying partition list in listPartitionsByExpr</summary>
      <description>This is an expensive call, I am not sure why deepCopy is required.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22632" opendate="2019-12-12 00:00:00" fixdate="2019-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve estimateRowSizeFromSchema</summary>
      <description>estimateRowSizeFromSchema un-necessarily iterate and do look-up. This could be avoided.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22680" opendate="2019-12-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in druid-handler Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.security.DruidKerberosUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22681" opendate="2019-12-29 00:00:00" fixdate="2019-6-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Replace Base64 in hcatalog-webhcat Package</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.repl.ReplicationUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22940" opendate="2020-2-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make the datasketches functions available as predefined functions</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2304" opendate="2011-7-25 00:00:00" fixdate="2011-6-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support PreparedStatement.setObject</summary>
      <description>PreparedStatement.setObject is important for spring's jdbcTemplate support</description>
      <version>0.7.1</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HivePreparedStatement.java</file>
    </fixedFiles>
  </bug>
  <bug id="23040" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing for repl dump incremental phase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.DumpMetaData.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplAck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestScheduledReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTablesMetaDataOnly.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="23042" opendate="2020-3-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Merge queries to a single one for updating MIN_OPEN_TXNS table</summary>
      <description>When opening a new transaction we issue 2 queries to update the MIN_OPEN_TXN table.&lt;SELECT MIN("TXN_ID") FROM "TXNS" WHERE "TXN_STATE" = 'o'&gt;&lt;insert into "MIN_HISTORY_LEVEL" ("MHL_TXNID", "MHL_MIN_OPEN_TXNID") values(763, 763)&gt;This could be archived with a single query faster, if we do not open transactions in batch, like:&lt;INSERT INTO "MIN_HISTORY_LEVEL" ("MHL_TXNID", "MHL_MIN_OPEN_TXNID") SELECT ?, MIN("TXN_ID") FROM "TXNS" WHERE "TXN_STATE" = 'o'&gt;</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.tools-common.src.main.java.org.apache.hadoop.hive.metastore.tools.HMSClient.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.src.main.java.org.apache.hadoop.hive.metastore.tools.HMSBenchmarks.java</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.src.main.java.org.apache.hadoop.hive.metastore.tools.BenchmarkTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2307" opendate="2011-7-26 00:00:00" fixdate="2011-7-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Schema creation scripts for PostgreSQL use bit(1) instead of boolean</summary>
      <description>The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.hive&gt; create table test (id int); FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object "org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7" using statement "INSERT INTO "SDS" ("SD_ID","INPUT_FORMAT","OUTPUT_FORMAT","LOCATION","SERDE_ID","NUM_BUCKETS","IS_COMPRESSED") VALUES (?,?,?,?,?,?,?)" failed : ERROR: column "IS_COMPRESSED" is of type bit but expression is of type boolean</description>
      <version>0.5.0,0.6.0,0.7.0,0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.6.0-to-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.7.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.5.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.1.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.4.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.3.0.postgres.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23073" opendate="2020-3-25 00:00:00" fixdate="2020-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Shade netty and upgrade to netty 4.1.48.Final</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23096" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Review Code Path for getResults</summary>
      <description>Log when a buffer returned to a client is completely full. This can be helpful for identifying misconfigured clients that have a fetchSize that is too small See below code snippet Rows are loaded into an ArrayList called "convey." This ArrayList is instantiated with the default size (10) and is expanded on-demand as it is filled. I changed it so that this ArrayList's size is set once, on the first call to fetch, so that it doesn't have to "grow" and waste cycles and memory with GC thrashing for that. Some minor cleanup try { ss = Utilities.readColumn(driverContext.getResStream(), bos); if (bos.getLength() &gt; 0) { // use JDK StandardCharsets row = new String(bos.getData(), 0, bos.getLength(), "UTF-8"); } else if (ss == Utilities.StreamStatus.TERMINATED) { // Do not create a new string for this. Just use a hard-coded empty string. JDK: "Note that use of this constructor is unnecessary since Strings are immutable." row = new String(); } ...}</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="23097" opendate="2020-3-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: LLAPServiceDriver is wrongly using fixed llap- prefix for tarball name</summary>
      <description>LLAP: LLAPServiceDriver is wrongly using fixed llap- prefix for tarball nameMissed modifying this change too in the JiraÂ https://issues.apache.org/jira/browse/HIVE-22937Â </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.service.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="23103" opendate="2020-3-30 00:00:00" fixdate="2020-4-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Oracle statement batching</summary>
      <description>Examine how to really get better performance for oracle statement batches.Oracle JDBC doc describes:The Oracle implementation of standard update batching does not implement true batching for generic statements and callable statements. Even though Oracle JDBC supports the use of standard batching forÂ StatementÂ andÂ CallableStatementÂ objects, you are unlikely to see performance improvement.I would look for connection properties to set, so it is handled anyway, or if not, then use:begin query1; query2; query3;end;to we will have only a single roundtrip for the db.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2315" opendate="2011-7-27 00:00:00" fixdate="2011-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DatabaseMetadata.getColumns() does not return partition column names for a table</summary>
      <description>getColumns() method of DatabaseMetadata for HIVE JDBC Driver does not return the partition column names. Where as from HIVE CLI, if you do a 'describe tablename' you get all columns including the partition columns. It would be nice if getColumns() method returns all columns.</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hadoop.hive.jdbc.HiveDatabaseMetaData.java</file>
    </fixedFiles>
  </bug>
  <bug id="23151" opendate="2020-4-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: default hive.llap.file.cleanup.delay.seconds=0s</summary>
      <description>The current default value (300s) reflects more a debugging scenario, let's set this to 0s in order to make shuffle local files be cleaned up immediately after dag complete.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23201" opendate="2020-4-14 00:00:00" fixdate="2020-4-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve logging in locking</summary>
      <description>Currently it can be quite difficult to troubleshoot issues related to locking. To understand why a particular txn gave up after a while on acquiring a lock, you have to connect directly to the backend DB, since we are not logging right now which exact locks the txn is waiting for.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
    </fixedFiles>
  </bug>
  <bug id="23209" opendate="2020-4-15 00:00:00" fixdate="2020-4-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ptest2 compilation failure after HIVE-21603 - upgrade mockito-core in testutils/ptest2</summary>
      <description>[ERROR] COMPILATION ERROR : [INFO] -------------------------------------------------------------[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/context/TestCloudExecutionContextProvider.java:[21,26] cannot find symbol symbol: class ArgumentMatchers location: package org.mockito[ERROR] /home/jenkins/jenkins-slave/workspace/PreCommit-HIVE-Build/hive/build/hive/testutils/ptest2/src/test/java/org/apache/hive/ptest/execution/context/TestCloudExecutionContextProvider.java:[21,1] static import only from classes and interfacesreprocd testutils/ptest2mvn clean install -DskipTests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23252" opendate="2020-4-20 00:00:00" fixdate="2020-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change spark related tests to be optional</summary>
      <description>HIVE-23137 have disabled the execution of some spark related tests; but they would be still considered by a plain maven command - and the spark artifacts are (unneccessarily) still downloaded</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">itests.pom.xml</file>
      <file type="M">itests.hive-unit.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23280" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Trigger compaction with old aborted txns</summary>
      <description>When a txn is aborted and the compaction threshold for number of aborted txns is not reached then the aborted transaction can remain forever in the RDBMS database. This could result in several serious performance degradations: getOpenTxns has to list this aborted txn forever TXN_TO_WRITE_ID table is not cleanedWe should add a threshold, so after a given time the compaction is started anyway.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionInfoStruct.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestInitiator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23283" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Generate random temp ID for lock enqueue and commitTxn</summary>
      <description>In order to optimize the S4U scope of enqueue lock and commitTxn, currently a hardcoded constant (-1) is used to first insert all the lock and ws entries with a temporary lockID/commitID. However, in a concurrent environment this seems to cause some performance degradation (and deadlock issues with some rdbms) as multiple concurrent transactions are trying to insert rows with the same primary key (e.g. (-1, 1), (-1, 2), (-1, 3), .. etc. for (extID/intID) in HIVE_LOCKS). The proposed solution is to replace the constant with a random generated negative number, which seems to resolve this issue.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23284" opendate="2020-4-23 00:00:00" fixdate="2020-5-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove dependency on mariadb-java-client</summary>
      <description>It hasÂ GNU Lesser General Public License which is Category X.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.Mysql.java</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.DEV-README</file>
    </fixedFiles>
  </bug>
  <bug id="23287" opendate="2020-4-23 00:00:00" fixdate="2020-4-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on icu4j</summary>
      <description>Brought in transitively via druid.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23293" opendate="2020-4-24 00:00:00" fixdate="2020-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Locks: Implement zero-wait readers</summary>
      <description>With a new lock type (EXCL_WRITE) for INSERT_OVERWRITE, SHARED_READ does not have to wait for any lock - it can fails fast for a pending EXCLUSIVE, because even if there is an EXCL_WRITE or SHARED_WRITE pending, there's no semantic reason to wait for them.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.LockRequestBuilder.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">ql.src.test.results.clientnegative.lockneg.try.drop.locked.db.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.insert.into1.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="23344" opendate="2020-4-30 00:00:00" fixdate="2020-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bump scala version to 2.12.4, spark to 2.4.5</summary>
      <description>And bump up spark version, as 2.3.3 is not compatible with scala 2.12</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23482" opendate="2020-5-16 00:00:00" fixdate="2020-6-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use junit5 to execute tests</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.tools-common.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">service-rpc.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">jdbc-handler.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2398" opendate="2011-8-19 00:00:00" fixdate="2011-9-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive server doesn&amp;#39;t return schema for &amp;#39;set&amp;#39; command</summary>
      <description>The Hive server does process the CLI commands like 'set', 'set -v' sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.</description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hadoop.hive.service.HiveServer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.SetProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorResponse.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2445" opendate="2011-9-14 00:00:00" fixdate="2011-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>The PerfLogger should log the full name of hooks, not just the simple name.</summary>
      <description>Sometimes the simple name of a hook is not enough to identify it, so the PerfLogger should log the full name instead.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="2448" opendate="2011-9-15 00:00:00" fixdate="2011-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade JavaEWAH to 0.3</summary>
      <description>It contains performance improvements and should be a drop-in replacement.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">lib.javaewah-0.2.jar</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2451" opendate="2011-9-15 00:00:00" fixdate="2011-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TABLESAMBLE(BUCKET xxx) sometimes doesn&amp;#39;t trigger input pruning as regression of HIVE-1538</summary>
      <description>Example:select count(1) from &lt;bucket_table&gt; TABLESAMPLE(BUCKET xxx out of yyy) where &lt;partition_column&gt; = 'xxx'will not trigger input pruning.The reason is that we assume sample filtering operator only happens as the second filter after table scan, which is broken by HIVE-1538, even if the feature doesn't turn on.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.sample6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.sample2.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.sample9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample10.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SamplePruner.java</file>
    </fixedFiles>
  </bug>
  <bug id="2453" opendate="2011-9-16 00:00:00" fixdate="2011-9-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Need a way to categorize queries in hooks for improved logging</summary>
      <description>We need a way to categorize queries, such as whether or not the include a join clause, a group by clause, etc., in the hooks. This will allow for better performance logging.Currently the only way I can find is to go through the operators in the tasks, but which operators are used for the different types of queries may change over time.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.QueryPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2455" opendate="2011-9-18 00:00:00" fixdate="2011-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Pass correct remoteAddress in proxy user authentication</summary>
      <description></description>
      <version>0.7.1,0.8.0</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2457" opendate="2011-9-20 00:00:00" fixdate="2011-9-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Files in Avro-backed Hive tables do not have a ".avro" extension</summary>
      <description>When using the Avro SerDe (see HIVE-895, https://github.com/jghoman/haivvreo) the files created for an Avro table do not have a ".avro" extension, which causes problems for tools like Avro MapReduce or Sqoop which expect the extension.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="246" opendate="2009-1-23 00:00:00" fixdate="2009-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Msck incorrectly finds tables not in ms</summary>
      <description>The msck command looks for tables on HDFS that are missing in the Metastore. It tries to find these tables in the same dir as already existing tables. Due to a bug it also does this for external tables, causing it to find unwanted results.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHiveMetaStoreChecker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveMetaStoreChecker.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2466" opendate="2011-9-23 00:00:00" fixdate="2011-11-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>mapjoin_subquery dump small table (mapjoin table) to the same file</summary>
      <description>in mapjoin_subquery.q there is a queryï¼SELECT /*+ MAPJOIN(z) */ subq.key1, z.valueFROM(SELECT /*+ MAPJOIN */ x.key as key1, x.value as value1, y.key as key2, y.value as value2 FROM src1 x JOIN src y ON (x.key = y.key)) subq JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);when dump x and z to a local file,there all dump to the same file, so we lost the data of x</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HashTableSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.GenMRSkewJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2487" opendate="2011-10-5 00:00:00" fixdate="2011-10-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bug from HIVE-2446, the code that calls client stats publishers run() methods is in wrong place, should be in the same method but inside of while (!rj.isComplete()) {} loop</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.HadoopJobExecHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="2488" opendate="2011-10-6 00:00:00" fixdate="2011-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PDK tests failing on Hudson because HADOOP_HOME is not defined</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pdk.scripts.build-plugin.xml</file>
      <file type="M">pdk.build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2497" opendate="2011-10-12 00:00:00" fixdate="2011-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition pruning prune some right partition under specific conditions</summary>
      <description>create table src3(key string, value string) partitioned by (pt string)row format delimited fields terminated by ',';ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110911000000') ;ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110912000000') ;ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110913000000') ;explain extendedselect user_id from ( select cast(key as int) as user_id ,case when (value like 'aaa%' or value like 'vvv%') then 1 else 0 end as tag_student from src3 ) subwhere sub.tag_student &gt; 0;STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Map Reduce Alias -&gt; Map Operator Tree: sub:src3 TableScan alias: src3 Filter Operator isSamplingPred: false predicate: expr: (CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END &gt; 0) type: boolean Select Operator expressions: expr: UDFToInteger(key) type: int expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END type: int outputColumnNames: _col0, _col1 Filter Operator isSamplingPred: false predicate: expr: (_col1 &gt; 0) type: boolean Select Operator expressions: expr: _col0 type: int outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-26-12_894_9085644225727185586/-ext-10001 NumFilesPerFileSink: 1 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: columns _col0 columns.types int serialization.format 1 TotalFiles: 1 MultiFileSpray: false Needs Tagging: false Stage: Stage-0 Fetch Operator limit: -1if we set hive.optimize.ppd=false;STAGE DEPENDENCIES: Stage-1 is a root stage Stage-0 is a root stageSTAGE PLANS: Stage: Stage-1 Map Reduce Alias -&gt; Map Operator Tree: sub:src3 TableScan alias: src3 Select Operator expressions: expr: UDFToInteger(key) type: int expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END type: int outputColumnNames: _col0, _col1 Filter Operator isSamplingPred: false predicate: expr: (_col1 &gt; 0) type: boolean Select Operator expressions: expr: _col0 type: int outputColumnNames: _col0 File Output Operator compressed: false GlobalTableId: 0 directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-27-22_527_1729287213481398480/-ext-10001 NumFilesPerFileSink: 1 table: input format: org.apache.hadoop.mapred.TextInputFormat output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat properties: columns _col0 columns.types int serialization.format 1 TotalFiles: 1 MultiFileSpray: false Needs Tagging: false Path -&gt; Alias: hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 &amp;#91;sub:src3&amp;#93; hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110912000000 &amp;#91;sub:src3&amp;#93; hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110913000000 &amp;#91;sub:src3&amp;#93; Path -&gt; Partition: hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 Partition base file name: pt=20110911000000</description>
      <version>0.7.1</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.ExprProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="250" opendate="2009-1-25 00:00:00" fixdate="2009-4-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>shared memory java dbm for map-side joins</summary>
      <description>can use either: sdbm: http://freshmeat.net/projects/solingerjavasdbm/ jdbm: http://sourceforge.net/projects/jdbm/both need modifications to use file mmaps instead of regular file io. will do some testing to see if there's a major difference between the two.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2500" opendate="2011-10-12 00:00:00" fixdate="2011-10-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow Hive to be debugged remotely</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
      <file type="M">conf.hive-env.sh.template</file>
      <file type="M">bin.hive</file>
      <file type="M">bin.ext.help.sh</file>
    </fixedFiles>
  </bug>
  <bug id="2515" opendate="2011-10-19 00:00:00" fixdate="2011-10-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Constant OIs work with UDTFs.</summary>
      <description>UDTFs are the last shoe to drop for constant OIs.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnInfo.java</file>
    </fixedFiles>
  </bug>
  <bug id="254" opendate="2009-1-28 00:00:00" fixdate="2009-4-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Tracker page should provide a link to the "next step"</summary>
      <description>This may belong in Hadoop Core, or maybe more about Hive integration with Hadoop, but many queries these days take multiple steps or mapreduce phases. It would be lovely if a "finished" mapreduce from a Hive query that has several steps could provide a link to the tracker page for the next mapreduce step.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Wish</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hwi.web.session.manage.jsp</file>
      <file type="M">hwi.web.session.history.jsp</file>
      <file type="M">hwi.src.java.org.apache.hadoop.hive.hwi.HWISessionItem.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2543" opendate="2011-11-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compact index table&amp;#39;s files merged in creation</summary>
      <description>When a compact index is built there is the possibility of a merge task at the end of the task tree. If this happens, the index table's files will no longer be sorted.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.AggregateIndexHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2585" opendate="2011-11-17 00:00:00" fixdate="2011-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collapse hive.metastore.uris and hive.metastore.local</summary>
      <description>We should just have hive.metastore.uris. If it is empty, we shall assume local mode, if non-empty we shall use that string to connect to remote metastore. Having two different keys for same information is confusing.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStoreIpAddress.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestRemoteHiveMetaStore.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreAuthorization.java</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMarkPartitionRemote.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2735" opendate="2012-1-22 00:00:00" fixdate="2012-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2012-1-25 00:00:00" fixdate="2012-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2771" opendate="2012-2-1 00:00:00" fixdate="2012-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filter pushdown for key ranges in hbase for keys of type string</summary>
      <description>This is a subtask of HIVE-1643</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2012-2-22 00:00:00" fixdate="2012-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter pushdown in hbase for keys stored in binary format</summary>
      <description>This patch enables filter pushdown for keys stored in binary format in hbase</description>
      <version>0.6.0,0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="283" opendate="2009-2-9 00:00:00" fixdate="2009-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>partition pruning not happening for uppercase table aliases</summary>
      <description>With upper case aliases partition pruning breaks.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.PartitionPruner.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2831" opendate="2012-2-28 00:00:00" fixdate="2012-3-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestContribCliDriver.dboutput and TestCliDriver.input45 fail on 0.23</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="2833" opendate="2012-3-2 00:00:00" fixdate="2012-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test failures caused by HIVE-2716</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2835" opendate="2012-3-2 00:00:00" fixdate="2012-3-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Change default configuration for hive.exec.dynamic.partition</summary>
      <description>I think we should enable dynamic partitions by default.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2837" opendate="2012-3-3 00:00:00" fixdate="2012-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert into external tables should not be allowed</summary>
      <description>This is a very risky thing to allow. Since, the external tables can point to any user location, which can potentially corrupt some other tables.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2838" opendate="2012-3-4 00:00:00" fixdate="2012-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>cleanup readentity/writeentity</summary>
      <description>Ideally, there should be one common entity instead of readentity/writeentity.Unfortunately, that would be a backward incompatible change since users os hive might have writtenthere own hooks, where they are using readentity/writeentity.We should atleast create a common class, and then we can deprecate read/write entity later, for a new release.For now, I propose to make a backward compatible change.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.WriteEntity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.ReadEntity.java</file>
    </fixedFiles>
  </bug>
  <bug id="2874" opendate="2012-3-15 00:00:00" fixdate="2012-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming external partition changes location</summary>
      <description>Renaming an external partition will change the location of that partition to the default location of a managed partition with the same name.E.g. If ex_table is external and has partition part=1 with location /.../managed_table/part=1Calling ALTER TABLE ex_table PARTITION (part = '1') RENAME TO PARTITION (part = '2');Will change the location of the partition to /.../ex_table/part=2</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2875" opendate="2012-3-15 00:00:00" fixdate="2012-3-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Renaming partition changes partition location prefix</summary>
      <description>Renaming a partition changes the location of the partition to the default location of the table, followed by the partition specification. It should just change the partition specification of the path.If the path does not end with the old partition specification, we should probably throw an exception because renaming a partition should not change the path so dramatically, and not changing the path to reflect the new partition name could leave the partition in a very confusing state.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="3066" opendate="2012-5-30 00:00:00" fixdate="2012-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the option -database DATABASE in hive cli to specify a default database to use for the cli session.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3068" opendate="2012-5-31 00:00:00" fixdate="2012-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to export table metadata as JSON on table drop</summary>
      <description>When a table is dropped, the contents go to the users trash but the metadata is lost. It would be super neat to be able to save the metadata as well so that tables could be trivially re-instantiated via thrift.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3086" opendate="2012-6-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skewed Join Optimization</summary>
      <description>During a join operation, if one of the columns has a skewed key, it can cause that particular reducer to become the bottleneck. The following feature will address it:https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3387" opendate="2012-8-15 00:00:00" fixdate="2012-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>meta data file size exceeds limit</summary>
      <description>The cause is certainly that we use an array list instead of a set structure in the split locations API. Looks like a bug in Hive's CombineFileInputFormat.Reproduce:Set mapreduce.jobtracker.split.metainfo.maxsize=100000000 when submitting the Hive query. Run a big hive query that write data into a partitioned table. Due to the large number of splits, you encounter an exception on the job submitted to Hadoop and the exception said:meta data size exceeds 100000000.</description>
      <version>0.7.1</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
    </fixedFiles>
  </bug>
  <bug id="3388" opendate="2012-8-15 00:00:00" fixdate="2012-9-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Improve Performance of UDF PERCENTILE_APPROX()</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.NumericHistogram.java</file>
    </fixedFiles>
  </bug>
  <bug id="339" opendate="2009-3-11 00:00:00" fixdate="2009-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[Hive] problem in count distinct in 1mapreduce job with map side aggregation</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.union.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby6.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby5.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby4.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby3.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby2.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.groupby1.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.groupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3409" opendate="2012-8-27 00:00:00" fixdate="2012-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Increase test.junit.timeout value</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="3654" opendate="2012-11-2 00:00:00" fixdate="2012-11-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>block relative path access in hive</summary>
      <description>metadata only optimizer uses relative paths</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.MetadataOnlyOptimizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3693" opendate="2012-11-9 00:00:00" fixdate="2012-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance regression introduced by HIVE-3483</summary>
      <description>https://issues.apache.org/jira/browse/HIVE-3483 introduced a performance regression in the client side during split computation.The client side spends a lot more time in the split computation phase. The problem is checkFilterPathContains method.While investigating, can you create a config to disable it by default?thanks</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="410" opendate="2009-4-14 00:00:00" fixdate="2009-5-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Heartbeating for streaming jobs should not depend on stdout</summary>
      <description>jobs that require iterative processing may take longer than 10 mins to produce rows. This shouldn't be cause to kill the job. Producing keepalive dummy rows to stdout is bad if the data has to go into a Hive table or other Hive steps.If we adopt the solution of using stderr to indicate heartbeats, can that be combined with streaming counters (http://hadoop.apache.org/core/docs/current/streaming.html#How+do+I+update+counters+in+streaming+applications%3F )? Also, will limitations on size of stderr break this?</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="4531" opendate="2013-5-9 00:00:00" fixdate="2013-9-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>[WebHCat] Collecting task logs to hdfs</summary>
      <description>It would be nice we collect task logs after job finish. This is similar to what Amazon EMR does.</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hcatalog.webhcat.svr.src.test.java.org.apache.hive.hcatalog.templeton.tool.TestTempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonUtils.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.StreamingDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.Server.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.PigDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.LauncherDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.JarDelegator.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HiveDelegator.java</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.pig.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.mapreducestreaming.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.mapreducejar.xml</file>
      <file type="M">hcatalog.src.docs.src.documentation.content.xdocs.hive.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
