<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="2498" opendate="2011-10-12 00:00:00" fixdate="2011-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Group by operator does not estimate size of Timestamp &amp; Binary data correctly</summary>
      <description>It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2519" opendate="2011-10-20 00:00:00" fixdate="2011-10-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema</summary>
      <description>Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:insert overwrite ... partition (p2="...", p1);which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2712" opendate="2012-1-12 00:00:00" fixdate="2012-2-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make ZooKeeper token store ACL configurable</summary>
      <description>ACL needs to be set to secure the token store with ZK 3.4.The patch will also include the review changes from HIVE-2467 that were not committed.</description>
      <version>0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestHadoop20SAuthBridge.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2735" opendate="2012-1-22 00:00:00" fixdate="2012-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PlanUtils.configureTableJobPropertiesForStorageHandler() is not called for partitioned table</summary>
      <description>As a result, if there is a query which results in a MR job which needs to be configured via storage handler, it returns in failure.</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1,0.9.0</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2748" opendate="2012-1-25 00:00:00" fixdate="2012-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Hbase and ZK dependcies</summary>
      <description>Both softwares have moved forward with significant improvements. Lets bump compile time dependency to keep up</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.test.org.apache.hadoop.hive.thrift.TestZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenStore.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">ivy.ivysettings.xml</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2762" opendate="2012-1-30 00:00:00" fixdate="2012-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Alter Table Partition Concatenate Fails On Certain Characters</summary>
      <description>Alter table partition concatenate creates a Java URI object for the location of a partition. If the partition name contains certain characters, such as } or space ' ', the object constructor fails, causing the query to fail.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2771" opendate="2012-2-1 00:00:00" fixdate="2012-3-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add support for filter pushdown for key ranges in hbase for keys of type string</summary>
      <description>This is a subtask of HIVE-1643</description>
      <version>0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ppd.OpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2792" opendate="2012-2-9 00:00:00" fixdate="2012-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SUBSTR(CAST(&lt;string&gt; AS BINARY)) produces unexpected results</summary>
      <description></description>
      <version>0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.substr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ba.table.udfs.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.substr.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ba.table.udfs.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFSubstr.java</file>
    </fixedFiles>
  </bug>
  <bug id="2794" opendate="2012-2-9 00:00:00" fixdate="2012-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Aggregations without grouping should return NULL when applied to partitioning column of a partitionless table</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.metadataonly1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="2795" opendate="2012-2-9 00:00:00" fixdate="2012-2-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>View partitions do not have a storage descriptor</summary>
      <description>Besides being an inconsistency, it causes errors.Calling describe formatted on a view partition throws an exceptionjava.lang.NullPointerException at org.apache.hadoop.hive.ql.metadata.Partition.getCols(Partition.java:505) at org.apache.hadoop.hive.ql.exec.DDLTask.describeTable(DDLTask.java:2570)because it does not have a column descriptor, which is part of the storage descriptor.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2815" opendate="2012-2-22 00:00:00" fixdate="2012-3-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Filter pushdown in hbase for keys stored in binary format</summary>
      <description>This patch enables filter pushdown for keys stored in binary format in hbase</description>
      <version>0.6.0,0.7.0,0.7.1,0.8.0,0.8.1</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="2853" opendate="2012-3-8 00:00:00" fixdate="2012-4-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add pre event listeners to metastore</summary>
      <description>Currently there are event listeners in the metastore which run after the completion of a method. It would be useful to have similar hooks which run before the metastore method is executed. These can be used to make validating names, locations, etc. customizable.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2965" opendate="2012-4-19 00:00:00" fixdate="2012-4-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2612</summary>
      <description>In 4/19 contrib meeting it was decided to revert HIVE-2612.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.fileformat.text.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.inputddl1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input15.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">contrib.src.test.results.clientnegative.serde.regex.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.fileformat.base64.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.regex.q.out</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
      <file type="M">metastore.scripts.upgrade.derby.009-HIVE-2612.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.9.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.8.0-to-0.9.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.009-HIVE-2612.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.9.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.8.0-to-0.9.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.009-HIVE-2612.postgres.sql</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RegionStorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.StorageDescriptor.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.constants.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MRegionStorageDescriptor.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MStorageDescriptor.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableDesc.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.metadata.TestHive.java</file>
      <file type="M">ql.src.test.results.clientpositive.create.union.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fileformat.sequencefile.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="3009" opendate="2012-5-8 00:00:00" fixdate="2012-1-8 01:00:00" resolution="Unresolved">
    <buginformation>
      <summary>do authorization for all metadata operations</summary>
      <description>Most of the metadata read operations and some write operations are not checking for authorization. See org.apache.hadoop.hive.ql.plan.HiveOperation . Operations such as DESCTABLE and DROPDATABASE have null for required privileges.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.thrift.TUGIContainingTransport.java</file>
    </fixedFiles>
  </bug>
  <bug id="3012" opendate="2012-5-9 00:00:00" fixdate="2012-5-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive custom scripts do not work well if the data contains new lines</summary>
      <description>If the data contain newline, it will be passed as is to the script.The script has no way of splitting the data based on the new line.An option should be added to hive to escape/unescape the new lines.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordWriter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TextRecordReader.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3048" opendate="2012-5-23 00:00:00" fixdate="2012-6-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Collect_set Aggregate does uneccesary check for value.</summary>
      <description>Sets already de-duplicate for free no need for existence check. private void putIntoSet(Object p, MkArrayAggregationBuffer myagg) { if (myagg.container.contains(p)) return; Object pCopy = ObjectInspectorUtils.copyToStandardObject(p, this.inputOI); myagg.container.add(pCopy);</description>
      <version>0.8.1</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="305" opendate="2009-2-24 00:00:00" fixdate="2009-7-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Port Hadoop streaming&amp;#39;s counters/status reporters to Hive Transforms</summary>
      <description>https://issues.apache.org/jira/browse/HADOOP-1328" Introduced a way for a streaming process to update global counters and status using stderr stream to emit information. Use "reporter:counter:&lt;group&gt;,&lt;counter&gt;,&lt;amount&gt; " to update a counter. Use "reporter:status:&lt;message&gt;" to update status. "</description>
      <version>None</version>
      <fixedVersion>0.12.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3058" opendate="2012-5-26 00:00:00" fixdate="2012-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.transform.escape.input breaks tab delimited data</summary>
      <description>With the hive.transform.escape.input set, all tabs going into a script, including those used to separate columns are escaped.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.newline.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.newline.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.HiveUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="3059" opendate="2012-5-28 00:00:00" fixdate="2012-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>revert HIVE-2703</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">jdbc.src.test.org.apache.hadoop.hive.jdbc.TestJdbcDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="306" opendate="2009-2-25 00:00:00" fixdate="2009-7-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support "INSERT [INTO] destination"</summary>
      <description>Currently hive only supports "INSERT OVERWRITE destination". We should support "INSERT &amp;#91;INTO&amp;#93; destination".</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.errors.missing.overwrite.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ErrorMsg.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.StatsTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="3061" opendate="2012-5-28 00:00:00" fixdate="2012-6-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive.binary.record.max.length is a magic string</summary>
      <description></description>
      <version>0.8.1</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.BinaryRecordReader.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3063" opendate="2012-5-30 00:00:00" fixdate="2012-5-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>drop partition for non-string columns is failing</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.1,0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PartitionSpec.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3066" opendate="2012-5-30 00:00:00" fixdate="2012-6-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the option -database DATABASE in hive cli to specify a default database to use for the cli session.</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.OptionsProcessor.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliSessionState.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="3068" opendate="2012-5-31 00:00:00" fixdate="2012-9-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to export table metadata as JSON on table drop</summary>
      <description>When a table is dropped, the contents go to the users trash but the metadata is lost. It would be super neat to be able to save the metadata as well so that tables could be trivially re-instantiated via thrift.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3079" opendate="2012-6-1 00:00:00" fixdate="2012-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Revert HIVE-2989</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.tablelink.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.view.failure1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.drop.table.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.table.failure5.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.create.tablelink.failure1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.create.tablelink.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.table.failure5.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure2.q</file>
      <file type="M">ql.src.test.queries.clientnegative.create.tablelink.failure1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.HiveOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DropTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.CreateTableLinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.Hive.g</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTable.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.TableType.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.types.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableIdentifier.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Schema.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Index.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.EnvironmentContext.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.types.cpp</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.010-HIVE-2989.mysql.sql</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="3086" opendate="2012-6-5 00:00:00" fixdate="2012-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skewed Join Optimization</summary>
      <description>During a join operation, if one of the columns has a skewed key, it can cause that particular reducer to become the bottleneck. The following feature will address it:https://cwiki.apache.org/confluence/display/Hive/Skewed+Join+Optimization</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.SelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FilterOperator.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="3173" opendate="2012-6-22 00:00:00" fixdate="2012-9-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>implement getTypeInfo database metadata method</summary>
      <description>The JDBC driver does not implement the database metadata method getTypeInfo. Hence, an application cannot dynamically determine the available type information and associated properties.</description>
      <version>0.8.1</version>
      <fixedVersion>2.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="3178" opendate="2012-6-22 00:00:00" fixdate="2012-6-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>retry not honored in RetryingRawMetastore</summary>
      <description>The retrymetastore catches JDOException, but they are always wrapped by reflection.</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RetryingRawStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="3181" opendate="2012-6-23 00:00:00" fixdate="2012-12-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>getDatabaseMajor/Minor version does not return values</summary>
      <description>This is really a sub-issue of HIVE-3174 (which is a lot of properties) but given that the driver will return databaseProductVersion it makes no sense to not have implemented these as well.</description>
      <version>0.8.1</version>
      <fixedVersion>0.13.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.Utils.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDatabaseMetaData.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="3333" opendate="2012-8-3 00:00:00" fixdate="2012-8-3 01:00:00" resolution="Not A Problem">
    <buginformation>
      <summary>Specified SerDe does not get used when executing a query over JSON data</summary>
      <description>I found a JSON SerDe that I wanted to try out, and I ran into some issues attempting to use it. The script I was executing looks like this:ADD JAR /home/natty/hive-test-case/hive-json-serde-0.2.jar;CREATE TABLE bar ( id INT, integers ARRAY&lt;INT&gt;, datum STRING) ROW FORMAT SERDE 'org.apache.hadoop.hive.contrib.serde2.JsonSerde';LOAD DATA LOCAL INPATH '/home/natty/sample_data/json.sample' OVERWRITE INTO TABLE bar;SELECT * FROM bar;The data I loaded in looks like this:{ "id": 1, "integers": [ 1, 2, 3 ], "datum": "hello" },When the "SELECT * FROM bar" query executes, it returns with a failure:hive&gt; ADD JAR /home/natty/hive-test-case/hive-json-serde-0.2.jar;Added /home/natty/hive-test-case/hive-json-serde-0.2.jar to class pathAdded resource: /home/natty/hive-test-case/hive-json-serde-0.2.jarhive&gt; SELECT * FROM bar;OKFailed with exception java.io.IOException:java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object;Time taken: 2.335 secondsNow, this alone doesn't bother me. What bothers me is that, if I look at the log file, I see the following exception:2012-08-03 13:12:11,407 ERROR CliDriver (SessionState.java:printError(380)) - Failed with exception java.io.IOException:java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object;java.io.IOException: java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object; at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:173) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1383) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:266) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:212) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:671) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:554) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.RunJar.main(RunJar.java:208)Caused by: java.lang.ClassCastException: org.json.JSONArray cannot be cast to [Ljava.lang.Object; at org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector.getList(StandardListObjectInspector.java:98) at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:287) at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:213) at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:59) at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:365) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:163) ... 11 moreNote that this exception indicates that Hive is executing code for the DelimitedJSONSerDe, rather than the one that I specified (JsonSerde from the jar file). Seems incorrect.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">LICENSE</file>
    </fixedFiles>
  </bug>
  <bug id="3375" opendate="2012-8-13 00:00:00" fixdate="2012-8-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>bucketed map join should check that the number of files match the number of buckets</summary>
      <description>Currently, we get NPE if that is not the case</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin.10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.smb.mapjoin9.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketMapJoinOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3379" opendate="2012-8-14 00:00:00" fixdate="2012-8-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>stats are not being collected correctly for analyze table with dynamic partitions</summary>
      <description>analyze table T partition (ds, hr);does not collect stats correctly</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.stats8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.stats8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.stats10.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="3405" opendate="2012-8-23 00:00:00" fixdate="2012-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>UDF initcap to obtain a string with the first letter of each word in uppercase other letters in lowercase</summary>
      <description>Hive current releases lacks a INITCAP function which returns String with first letter of the word in uppercase.INITCAP returns String, with the first letter of each word in uppercase, all other letters in same case. Words are delimited by white space.This will be useful report generation.</description>
      <version>0.8.1,0.9.0,0.9.1,0.10.0,0.11.0,0.13.0,0.14.0,0.14.1,0.15.0</version>
      <fixedVersion>1.1.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="3465" opendate="2012-9-14 00:00:00" fixdate="2012-9-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>insert into statement overwrites if target table is prefixed with database name</summary>
      <description>insert into statement works as expected when the target table is not prefixed with database name. It does append correctly.However, if I specify a database name prefixing the target table, the statement just overwrites instead of appends:insert into table my_database.target select * from source;</description>
      <version>0.8.1</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.insert1.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QBParseInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3505" opendate="2012-9-24 00:00:00" fixdate="2012-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>log4j template has logging threshold that hides all audit logs</summary>
      <description>With the "template" for log4j configuration provided in the tarball, audit logging is hidden (it's logged as "INFO"). By making the log threshold a parameter, this information remains hidden when using the CLI (which is desired) but can be overridden when starting services to enable audit-logging.(This is primarily so that Hive is more functional out-of-the-box as installed by Apache Bigtop).</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.conf.hive-exec-log4j.properties</file>
      <file type="M">common.src.java.conf.hive-log4j.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3507" opendate="2012-9-25 00:00:00" fixdate="2012-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Some of the tests are not deterministic</summary>
      <description>skewjoinopt* tests are not deterministic</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt9.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt8.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt7.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt6.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt4.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt3.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt20.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt19.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt18.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt17.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt16.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt15.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt14.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt13.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt12.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt11.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt10.q</file>
      <file type="M">ql.src.test.queries.clientpositive.skewjoinopt1.q</file>
    </fixedFiles>
  </bug>
  <bug id="3678" opendate="2012-11-6 00:00:00" fixdate="2012-11-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add metastore upgrade scripts for column stats schema changes</summary>
      <description>Add upgrade script for column statistics schema changes for Postgres/MySQL/Oracle/Derby</description>
      <version>None</version>
      <fixedVersion>0.10.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.model.package.jdo</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">metastore.src.model.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.scripts.upgrade.postgres.upgrade-0.9.0-to-0.10.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.postgres.hive-schema-0.10.0.postgres.sql</file>
      <file type="M">metastore.scripts.upgrade.oracle.hive-schema-0.10.0.oracle.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.upgrade-0.9.0-to-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.mysql.hive-schema-0.10.0.mysql.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.upgrade-0.9.0-to-0.10.0.derby.sql</file>
      <file type="M">metastore.scripts.upgrade.derby.hive-schema-0.10.0.derby.sql</file>
    </fixedFiles>
  </bug>
  <bug id="3682" opendate="2012-11-7 00:00:00" fixdate="2012-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>when output hive table to file,users should could have a separator of their own choice</summary>
      <description>By default,when output hive table to file ,columns of the Hive table are separated by ^A character (that is \001).But indeed users should have the right to set a seperator of their own choice.Usage Example:create table for_test (key string, value string);load data local inpath './in1.txt' into table for_testselect * from for_test;UT-01：default separator is \001 line separator is \ninsert overwrite local directory './test-01' select * from src ;create table array_table (a array&lt;string&gt;, b array&lt;string&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ',';load data local inpath "../hive/examples/files/arraytest.txt" overwrite into table table2;CREATE TABLE map_table (foo STRING , bar MAP&lt;STRING, STRING&gt;)ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'STORED AS TEXTFILE;UT-02：defined field separator as ':'insert overwrite local directory './test-02' row format delimited FIELDS TERMINATED BY ':' select * from src ;UT-03: line separator DO NOT ALLOWED to define as other separator insert overwrite local directory './test-03' row format delimited FIELDS TERMINATED BY ':' select * from src ;UT-04: define map separators insert overwrite local directory './test-04' row format delimited FIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'select * from src;</description>
      <version>0.8.1</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.QB.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
    </fixedFiles>
  </bug>
  <bug id="3861" opendate="2013-1-5 00:00:00" fixdate="2013-4-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade hbase dependency to 0.94</summary>
      <description>Hive tests fail to run against hbase v0.94.2. Proposing to upgrade the dependency and change the test setup to properly work with the newer version.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">hbase-handler.src.test.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">hbase-handler.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="387" opendate="2009-4-6 00:00:00" fixdate="2009-4-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore should not use HADOOPFS variable value directly</summary>
      <description>hadoop uses a uri for the file system that first does a reverse resolution of the fs.default.name ip address section (in case ip address is used). hive uses the un-resolved one. this causes url mismatch exceptions in load command (for example - localhost.localdomain does not match 127.0.0.1).fix is to use the uri given back by the filesystem object.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="3872" opendate="2013-1-9 00:00:00" fixdate="2013-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MAP JOIN for VIEW throws NULL pointer exception error</summary>
      <description>I have created a view as shown below. CREATE VIEW V1 ASselect /*+ MAPJOIN(t1) ,MAPJOIN(t2) */ t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3 from TABLE1 t1 join TABLE t2 on ( t1.f2= t2.f2 and t1.f3 = t2.f3 and t1.f4 = t2.f4 ) group by t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3View get created successfully however when I execute below mentioned SQL or any SQL on the view get NULLPOINTER exception errorhive&gt; select count from V1;FAILED: NullPointerException nullhive&gt;Is there anything wrong with the view creation ?Next I created view without MAPJOIN hints CREATE VIEW V1 ASselect t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3 from TABLE1 t1 join TABLE t2 on ( t1.f2= t2.f2 and t1.f3 = t2.f3 and t1.f4 = t2.f4 ) group by t1.f1, t1.f2, t1.f3, t1.f4, t2.f1, t2.f2, t2.f3Before executing select SQL I excute set hive.auto.convert.join=true; I am getting beloow mentioned warningsjava.lang.InstantiationException: org.apache.hadoop.hive.ql.parse.ASTNodeOriginContinuing ...java.lang.RuntimeException: failed to evaluate: &lt;unbound&gt;=Class.new();Continuing ...And I see from log that total 5 mapreduce jobs are started however when don't set auto.convert.join to true, I see only 3 mapreduce jobs getting invoked.Total MapReduce jobs = 5Ended Job = 1116112419, job is filtered out (removed at runtime).Ended Job = -33256989, job is filtered out (removed at runtime).WARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ErrorMsg.java</file>
    </fixedFiles>
  </bug>
  <bug id="3874" opendate="2013-1-9 00:00:00" fixdate="2013-3-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Create a new Optimized Row Columnar file format for Hive</summary>
      <description>There are several limitations of the current RC File format that I'd like to address by creating a new format: each column value is stored as a binary blob, which means: the entire column value must be read, decompressed, and deserialized the file format can't use smarter type-specific compression push down filters can't be evaluated the start of each row group needs to be found by scanning user metadata can only be added to the file when the file is created the file doesn't store the number of rows per a file or row group there is no mechanism for seeking to a particular row number, which is required for external indexes. there is no mechanism for storing light weight indexes within the file to enable push-down filters to skip entire row groups. the type of the rows aren't stored in the file</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.ivy.xml</file>
      <file type="M">ql.build.xml</file>
      <file type="M">ivy.libraries.properties</file>
      <file type="M">build.xml</file>
      <file type="M">build.properties</file>
    </fixedFiles>
  </bug>
  <bug id="3907" opendate="2013-1-16 00:00:00" fixdate="2013-5-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive should support adding multiple resources at once</summary>
      <description>Currently hive adds resources in one by one manner. And for JAR resources, one classloader is created for each jar file, which seemed not good idea.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.udf.nonexistent.resource.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.session.TestSessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="3937" opendate="2013-1-24 00:00:00" fixdate="2013-2-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Profiler</summary>
      <description>Adding a Hive Profiler implementation which tracks inclusive wall times and call counts of the operators</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="4537" opendate="2013-5-10 00:00:00" fixdate="2013-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select * fails on orc table when vectorization is enabled</summary>
      <description>hive&gt; select * from intdataorc;OKFailed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: Error evaluating cint0Time taken: 0.213 seconds</description>
      <version>None</version>
      <fixedVersion>vectorization-branch,0.13.0</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="473" opendate="2009-5-6 00:00:00" fixdate="2009-7-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Clean up after tests</summary>
      <description>The test suite creates a lot of temporary files that aren't cleaned up. For example plan xml files, mapred/local and mapred/system files.</description>
      <version>None</version>
      <fixedVersion>0.4.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="7634" opendate="2014-8-6 00:00:00" fixdate="2014-8-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use Configuration.getPassword() if available to eliminate passwords from hive-site.xml</summary>
      <description>HADOOP-10607 provides a Configuration.getPassword() API that allows passwords to be retrieved from a configured credential provider, while also being able to fall back to the HiveConf setting if no provider is set up. Hive should use this API for versions of Hadoop that support this API. This would give users the ability to remove the passwords from their Hive configuration files.</description>
      <version>None</version>
      <fixedVersion>0.14.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.0.20.src.main.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.HiveSchemaTool.java</file>
      <file type="M">beeline.pom.xml</file>
    </fixedFiles>
  </bug>
</bugrepository>
