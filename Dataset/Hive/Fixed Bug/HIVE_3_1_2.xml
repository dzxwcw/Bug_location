<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="16353" opendate="2017-4-3 00:00:00" fixdate="2017-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Jetty 9 upgrade breaks hive master LLAP</summary>
      <description>HIVE-16049 upgraded to jetty 9. It is committed to apache master which is still 2.3.0-snapshot. This breaks couple of other components like LLAP and ends up throwing the following error during runtime.2017-04-02T20:17:45,435 WARN [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exceptionjava.lang.NoClassDefFoundError: org/eclipse/jetty/rewrite/handler/Ruleat org.apache.hive.http.HttpServer$Builder.build(HttpServer.java:125) ~[hive-exec-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]at org.apache.hadoop.hive.llap.daemon.services.impl.LlapWebServices.serviceInit(LlapWebServices.java:102) ~[hive-llap-server-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) ~[hadoop-common-2.7.3.2.5.0.0-1245.jar:?]at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107) ~[hadoop-common-2.7.3.2.5.0.0-1245.jar:?]at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.serviceInit(LlapDaemon.java:385) ~[hive-llap-server-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163) ~[hadoop-common-2.7.3.2.5.0.0-1245.jar:?]at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:528) [hive-llap-server-2.3.0-SNAPSHOT.jar:2.3.0-SNAPSHOT]Caused by: java.lang.ClassNotFoundException: org.eclipse.jetty.rewrite.handler.Ruleat java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_77]at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_77]at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_77]at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_77]... 7 more2017-04-02T20:17:45,441 INFO [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: LlapDaemon shutdown invoked</description>
      <version>None</version>
      <fixedVersion>3.0.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapServiceDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21753" opendate="2019-5-20 00:00:00" fixdate="2019-6-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update HiveMetastore authorization to enable use of HiveAuthorizer implementation</summary>
      <description>Currently HMS supports authorization using StorageBasedAuthorizationProvider which relies on permissions at filesystem – like HDFS. Hive supports a pluggable authorization interface, and multiple authorizer implementations (like SQLStd, Ranger, Sentry) are available to authorizer access in Hive. Extending HiveMetastore to use the same authorization interface as Hive will enable use of pluggable authorization implementations; and will result in consistent authorization across Hive, HMS and other services that use HMS (like Spark).</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.fallback.FallbackHiveAuthorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21829" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveMetaStore authorization issue with AlterTable and DropTable events</summary>
      <description>With HIVE-21753, we have HiveMetastore authorizer which uses HiveAuthorizer interface to authorizer metastore events.This jira is to fix a bug in HIVE-21753 which failed to authorizer Alter and DropTable events</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.TestHiveMetaStoreAuthorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.HiveMetaStoreAuthorizer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22162" opendate="2019-8-30 00:00:00" fixdate="2019-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MVs are not using ACID tables by default</summary>
      <description>SET hive.support.concurrency=true;SET hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;SET metastore.strict.managed.tables=true;SET hive.default.fileformat=textfile;SET hive.default.fileformat.managed=orc;SET metastore.create.as.acid=true;CREATE TABLE cmv_basetable_n4 (a int, b varchar(256), c decimal(10,2));INSERT INTO cmv_basetable_n4 VALUES (1, 'alfred', 10.30),(2, 'bob', 3.14),(2, 'bonnie', 172342.2),(3, 'calvin', 978.76),(3, 'charlie', 9.8);CREATE MATERIALIZED VIEW cmv_mat_view_n4 disable rewriteAS SELECT a, b, c FROM cmv_basetable_n4;DESCRIBE FORMATTED cmv_mat_view_n4;POSTHOOK: query: DESCRIBE FORMATTED cmv_mat_view_n4...Table Type: MATERIALIZED_VIEW Table Parameters: COLUMN_STATS_ACCURATE {\"BASIC_STATS\":\"true\",\"COLUMN_STATS\":{\"a\":\"true\",\"b\":\"true\",\"c\":\"true\"}} bucketing_version 2 numFiles 1 numRows 5 rawDataSize 1025 totalSize 509 Missing table parametertransaction = truecc.: ashutoshc, gopalv, jcamachorodriguez</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22163" opendate="2019-8-30 00:00:00" fixdate="2019-9-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO: Enabling CBO turns on stats estimation, even when the estimation is disabled</summary>
      <description>create table claims(claim_rec_id bigint, claim_invoice_num string, typ_c int);alter table claims update statistics set ('numRows'='1154941534','rawDataSize'='1135307527922');set hive.stats.estimate=false;explain extended select count(1) from claims where typ_c=3;set hive.stats.ndv.estimate.percent=5e-7;explain extended select count(1) from claims where typ_c=3;Expecting the standard /2 for the single filter, but we instead get 5 rows.' Map Operator Tree:'' TableScan'' alias: claims'' filterExpr: (typ_c = 3) (type: boolean)'' Statistics: Num rows: 1154941534 Data size: 4388777832 Basic stats: COMPLETE Column stats: NONE'' GatherStats: false'' Filter Operator'' isSamplingPred: false'' predicate: (typ_c = 3) (type: boolean)'' Statistics: Num rows: 5 Data size: 19 Basic stats: COMPLETE Column stats: NONE'The estimation is in effect, as changing the estimate.percent changes this.' filterExpr: (typ_c = 3) (type: boolean)'' Statistics: Num rows: 1154941534 Data size: 4388777832 Basic stats: COMPLETE Column stats: NONE'' GatherStats: false'' Filter Operator'' isSamplingPred: false'' predicate: (typ_c = 3) (type: boolean)'' Statistics: Num rows: 230988307 Data size: 877755567 Basic stats: COMPLETE Column stats: NONE'</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.join.reordering.no.stats.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="22170" opendate="2019-9-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="22195" opendate="2019-9-11 00:00:00" fixdate="2019-9-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Configure authentication type for Zookeeper when different from the default cluster wide</summary>
      <description>This could be useful in case when cluster is kerberized, but Zookeeper is not.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HS2ActivePassiveHARegistryClient.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZkRegistryBase.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="2226" opendate="2011-6-16 00:00:00" fixdate="2011-7-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add API to retrieve table names by an arbitrary filter, e.g., by owner, retention, parameters, etc.</summary>
      <description>Create a function called get_table_names_by_filter that returns a list of table names in a database that match a certain filter. The filter should operate similar to the one HIVE-1609. Initially, you should be able to prune the table list based on owner, retention, or table parameter key/values. The filtering should take place at the JDO level for efficiency/speed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.Filter.g</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.parser.ExpressionTree.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.hive.metastore.constants.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Constants.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.hive.metastore.constants.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="22261" opendate="2019-9-27 00:00:00" fixdate="2019-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support for materialized view rewriting with window functions</summary>
      <description>Materialized views don't support window functions.  At a minimum, we should print a friendlier message when the rewrite fails (it can still be created with a "disable rewrite")Script is attached </description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query67.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.perf.mv.query67.q</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22263" opendate="2019-9-27 00:00:00" fixdate="2019-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MV rewriting for distinct and count(distinct) not being triggered</summary>
      <description>Count distinct issues with materialized views.  Two scripts attached1) create materialized view base_aview stored as orc as select distinct c1 c1, c2 c2 from base;explain extended select count(distinct c1) from base group by c2 ;2)create materialized view base_aview stored as orc as SELECT c1 c1, c2 c2, sum(c2) FROM base group by 1,2;explain extended select count(distinct c1) from base group by c2;</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22274" opendate="2019-9-30 00:00:00" fixdate="2019-11-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade Calcite version to 1.21.0</summary>
      <description>Click to add description</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.dec.str.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.remove.exprs.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.udf.col.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.filter.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.assertion.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ANY.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.ALL.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.in.clause.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.BaseJdbcWithMiniLlap.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestNewGetSplitsFormat.java</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveDefaultRelMetadataProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelFactories.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelOptUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveSubQRemoveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveAggregate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveMultiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.reloperators.HiveSemiJoin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveAggregateJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinAddNotNullRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveJoinConstraintsRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveProjectJoinTransposeRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRemoveGBYSemiJoinRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.jdbc.JDBCSortPushDownRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdDistinctRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdPredicates.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdRowCount.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSelectivity.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.stats.HiveRelMdSize.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.const.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constprog.semijoin.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="22278" opendate="2019-10-1 00:00:00" fixdate="2019-10-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade log4j to 2.12.1</summary>
      <description>Hive's currently using log4j 2.10.0 and according to HIVE-21273, a number of issues are present in it, which can be resolved by upgrading to 2.12.1:Curly braces in parameters are treated as placeholders affectsVersions:2.8.2;2.9.0;2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2032?filter=allopenissues Remove Log4J API dependency on Management APIs affectsVersions:2.9.1;2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2126?filter=allopenissues Log4j2 throws NoClassDefFoundError in Java 9 affectsVersions:2.10.0;2.11.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2129?filter=allopenissues ThreadContext map is cleared =&gt; entries are only available for one log event affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2158?filter=allopenissues Objects held in SortedArrayStringMap cannot be filtered during serialization affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2163?filter=allopenissues NullPointerException at org.apache.logging.log4j.util.Activator.loadProvider(Activator.java:81) in log4j 2.10.0 affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2182?filter=allopenissues MarkerFilter onMismatch invalid attribute in .properties affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2202?filter=allopenissues Configuration builder classes should look for "onMismatch"; not "onMisMatch". affectsVersions:2.4;2.4.1;2.5;2.6;2.6.1;2.6.2;2.7;2.8;2.8.1;2.8.2;2.9.0;2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2219?filter=allopenissues Empty Automatic-Module-Name Header affectsVersions:2.10.0;2.11.0;3.0.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2254?filter=allopenissues ConcurrentModificationException from org.apache.logging.log4j.status.StatusLogger.&lt;clinit&gt;(StatusLogger.java:71) affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2276?filter=allopenissues Allow SystemPropertiesPropertySource to run with a SecurityManager that rejects system property access affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2279?filter=allopenissues ParserConfigurationException when using Log4j with oracle.xml.jaxp.JXDocumentBuilderFactory affectsVersions:2.10.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2283?filter=allopenissues Log4j 2.10+not working with SLF4J 1.8 in OSGI environment affectsVersions:2.10.0;2.11.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2305?filter=allopenissues fix the CacheEntry map in ThrowableProxy#toExtendedStackTrace to be put and gotten with same key affectsVersions:2.6.2;2.7;2.8;2.8.1;2.8.2;2.9.0;2.9.1;2.10.0;2.11.0 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2389?filter=allopenissues NullPointerException when closing never used RollingRandomAccessFileAppender affectsVersions:2.10.0;2.11.1 https://issues.apache.org/jira/projects/LOG4J2/issues/LOG4J2-2418?filter=allopenissues</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.log.SlidingFilenameRolloverStrategy.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.service.cli.operation.TestOperationLoggingLayout.java</file>
    </fixedFiles>
  </bug>
  <bug id="22289" opendate="2019-10-3 00:00:00" fixdate="2019-10-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regenerate test output for tests broken due to commit race</summary>
      <description>HIVE-22042 got committed which changed the plans of a few tests (by enabling nonstrict partitioning mode by default) then HIVE-22269 got committed which fixes a bug with stats not being correctly calculated on some operators. Each patch got green runs individually but together causes test output differences.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.spark.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="22305" opendate="2019-10-8 00:00:00" fixdate="2019-10-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add the kudu-handler to the packaging module</summary>
      <description>The hive-kudu-handler needs to be added to the packaging module to ensure the jars are packaged into the tar distribution.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">packaging.src.main.assembly.src.xml</file>
      <file type="M">packaging.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22311" opendate="2019-10-9 00:00:00" fixdate="2019-11-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Propagate min/max column values from statistics to the optimizer for timestamp type</summary>
      <description>Currently stats annotation does not consider timestamp type e.g. for estimates with range predicates.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.StatObjectConverter.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MTableColumnStatistics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.model.MPartitionColumnStatistics.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.ColumnStatsMergerFactory.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.ColumnsStatsUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.ColumnStatsAggregatorFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.udf.trunc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.non.constant.in.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.cast.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partitioned.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ColumnStatsUpdateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.ColumnStatisticsObjTranslator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFComputeStats.java</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.table.update.status.disable.bitvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.desc.table.formatted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.confirm.initial.tbl.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.desc.table.formatted.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.interval.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.nonvector.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.cast.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="22327" opendate="2019-10-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl: Ignore read-only transactions in notification log</summary>
      <description>Read txns need not be replicated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.CommitTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AllocWriteIdEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AbortTxnEvent.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="22328" opendate="2019-10-11 00:00:00" fixdate="2019-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Min value for column in stats is not set correctly for some data types in partitioned tables</summary>
      <description>This is a follow up Jira for HIVE-22248. For partitioned tables the statistics aggregation happens at in the *ColumnStatsAggregator classes instead of the *ColumnStatsMerger classes, and they still fail to handle the unset low values correctly. Beside that they need to be fixed the two classes should use the same codes for merging statistics.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22345" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HIVE-21327 commit message is wrong</summary>
      <description>https://github.com/apache/hive/commit/a0ccbff838afb440461a4d6df335f824c1dccbccAccidentally used wrong commit message.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">errata.txt</file>
    </fixedFiles>
  </bug>
  <bug id="22346" opendate="2019-10-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Yetus is failing rat check</summary>
      <description>Lines that start with ????? in the ASF License report indicate files that do not have an Apache license header: !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/ad.example.com.ldif !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/example.com.ldif !????? /data/hiveptest/working/yetus_PreCommit-HIVE-Build-18996/standalone-metastore/metastore-server/src/test/resources/ldap/microsoft.schema.ldif</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22366" opendate="2019-10-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Multiple metastore calls for same table and constraints during planning</summary>
      <description>e.g. for a simple query likeexplain select count(i) from test2 group by j limit 3;planner makes the following calls:2019-10-17T22:44:49,892 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_table : tbl=hive.default.test22019-10-17T22:44:49,908 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_primary_keys : tbl=hive.default.test22019-10-17T22:44:49,910 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_foreign_keys : parentdb=null parenttbl=null foreigndb=default foreigntbl=test22019-10-17T22:44:49,911 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_unique_constraints : tbl=hive.default.test22019-10-17T22:44:49,913 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_not_null_constraints : tbl=hive.default.test22019-10-17T22:44:49,979 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_partitions : tbl=hive.default.test22019-10-17T22:44:49,997 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_aggr_stats_for: table=hive.default.test22019-10-17T22:44:50,014 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_table : tbl=hive.default.test22019-10-17T22:44:50,023 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_primary_keys : tbl=hive.default.test22019-10-17T22:44:50,024 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_foreign_keys : parentdb=null parenttbl=null foreigndb=default foreigntbl=test22019-10-17T22:44:50,025 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_unique_constraints : tbl=hive.default.test22019-10-17T22:44:50,026 INFO [812910d4-bc6a-450c-aa8b-3487b4edceab main] HiveMetaStore.audit: ugi=vgarg ip=unknown-ip-addr cmd=get_not_null_constraints : tbl=hive.default.test2</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
    </fixedFiles>
  </bug>
  <bug id="22367" opendate="2019-10-18 00:00:00" fixdate="2019-10-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Transaction type not retrieved from OpenTxnRequest</summary>
      <description>When opening a transaction, its type should be extracted from OpenTxnRequest object. Currently it's hardcoded with TxnType.DEFAULT.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStoreTxns.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22374" opendate="2019-10-21 00:00:00" fixdate="2019-11-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade commons-compress version to 1.19</summary>
      <description>As described in CVE-2019-12402, commons-compress:1.18 has an issue where certain inputs may cause an infinite loop which leads to a denial of service attack.This patch simply upgrades common-compress versions from 1.18 to 1.19 which is the latest minor version at the date of filing this issue (Maven repo).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22376" opendate="2019-10-21 00:00:00" fixdate="2019-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Cancelled query still prints exception if it was stuck in waiting for lock</summary>
      <description>The query waits for locks, then cancelled.It prints this to the logs, which is unnecessary and missleading:apache.hive.service.cli.operation.SQLOperation$BackgroundWork$1.run(SQLOperation.java:326) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) at org.apache.hive.service.cli.operation.SQLOperation$BackgroundWork.run(SQLOperation.java:344) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: NoSuchLockException(message:No such lock lockid:272) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$check_lock_result$check_lock_resultStandardScheme.read(ThriftHiveMetastore.java) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$check_lock_result$check_lock_resultStandardScheme.read(ThriftHiveMetastore.java) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$check_lock_result.read(ThriftHiveMetastore.java) at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:86) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_check_lock(ThriftHiveMetastore.java:5730) at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.check_lock(ThriftHiveMetastore.java:5717) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.checkLock(HiveMetaStoreClient.java:3128) at sun.reflect.GeneratedMethodAccessor351.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) at com.sun.proxy.$Proxy59.checkLock(Unknown Source) at sun.reflect.GeneratedMethodAccessor351.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:3333) at com.sun.proxy.$Proxy59.checkLock(Unknown Source) at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.lock(DbLockManager.java:115) ... 25 more</description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22378" opendate="2019-10-21 00:00:00" fixdate="2019-10-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove code duplicatoins from return path handling</summary>
      <description>Return path handling have some code duplications, they should be removed.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveOpConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22394" opendate="2019-10-23 00:00:00" fixdate="2019-10-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Duplicate Jars in druid classpath causing issues</summary>
      <description>hive-druid-handler jar has shaded version of druid classes, druid-hdfs-storage also has non-shaded classes. [hive@hiveserver2-1 lib]$ ls |grep druidcalcite-druid-1.19.0.7.0.2.0-163.jardruid-bloom-filter-0.15.1.7.0.2.0-163.jardruid-hdfs-storage-0.15.1.7.0.2.0-163.jarhive-druid-handler-3.1.2000.7.0.2.0-163.jarhive-druid-handler.jarException below - Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)  at org.apache.hive.druid.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)  at org.apache.hadoop.hive.druid.io.DruidRecordWriter.pushSegments(DruidRecordWriter.java:177)  ... 22 moreCaused by: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:765)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$push$1(AppenderatorImpl.java:630)  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$1.apply(Futures.java:713)  at org.apache.hive.druid.com.google.common.util.concurrent.Futures$ChainingListenableFuture.run(Futures.java:861)  ... 3 moreCaused by: java.lang.RuntimeException: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:96)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:114)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:104)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.mergeAndPush(AppenderatorImpl.java:743)  ... 6 moreCaused by: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.fs.HadoopFsWrapper  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.copyFilesWithChecks(HdfsDataSegmentPusher.java:163)  at org.apache.hive.druid.org.apache.druid.storage.hdfs.HdfsDataSegmentPusher.push(HdfsDataSegmentPusher.java:145)  at org.apache.hive.druid.org.apache.druid.segment.realtime.appenderator.AppenderatorImpl.lambda$mergeAndPush$4(AppenderatorImpl.java:747)  at org.apache.hive.druid.org.apache.druid.java.util.common.RetryUtils.retry(RetryUtils.java:86)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22395" opendate="2019-10-23 00:00:00" fixdate="2019-1-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to read Druid metastore password from jceks</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22411" opendate="2019-10-28 00:00:00" fixdate="2019-11-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Performance degradation on single row inserts</summary>
      <description>Executing single insert statements on a transactional table effects write performance on a s3 file system. Each insert creates a new delta directory. After each insert hive calculates statistics like number of file in the table and total size of the table. In order to calculate these, it traverses the directory recursively. During the recursion for each path a separate listStatus call is executed. In the end the more delta directory you have the more time it takes to calculate the statistics.Therefore insertion time goes up linearly:The fix is to use fs.listFiles(path, /*recursive*/ true) instead the handcrafter recursive method/</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.HiveStatsUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="2245" opendate="2011-6-30 00:00:00" fixdate="2011-7-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make CombineHiveInputFormat the default hive.input.format</summary>
      <description>We should use CombineHiveInputFormat as the default Hive input format.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22451" opendate="2019-11-4 00:00:00" fixdate="2019-11-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Secure LLAP configurations are still deemed unsecure in Tez AM processes</summary>
      <description>Due to the change in HIVE-22354 and HIVE-22195 Zookeeper discovery of LLAP workers is not working when invoked from within a Tez AM process: a Tez AM process does not log on using Kerberos even in secure environments, hence UserGroupInformation.getLoginUser().hasKerberosCredentials() will return false for security-enabled clusters too.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.registry.impl.TestZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
      <file type="M">llap-client.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22453" opendate="2019-11-4 00:00:00" fixdate="2019-2-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Describe table unnecessarily fetches partitions</summary>
      <description>The simple describe table command without EXTENDED and FORMATTED (i.e., DESCRIBE table_name) fetches all partitions when no partition is specified, although it does not display partition statistics in nature.The command should not fetch partitions since it can take a long time for a large amount of partitions.For instance, in our environment, the command takes around 8 seconds for a table with 8760 (24 * 365) partitions.</description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.info.desc.DescTableOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="22458" opendate="2019-11-5 00:00:00" fixdate="2019-5-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add more constraints on showing partitions</summary>
      <description>When we showing partitions of a table with thousands of partitions,  all the partitions will be returned and it's not easy to catch the specified one from them, this make showing partitions hard to use. We can add where/limit/order by constraints to show partitions like: show partitions table_name &amp;#91;partition_specs&amp;#93; where partition_key &gt;= value order by partition_key desc limit n; </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreForJdoConnection.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.DummyRawStoreControlledCommit.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionsByExprRequest.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.show.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.showparts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.drop.partitions.filter.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionsOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionsDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.show.ShowPartitionAnalyzer.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.DummyRawStoreFailEvent.java</file>
    </fixedFiles>
  </bug>
  <bug id="2257" opendate="2011-7-5 00:00:00" fixdate="2011-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TestHadoop20SAuthBridge</summary>
      <description>Looks like this test was accidentally disabled in HIVE-818.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22580" opendate="2019-12-4 00:00:00" fixdate="2019-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flakyness in TestDbTxnManager2</summary>
      <description>Different TestDbTxnManager2 tests are failing intermittently with:2019-12-03T20:18:49,269 DEBUG [main] pool.PoolBase: HikariPool-1 - Reset (autoCommit) on connection org.apache.derby.impl.jdbc.EmbedConnection@912054991 (XID = 347), (SESSIONID = 7), (DATABASE = memory:/data/jenkins/workspace/hive-TestDbTxnManager2/hive/ql/target/tmp/junit_metastore_db), (DRDAID = null) 2019-12-03T20:18:49,270 ERROR [main] metastore.RetryingHMSHandler: MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: Table/View 'NEXT_TXN_ID' does not exist. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source) at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source) at com.zaxxer.hikari.pool.ProxyStatement.executeQuery(ProxyStatement.java:108) at com.zaxxer.hikari.pool.HikariProxyStatement.executeQuery(HikariProxyStatement.java) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:604) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:560) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:8095) at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy38.open_txns(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxnsIntr(HiveMetaStoreClient.java:3139) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(HiveMetaStoreClient.java:3103) at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) at com.sun.proxy.$Proxy39.openTxn(Unknown Source) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:250) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:234) at org.apache.hadoop.hive.ql.Driver.openTransaction(Driver.java:915) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:640) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1504) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1564) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1387) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1376) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.dropTable(TestDbTxnManager2.java:974) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testLockBlockedBy(TestDbTxnManager2.java:396) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)Caused by: ERROR 42X05: Table/View 'NEXT_TXN_ID' does not exist. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) at org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source) at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source) at org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source) at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source) at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source) at org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source) at org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source) at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source) at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source) at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source) ... 59 more) at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:570) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:8095) at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) at com.sun.proxy.$Proxy38.open_txns(Unknown Source) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxnsIntr(HiveMetaStoreClient.java:3139) at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.openTxn(HiveMetaStoreClient.java:3103) at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:212) at com.sun.proxy.$Proxy39.openTxn(Unknown Source) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:250) at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.openTxn(DbTxnManager.java:234) at org.apache.hadoop.hive.ql.Driver.openTransaction(Driver.java:915) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:640) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1504) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1564) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1387) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1376) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.dropTable(TestDbTxnManager2.java:974) at org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.testLockBlockedBy(TestDbTxnManager2.java:396) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27) at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) at org.junit.rules.RunRules.evaluate(RunRules.java:20) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229) at org.junit.runners.ParentRunner.run(ParentRunner.java:309) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:365) at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:273) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:238) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:159) at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:379) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:340) at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:125) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:413)Before this failure we are seeing this:2019-12-03T20:18:49,180 WARN [Heartbeater-1] pool.ProxyConnection: HikariPool-3 - Connection org.apache.derby.impl.jdbc.EmbedConnection@1077215639 (XID = 389), (SESSIONID = 49), (DATABASE = memory:/data/jenkins/workspace/hive-TestDbTxnManager2/hive/ql/target/tmp/junit_metastore_db), (DRDAID = null) marked as broken because of SQLSTATE(08000), ErrorCode(40000)java.sql.SQLNonTransientConnectionException: Connection closed by unknown interrupt. at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.closeOnTransactionError(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.movePosition(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.jdbc.EmbedResultSet.next(Unknown Source) ~[derby-10.14.2.0.jar:?] at com.zaxxer.hikari.pool.HikariProxyResultSet.next(HikariProxyResultSet.java) ~[HikariCP-2.6.1.jar:?] at org.datanucleus.store.rdbms.query.ForwardQueryResult.initialise(ForwardQueryResult.java:99) ~[datanucleus-rdbms-4.1.19.jar:?] at org.datanucleus.store.rdbms.query.JDOQLQuery.performExecute(JDOQLQuery.java:703) ~[datanucleus-rdbms-4.1.19.jar:?] at org.datanucleus.store.query.Query.executeQuery(Query.java:1855) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.store.query.Query.executeWithArray(Query.java:1744) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.store.query.Query.execute(Query.java:1726) ~[datanucleus-core-4.1.17.jar:?] at org.datanucleus.api.jdo.JDOQuery.executeInternal(JDOQuery.java:374) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:216) ~[datanucleus-api-jdo-4.2.4.jar:?] at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.ensureDbInit(MetaStoreDirectSql.java:246) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.&lt;init&gt;(MetaStoreDirectSql.java:188) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:513) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:432) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:385) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77) ~[hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137) ~[hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.hive.metastore.RawStoreProxy.&lt;init&gt;(RawStoreProxy.java:59) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:781) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:749) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:743) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:605) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at sun.reflect.GeneratedMethodAccessor87.invoke(Unknown Source) ~[?:?] at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_202] at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:147) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:108) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.&lt;init&gt;(RetryingHMSHandler.java:80) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:93) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:9820) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:172) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:112) ~[classes/:?] at sun.reflect.GeneratedConstructorAccessor281.newInstance(Unknown Source) ~[?:?] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_202] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_202] at org.apache.hadoop.hive.metastore.utils.JavaUtils.newInstance(JavaUtils.java:84) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:95) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:148) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:119) ~[hive-standalone-metastore-3.1.2000.7.1.0.0-SNAPSHOT.jar:3.1.2000.7.1.0.0-SNAPSHOT] at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:4842) ~[classes/:?] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4910) ~[classes/:?] at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:4890) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getMS(DbTxnManager.java:192) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.heartbeat(DbTxnManager.java:632) ~[classes/:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.lambda$run$0(DbTxnManager.java:1038) ~[classes/:?] at java.security.AccessController.doPrivileged(Native Method) [?:1.8.0_202] at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_202] at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1876) [hadoop-common-3.1.1.7.1.0.0-SNAPSHOT.jar:?] at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager$Heartbeater.run(DbTxnManager.java:1037) [classes/:?] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_202] at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308) [?:1.8.0_202] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_202] at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294) [?:1.8.0_202] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202]Caused by: org.apache.derby.iapi.error.StandardException: Connection closed by unknown interrupt. at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.error.StandardException.newException(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.util.InterruptStatus.setInterrupted(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.iapi.util.InterruptStatus.throwIf(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.checkCancellationFlag(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BulkTableScanResultSet.getNextRowCore(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source) ~[derby-10.14.2.0.jar:?] at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.getNextRow(Unknown Source) ~[derby-10.14.2.0.jar:?] ... 57 more</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="22588" opendate="2019-12-6 00:00:00" fixdate="2019-12-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode</summary>
      <description>Flush the remaining rows for the rest of the grouping sets when switching the vector groupby mode</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="2259" opendate="2011-7-5 00:00:00" fixdate="2011-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Skip comments in hive script</summary>
      <description>If you specify something like:&amp;#8211; This is a commentadd jar jar_path;select * from my_table;This fails.I have created a fix to skip the commented lines.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22606" opendate="2019-12-9 00:00:00" fixdate="2019-12-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AvroSerde logs avro.schema.literal under INFO level</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="22652" opendate="2019-12-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopNKey push through Group by with Grouping sets</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.topnkey.TopNKeyPushdownProcessor.java</file>
    </fixedFiles>
  </bug>
  <bug id="22653" opendate="2019-12-17 00:00:00" fixdate="2019-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove commons-lang leftovers</summary>
      <description>HIVE-7145 removed commons-lang - in favor of commons-lang3 - as a direct dependency, however a high number of files still refer to commons-lang, which is transitively brought in either way.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestResultProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.HostExecutor.java</file>
      <file type="M">testutils.ptest2.src.main.java.org.apache.hive.ptest.execution.conf.UnitTestPropertiesParser.java</file>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.CacheTag.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.VerifyingObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.tools.schematool.TestSchemaToolForMetastore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.rules.DatabaseRule.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.client.TestAppendPartitions.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreServerUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.SchemaToolTaskValidate.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.tools.schematool.MetastoreSchemaTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenTool.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.DBTokenStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.RawStoreProxy.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.PersistenceManagerProvider.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetastoreDirectSqlUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.client.builder.ConstraintBuilder.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CacheUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.AuthFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.HdfsUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.MetaStoreLdapAuthenticationProviderImpl.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ldap.LdapUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ldap.GroupFilterFactory.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.SparkClientUtilities.java</file>
      <file type="M">spark-client.src.main.java.org.apache.hive.spark.client.rpc.RpcConfiguration.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.common.src.main.java.org.apache.hadoop.hive.io.HdfsUtils.java</file>
      <file type="M">shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">service.src.test.org.apache.hive.service.cli.TestHiveSQLException.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.KillQueryImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.cli.session.HiveSessionImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.ldap.LdapUtils.java</file>
      <file type="M">service.src.java.org.apache.hive.service.auth.LdapAuthenticationProviderImpl.java</file>
      <file type="M">service.src.java.org.apache.hive.http.JdbcJarDownloadServlet.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.objectinspector.TestReflectionObjectInspectors.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableSerDe.java</file>
      <file type="M">serde.src.test.org.apache.hadoop.hive.serde2.binarysortable.TestBinarySortableFast.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataOutputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.teradata.TeradataBinaryDataInputStream.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyObjectInspectorParametersImpl.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroLazyObjectInspector.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.stats.TestStatsUtils.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedMapColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestVectorizedListColumnReader.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.VectorRandomRowSource.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.TestMapJoinOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.MapJoinTestConfig.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.xml.GenericUDFXPath.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.ptf.WindowingTableFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFSubstringIndex.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFRTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFLTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUnixTime.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFField.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFBaseTrim.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MinorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.OperationLog.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.DropDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.CreateDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AlterDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.metastore.events.AddPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.ListResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.CommandProcessorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.VectorPTFDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ptf.PTFInputDef.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeFieldDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ExplainLockDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.StorageFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.CreateDatabaseHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.MergeSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SortedDynPartitionTimeGranularityOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SimpleFetchOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.SharedWorkOptimizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.pcr.PcrOpProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.listbucketingpruner.LBPartitionProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.lineage.LineageCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HivePartitionPruneRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.JarUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.MetaDataFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.JsonMetaDataFormatter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLock.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.HiveLockObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.truncate.ColumnTruncateTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.merge.MergeFileTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.hooks.LineageInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedBatchUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorColumnOrderedMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinGenerateResultOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStartLen.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringSubstrColStart.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.StringInitCap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.WorkloadManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.RemoteSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.status.LocalSparkJobMonitor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlan.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.LoadFunction.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSDatabaseEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.BytesBytesMultiHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OrcFileMergeOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.debug.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.misc.AlterTableSetPropertiesOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.info.DescTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.create.show.ShowCreateTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.AbstractAlterTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.function.desc.DescFunctionOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.DDLUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.alter.location.AlterDatabaseSetLocationOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.SerDeStorageSchemaReader.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveClientCache.java</file>
      <file type="M">llap-tez.src.java.org.apache.hadoop.hive.llap.tezplugins.LlapTaskSchedulerService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapTaskReporter.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-common.src.test.org.apache.hadoop.hive.llap.TestRow.java</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.security.LlapTokenIdentifier.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hive.beeline.QFile.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestSyntaxUtil.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.AccumuloHiveRow.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.ColumnMapper.java</file>
      <file type="M">accumulo-handler.src.java.org.apache.hadoop.hive.accumulo.columns.HiveAccumuloMapColumnMapping.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.columns.TestColumnMapper.java</file>
      <file type="M">accumulo-handler.src.test.org.apache.hadoop.hive.accumulo.serde.FirstCharAccumuloCompositeRowId.java</file>
      <file type="M">beeline.src.java.org.apache.hive.beeline.SeparatedValuesOutputFormat.java</file>
      <file type="M">cli.src.java.org.apache.hadoop.hive.cli.CliDriver.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.cli.HiveFileProcessor.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.FileUtils.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.log.InPlaceUpdate.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveBaseChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.type.HiveChar.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConfUtil.java</file>
      <file type="M">common.src.java.org.apache.hive.common.util.HiveStringUtils.java</file>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.ColumnMappings.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.HCatCli.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.cli.SemanticAnalysis.CreateTableHook.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HCatUtil.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.common.HiveClientCache.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.schema.HCatFieldSchema.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.FileOutputCommitterContainer.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.HCatBaseInputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.MultiOutputFormat.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.TaskCommitContextRegistry.java</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.src.main.java.org.apache.hive.hcatalog.pig.HCatBaseStorer.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.messaging.jms.MessagingUtils.java</file>
      <file type="M">hcatalog.streaming.src.java.org.apache.hive.hcatalog.streaming.StrictRegexWriter.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatClientHMSImpl.java</file>
      <file type="M">hcatalog.webhcat.java-client.src.main.java.org.apache.hive.hcatalog.api.HCatTable.java</file>
      <file type="M">hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.HcatDelegator.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Copy.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.Function.java</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.functions.FunctionDatetime.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.session.TestClearDanglingScratchDir.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.schematool.TestSchemaTool.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.beeline.TestBeeLineWithArgs.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.AbstractCliConfig.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.hbase.HBaseTestSetup.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckColumnAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.CheckTableAccessHook.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.hooks.VerifySessionStateStackTracesHook.java</file>
    </fixedFiles>
  </bug>
  <bug id="22698" opendate="2020-1-7 00:00:00" fixdate="2020-4-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support Statement#closeOnCompletion()</summary>
      <description>I am a member of MyBatis team and a user reported that Hive does not support java.sql.Statement#closeOnCompletion() yet.</description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveStatement.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveQueryResultSet.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcDriver2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22703" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compaction configuration check when starting HMS/HS2</summary>
      <description>Currently when starting HMS we can have bugous configuration which prevents compatction to work. We should find a way to inform the admin about the configuration error, or even prevent HMS to start in this case.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">service.src.java.org.apache.hive.service.server.HiveServer2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22705" opendate="2020-1-8 00:00:00" fixdate="2020-1-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP cache is polluted by query-based compactor</summary>
      <description>One of the steps that query-based compaction does is the verification of ACID sort order by using the validate_acid_sort_order UDF. This is a prerequisite before the actual compaction can happen, and is done by a query that reads the whole table content.This results in the whole table content being populated into the cache. The problem is that this content is not useful and will rather pollute the cache space, as it can never be used again: cache content binds to files (file IDs) that obviously will be changed in this case by compaction.I propose we disable LLAP caching in the session of query-based compaction's queries.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
    </fixedFiles>
  </bug>
  <bug id="22713" opendate="2020-1-9 00:00:00" fixdate="2020-1-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation shouldn&amp;#39;t be done for Join-Fil(*)-RS structure</summary>
      <description>Constant propagation shouldn't be done for Join-Fil-RS structure too. Since we output columns from the join if the structure is Join-Fil-RS, the expressions shouldn't be modified.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22721" opendate="2020-1-13 00:00:00" fixdate="2020-2-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option for queries to only read from LLAP cache</summary>
      <description>Testing features of LLAP cache sometimes requires to validate if e.g. a particular table/partition is cached, or not.This is to avoid relying on counters that are dependent on the underlying (ORC) file format (which may produce different number of bytes among its different versions).</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.ReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.Reader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapHiveUtils.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.OrcEncodedDataReader.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22722" opendate="2020-1-13 00:00:00" fixdate="2020-4-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>timestamptz_2 test failure</summary>
      <description>the min/max value seems to be off in some cases; this was highly non deterministic; and hard to reproduce - but in the recent QA runs it started failing more....</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.timestamptz.2.q.out</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CliConfigs.java</file>
    </fixedFiles>
  </bug>
  <bug id="22726" opendate="2020-1-14 00:00:00" fixdate="2020-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TopN Key optimizer should use array instead of priority queue</summary>
      <description>The TopN key optimizer currently uses a priority queue for keeping track of the largest/smallest rows. Its max size is the same as the user specified limit. This should be replaced a more cache line friendly array with a small (128) maximum size and see how much performance is gained.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestTopNKeyFilter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.topnkey.TopNKeyProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperGeneralComparator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.wrapper.VectorHashKeyWrapperBatch.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.TopNKeyFilter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22729" opendate="2020-1-15 00:00:00" fixdate="2020-2-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Provide a failure reason for failed compactions</summary>
      <description>We should provide a compaction failure reason as easily accessible as possible. Like in the result of the SHOW COMPACTIONS command.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.CompactionInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponseElement.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionInfoStruct.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dbtxnmgr.showlocks.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestCompactionTxnHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.show.compactions.ShowCompactionsDesc.java</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="2278" opendate="2011-7-11 00:00:00" fixdate="2011-11-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support archiving for multiple partitions if the table is partitioned by multiple columns</summary>
      <description>If a table is partitioned by ds,hrit should be possible to archive all the files in ds to reduce the number of files</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.archive2.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.archive1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.DummyPartition.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22780" opendate="2020-1-27 00:00:00" fixdate="2020-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade slf4j version to 1.7.30</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.pom.xml</file>
      <file type="M">pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">itests.qtest.pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22781" opendate="2020-1-27 00:00:00" fixdate="2020-2-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add ability to immediately execute a scheduled query</summary>
      <description>there are some differences when the system invokes the scheduled query / the user executes it in a shell - forcing the schedule to run might be usefull in developing/debugging schedulessomething like:alter scheduled query a execute</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.schq.TestScheduledQueryStatements.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ScheduledQueryAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.schq.ScheduledQueryMaintenanceTask.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.IdentifiersParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveLexer.g</file>
    </fixedFiles>
  </bug>
  <bug id="2281" opendate="2011-7-12 00:00:00" fixdate="2011-7-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Regression introduced from HIVE-2155</summary>
      <description>EXPLAIN SELECT key, count(1) FROM src; throws a null pointer exception due to the operator stack not being checked prior to use for constructing the error message, due to the change introduced in HIVE-2155 to improve error message context tokens.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="22814" opendate="2020-1-31 00:00:00" fixdate="2020-2-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ArrayIndexOutOfBound in the vectorization getDataTypePhysicalVariation</summary>
      <description>ArrayIndexOutOfBound in the vectorization getDataTypePhysicalVariation</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22815" opendate="2020-2-1 00:00:00" fixdate="2020-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>reduce the unnecessary file system object creation in MROutput</summary>
      <description>MROutput generates unnecessary file system object which may create long latency in Cloud environment. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22816" opendate="2020-2-1 00:00:00" fixdate="2020-2-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryCache: Queries using views can have them cached after CTE expansion</summary>
      <description>create view ss_null as select * from store_Sales where ss_Sold_date_sk is null;select count(ss_ticket_number) from ss_null;with ss_null_cte as (select * from store_Sales where ss_Sold_date_sk is null)select count(ss_ticket_number) from ss_null_cte;Are treated differently by the query cache, however their execution is identical.CBO rewrites the view query into AST form as followsSELECT COUNT(`ss_ticket_number`) AS `$f0`FROM `tpcds_bin_partitioned_acid_orc_10000`.`store_sales`WHERE `ss_sold_date_sk` IS NULLBut retains the write-entity for the VIRTUAL_VIEW for Ranger authorization 0: jdbc:hive2://localhost:10013&gt; explain dependency select count(distinct ss_ticket_number) from ss_null;+----------------------------------------------------+| Explain |+----------------------------------------------------+| {"input_tables":[{"tablename":"tpcds_bin_partitioned_acid_orc_10000@ss_null","tabletype":"VIRTUAL_VIEW"},{"tablename":"tpcds_bin_partitioned_acid_orc_10000@store_sales","tabletype":"MANAGED_TABLE","tableParents":"[tpcds_bin_partitioned_acid_orc_10000@ss_null]"}],"input_partitions":[{"partitionName":"tpcds_bin_partitioned_acid_orc_10000@store_sales@ss_sold_date_sk=__HIVE_DEFAULT_PARTITION__"}]} |+----------------------------------------------------+Causing Query cache to print outparse.CalcitePlanner: Not eligible for results caching - query contains non-transactional tables [ss_null]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="22818" opendate="2020-2-3 00:00:00" fixdate="2020-2-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Preparation for jetty 9.4.26 upgrade</summary>
      <description>Make some code adjustment, before upgrading jetty to 9.4.26.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="22821" opendate="2020-2-3 00:00:00" fixdate="2020-4-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add necessary endpoints for proactive cache eviction</summary>
      <description>Implement the parts required for iHS2 -&gt; LLAP daemons communication: protobuf message schema and endpoints Hive configuration for use cases: dropping db dropping table dropping partition from a table</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.partition.drop.AlterTableDropPartitionOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.drop.DropTableOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.database.drop.DropDatabaseOperation.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.llap.LlapHiveUtils.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestLowLevelCacheImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapProtocolServerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelLrfuCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelFifoCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.CacheContentsTracker.java</file>
      <file type="M">llap-common.src.protobuf.LlapDaemonProtocol.proto</file>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.impl.LlapManagementProtocolClientImpl.java</file>
      <file type="M">llap-common.src.gen.protobuf.gen-java.org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.io.api.LlapIo.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22906" opendate="2020-2-18 00:00:00" fixdate="2020-3-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Redundant checkLock Mutex blocks concurrent Lock requests</summary>
      <description>enqueueLocks is already serialising locks creation via (SELECT NL_NEXT FROM NEXT_LOCK_ID FOR UPDATE). Requested locks would be assigned 'W' state.checkLock is iterating over the sorted set of conflicting locks below current EXT_LOCK_ID. It does handle the situation when there is conflicting lock with lower ID in 'W' state - lock request would be denied and retried later.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="22908" opendate="2020-2-19 00:00:00" fixdate="2020-2-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>AM caching connections to LLAP based on hostname and port does not work in kubernetes</summary>
      <description>AM is caching all connections to LLAP services using combination of hostname and port which does not work in kubernetes environment where hostname of pod and port can be same with statefulset. This causes AM to talk to old LLAP which could have died or OOM/Pod kill etc. </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-common.src.java.org.apache.hadoop.hive.llap.AsyncPbRpcProxy.java</file>
    </fixedFiles>
  </bug>
  <bug id="22914" opendate="2020-2-20 00:00:00" fixdate="2020-2-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make Hive Connection ZK Interactions Easier to Troubleshoot</summary>
      <description>Add better logging and make errors more consistent and meaningful.Recently was trying to troubleshoot an issue where the ZK namespace of the client and the HS2 were different and it was way too difficult to diagnose.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.ZooKeeperHiveClientHelper.java</file>
    </fixedFiles>
  </bug>
  <bug id="22931" opendate="2020-2-26 00:00:00" fixdate="2020-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HoS dynamic partitioning fails with blobstore optimizations off</summary>
      <description>Reproduction steps: Create s3a backed table and normal table.CREATE TABLE source ( a string, b int, c int); CREATE TABLE target ( a string)PARTITIONED BY ( b int, c int)STORED AS parquetLOCATION 's3a://somepath'; Insert values into normal table.INSERT INTO TABLE source VALUES ("a", "1", "1"); Do an insert overwrite with dynamic partitions:set hive.exec.dynamic.partition.mode=nonstrict;set hive.blobstore.optimizations.enabled=false;set hive.execution.engine=spark;INSERT OVERWRITE TABLE target partition (b,c)SELECT *FROM source;This fails only with Spark execution engine + blobstorage optimizations being turned off with:2020-01-16 15:24:56,064 ERROR hive.ql.metadata.Hive: [load-dynamic-partitions-5]: Exception when loading partition with parameters partPath=hdfs://nameservice1/tmp/hive/hive/6bcee075-b637-429e-9bf0-a2658355415e/hive_2020-01-16_15-24-01_156_4299941251929377815-4/-mr-10000/.hive-staging_hive_2020-01-16_15-24-01_156_4299941251929377815-4/-ext-10002, table=email_click_base, partSpec={b=null, c=null}, replace=true, listBucketingEnabled=false, isAcid=false, hasFollowingStatsTask=trueorg.apache.hadoop.hive.ql.metadata.HiveException: MetaException(message:Partition spec is incorrect. {companyid=null, eventmonth=null}) at org.apache.hadoop.hive.ql.metadata.Hive.loadPartitionInternal(Hive.java:1666)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.spark.TestSparkUtilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22933" opendate="2020-2-26 00:00:00" fixdate="2020-5-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Allow connecting kerberos-enabled Hive to connect to a non-kerberos druid cluster</summary>
      <description>Currently, If kerberos is enabled for hive, it can only connect to external druid clusters which are kerberos enabled, Since the Druid client used to connect to druid is always KerberosHTTPClient, This task is to allow a kerberos enabled hiverserver2 to connect to non-kerberized druid cluster.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22937" opendate="2020-2-27 00:00:00" fixdate="2020-3-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP : Use unique names for the zip and tarball bundle for LLAP</summary>
      <description>LLAP : Use unique names for the zip and tarball bundle for LLAP</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.main.resources.templates.py</file>
      <file type="M">llap-server.src.main.resources.package.py</file>
    </fixedFiles>
  </bug>
  <bug id="22948" opendate="2020-2-29 00:00:00" fixdate="2020-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>QueryCache: Treat query cache locations as temporary storage</summary>
      <description>The WriteEntity with a query cache query is considered for user authorization without having direct access for users.https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/command/CommandAuthorizerV2.java#L111 if (privObject instanceof WriteEntity &amp;&amp; ((WriteEntity)privObject).isTempURI()) { // do not authorize temporary uris continue; }is not satisfied by the queries qualifying for the query cache.</description>
      <version>3.1.2,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Context.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.security.authorization.plugin.TestHiveAuthorizerCheckInvocation.java</file>
    </fixedFiles>
  </bug>
  <bug id="22967" opendate="2020-3-3 00:00:00" fixdate="2020-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support hive.reloadable.aux.jars.path for Hive on Tez</summary>
      <description>The jars in hive.reloadable.aux.jars.path are not localized in Tez containers. As a result, any query utilizing those reloadable jars fails for Hive on Tez due to ClassNotFoundException.Error: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1578856704640_0087_1_00, diagnostics=[Task failed, taskId=task_1578856704640_0087_1_00_000001, diagnostics=[TaskAttempt 0 failed, info=[Error: Error while running task ( failure) : attempt_1578856704640_0087_1_00_000001_0:java.lang.RuntimeException: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:296) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:250) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:374) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:73) at org.apache.tez.runtime.task.TaskRunner2Callable$1.run(TaskRunner2Callable.java:61) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:61) at org.apache.tez.runtime.task.TaskRunner2Callable.callInternal(TaskRunner2Callable.java:37) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108) at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41) at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:354) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:266) ... 16 moreCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:71) at org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.initializeOp(VectorFilterOperator.java:83) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.initializeMapOperator(VectorMapOperator.java:591) at org.apache.hadoop.hive.ql.exec.tez.MapRecordProcessor.init(MapRecordProcessor.java:317) ... 17 moreCaused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:134) at org.apache.hadoop.hive.ql.exec.FunctionRegistry.isStateful(FunctionRegistry.java:1492) at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.&lt;init&gt;(ExprNodeGenericFuncEvaluator.java:111) at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluatorFactory.get(ExprNodeEvaluatorFactory.java:58) at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:63) ... 24 moreCaused by: java.lang.ClassNotFoundException: com.example.hive.udf.Lower at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClassInternal(GenericUDFBridge.java:142) at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.getUdfClass(GenericUDFBridge.java:132) ... 28 more], TaskAttempt 1 failed, (omitted)HIVE-14037 and HIVE-14142 resolved the similar issues for MapReduce and Hive on Spark, respectively, in the past. </description>
      <version>2.3.6,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22968" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Set hive.parquet.timestamp.time.unit default to micros</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.io.parquet.TestHiveSchemaConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="23035" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23039" opendate="2020-3-18 00:00:00" fixdate="2020-4-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Checkpointing for repl dump bootstrap phase</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.repl.TestReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.PartitionExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.DirCopyWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.TableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSPartitionEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExportTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcrossInstances.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenarios.java</file>
    </fixedFiles>
  </bug>
  <bug id="23077" opendate="2020-3-25 00:00:00" fixdate="2020-3-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-jdbc</summary>
      <description>Only one "tricky" change. Throw an Exception instead of printStackTrace in the static Driver loader as suggested from the reference here:https://github.com/mariadb-corporation/mariadb-connector-j/blob/3bc66153b51aca188afc50ff35a0123f16c099ed/src/main/java/org/mariadb/jdbc/Driver.java#L72</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveDriver.java</file>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveBaseResultSet.java</file>
    </fixedFiles>
  </bug>
  <bug id="23079" opendate="2020-3-26 00:00:00" fixdate="2020-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Calls to printStackTrace in Module hive-serde</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeSet.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeTypeMap.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="23089" opendate="2020-3-27 00:00:00" fixdate="2020-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add constraint checks to CBO plan</summary>
      <description>create table acid_uami(i int, de decimal(5,2) constraint nn1 not null enforced, vc varchar(128) constraint nn2 not null enforced) clustered by (i) into 2 buckets stored as orc TBLPROPERTIES ('transactional'='true');explainupdate acid_uami set de=null where i=1;Non-CBO path:Map Operator Tree:TableScan alias: acid_uami filterExpr: ((i = 1) and enforce_constraint(vc is not null)) (type: boolean) Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: ((i = 1) and enforce_constraint(vc is not null)) (type: boolean)CBO path: Map Reduce Map Operator Tree: TableScan alias: acid_uami filterExpr: (i = 1) (type: boolean) Statistics: Num rows: 1 Data size: 216 Basic stats: COMPLETE Column stats: NONE Filter Operator predicate: (i = 1) (type: boolean)... Reduce Operator Tree:... Filter Operator predicate: enforce_constraint((null is not null and _col3 is not null)) (type: boolean)In CBO path the enforce_constraint function is added to the plan when CBO plan is already generated and optimized.HiveSortExchange(distribution=[any], collation=[[0]]) HiveProject(row__id=[$5], i=[CAST(1):INTEGER], _o__c2=[null:NULL], vc=[$2]) HiveFilter(condition=[=($0, 1)]) HiveTableScan(table=[[default, acid_uami]], table:alias=[acid_uami])</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.update.notnull.constraint.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.type.RexNodeTypeCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.type.ExprNodeTypeCheck.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23178" opendate="2020-4-10 00:00:00" fixdate="2020-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description></description>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23181" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23184" opendate="2020-4-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade druid to 0.17.1</summary>
      <description>Upgrade to druid latest release 0.17.1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.semijoin.reduction.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.delimited.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">data.scripts.kafka.init.data.csv</file>
    </fixedFiles>
  </bug>
  <bug id="2319" opendate="2011-7-28 00:00:00" fixdate="2011-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="232" opendate="2009-1-14 00:00:00" fixdate="2009-1-14 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>metastore.warehouse configuration should use inherited hadoop configuration</summary>
      <description>the hive.metastore.warehouse.dir configuration property in hive-*.xml needs to use the protocol, host, and port when it is inherited from the fs.name property in hadoop-site.xml.When it doesn't and no protocol is found then a broad range of "Move" operations when the source and target are both in the DFS will fail.Currently this can be worked around by prepending the protocol, host and port of the hadoop nameserver into the value of the hive.metastore.warehouse.dir property.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.Warehouse.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23267" opendate="2020-4-21 00:00:00" fixdate="2020-4-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Reduce dependency on groovy</summary>
      <description>Transitively pulled where its unneeded.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23268" opendate="2020-4-22 00:00:00" fixdate="2020-4-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Eliminate beanutils transitive dependency</summary>
      <description>Transitively retrieved from hadoop-commons</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">upgrade-acid.pre-upgrade.pom.xml</file>
      <file type="M">streaming.pom.xml</file>
      <file type="M">storage-api.pom.xml</file>
      <file type="M">standalone-metastore.pom.xml</file>
      <file type="M">standalone-metastore.metastore-tools.metastore-benchmarks.pom.xml</file>
      <file type="M">standalone-metastore.metastore-server.pom.xml</file>
      <file type="M">standalone-metastore.metastore-common.pom.xml</file>
      <file type="M">shims.scheduler.pom.xml</file>
      <file type="M">shims.common.pom.xml</file>
      <file type="M">shims.0.23.pom.xml</file>
      <file type="M">service.pom.xml</file>
      <file type="M">serde.pom.xml</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">metastore.pom.xml</file>
      <file type="M">llap-tez.pom.xml</file>
      <file type="M">llap-server.pom.xml</file>
      <file type="M">llap-ext-client.pom.xml</file>
      <file type="M">llap-common.pom.xml</file>
      <file type="M">llap-client.pom.xml</file>
      <file type="M">kudu-handler.pom.xml</file>
      <file type="M">kryo-registrator.pom.xml</file>
      <file type="M">kafka-handler.pom.xml</file>
      <file type="M">jdbc.pom.xml</file>
      <file type="M">hplsql.pom.xml</file>
      <file type="M">hcatalog.webhcat.svr.pom.xml</file>
      <file type="M">hcatalog.webhcat.java-client.pom.xml</file>
      <file type="M">hcatalog.streaming.pom.xml</file>
      <file type="M">hcatalog.server-extensions.pom.xml</file>
      <file type="M">hcatalog.pom.xml</file>
      <file type="M">hcatalog.hcatalog-pig-adapter.pom.xml</file>
      <file type="M">hcatalog.core.pom.xml</file>
      <file type="M">hbase-handler.pom.xml</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">contrib.pom.xml</file>
      <file type="M">common.pom.xml</file>
      <file type="M">cli.pom.xml</file>
      <file type="M">beeline.pom.xml</file>
      <file type="M">accumulo-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23309" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Lazy Initialization of Hadoop Shims</summary>
      <description>Initialize hadoop-shims only if CM is enabled</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.ReplChangeManager.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.FunctionSerializer.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.TestMetaStoreMultipleEncryptionZones.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="2331" opendate="2011-8-2 00:00:00" fixdate="2011-8-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off compression when generating index intermediate results</summary>
      <description>HiveIndexResult is not compression-aware, so for any index to work (regardless of compact/bitmap) we need to not compress the index intermediate file when we generate it.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="23314" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to Kudu 1.12</summary>
      <description>we need KUDU-3044 because it could cause random failures...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23315" opendate="2020-4-28 00:00:00" fixdate="2020-5-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove empty line from the end of SHOW EXTENDED TABLES and SHOW MATERIALIZED VIEWS</summary>
      <description>At the end of each SHOW EXTENDED TABLES; and SHOW MATERIALIZED VIEWS; command there is an empty line like this:+------------+----------------+|  tab_name  |   table_type   |+------------+----------------+| sample_07  | MANAGED_TABLE  || sample_08  | MANAGED_TABLE  || web_logs   | MANAGED_TABLE  ||            | NULL           |+------------+----------------+It should be removed.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.show.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.show.materialized.views.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.formatting.TextMetaDataFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="23498" opendate="2020-5-19 00:00:00" fixdate="2020-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable HTTP Trace method on ThriftHttpCliService</summary>
      <description></description>
      <version>3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpCLIService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23499" opendate="2020-5-19 00:00:00" fixdate="2020-6-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL: Immutable repl dumps should be reusable across multiple repl loads</summary>
      <description>"hive.repl.dump.metadata.only=true" is not currently honored during "repl load". Currently, it ends up copying all files even if this option is specified in "repl load".  E.grepl load airline_ontime_orc into another_airline_ontime_orc with ('hive.repl.rootdir'='s3a://blah/', 'hive.repl.dump.metadata.only'='true'); Alternatively, we can use hive.repl.dump.skip.immutable.data.copy and skip the load ack file. This would enable users to reuse immutable repl dumps.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTables.java</file>
    </fixedFiles>
  </bug>
  <bug id="235" opendate="2009-1-16 00:00:00" fixdate="2009-2-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DynamicSerDe does not work with Thrift Protocols that can have missing fields for null values</summary>
      <description>The current DynamicSerDe code assumes all fields are there and no fields are missing.However Thrift Protocols can have missing fields, in case the field is null.In that case, DynamicSerDe may commit 2 behavior:1. array index out of bound error because DynamicSerDe assumes the number of fields in the record should be equal to that in the DDL;2. fields with null values will take the value from the last record. This may produce wrong result for queries.In order to fix this, we need to:1. Pass ObjectInspector/TypeInfo recursively so that we know the number of fields when deserializing the record.2. Clear out fields that are missing from the record.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="23585" opendate="2020-6-1 00:00:00" fixdate="2020-6-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Retrieve replication instance metrics details</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.strict.managed.tables.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.resourceplan.q.out</file>
      <file type="M">metastore.scripts.upgrade.hive.upgrade-3.1.0-to-4.0.0.hive.sql</file>
      <file type="M">metastore.scripts.upgrade.hive.hive-schema-4.0.0.hive.sql</file>
    </fixedFiles>
  </bug>
  <bug id="23617" opendate="2020-6-5 00:00:00" fixdate="2020-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix FindBug issues in storage-api</summary>
      <description>mvn test-compile findbugs:findbugs -pl storage-apimvn findbugs:gui</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">storage-api.src.test.org.apache.hive.common.util.TestMurmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.Murmur3.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomKFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hive.common.util.BloomFilter.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritableV1.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.serde2.io.HiveDecimalWritable.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.io.sarg.SearchArgumentImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.TimestampColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidReadTxnList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.ValidReaderWriteIdList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.RandomTypeUtil.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.HiveIntervalDayTime.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimalImpl.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.type.FastHiveDecimal.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.encoded.EncodedColumnBatch.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DiskRangeList.java</file>
      <file type="M">storage-api.src.java.org.apache.hadoop.hive.common.io.DataCache.java</file>
      <file type="M">Jenkinsfile</file>
    </fixedFiles>
  </bug>
  <bug id="23706" opendate="2020-6-17 00:00:00" fixdate="2020-6-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix nulls first sorting behavior</summary>
      <description>INSERT INTO t(a) VALUES (1), (null), (3), (2), (2), (2);select a from t order by a desc;instead of 3, 2, 2, 2, 1, nullshould return null, 3, 2 ,2 ,2, 1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.windowspec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.rank.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.range.multiorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.multipartitioning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.simple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf3.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">parser.src.java.org.apache.hadoop.hive.ql.parse.HiveParser.g</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.decimal.1.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.gen.udf.example.add10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hypothetical.set.aggregates.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.temp.table.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.grouping.sets.order.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.order.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.percentile.cont.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.udaf.percentile.disc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.numeric.overflows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.order.null.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="239" opendate="2009-1-17 00:00:00" fixdate="2009-1-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>alter table repalce columns allows adding of new columns with the same name as that of a partition column which corrupts metadata</summary>
      <description>there is no way for user to revert back and this check should be done in alter table function.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientnegative.invalid.create.tbl1.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Table.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2405" opendate="2011-8-24 00:00:00" fixdate="2011-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>get_privilege does not get user level privilege</summary>
      <description>hive&gt; set hive.security.authorization.enabled=true;hive&gt; grant all to user heyongqiang; hive&gt; show grant user heyongqiang; principalName heyongqiang principalType USER privilege All grantTime Wed Aug 24 11:51:54 PDT 2011 grantor heyongqiang Time taken: 0.032 secondshive&gt; CREATE TABLE src (foo INT, bar STRING); Authorization failed:No privilege 'Create' found for outputs { database:default}. Use show grant to get more details.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="2611" opendate="2011-11-28 00:00:00" fixdate="2011-12-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make index table output of create index command if index is table based</summary>
      <description>If an index is table based, when that index is created a table is created to contain that index. This should be listed in the output of the command.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.show.indexes.edge.cases.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ql.rewrite.gbtoidx.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.stale.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.creation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.binary.search.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.rc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.compression.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.bitmap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.update.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.unused.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.self.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.compact.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.mult.tables.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.multiple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.file.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auto.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.index.auth.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.index.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.merge.negative.1.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.size.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.compact.entry.limit.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.index.bitmap.no.map.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.alter.concatenate.indexed.table.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.DDLWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="2622" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive POMs reference the wrong Hadoop artifacts</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2691" opendate="2012-1-4 00:00:00" fixdate="2012-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Specify location of log4j configuration files via configuration properties</summary>
      <description>Oozie needs to be able to override the default location of the log4j configurationfiles from the Hive command line, e.g:hive -hiveconf hive.log4j.file=/home/carl/hive-log4j.properties -hiveconf hive.log4j.exec.file=/home/carl/hive-exec-log4j.properties</description>
      <version>None</version>
      <fixedVersion>0.11.0</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml.template</file>
      <file type="M">common.src.test.org.apache.hadoop.hive.conf.TestHiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.LogUtils.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
