<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="2619" opendate="2011-12-1 00:00:00" fixdate="2011-12-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add hook to run in metastore&amp;#39;s endFunction which can collect more fb303 counters</summary>
      <description>Create the potential for hooks to run in the endFunction method of HMSHandler which take the name of a function and whether or not it succeeded. Also, override getCounters from fb303 to allow these hooks to add counters which they collect, should this be desired. These hooks can be similar to EventListeners, but they should be more generic.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">conf.hive-default.xml</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="262" opendate="2009-1-30 00:00:00" fixdate="2009-2-30 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>outer join gets some duplicate rows in some scenarios</summary>
      <description>SELECT * FROM src src1 JOIN src src2 ON (src1.key = src2.key AND src1.key &lt; 10) RIGHT OUTER JOIN src src3 ON (src1.key = src3.key AND src3.key &lt; 20);returns duplicate rows for outer join</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.compiler.plan.join7.q.xml</file>
      <file type="M">ql.src.test.results.compiler.plan.join3.q.xml</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.JoinOperator.java</file>
      <file type="M">CHANGES.txt</file>
    </fixedFiles>
  </bug>
  <bug id="2622" opendate="2011-12-2 00:00:00" fixdate="2011-12-2 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive POMs reference the wrong Hadoop artifacts</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.ivy.xml</file>
      <file type="M">serde.ivy.xml</file>
      <file type="M">ql.ivy.xml</file>
      <file type="M">metastore.ivy.xml</file>
      <file type="M">hwi.ivy.xml</file>
      <file type="M">hbase-handler.ivy.xml</file>
      <file type="M">contrib.ivy.xml</file>
      <file type="M">common.ivy.xml</file>
      <file type="M">cli.ivy.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2629" opendate="2011-12-6 00:00:00" fixdate="2011-1-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make a single Hive binary work with both 0.20.x and 0.23.0</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.8.1,0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.SchemaAwareCompressionInputStream.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.thrift.DelegationTokenSelector23.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.thrift.DelegationTokenIdentifier23.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.DelegationTokenSelector.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.DelegationTokenIdentifier.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.java</file>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.ShimLoader.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.MemoryTokenStore.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenSelector.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenSecretManager.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.DelegationTokenIdentifier.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.23.java.org.apache.hadoop.hive.shims.Hadoop23Shims.java</file>
      <file type="M">shims.src.0.20S.java.org.apache.hadoop.hive.shims.Hadoop20SShims.java</file>
      <file type="M">shims.ivy.xml</file>
      <file type="M">shims.build.xml</file>
      <file type="M">build.properties</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2648" opendate="2011-12-12 00:00:00" fixdate="2011-12-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Parallel tests fail if master directory is not present</summary>
      <description>Parallel tests should create directories as needed.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest.hivetest.py</file>
    </fixedFiles>
  </bug>
  <bug id="2706" opendate="2012-1-10 00:00:00" fixdate="2012-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>StackOverflowError when using custom UDF after adding archive after adding jars</summary>
      <description>When a custom UDF is used in a query after add an archive, such as a zip file, after adding jars, the XMLEncoder enters an infinite loop when serializing the map reduce task, as part of sending it to be executed. This results in a stack overflow error.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.DeleteResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.processors.AddResourceProcessor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="2727" opendate="2012-1-18 00:00:00" fixdate="2012-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>add a testcase for partitioned view on union and base tables have index</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Test</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.IndexUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.IndexMetadataChangeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2749" opendate="2012-1-25 00:00:00" fixdate="2012-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CONV returns incorrect results sometimes</summary>
      <description>...because it fails to reset state.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.udf.conv.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf.conv.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFConv.java</file>
    </fixedFiles>
  </bug>
  <bug id="2750" opendate="2012-1-26 00:00:00" fixdate="2012-1-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive multi group by single reducer optimization causes invalid column reference error</summary>
      <description>After the optimization, if two query blocks have the same distinct clause and the same group by keys, but the first query block does not reference all the rows the second query block does, an invalid column reference error is raised for the columns unreferenced in the first query block.E.g.FROM srcINSERT OVERWRITE TABLE dest_g2 SELECT substr(src.key,1,1), count(DISTINCT src.key) WHERE substr(src.key,1,1) &gt;= 5 GROUP BY substr(src.key,1,1)INSERT OVERWRITE TABLE dest_g3 SELECT substr(src.key,1,1), count(DISTINCT src.key), count(src.value) WHERE substr(src.key,1,1) &lt; 5 GROUP BY substr(src.key,1,1);This results in an invalid column reference error on src.value</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="2764" opendate="2012-1-31 00:00:00" fixdate="2012-4-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Obtain delegation tokens for MR jobs in secure hbase setup</summary>
      <description>As discussed in HCATALOG-244, in a secure hbase setup with 0.92, we need to obtain delegation tokens for hbase and save it in jobconf, so that tasks can access region servers.</description>
      <version>None</version>
      <fixedVersion>0.9.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">shims.src.common.java.org.apache.hadoop.hive.shims.HadoopShims.java</file>
      <file type="M">shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.java</file>
      <file type="M">shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.MapredWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RCFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveNullValueSequenceFileOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExecDriver.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHFileOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableOutputFormat.java</file>
      <file type="M">hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
