<?xml version="1.0" encoding="UTF-8"?>

<bugrepository name="HIVE">
  <bug id="13782" opendate="2016-5-18 00:00:00" fixdate="2016-5-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Compile async query asynchronously</summary>
      <description>Currently, when an async query is submitted to HS2, HS2 does the preparation synchronously. One of the preparation step is to compile the query, which may take some time. It will be helpful to provide an option to do the compilation asynchronously.</description>
      <version>None</version>
      <fixedVersion>2.0.1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.java.org.apache.hive.service.cli.operation.SQLOperation.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="14737" opendate="2016-9-12 00:00:00" fixdate="2016-6-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Problem accessing /logs in a Kerberized Hive Server 2 Web UI</summary>
      <description>The /logs menu fails with error &amp;#91;1&amp;#93; when the cluster is Kerberized. Other menu items are working properly.&amp;#91;1&amp;#93; HTTP ERROR: 401Problem accessing /logs/. Reason: Unauthenticated users are not authorized to access this page.Powered by Jetty://</description>
      <version>1.1.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="20880" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update default value for hive.stats.filter.in.min.ratio</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="20881" opendate="2018-11-7 00:00:00" fixdate="2018-11-7 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation oversimplifies projections</summary>
      <description>create table cx2(bool1 boolean);insert into cx2 values (true),(false),(null);set hive.cbo.enable=true;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+--------+| _c0 |+--------+| true || false || NULL |+--------+set hive.cbo.enable=false;select bool1 IS TRUE OR (cast(NULL as boolean) AND bool1 IS NOT TRUE AND bool1 IS NOT FALSE) from cx2;+-------+| _c0 |+-------+| true || NULL || NULL |+-------+from explain it seems the expression was simplified to: (_col0 is true or null)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
    </fixedFiles>
  </bug>
  <bug id="209" opendate="2009-1-5 00:00:00" fixdate="2009-1-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable assertions for unit tests</summary>
      <description>We should enable assertions while running unit tests. Many assertions currently fail while running tests.</description>
      <version>None</version>
      <fixedVersion>0.3.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.src.test.org.apache.hadoop.hive.service.TestHiveServer.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDeFieldList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.typeinfo.StructTypeInfo.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">CHANGES.txt</file>
      <file type="M">build-common.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20934" opendate="2018-11-16 00:00:00" fixdate="2018-1-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Query based compactor for minor compaction</summary>
      <description>Follow up of HIVE-20699. This is to enable running minor compactions as a HiveQL query</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForOrcMmTable.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommandsForMmTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactorFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.QueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MmMajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.MajorQueryCompactor.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcSplit.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.SplitGrouper.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.TestCompactor.java</file>
    </fixedFiles>
  </bug>
  <bug id="20935" opendate="2018-11-16 00:00:00" fixdate="2018-11-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upload of llap package tarball fails in EC2 causing LLAP service start failure</summary>
      <description>Even though package dir is defined as below (with a / at the end) -LLAP_PACKAGE_DIR = ".yarn/package/LLAP/";copyLocalFileToHdfs API fails to create the dir hierarchy of .yarn/package/LLAP/ first and then copy the file under it. It instead uploads the file under .yarn/package with name LLAP.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cli.LlapSliderUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="20940" opendate="2018-11-19 00:00:00" fixdate="2018-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bridge cases in which Calcite&amp;#39;s type resolution is more stricter than Hive.</summary>
      <description>Calcite is more stricter w.r.t common types than Hive.For example in case a CASE with different types on the branches is not something Calcite likes:CASE WHEN cond1 THEN booleanCol WHEN cond2 THEN stringCol WHEN cond3 THEN floatCol ELSE doubleColENDissues will only happen with 1.18-SNAPSHOT version</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.kryo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.cond.pushdown.unqual5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.conversion.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
    </fixedFiles>
  </bug>
  <bug id="20951" opendate="2018-11-20 00:00:00" fixdate="2018-11-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Set Xms to 50% always</summary>
      <description>The lack of GC pauses is killing LLAP containers whenever the significant amount of memory is consumed by the off-heap structures which aren't cleaned up automatically until the GC runs.There's a java.nio.DirectByteBuffer.Deallocator which runs when the Direct buffers are garbage collected, which actually does the cleanup of the underlying off-heap buffers.The lack of Garbage collection activity for several hours while responding to queries triggers a build-up of these off-heap structures which end up forcing YARN to kill the process instead.It is better to hit a GC pause occasionally rather than to lose a node every few hours.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.bin.runLlapDaemon.sh</file>
    </fixedFiles>
  </bug>
  <bug id="20982" opendate="2018-11-29 00:00:00" fixdate="2018-12-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid the un-needed object creation within hotloop</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidScanQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.TaskReportData.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidVectorizedWrapper.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.conf.DruidConstants.java</file>
      <file type="M">druid-handler.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="20983" opendate="2018-11-29 00:00:00" fixdate="2018-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Vectorization: Scale up small hashtables, when collisions are detected</summary>
      <description>Hive's hashtable estimates are getting better with HyperLogLog stats in place, but an accurate estimate does not always result in a low number of collisions.The hashtables which contain a very small number of items tend to lose their O(1) lookup performance where there are collisions. Since collisions are easy to detect within the fast hashtable implementation, a rehashing to a higher size will help these small hashtables avoid collisions and go back to O(1) perf.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMultiSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMapNonMatched.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastLongHashMap.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.TestVectorMapJoinFastBytesHashMap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastLongHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashTable.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMultiSet.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.fast.VectorMapJoinFastBytesHashMap.java</file>
    </fixedFiles>
  </bug>
  <bug id="21001" opendate="2018-12-4 00:00:00" fixdate="2018-3-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade to calcite-1.19</summary>
      <description>XLEAR LIBRARY CACHE</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.offcbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isops.simplify.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.use.ts.stats.for.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.16.q.out.spark</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.row..id.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.mv.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.ext.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.ext.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.ppd.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.optimize.filter.literal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.navfn.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.varchar.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.date.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.case.when.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.adaptor.usage.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.ints.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.string.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.math.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.casts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.div0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedwork.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.join.ptp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lineage3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.kryo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.rollup.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.default.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketpruning1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.multiskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.filters.overlap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.join.preds.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.predicate.pushdown.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.ppd.key.range.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.pushdown.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveRelDecorrelator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.rules.HiveSubQueryRemoveRule.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.ASTConverter.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ppr.PartitionPruner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.test.queries.clientpositive.druidmini.expressions.q</file>
      <file type="M">ql.src.test.queries.clientpositive.join.constraints.optimization.q</file>
      <file type="M">ql.src.test.queries.clientpositive.subquery.multi.q</file>
      <file type="M">ql.src.test.results.clientpositive.allcolref.in.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alter.partition.coltype.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join.stats2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.simple.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.char.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constantPropWhen.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.when.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cte.mat.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.extractTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.floorTime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.test.ts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.join.unencrypted.tbl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.except.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.to.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21004" opendate="2018-12-4 00:00:00" fixdate="2018-12-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Less object creation for Hive Kafka reader</summary>
      <description>Reduce the amount of un-needed object allocation by using a row boat as way to carry data around.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.VectorizedKafkaRecordReader.java</file>
      <file type="M">kafka-handler.src.java.org.apache.hadoop.hive.kafka.KafkaSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="21007" opendate="2018-12-5 00:00:00" fixdate="2018-12-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Semi join + Union can lead to wrong plans</summary>
      <description>Tez compiler has the ability to push JOIN within UNION (by replicating join on each branch). If this JOIN had a SJ branch outgoing (or incoming) it could mess up the plan and end up generating incorrect or wrong plan.As a safe measure any SJ branch after UNION should be removed (until we improve the logic to better handle SJ branches)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="21029" opendate="2018-12-11 00:00:00" fixdate="2018-1-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>External table replication for existing deployments running incremental replication.</summary>
      <description>Existing deployments using hive replication do not get external tables replicated. For such deployments to enable external table replication they will have to provide a specific switch to first bootstrap external tables as part of hive incremental replication, following which the incremental replication will take care of further changes in external tables.The switch will be provided by an additional hive configuration (for ex: hive.repl.bootstrap.external.tables) and is to be used in WITH clause of REPL DUMP command. Additionally the existing hive config hive.repl.include.external.tables will always have to be set to "true" in the above clause. Proposed usage for enabling external tables replication on existing replication policy.1. Consider an ongoing repl policy &lt;db1&gt; in incremental phase.Enable hive.repl.include.external.tables=true and hive.repl.bootstrap.external.tables=true in next incremental REPL DUMP. Dumps all events but skips events related to external tables. Instead, combine bootstrap dump for all external tables under “_bootstrap” directory. Also, includes the data locations file "_external_tables_info”. LIMIT or TO clause shouldn’t be there to ensure the latest events are dumped before bootstrap dumping external tables.2. REPL LOAD on this dump applies all the events first, copies external tables data and then bootstrap external tables (metadata). It is possible that the external tables (metadata) are not point-in time consistent with rest of the tables. But, it would be eventually consistent when the next incremental load is applied. This REPL LOAD is fault tolerant and can be retried if failed.3. All future REPL DUMPs on this repl policy should set hive.repl.bootstrap.external.tables=false. If not set to false, then target might end up having inconsistent set of external tables as bootstrap wouldn’t clean-up any dropped external tables.</description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.io.TableSerializer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.HiveWrapper.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdateTableColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.UpdatePartColStatHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.InsertHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.CreateTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterTableHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AlterPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AddPartitionHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.events.AbstractConstraintEventHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ReplicationSemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.util.ReplUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadWork.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplExternalTables.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.load.table.LoadPartitions.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.FSTableEvent.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.DatabaseEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.ConstraintEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.bootstrap.events.filesystem.BootstrapEventsIterator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosIncrementalLoadAcidTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21055" opendate="2018-12-18 00:00:00" fixdate="2018-2-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>REPL LOAD command executing copy in serial mode even if parallel execution is enabled using WITH clause</summary>
      <description>For repl load command use can specify the execution mode as part of "with" clause. But the config for executing task in parallel or serial is not read from the command specific config. It is read from the hive server config. So even if user specifies to run the tasks in parallel during repl load command, the tasks are getting executed serially.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Task.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21073" opendate="2018-12-27 00:00:00" fixdate="2018-1-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove Extra String Object</summary>
      <description>public static String generatePath(Path baseURI, String filename) { String path = new String(baseURI + Path.SEPARATOR + filename); return path; } public static String generateFileName(Byte tag, String bigBucketFileName) { String fileName = new String("MapJoin-" + tag + "-" + bigBucketFileName + suffix); return fileName; }It's a bit odd to be performing string concatenation and then wrapping the results in a new string. This is creating superfluous String objects.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
    </fixedFiles>
  </bug>
  <bug id="21113" opendate="2019-1-10 00:00:00" fixdate="2019-1-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>For HPL/SQL that contains boolean expression with NOT, incorrect SQL may be generated.</summary>
      <description>In HPL/SQL, ' SELECT * FROM a WHERE NOT (1 = 2) ' will generate to incorrect SQL ' SELECT * FROM a WHERE (1 = 2) ', the 'NOT' in boolean expression is missing.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">hplsql.src.test.results.offline.select.out.txt</file>
      <file type="M">hplsql.src.test.queries.offline.select.sql</file>
      <file type="M">hplsql.src.main.java.org.apache.hive.hplsql.Expression.java</file>
    </fixedFiles>
  </bug>
  <bug id="21164" opendate="2019-1-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: explore how we can avoid a move step during inserts/compaction</summary>
      <description>Currently, we write compacted data to a temporary location and then move the files to a final location, which is an expensive operation on some cloud file systems. Since HIVE-20823 is already in, it can control the visibility of compacted data for the readers. Therefore, we can perhaps avoid writing data to a temporary location and directly write compacted data to the intended final path.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">streaming.src.test.org.apache.hive.streaming.TestStreaming.java</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.overwrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.encrypted.encryption.insert.partition.dynamic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.transactional.full.acid.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.subquery.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.TestWorker.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.txn.compactor.CompactorTest.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TxnCommandsBaseForTests.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnNoBuckets.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnLoadData.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnExIm.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnConcatenate.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands3.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnCommands.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.TestTxnAddPartition.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.lockmgr.TestDbTxnManager2.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestFileSinkOperator.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExecDriver.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.util.UpgradeTool.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.LoadTableDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.FileSinkDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.RecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.VectorizedOrcAcidRowBatchReader.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRecordUpdater.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidOutputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.AcidInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MoveTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FileSinkOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorTestUtil.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestAcidOnTez.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.history.TestHiveHistory.java</file>
      <file type="M">hcatalog.streaming.src.test.org.apache.hive.hcatalog.streaming.TestStreaming.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21167" opendate="2019-1-25 00:00:00" fixdate="2019-2-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Bucketing: Bucketing version 1 is incorrectly partitioning data</summary>
      <description>Using murmur hash for bucketing columns was introduced in HIVE-18910, following which 'bucketing_version'='1' stands for the old behaviour (where for example integer columns were partitioned based on mod values). Looks like we have a bug in the old bucketing scheme now. I could repro it when modified the existing schema using an alter table add column and adding new data. Repro:0: jdbc:hive2://localhost:10010&gt; create transactional table acid_ptn_bucket1 (a int, b int) partitioned by(ds string) clustered by (a) into 2 buckets stored as ORC TBLPROPERTIES('bucketing_version'='1', 'transactional'='true', 'transactional_properties'='default');No rows affected (0.418 seconds)0: jdbc:hive2://localhost:10010&gt; insert into acid_ptn_bucket1 partition (ds) values(1,2,'today'),(1,3,'today'),(1,4,'yesterday'),(2,2,'yesterday'),(2,3,'today'),(2,4,'today');6 rows affected (3.695 seconds)Data from ORC file (data as expected):/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00000{"operation": 0, "originalTransaction": 1, "bucket": 536870912, "rowId": 0, "currentTransaction": 1, "row": {"a": 2, "b": 4}}{"operation": 0, "originalTransaction": 1, "bucket": 536870912, "rowId": 1, "currentTransaction": 1, "row": {"a": 2, "b": 3}}/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000001_0000001_0000/bucket_00001{"operation": 0, "originalTransaction": 1, "bucket": 536936448, "rowId": 0, "currentTransaction": 1, "row": {"a": 1, "b": 3}}{"operation": 0, "originalTransaction": 1, "bucket": 536936448, "rowId": 1, "currentTransaction": 1, "row": {"a": 1, "b": 2}}Modifying table schema and inserting new data:0: jdbc:hive2://localhost:10010&gt; alter table acid_ptn_bucket1 add columns(c int);No rows affected (0.541 seconds)0: jdbc:hive2://localhost:10010&gt; insert into acid_ptn_bucket1 partition (ds) values(3,2,1000,'yesterday'),(3,3,1001,'today'),(3,4,1002,'yesterday'),(4,2,1003,'today'), (4,3,1004,'yesterday'),(4,4,1005,'today');6 rows affected (3.699 seconds)Data from ORC file (wrong partitioning):/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00000{"operation": 0, "originalTransaction": 3, "bucket": 536870912, "rowId": 0, "currentTransaction": 3, "row": {"a": 3, "b": 3, "c": 1001}}/apps/hive/warehouse/acid_ptn_bucket1/ds=today/delta_0000003_0000003_0000/bucket_00001{"operation": 0, "originalTransaction": 3, "bucket": 536936448, "rowId": 0, "currentTransaction": 3, "row": {"a": 4, "b": 4, "c": 1005}}{"operation": 0, "originalTransaction": 3, "bucket": 536936448, "rowId": 1, "currentTransaction": 3, "row": {"a": 4, "b": 2, "c": 1003}}As seen above, the expected behaviour is that new data with column 'a' being 3 should go to bucket1 and column 'a' being 4 should go to bucket0, but the partitioning is wrong.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.murmur.hash.migration.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.murmur.hash.migration.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2122" opendate="2011-4-21 00:00:00" fixdate="2011-8-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove usage of deprecated methods from org.apache.hadoop.io package</summary>
      <description>Serde code uses some deprecated methods from org.apache.hadoop.io. package. We should remove them.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.dynamic.type.DynamicSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.ByteStreamTypedSerDe.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.binarysortable.BinarySortableSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritableOutput.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesWritable.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.util.typedbytes.TypedBytesRecordWriter.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.TypedBytesSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.s3.S3LogDeserializer.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.fileformat.base64.Base64TextOutputFormat.java</file>
    </fixedFiles>
  </bug>
  <bug id="21240" opendate="2019-2-11 00:00:00" fixdate="2019-5-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JSON SerDe Re-Write</summary>
      <description>The JSON SerDe has a few issues, I will link them to this JIRA. Use Jackson Tree parser instead of manually parsing Added support for base-64 encoded data (the expected format when using JSON) Added support to skip blank lines (returns all columns as null values) Current JSON parser accepts, but does not apply, custom timestamp formats in most cases Added some unit tests Added cache for column-name to column-index searches, currently O&amp;#40;n&amp;#41; for each row processed, for each column in the row</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.json.HiveJsonStructReader.java</file>
      <file type="M">serde.src.java.org.apache.hadoop.hive.serde2.JsonSerDe.java</file>
      <file type="M">ql.src.test.results.clientpositive.kafka.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.kafka.storage.handler.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFJsonRead.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFJsonRead.java</file>
      <file type="M">hcatalog.core.src.main.java.org.apache.hive.hcatalog.data.JsonSerDe.java</file>
    </fixedFiles>
  </bug>
  <bug id="21399" opendate="2019-3-6 00:00:00" fixdate="2019-3-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Adjust hive.map.aggr.hash.min.reduction statically depending on group by statistics</summary>
      <description>Currently, the value is set statically from config variable. If stats are available, we could try to adjust this value at optimization time to favor turning off hash aggregation earlier.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.view.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.tablesample.rows.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.duplicate.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join.no.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.empty.where.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.delete.orig.table.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.parquet.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.nested.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.plan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.pos.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union.lateralview.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.parse.url.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.json.tuple.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udtf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.reverse.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.octet.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.isops.simplify.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.explode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.character.length.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.10.trims.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.number.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.binarysetfunctions.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udaf.binarysetfunctions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.vector.join.part.col.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.acid.vectorization.original.tez.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.temp.table.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tablevalues.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.symlink.text.input.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq.where.serialization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqual.corr.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notin.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.notexists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subq2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stat.estimate.drill.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.sizebug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.ppr.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.nonpart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats.empty.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.parquet.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.nested.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union.remove.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.union10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats.noscan.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.stats1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.use.op.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.in.process.launcher.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explain.groupbyshuffle.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.mapjoin.only.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.runtime.skewjoin.mapjoin.spark.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.dynamic.rdd.cache.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sort.merge.join.desc.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.smb.mapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.onesideskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.noskew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.mapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoinopt1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.setop.subq.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.semijoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample.islocalmode.hook.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.runtime.skewjoin.mapjoin.spark.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.reduce.deduplicate.extended2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rcfile.null.value.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.rand.partitionpruner2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quotedid.basic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.quote1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ptfgroupbyjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.filter.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.constant.expr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.position.alias.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partition.boolexpr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.partial.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.types.non.dictionary.encoding.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.order3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.orc.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullgroup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nullformatCTAS.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.notable.alias1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonreserved.keywords.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonblock.op.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.with.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.move.tasks.share.dependencies.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.mixed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multi.insert.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.multigroupby.singlemr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.mv.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.streaming.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.when.case.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.udf1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.string.concat.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reuse.scratchcols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.retry.failure.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce.groupby.duplicate.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.reduce.groupby.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.ptf.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partition.diff.num.cols.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.partitioned.date.time.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.reference.windowed.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orderby.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orc.nested.column.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.orc.merge.incompat.schema.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.number.compare.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.null.projection.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mr.diff.schema.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.complex.values.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.leftsemi.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.inner.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.include.no.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.grouping.sets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets3.dec.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.grouping.id1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.groupby.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.distinct.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.aggregate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.data.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.complex.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.coalesce.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.varchar.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.char.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.cast.constant.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.binary.join.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.between.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.without.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.aggregate.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.adaptor.usage.mode.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.parquet.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.multi.output.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.distinct.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.nested.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.input.format.excludes.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.top.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union.remove.26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.topnkey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.multiinsert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.group.by.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.dynamic.partition.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.reduce.side.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.main.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.smb.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.nway.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.join.hash.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.input.counters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dml.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sysdb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.null.agg.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.corr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats.only.external.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.stats11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sqlmerge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoinopt15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sharedworkext.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.reddedup.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.selectDistinctStar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample10.mm.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.sample1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.with.masking.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.transactional.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.temptable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.quoted.identifiers.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.lifetime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.invalidation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.empty.result.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.capacity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.results.cache.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.extended.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reduce.deduplicate.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.rcfile.createas1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.q93.with.constraints.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.multilevels.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partition.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.map.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parquet.complex.types.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.colstats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.parallel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.struct.type.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.predicate.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.ppd.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.incompat1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge.diff.fs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.optimize.join.ptp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.ppd.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.offset.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.insert.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multi.count.distinct.null.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.multiMapJoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mrr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.bhif.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mm.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadata.only.queries.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.metadataonly1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mergejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.merge2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.merge1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.ssb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.time.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.rebuild.dummy.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.create.rewrite.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapreduce1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.hint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.vector.nohybridgrace.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.ptf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.smb.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.partitioned.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llap.decimal64.reader.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.llapdecider.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.list.bucket.dml.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.limit.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.lateral.view.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.kryo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.reordering.no.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.jdbc.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.merge.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.intersect.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.values.orig.table.use.metadata.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into.default.keyword.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.rollup.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.resolution.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.except.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.enforce.constraint.notnull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.opt.vectorization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynpart.sort.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.user.level.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.pruning.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.join.noncbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.prod.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.count.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.correlationoptimizer1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.constraints.optimization.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.orc.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.column.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.columnStatsUpdateForStatsOptimizer.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.check.constraint.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.many.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketmapjoin1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.nulls.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.vectorization.original.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.query.oneskew.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lineage1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.limit.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.explode2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.lateral.view.cp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.keep.uniform.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.kafka.kafka.storage.handler.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.merge.multi.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join.grp.diff.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join28.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.varchar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.pointlook.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.char.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert.into1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insertoverwrite.bucket.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert2.overwrite.partitions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.insert1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testxpath.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.testsequencefile.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input3.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input39.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.innerjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.num.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.map.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.grouping.operators.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.infer.bucket.sort.dyn.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.hll.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.test.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.skew.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.1.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup.empty.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.rollup1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppd.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.position.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.single.reducer2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multi.insert.common.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.multialias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.map.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.join.pushdown.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.window.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets.grouping.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.sets1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.grouping.id3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.duplicate.key.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.distinct.samekey.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube.multi.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.cube1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.complex.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby8.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.multi.single.reducer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby7.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby6.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby5.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby4.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby3.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby2.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.skew.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.nomap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.map.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby1.limit.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.gby.star.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fp.literal.arithmetic.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.eq.with.case.when.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fold.case.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fm-sketch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.union.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown.HIVE.15647.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.cond.pushdown2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.aggr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.fetch.aggregation.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.rearrange.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.outputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.logical.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.explain.ddl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.exec.parallel.column.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.except.all.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.no.cbo.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.windowing.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.distinct.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.display.colstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.decimal.precision.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.uses.database.location.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.colname.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ctas.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cross.product.check.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.create.genericudaf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cp.sel.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.count.dist.rewrite.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlated.join.keys.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.type.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constGby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constant.prop.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.compute.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.complex.alias.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.comments.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.combine2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.tbllvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.quoting.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.dp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnStatsUpdateForStatsOptimizer.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.udaf.percentile.approx.23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.gby2.map.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.cross.product.check.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.auto.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cbo.rp.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.cast1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.case.sensitivity.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.spark1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket.map.join.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketsortoptimize.insert.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin.negative.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketmapjoin10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucketcontext.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.bucket1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binary.output.format.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.binarysortable.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.smb.mapjoin.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.explain.outputs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join22.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.inclause.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.multi.distinct.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join1.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.queries.q.out</file>
      <file type="M">accumulo-handler.src.test.results.positive.accumulo.single.sourced.multi.insert.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes2.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes3.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.serde.typedbytes5.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.avg.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.group.concat.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.max.n.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.q.out</file>
      <file type="M">contrib.src.test.results.clientpositive.udaf.example.min.n.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbasestats.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.queries.q.out</file>
      <file type="M">hbase-handler.src.test.results.positive.hbase.single.sourced.multi.insert.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.explain.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.into.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.insert.overwrite.table.q.out</file>
      <file type="M">itests.hive-blobstore.src.test.results.clientpositive.write.final.output.blobstore.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.GroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.HiveGBOpConvUtil.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.DynamicPartitionPruningOptimization.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.GroupByDesc.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.results.clientnegative.masking.mv.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.spark.spark.job.max.tasks.q.out</file>
      <file type="M">ql.src.test.results.clientnegative.spark.spark.stage.max.tasks.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.nullscan.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.table.stats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.alterColumnStatsPart.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.deep.filters.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.groupby2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.5a.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.9.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21416" opendate="2019-3-8 00:00:00" fixdate="2019-3-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log git apply tries with p0, p1, and p2</summary>
      <description>Currently when the PreCommit-HIVE-Build Jenkins job is trying to apply the patch it tries it first with -p0, then if it wasn't successful with -p1, then finally if it still wasn't successful with -p2. The 3 tries are not separated by anything, so the error messages of  the potential failures are mixed together. There should be a log message before each try.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.src.main.resources.smart-apply-patch.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21423" opendate="2019-3-11 00:00:00" fixdate="2019-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not check for whitespace issues in generated code</summary>
      <description></description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">dev-support.hive-personality.sh</file>
    </fixedFiles>
  </bug>
  <bug id="21424" opendate="2019-3-11 00:00:00" fixdate="2019-3-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable AggregateStatsCache by default</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.sample8.q.out</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.updateBasicStats.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.stats2.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.acid.stats4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.autoColumnStats.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.columnstats.partlvl.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.dynamic.partition.skip.default.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.extrapolate.part.stats.date.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.groupby.sort.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.list.bucket.dml.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.acid.no.buckets.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucketsortoptimize.insert.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.groupby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.semijoin.reduction.sw2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.no.join.opt.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.materialized.view.rewrite.part.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.orc.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.nonvec.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vecrow.part.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.schema.evol.text.vec.part.all.primitive.llap.io.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.load.dyn.part3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.nonmr.fetch.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.offset.limit.global.optimizer.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.outer.join.ppr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcr.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.pointlookup3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppr.allchildsarenull.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.regex.col.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21440" opendate="2019-3-13 00:00:00" fixdate="2019-3-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix test_teradatabinaryfile to not run into stackoverflows</summary>
      <description>this test seems to be failing in recent runs; taking a closer look shows that it might be some kryo related stackoverflowCaused by: java.lang.IllegalArgumentException: Unable to create serializer "org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer" for class: org.apache.hadoop.hive.ql.io.TeradataBinaryFileOutputFormat at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:67) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:45) at org.apache.hive.com.esotericsoftware.kryo.Kryo.newDefaultSerializer(Kryo.java:380) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getDefaultSerializer(Kryo.java:364) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.registerImplicit(DefaultClassResolver.java:74) at org.apache.hive.com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:490) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:97) at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:517) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:321) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.write(DefaultSerializers.java:314) at org.apache.hive.com.esotericsoftware.kryo.Kryo.writeObjectOrNull(Kryo.java:606) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:87) ... 104 moreCaused by: java.lang.reflect.InvocationTargetException at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) at java.lang.reflect.Constructor.newInstance(Constructor.java:423) at org.apache.hive.com.esotericsoftware.kryo.factories.ReflectionSerializerFactory.makeSerializer(ReflectionSerializerFactory.java:54) ... 115 moreCaused by: java.lang.StackOverflowError at java.util.HashMap.hash(HashMap.java:338) at java.util.HashMap.get(HashMap.java:556) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:61) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62) at org.apache.hive.com.esotericsoftware.kryo.Generics.getConcreteClass(Generics.java:62)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2146" opendate="2011-5-3 00:00:00" fixdate="2011-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Block Sampling should adjust number of reducers accordingly to make it useful</summary>
      <description>Now number of reducers of block sampling is not modified, so that queries like:select c from tab tablesample(1 percent) group by c;can generate huge number of reducers although the input is sampled to be small.We need to shrink number of reducers to make block sampling more useful.Since now number of reducers are determined before get splits, the way to do it probably is not clean enough, but we can do a good guess.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.MapRedTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="21460" opendate="2019-3-16 00:00:00" fixdate="2019-3-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>ACID: Load data followed by a select * query results in incorrect results</summary>
      <description>This affects current master as well. Created an orc file such that it spans multiple stripes and ran a simple select *, and got incorrect row counts (when comparing with select count. The problem seems to be that after split generation and creating min/max rowId for each row (note that since the loaded file is not written by Hive ACID, it does not have ROW_ID in the file; but the ROWID is applied on read by discovering min/max bounds which are used for calculating ROW_ID.rowId for each row of a split), Hive is only reading the last split.</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>3.1.1,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.java</file>
    </fixedFiles>
  </bug>
  <bug id="21487" opendate="2019-3-21 00:00:00" fixdate="2019-2-21 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>COMPLETED_COMPACTIONS and COMPACTION_QUEUE table missing appropriate indexes</summary>
      <description>Looking at a MySQL install where HMS is pointed on Hive 3.1, I see a constant stream of queries of the form:select CC_STATE from COMPLETED_COMPACTIONS where CC_DATABASE = 'tpcds_orc_exact_1000' and CC_TABLE = 'catalog_returns' and CC_PARTITION = 'cr_returned_date_sk=2452851' and CC_STATE != 'a' order by CC_ID desc;but the COMPLETED_COMPACTIONS table has no index. In this case it's resulting in a full table scan over 115k rows, which takes around 100ms.</description>
      <version>3.1.1</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.upgrade-3.2.0-to-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.postgres.hive-schema-4.0.0.postgres.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.upgrade-3.2.0-to-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.oracle.hive-schema-4.0.0.oracle.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.upgrade-3.2.0-to-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mysql.hive-schema-4.0.0.mysql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-3.2.0-to-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.hive-schema-4.0.0.mssql.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.upgrade-3.2.0-to-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.derby.hive-schema-4.0.0.derby.sql</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnDbUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21507" opendate="2019-3-26 00:00:00" fixdate="2019-3-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive swallows NPE if no delegation token found</summary>
      <description>In case if there is no delegation token put into token file, this line will cause a NullPointerException which is not handled and the user is not notified in any way.To cause NPE the use case is to have an Oozie Sqoop import to Hive in a kerberized cluster. Oozie puts the delegation token into the token file with id: HIVE_DELEGATION_TOKEN_hiveserver2ClientToken. So with id hive it is not working. However, fallback code uses the key which Oozie provides this way.I suggest to have warning message to user that key with id hive cannot be used and falling back to get delegation token from the session.I am creating the patch.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">jdbc.src.java.org.apache.hive.jdbc.HiveConnection.java</file>
    </fixedFiles>
  </bug>
  <bug id="21518" opendate="2019-3-27 00:00:00" fixdate="2019-4-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>GenericUDFOPNotEqualNS does not run in LLAP</summary>
      <description>GenericUDFOPNotEqualNS (Not equal nullsafe operator) does not run in LLAP mode, because it is not registered as a built-in function.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21540" opendate="2019-3-29 00:00:00" fixdate="2019-4-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Query with join condition having date literal throws SemanticException.</summary>
      <description>This semantic exception is thrown for the following query. SemanticException '2019-03-20' encountered with 0 childrencreate table date_1 (key int, dd date);create table date_2 (key int, dd date);select d1.key, d2.dd from( select key, dd as start_dd, current_date as end_dd from date_1) d1 join date_2 as d2 on d1.key = d2.key where d2.dd between start_dd and end_dd;When the WHERE condition below is commented out, the query completes successfully.where d2.dd between start_dd and end_dd------------------------------------------------</description>
      <version>3.1.0,3.1.1</version>
      <fixedVersion>3.1.2,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21541" opendate="2019-3-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix missing asf headers from HIVE-15406</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloatNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncFloat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimalNoScale.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDecimal.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.TruncDateFromDate.java</file>
    </fixedFiles>
  </bug>
  <bug id="21543" opendate="2019-3-29 00:00:00" fixdate="2019-2-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Use FilterHooks for show compactions</summary>
      <description>Use FilterHooks for checking dbs/tables/partitions for showCompactions</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestFilterHooks.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.utils.FilterUtils.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="21544" opendate="2019-3-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Constant propagation corrupts coalesce/case/when expressions during folding</summary>
      <description>set hive.fetch.task.conversion=none;set hive.optimize.ppd=false;create table t (s1 string,s2 string);insert into t values (null,null);explainselect coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;select coalesce(s1, 'null_value' ), coalesce(s2, 'null_value' ), coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ), case when coalesce(s1, 'null_value' )=coalesce(s2, 'null_value' ) then 'eq' else 'noteq' endfrom t;incorrect result is:null_value null_value NULL noteqexpected result:null_value null_value true eq</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21641" opendate="2019-4-22 00:00:00" fixdate="2019-11-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Llap external client returns decimal columns in different precision/scale as compared to beeline</summary>
      <description>Llap external client gives different precision/scale as compared to when the query is executed beeline. Consider the following results:Query: select avg(ss_ext_sales_price) my_avg from store_sales; Result from Beeline +----------------------------+| my_avg |+----------------------------+| 37.8923531030581611189434 |+----------------------------+ Result from Llap external client+---------+| my_avg|+---------+|37.892353|+---------+ This is due to Driver(beeline path) calls analyzeInternal() for getting result set schema which initializes resultSchema after some more transformations as compared to llap-ext-client which calls genLogicalPlan()Replacing genLogicalPlan() by analyze() resolves this.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.ParseUtils.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcGenericUDTFGetSplits.java</file>
    </fixedFiles>
  </bug>
  <bug id="21645" opendate="2019-4-24 00:00:00" fixdate="2019-4-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Include CBO json plan in explain formatted</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.plan.json.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.input4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.concat.op.q.out</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.ExplainTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21651" opendate="2019-4-26 00:00:00" fixdate="2019-4-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move protobuf serde into hive-exec.</summary>
      <description>The serde and input format is not accessible without doing an add jar or modifying hive aux libs. Moving it to hive-exec will let us use the serde. Can't move the serde to hive/serde since it depends on ProtobufMessageWriter which is in hive-exec.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.protobuf.HiveEvents.proto</file>
      <file type="M">ql.pom.xml</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.io.encoded.TestVectorDeserializeOrcWriter.java</file>
      <file type="M">contrib.src.test.org.apache.hadoop.hive.contrib.serde2.TestProtoMessageSerDe.java</file>
      <file type="M">contrib.src.protobuf-test.SampleProtos.proto</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufMessageSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.serde2.ProtobufBytesWritableSerDe.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.ProtobufMessageInputFormat.java</file>
      <file type="M">contrib.src.java.org.apache.hadoop.hive.contrib.input.package-info.java</file>
      <file type="M">contrib.src.gen-test.protobuf.gen-java.org.apache.hadoop.hive.contrib.serde2.SampleProtos.java</file>
      <file type="M">contrib.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21660" opendate="2019-4-29 00:00:00" fixdate="2019-3-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Wrong result when union all and later view with explode is used</summary>
      <description>There is a data loss when the data is inserted to a partitioned table using union all and lateral view with explode.  Steps to reproduce: create table t1 (id int, dt string);insert into t1 values (2, '2019-04-01');create table t2( id int, dates array&lt;string&gt;);insert into t2 select 1 as id, array('2019-01-01','2019-01-02','2019-01-03') as dates;create table dst (id int) partitioned by (dt string);set hive.exec.dynamic.partition.mode=nonstrict;set hive.exec.dynamic.partition=true;insert overwrite table dst partition (dt)select t.id, t.dt from (select id, dt from t1union allselect id, dts as dt from t2 tt2 lateral view explode(tt2.dates) dd as dts ) t;select * from dst;  Actual Result:+--------------+--------------+| 2 | 2019-04-01 |+--------------+--------------+ Expected Result (Run only the select part from the above insert query): +-------+------------+| 2     | 2019-04-01 || 1     | 2019-01-01 || 1     | 2019-01-02 || 1     | 2019-01-03 |+-------+------------+ Data retrieved using union all and lateral view with explode from second table is missing. </description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="21661" opendate="2019-4-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add option to check q file syntax before execution</summary>
      <description>Currently q file tests are executed line-by-line. This could lead to problems, e.g. when a q file has some long running queries and the last line contains a syntax error. In this case everything before the wrong line will be executed first, which could take a lot of time and only then will the test fail due to the syntax error.I propose a simple syntax checker that will check the q file for errors before executing any statements. This check can will be turned off by default and can be enabled with the following option: -Dtest.check.syntax=true</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
    </fixedFiles>
  </bug>
  <bug id="21663" opendate="2019-4-29 00:00:00" fixdate="2019-5-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Hive Metastore Translation Layer</summary>
      <description>This task is for the implementation of the default provider for translation, that is extensible if needed for a custom translator. Please refer the spec for additional details on the translation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.TableValidWriteIds.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClientPreCatalog.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.thrift.hive.metastore.thrift</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">standalone-metastore.metastore-common.src.main.java.org.apache.hadoop.hive.metastore.conf.MetastoreConf.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.types.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-rb.hive.metastore.constants.rb</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ttypes.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-py.hive.metastore.constants.py</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.Types.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-php.metastore.ThriftHiveMetastore.php</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WriteNotificationLogRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMValidateResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetTriggersForResourePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMGetAllResourcePlanResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.WMFullResourcePlan.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AbortTxnsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AddDynamicPartitions.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AllocateTableWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.AlterPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClearFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ClientCapabilities.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ColumnStatistics.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CommitTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.CompactionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FindSchemasByColsResp.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.FireEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Function.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetAllFunctionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataByExprResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetFileMetadataResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsInfoResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsByNamesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsFilterSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsProjectionSpec.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetPartitionsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTableRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetTablesResult.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.GetValidWriteIdsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.HeartbeatTxnRangeResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.hive.metastoreConstants.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.InsertEventRequestData.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.LockRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.NotificationEventResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.OpenTxnsResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Partition.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionSpecWithSharedSD.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PartitionWithoutSD.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.PutFileMetadataRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.RenamePartitionRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplLastIdInfo.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ReplTblWriteIdStateRequest.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.SchemaVersion.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowCompactResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ShowLocksResponse.java</file>
      <file type="M">standalone-metastore.metastore-common.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.Table.java</file>
    </fixedFiles>
  </bug>
  <bug id="21686" opendate="2019-5-3 00:00:00" fixdate="2019-5-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Brute Force eviction can lead to a random uncontrolled eviction pattern.</summary>
      <description>Current logic used by brute force eviction can lead to a perpetual random eviction pattern.For instance if the cache build a small pocket of free memory where the total size is greater than incoming allocation request, the allocator will randomly evict block that fits a particular size.This can happen over and over therefore all the eviction will be random.In Addition this random eviction will lead a leak in the linked list maintained by the policy since it does not know anymore about what is evicted and what not.The improvement of this patch is very substantial to TPC-DS benchmark. I have tested it with 10TB scale 9 llap nodes and 32GB cache size per node. The patch has showed very noticeable difference in the Hit rate for raw number Cache_hitrate_improvement.csv</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestOrcMetadataCache.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.MemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocatorMXBean.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21758" opendate="2019-5-20 00:00:00" fixdate="2019-5-20 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>DBInstall tests broken on master and branch-3.1</summary>
      <description>The Oracle and SqlServer install and upgrade tests in standalone-metastore fail in master and branch-3.1.  In the Oracle case it appears the docker container that was used no longer exists.  For SqlServer the cause of the failures is not immediately clear.</description>
      <version>3.1.1</version>
      <fixedVersion>3.1.2,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.dbinstall.ITestOracle.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.sql.mssql.upgrade-2.3.0-to-3.0.0.mssql.sql</file>
    </fixedFiles>
  </bug>
  <bug id="21777" opendate="2019-5-22 00:00:00" fixdate="2019-5-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Maven jar goal is producing warning due to missing dependency</summary>
      <description>org.apache.directory.client.ldap:ldap-client-directory is a test scope dependecy. Hive is using version 0.1 but 0.1-SNAPSHOT is also there as transitive dependency (omitted for collision with 0.1 which is already there on top level) causing warning in the maven default lifecycle execution:&amp;#91;WARNING&amp;#93; The POM for org.apache.directory.client.ldap:ldap-client-api:jar:0.1-SNAPSHOT is missing, no dependency information availableThe warning appears in the jar goal logs and it can easily be removed by excluding this transitive dependency. </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="21787" opendate="2019-5-24 00:00:00" fixdate="2019-6-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Metastore table cache LRU eviction</summary>
      <description>Metastore currently uses black/white list to specify patterns of tables to load into the cache. Cache is loaded in one shot "prewarm", and updated by a background thread. This is not a very efficient design. In this feature, we try to enhance the cache for Tables with LRU to improve cache utilization.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStore.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.SharedCache.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CacheUtils.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.metastore.cache.TestCachedStoreUpdateUsingEvents.java</file>
    </fixedFiles>
  </bug>
  <bug id="21791" opendate="2019-5-24 00:00:00" fixdate="2019-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Surrogate Key issue for insert with select with limit operations</summary>
      <description></description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21793" opendate="2019-5-24 00:00:00" fixdate="2019-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CBO retrieves column stats even if hive.stats.fetch.column.stats is set to false</summary>
      <description>When we go through CBO, we end up ignoring the flag value and we always retrieve column stats.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.views.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.select.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.notin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.leftsemijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.merging.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.cond.pushdown.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join.alt.syntax.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.lessSize.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.join32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.bucket.map.join.tez1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.sortmerge.join.6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.auto.join.without.localtask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query91.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query21.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query37.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query44.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query76.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query82.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.spark.query89.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21794" opendate="2019-5-24 00:00:00" fixdate="2019-5-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add materialized view parameters to sqlStdAuthSafeVarNameRegexes</summary>
      <description>&gt; set hive.materializedview.rewriting=false;Error: Error while processing statement: Cannot modify hive.materializedview.rewriting at runtime. It is not in list of params that are allowed to be modified at runtime (state=42000,code=1)</description>
      <version>None</version>
      <fixedVersion>3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.TestSQLStdHiveAccessControllerHS2.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21795" opendate="2019-5-27 00:00:00" fixdate="2019-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Rollup summary row might be missing when a mapjoin is happening on a partitioned table</summary>
      <description>join between 2 tables; the larger is partitioned mapjoin is selected dpp is sending events from the small table to the large on rollup: summary row is missing if dpp removes all input partitionsthe following should have a 1 row result.set hive.auto.convert.join=true;drop table if exists store_sales_s0;drop table if exists store_s0;CREATE TABLE store_sales_s0 (ss_item_sk int,payload string,payload2 string,payload3 string) PARTITIONED BY (ss_store_sk int) stored as orc TBLPROPERTIES( 'transactional'='false');CREATE TABLE store_s0 (s_item_sk int,s_store_sk int,s_state string) stored as orc TBLPROPERTIES( 'transactional'='false');insert into store_s0 values (1,10,'XX'), (2,20,'AA'), (3,30,'ZZ') ;insert into store_sales_s0 partition(ss_store_sk=9) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx');insert into store_sales_s0 partition(ss_store_sk=39) values (1,'xxx','xxx','xxx'),(2,'xxx','xxx','xxx'),(3,'xxx','xxx','xxx'),(4,'xxx','xxx','xxx'),(5,'xxx','xxx','xxx');explain select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state) order by s_state;select grouping(s_state) from store_s0, store_sales_s0 where ss_store_sk = s_store_sk and s_state in ('SD','FL', 'MI', 'LA', 'MO', 'SC') group by rollup(ss_item_sk, s_state) order by s_state;explain:STAGE PLANS: Stage: Stage-1 Tez#### A masked pattern was here #### Edges: Map 2 &lt;- Map 1 (BROADCAST_EDGE)[...]#### A masked pattern was here #### Vertices: Map 1 Map Operator Tree:[...] Dynamic Partitioning Event Operator Target column: ss_store_sk (int) Target Input: store_sales_s0 Partition key expr: ss_store_sk Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE Target Vertex: Map 2 Execution mode: vectorized, llap LLAP IO: all inputs[...] Map 2 Map Operator Tree: TableScan alias: store_sales_s0 filterExpr: ss_store_sk is not null (type: boolean) Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE Select Operator expressions: ss_item_sk (type: int), ss_store_sk (type: int) outputColumnNames: _col0, _col1 Statistics: Num rows: 10 Data size: 80 Basic stats: COMPLETE Column stats: COMPLETE Map Join Operator condition map: Inner Join 0 to 1 keys: 0 _col0 (type: int) 1 _col1 (type: int) outputColumnNames: _col1, _col2 input vertices: 0 Map 1 Statistics: Num rows: 10 Data size: 900 Basic stats: COMPLETE Column stats: COMPLETE[...] Execution mode: vectorized, llap LLAP IO: all inputs[...]</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DynamicPartitionPruner.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21832" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>New metrics to get the average queue/serving/response time</summary>
      <description>Simple DescriptiveStatistics with window size would do here. Time is not important in this case.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestLlapDaemonProtocolServerImpl.java</file>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.TestContainerRunnerImpl.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorInfo.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskRunnerCallable.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.TaskExecutorService.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.counters.WmFragmentCounters.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="21833" opendate="2019-6-4 00:00:00" fixdate="2019-6-4 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ranger Authorization in Hive based on object ownership</summary>
      <description>Background: Currently Hive Authorizer for Ranger does not provide owner information for Hive objects as part of AuthZ calls. This has resulted in gaps with respect to Sentry AuthZ and customers/partners cannot leverage privileges for owners in their authorization model. User Story: As an enterprise security admin, I need to be able to set privileges based on Hive object ownership for setting up access controls in Ranger so that I can provide appropriate protections and permissions for my enterprise users. Acceptance criteria:1) Owner information is available in Hive -Ranger AuthZ calls 2) Ranger admin users can use owner information to set policies based on object ownership in Ranger UI and APIs3) OWNER Macro based policies continue to work for Hive objects</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>New Feature</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.cache.CachedStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.Driver.java</file>
    </fixedFiles>
  </bug>
  <bug id="21834" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Avoid unnecessary calls to simplify filter conditions</summary>
      <description>Every time we create a filter, we try to simplify its condition. However, we already have a rule that simplifies the expressions and it is within the same loop as most of the rules that end up creating new filters. Hence, it may seem like we could remove some of the calls to simplify those conditions.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.external.jdbc.table.perf.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.HiveRelBuilder.java</file>
    </fixedFiles>
  </bug>
  <bug id="21836" opendate="2019-6-5 00:00:00" fixdate="2019-6-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Update apache directory server version to 1.5.7</summary>
      <description>I've bumped into some issues when downloading 1.5.6 artifacts...changing it to 1.5.7 worked fineit seems apacheds is only used during testing</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">service.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2192" opendate="2011-6-3 00:00:00" fixdate="2011-6-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Stats table schema incompatible after HIVE-2185</summary>
      <description>HIVE-2185 introduced a new column in the intermediate stats table. This introduces incompatibility between old and new branches (multiple branches could be deployed in production): the old branch will not work with the new schema, and the new branch will not work with the old schema. A solution would be to rename the stats table name (requires code change) or use a different database name (requires hive-default.xml conf change).</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.TestStatsPublisher.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsSetupConstants.java</file>
    </fixedFiles>
  </bug>
  <bug id="21927" opendate="2019-6-27 00:00:00" fixdate="2019-6-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer Web UI: Setting the HttpOnly option in the cookies</summary>
      <description>Intend of this JIRA is to introduce the HttpOnly option in the cookie.cookie: before changehdp32b FALSE / FALSE 0 JSESSIONID 8dkibwayfnrc4y4hvpu3vh74after change:#HttpOnly_hdp32b FALSE / FALSE 0 JSESSIONID e1npdkbo3inj1xnd6gdc6ihws</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21928" opendate="2019-6-27 00:00:00" fixdate="2019-7-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix for statistics annotation in nested AND expressions</summary>
      <description>Discovered while working on HIVE-21867. Having predicates with nested AND expressions may result in different stats, even if predicates are basically similar (from stats estimation standpoint).For instance, stats for AND(x=5, true, true) are different from x=5.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.llap.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.outer.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.outer.join4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.ppd.gby.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query59.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.12.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateStatsProcCtx.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.join29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.auto.sortmerge.join.9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.bucket.map.join.tez2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.convert.decimal64.to.decimal.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.filter.join.breaktask.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.mapjoin46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.orc.llap.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.semijoin7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.skewjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.smb.mapjoin.14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.having.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorization.short.regress.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.join46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.nested.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.shufflejoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.left.outer.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.nullsafe.join.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join0.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.outer.join2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.windowing.gby2.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="21968" opendate="2019-7-8 00:00:00" fixdate="2019-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove index related codes</summary>
      <description>Hive doesn't support indexes since 3.0.0, still some index related tests were left behind, and some code to disable them. Also some index related code is still in the codebase. They should be removed.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.cbo.2.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.cbo.1.q</file>
      <file type="M">ql.src.test.queries.clientpositive.ql.rewrite.gbtoidx.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.Operation2Privilege.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.TestDDLWithRemoteMetastoreSecondNamenode.java</file>
    </fixedFiles>
  </bug>
  <bug id="21972" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>"show transactions" display the header twice</summary>
      <description>show transactions;+-----------------+--------------------+----------------+----------------------+-------+-------------------+| txnid | state | startedtime | lastheartbeattime | user | host |+-----------------+--------------------+----------------+----------------------+-------+-------------------+| Transaction ID | Transaction State | Started Time | Last Heartbeat Time | User | Hostname || 896 | ABORTED | 1560209607000 | 1560209607000 | hive | hostname |+-----------------+--------------------+----------------+----------------------+-------+-------------------+</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.process.ShowTransactionsOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21973" opendate="2019-7-9 00:00:00" fixdate="2019-7-9 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>SHOW LOCKS prints the headers twice</summary>
      <description>show locks; &amp;#8211; output+----------+-----------+--------+------------+-------------+-------------+------------+-----------------+-----------------+--------------+-------+-----------+-------------+| lockid | database | table | partition | lock_state | blocked_by | lock_type | transaction_id | last_heartbeat | acquired_at | user | hostname | agent_info |+----------+-----------+--------+------------+-------------+-------------+------------+-----------------+-----------------+--------------+-------+-----------+-------------+| Lock ID | Database | Table | Partition | State | Blocked By | Type | Transaction ID | Last Heartbeat | Acquired At | User | Hostname | Agent Info |+----------+-----------+--------+------------+-------------+-------------+------------+-----------------+-----------------+--------------+-------+-----------+-------------+</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.lock.ShowLocksOperation.java</file>
    </fixedFiles>
  </bug>
  <bug id="21979" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestReplication tests time out regularily</summary>
      <description>I think we should add TestTableLevelReplicationScenarios and friends to be executed in isolationfrom a recent ptest execution:[INFO] Running org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 150.413 s - in org.apache.hadoop.hive.ql.TestCreateUdfEntities[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 32.084 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCleanerWithReplication[INFO] Running org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 61.062 s - in org.apache.hadoop.hive.ql.txn.compactor.TestCrudCompactorOnTez[INFO] Running org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 57.568 s - in org.apache.hadoop.hive.ql.TestWarehouseExternalDir[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 348.769 s - in org.apache.hadoop.hive.ql.parse.TestReplicationOfHiveStreaming[INFO] Running org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 50.089 s - in org.apache.hadoop.hive.ql.parse.TestExportImport[INFO] Running org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Tests run: 20, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1,044.666 s - in org.apache.hadoop.hive.ql.parse.TestTableLevelReplicationScenarios[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 225.734 s - in org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables[INFO] Running org.apache.hadoop.hive.ql.parse.TestReplAcrossInstancesWithJsonMessageFormat</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.conf.deployed.master-mr2.properties</file>
    </fixedFiles>
  </bug>
  <bug id="21986" opendate="2019-7-10 00:00:00" fixdate="2019-7-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>HiveServer Web UI: Setting the Strict-Transport-Security in default response header</summary>
      <description>Currently, HiveServer UI HTTP response header doesn't have Strict-Transport-Security set so will be adding this to default header.expected response after patch:HTTP/1.1 200 OKDate: Wed, 10 Jul 2019 22:47:34 GMTContent-Type: text/html;charset=utf-8Strict-Transport-Security: max-age=31536000; includeSubDomainsX-Content-Type-Options: nosniffX-FRAME-OPTIONS: SAMEORIGINX-XSS-Protection: 1; mode=blockSet-Cookie: JSESSIONID=fby9p6p5olb12xui7kj93uys;Path=/;HttpOnlyExpires: Thu, 01 Jan 1970 00:00:00 GMTContent-Length: 3824Server: Jetty(9.3.25.v20180904)</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.java.org.apache.hive.http.HttpServer.java</file>
    </fixedFiles>
  </bug>
  <bug id="21988" opendate="2019-7-11 00:00:00" fixdate="2019-7-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Do not consider nodes with 0 capacity when calculating host affinity</summary>
      <description>When a node is blacklisted (capacity set to 0) then we should not assign any more tasks to it</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestHostAffinitySplitLocationProvider.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.Utils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.HostAffinitySplitLocationProvider.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapZookeeperRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.LlapFixedRegistryImpl.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.llap.registry.impl.InactiveServiceInstance.java</file>
    </fixedFiles>
  </bug>
  <bug id="2199" opendate="2011-6-6 00:00:00" fixdate="2011-6-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>incorrect success flag passed to jobClose</summary>
      <description>For block level merging of RCFiles, jobClose is passed the incorrect variable as the success flag</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.rcfile.merge.BlockMergeTask.java</file>
    </fixedFiles>
  </bug>
  <bug id="22009" opendate="2019-7-18 00:00:00" fixdate="2019-7-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>CTLV with user specified location is not honoured</summary>
      <description>Steps to repro : CREATE TABLE emp_table (id int, name string, salary int);insert into emp_table values(1,'aaaaa',20000);CREATE VIEW emp_view AS SELECT * FROM emp_table WHERE salary&gt;10000;CREATE EXTERNAL TABLE emp_ext_table like emp_view LOCATION '/tmp/emp_ext_table';show create table emp_ext_table; +----------------------------------------------------+| createtab_stmt |+----------------------------------------------------+| CREATE EXTERNAL TABLE `emp_ext_table`( || `id` int, || `name` string, || `salary` int) || ROW FORMAT SERDE || 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' || STORED AS INPUTFORMAT || 'org.apache.hadoop.mapred.TextInputFormat' || OUTPUTFORMAT || 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' || LOCATION || 'hdfs://nn:8020/warehouse/tablespace/external/hive/emp_ext_table' || TBLPROPERTIES ( || 'bucketing_version'='2', || 'transient_lastDdlTime'='1563467962') |+----------------------------------------------------+Table Location is not '/tmp/emp_ext_table', instead location is set to default warehouse path.  </description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.ddl.table.creation.CreateTableLikeOperation.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hive.jdbc.TestJdbcWithMiniHS2.java</file>
    </fixedFiles>
  </bug>
  <bug id="22055" opendate="2019-7-26 00:00:00" fixdate="2019-9-26 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>select count gives incorrect result after loading data from text file</summary>
      <description>Add one more load to mm_loaddata.q:Load data 3 times (both kv1.txt and kv2.txt contains 500 records)create table load0_mm (key string, value string) stored as textfile tblproperties("transactional"="true", "transactional_properties"="insert_only");load data local inpath '../../data/files/kv1.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;load data local inpath '../../data/files/kv2.txt' into table load0_mm;select count(1) from load0_mm;Expected outputPREHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPREHOOK: type: LOAD#### A masked pattern was here ####PREHOOK: Output: default@load0_mmPOSTHOOK: query: load data local inpath '../../data/files/kv2.txt' into table load0_mmPOSTHOOK: type: LOAD#### A masked pattern was here ####POSTHOOK: Output: default@load0_mmPREHOOK: query: select count(1) from load0_mmPREHOOK: type: QUERYPREHOOK: Input: default@load0_mm#### A masked pattern was here ####POSTHOOK: query: select count(1) from load0_mmPOSTHOOK: type: QUERYPOSTHOOK: Input: default@load0_mm#### A masked pattern was here ####1500Got:&amp;#91;ERROR&amp;#93;   TestMiniLlapLocalCliDriver.testCliDriver:59 Client Execution succeeded but contained differences (error code = 1) after executing mm_loaddata.q63c63&lt; 1480—&gt; 1500 </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.io.encoded.SerDeEncodedDataReader.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22059" opendate="2019-7-29 00:00:00" fixdate="2019-9-29 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>hive-exec jar doesn&amp;#39;t contain (fasterxml) jackson library</summary>
      <description>While deploying master branch into a container I've noticed that the jackson libraries are not 100% sure that are available at runtime - this is probably due to the fact that we are still using the "old" codehaus jackson and also the "new" fasterxml one.]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1INFO : Completed executing command(queryId=vagrant_20190729141949_8d8c7f0d-0ac4-4d76-ba12-6ec01561b040); Time taken: 5.127 secondsINFO : Concurrency mode is disabled, not creating a lock managerError: Error while processing statement: FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Map 1, vertexId=vertex_1564408646590_0005_1_00, diagnostics=[Vertex vertex_1564408646590_0005_1_00 [Map 1] killed/failed due to:ROOT_INPUT_INIT_FAILURE, Vertex Input: _dummy_table initializer failed, vertex=vertex_1564408646590_0005_1_00 [Map 1], java.lang.NoClassDefFoundError: com/fasterxml/jackson/databind/ObjectMapperat org.apache.hadoop.hive.ql.exec.Utilities.&lt;clinit&gt;(Utilities.java:226)at org.apache.hadoop.hive.ql.io.HiveInputFormat.init(HiveInputFormat.java:428)at org.apache.hadoop.hive.ql.io.CombineHiveInputFormat.getSplits(CombineHiveInputFormat.java:508)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateOldSplits(MRInputHelpers.java:488)at org.apache.tez.mapreduce.hadoop.MRInputHelpers.generateInputSplitsToMem(MRInputHelpers.java:337)at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:122)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:278)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable$1.run(RootInputInitializerManager.java:269)at java.security.AccessController.doPrivileged(Native Method)at javax.security.auth.Subject.doAs(Subject.java:422)at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:269)at org.apache.tez.dag.app.dag.RootInputInitializerManager$InputInitializerCallable.call(RootInputInitializerManager.java:253)at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:108)at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:41)at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:77)at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)at java.lang.Thread.run(Thread.java:748)Caused by: java.lang.ClassNotFoundException: com.fasterxml.jackson.databind.ObjectMapperat java.net.URLClassLoader.findClass(URLClassLoader.java:382)at java.lang.ClassLoader.loadClass(ClassLoader.java:424)at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)at java.lang.ClassLoader.loadClass(ClassLoader.java:357)... 19 more]Vertex killed, vertexName=Reducer 2, vertexId=vertex_1564408646590_0005_1_01, diagnostics=[Vertex received Kill in INITED state., Vertex vertex_1564408646590_0005_1_01 [Reducer 2] killed/failed due to:OTHER_VERTEX_FAILURE]DAG did not succeed due to VERTEX_FAILURE. failedVertices:1 killedVertices:1 (state=08S01,code=2)</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22068" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Return the last event id dumped as repl status to avoid notification event missing error.</summary>
      <description>In repl load, update the status of target database to the last event dumped so that repl status returns that and next incremental can specify it as the event from which to start the dump. WIthout that repl status might return and old event which might cause, older events to be dumped again and/or a notification event missing error if the older events are cleaned by the cleaner.While at it Add more logging to DB notification listener cleaner thread The time when it considered cleaning, the interval and time before which events were cleared, the min and max id at that time how many events were cleared min and max id after the cleaning. In REPL::START document the starting event, end event if specified and the maximum number of events, if specified. *</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.ObjectStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.state.IncrementalDumpBegin.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.repl.dump.log.IncrementalDumpLogger.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplLoadTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.ReplDumpTask.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.repl.incremental.IncrementalLoadTasksBuilder.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.WarehouseInstance.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosExternalTables.java</file>
      <file type="M">itests.hive-unit.src.test.java.org.apache.hadoop.hive.ql.parse.TestReplicationScenariosAcidTablesBootstrap.java</file>
    </fixedFiles>
  </bug>
  <bug id="22074" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Slow compilation due to IN to OR transformation</summary>
      <description>Currently Hive transform IN expressions to OR to apply various CBO rules. This incur significant performance hit if IN consist of large number of expressions. It is better to not transform IN expressions to OR in such cases because overall benefit of various optimizations/transformations is unrealized due to the compilation overhead</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.in.typecheck.char.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.in.typecheck.char.q</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.CalcitePlanner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22075" opendate="2019-8-1 00:00:00" fixdate="2019-8-1 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix the max-reducers=1 regression from HIVE-14200</summary>
      <description>The condition does not kick in when minPartition=1, maxPartition=1, nReducers=1, maxReducers=1</description>
      <version>3.1.1,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LlapDecider.java</file>
    </fixedFiles>
  </bug>
  <bug id="22089" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jackson to 2.9.9</summary>
      <description></description>
      <version>3.1.0,3.0.0,3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22090" opendate="2019-8-8 00:00:00" fixdate="2019-8-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade jetty to 9.3.27</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22118" opendate="2019-8-15 00:00:00" fixdate="2019-10-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Log the table name while skipping the compaction because it&amp;#39;s sorted table/partitions</summary>
      <description>for debugging perspective it's good if we log the full table name while skipping the table for compaction otherwise it's tedious to know why the compaction is not happening for the target table.</description>
      <version>3.1.1</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.java</file>
    </fixedFiles>
  </bug>
  <bug id="22151" opendate="2019-8-27 00:00:00" fixdate="2019-8-27 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off hybrid grace hash join by default</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.tez.explainuser.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.explainanalyze.3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.unionDistinct.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.vector.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.tez.dynpart.hashjoin.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.partialdhj.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.join.max.hashtable.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.dynamic.partition.join.noncbo.q.out</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22170" opendate="2019-9-5 00:00:00" fixdate="2019-9-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>from_unixtime and unix_timestamp should use user session time zone</summary>
      <description>According to documentation, that is the expected behavior (since session time zone was not present, system time zone was being used previously). This was incorrectly changed by HIVE-12192 / HIVE-20007. This JIRA should fix this issue.</description>
      <version>3.1.0,3.1.1,3.1.2,3.2.0,4.0.0</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorGenericDateExpressions.java</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.to.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.from.unixtime.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf.folder.constants.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.udf5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.timestamp.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.timestamp.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vectorized.date.funcs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.current.date.timestamp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.foldts.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.date.udf.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.beeline.udf.unix.timestamp.q.out</file>
      <file type="M">ql.src.test.queries.clientpositive.udf5.q</file>
      <file type="M">ql.src.test.queries.clientpositive.foldts.q</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.udf.generic.TestGenericUDFToUnixTimestamp.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTypeCasts.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExtract.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorTimestampExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStructField.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorStringConcat.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNull.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorNegative.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorMathFunctions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorIndex.java</file>
      <file type="M">ql.src.gen.vectorization.ExpressionTemplates.FilterColumnBetweenDynamicValue.txt</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.AbstractFilterStringColLikeStringScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastCharToBinary.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastDecimalToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastStringToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.CastTimestampToLong.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.DecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDecimalColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterDoubleColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterLongColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FilterTimestampColumnInList.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.FuncLongToString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorInBloomFilterColDynamicValue.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddColScalar.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateAddScalarCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFDateDiffColCol.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFTimestampFieldTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampDate.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampTimestamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkCommonOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.reducesink.VectorReduceSinkObjectHashOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorFilterOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorGroupByOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSelectOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorSMBMapJoinOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorTopNKeyOperator.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.UDFFromUnixTime.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.aggregation.AggregationBase.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorArithmetic.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorBetweenIn.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCastStatement.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorCoalesceElt.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateAddSub.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateDiff.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorDateExpressions.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterCompare.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.vector.expressions.TestVectorFilterExpressions.java</file>
    </fixedFiles>
  </bug>
  <bug id="22175" opendate="2019-9-6 00:00:00" fixdate="2019-10-6 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>TestBudyAllocator#testMTT test is flaky</summary>
      <description>This test has a fail rate of about 20%-25%</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.cache.TestBuddyAllocator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.cache.BuddyAllocator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22236" opendate="2019-9-24 00:00:00" fixdate="2019-9-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fail to create View selecting View containing NOT IN subquery</summary>
      <description>Given a complicated view with a select statement that has subquery containing "NOT IN" Hive fails to create a simple view as SELECT * FROM complicated_view (with CBO disabled).The unparse replacements of the complicated view will be applied to the text of the simple view, resulting in IllegalArgumentException: replace: range invalid exceptions from org.antlr.runtime.TokenRewriteStream.replace.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.parse.SubQueryUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22238" opendate="2019-9-24 00:00:00" fixdate="2019-11-24 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>PK/FK selectivity estimation underscales estimations</summary>
      <description>at this point the parent operators rownum is scaled according to pkfkselectivityhowever pkfkselectivity is computed on a whole subtree.Scaling it by that amount will count in estimation already used when parentstats was calculated...so depending on the number of upstream joins - this may lead to severe underestimationswhat happened was: optimization was able to push the filter to the other side of the join as a result the incoming data was already filtered scaling down by the PK selectiviy - was actually already there...but a new "scaling" happened</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.tez.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.unqualcolumnrefs.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.spark.spark.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query30.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query3.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query29.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query24.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query99.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query98.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query97.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query96.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query95.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query94.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query93.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query92.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query91.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query90.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query89.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query88.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query87.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query86.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query85.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query84.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query83.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query81.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query80.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query8.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query79.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query78.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query77.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query75.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query74.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query73.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query72.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query71.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query70.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query7.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query69.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query68.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query67.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query66.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query65.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query64.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query63.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query61.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query60.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query6.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query58.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query57.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query56.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query55.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query53.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query52.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query51.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query50.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query5.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query49.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query48.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query46.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query43.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query42.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query40.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query4.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query38.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query36.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query35.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query34.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query32.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query31.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query30.q.out</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.OperatorUtils.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.plan.ColStatistics.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.java</file>
      <file type="M">ql.src.test.queries.clientpositive.stat.estimate.drill.q</file>
      <file type="M">ql.src.test.results.clientpositive.annotate.stats.join.pkfk.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.auto.join33.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.constprog.partitioner.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.correlationoptimizer9.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join45.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.join47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.explainuser.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.groupby.groupingset.bug.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.hybridgrace.hashjoin.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.dpp.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.reopt.semijoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.retry.failure.reorder.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.exists.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.in.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.multi.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.subquery.scalar.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.interval.mapjoin.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.mapjoin.reduce.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.mapjoin47.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.masking.disablecbo.2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.cbo.query54.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query10.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query11.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query12.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query13.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query14.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query15.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query16.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query17.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query18.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query19.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query1b.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query2.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query20.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query23.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query25.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query26.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query27.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.perf.tez.constraints.query29.q.out</file>
    </fixedFiles>
  </bug>
  <bug id="2224" opendate="2011-6-15 00:00:00" fixdate="2011-7-15 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Ability to add partitions atomically</summary>
      <description>I'd like to see an atomic version of the add_partitions() call.Whether this is to be done by config to affect add_partitions() behaviour (not my preference) or just changing add_partitions() default behaviour (my preference, but likely to affect current behaviour, so will need others' input) or by making a new add_partitions_atomic() call depends on discussion.This looks relatively doable to implement (will need a dependent add_partition_core to not do a ms.commit_partition() early, and to cache list of directories created to remove on rollback, and a list of AddPartitionEvent to trigger in one shot later)Thoughts? This also seems like something to implement for allowing HIVE-1805.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.IMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-rb.thrift.hive.metastore.rb</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore.py</file>
      <file type="M">metastore.src.gen.thrift.gen-py.hive.metastore.ThriftHiveMetastore-remote</file>
      <file type="M">metastore.src.gen.thrift.gen-php.hive.metastore.ThriftHiveMetastore.php</file>
      <file type="M">metastore.src.gen.thrift.gen-javabean.org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore.java</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.server.skeleton.cpp</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.h</file>
      <file type="M">metastore.src.gen.thrift.gen-cpp.ThriftHiveMetastore.cpp</file>
      <file type="M">metastore.if.hive.metastore.thrift</file>
    </fixedFiles>
  </bug>
  <bug id="22241" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Implement UDF to interpret date/timestamp using its internal representation and Gregorian-Julian hybrid calendar</summary>
      <description>UDF that converts a date/timestamp to new proleptic Gregorian calendar (ISO 8601 standard), which is produced by extending the Gregorian calendar backward to dates preceding its official introduction in 1582, assuming that its internal days/milliseconds since epoch is calculated using legacy Gregorian-Julian hybrid calendar, i.e., calendar that supports both the Julian and Gregorian calendar systems with the support of a single discontinuity, which corresponds by default to the Gregorian date when the Gregorian calendar was instituted.</description>
      <version>None</version>
      <fixedVersion>3.1.3,3.2.0,4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.show.functions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.FunctionRegistry.java</file>
    </fixedFiles>
  </bug>
  <bug id="22242" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Move TempTable and PartitionTree out of SessionHiveMetastoreClient</summary>
      <description>SessionHiveMetastoreClient is getting too crowded and unreadable. TempTable and PartitionTree should moved into a separate class.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Sub-task</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.java</file>
    </fixedFiles>
  </bug>
  <bug id="22243" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Align Apache Thrift version to 0.9.3-1 in standalone-metastore as well</summary>
      <description>Thrift was bumped to 0.9.3-1 in HIVE-21173, but standalone-metastore was left out of this.Thanks for pointing this out Ashutosh Bapat!</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22244" opendate="2019-9-25 00:00:00" fixdate="2019-9-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Added default ACLs for znodes on a non-kerberized cluster</summary>
      <description>Set default ACLs for znodes on a non-kerberized cluster: Create/Read/Delete/Write/Admin to the world</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
    </fixedFiles>
  </bug>
  <bug id="22245" opendate="2019-9-25 00:00:00" fixdate="2019-10-25 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Make qtest feature parser reuseable</summary>
      <description>right now we have a parser for --! qt:dataset ; to enable further addition of things (I would like to run scheduled query service for some qtests )</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">itests.util.src.test.java.org.apache.hadoop.hive.ql.dataset.TestDatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.QTestUtil.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.QTestDatasetHandler.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetParser.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.ql.dataset.DatasetCollection.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreCompareCliDriver.java</file>
      <file type="M">itests.util.src.main.java.org.apache.hadoop.hive.cli.control.CoreBeeLineDriver.java</file>
    </fixedFiles>
  </bug>
  <bug id="22327" opendate="2019-10-10 00:00:00" fixdate="2019-12-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Repl: Ignore read-only transactions in notification log</summary>
      <description>Read txns need not be replicated.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.OpenTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.CommitTxnEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AllocWriteIdEvent.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.events.AbortTxnEvent.java</file>
      <file type="M">ql.src.test.org.apache.hadoop.hive.metastore.txn.TestTxnHandler.java</file>
      <file type="M">itests.hcatalog-unit.src.test.java.org.apache.hive.hcatalog.listener.TestDbNotificationListener.java</file>
      <file type="M">hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="22328" opendate="2019-10-11 00:00:00" fixdate="2019-10-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Min value for column in stats is not set correctly for some data types in partitioned tables</summary>
      <description>This is a follow up Jira for HIVE-22248. For partitioned tables the statistics aggregation happens at in the *ColumnStatsAggregator classes instead of the *ColumnStatsMerger classes, and they still fail to handle the unset low values correctly. Beside that they need to be fixed the two classes should use the same codes for merging statistics.</description>
      <version>None</version>
      <fixedVersion>None</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.LongColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DoubleColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DecimalColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.merge.DateColumnStatsMerger.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.LongColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DoubleColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DecimalColumnStatsAggregator.java</file>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.columnstats.aggr.DateColumnStatsAggregator.java</file>
    </fixedFiles>
  </bug>
  <bug id="22354" opendate="2019-10-16 00:00:00" fixdate="2019-10-16 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP status driver may look for worker registration on &amp;#39;unsecure&amp;#39; ZK nodes</summary>
      <description>HIVE-22195 introduced a change in determining secure/unsecure environments:public static boolean isKerberosEnabled(Configuration conf) { try { return UserGroupInformation.getLoginUser().isFromKeytab() &amp;&amp; HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_ZOOKEEPER_USE_KERBEROS); } catch (IOException e) { return false; }} This won't work for cases where the JVM process was started after kinit (e.g. in a launcher shell script), where Kerberos authentication is not 'fromKeytab' but rather 'fromTicket' - it will return false even if we have a successfully authenticated principal.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">standalone-metastore.metastore-server.src.main.java.org.apache.hadoop.hive.metastore.security.ZooKeeperTokenStore.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.java</file>
      <file type="M">llap-client.src.test.org.apache.hadoop.hive.registry.impl.TestZookeeperUtils.java</file>
      <file type="M">llap-client.src.java.org.apache.hadoop.hive.registry.impl.ZookeeperUtils.java</file>
    </fixedFiles>
  </bug>
  <bug id="22510" opendate="2019-11-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Support decimal64 operations for column operands with different scales</summary>
      <description>Right now, if the operands on the decimal64 operations are columns with different scales, then we do not use the decimal64 vectorized version and fall back to HiveDecimal vectorized version of the operator. In this Jira, we will check if we can use decimal64 vectorized version, even if the scales are different.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">vector-code-gen.src.org.apache.hadoop.hive.tools.GenVectorCode.java</file>
      <file type="M">ql.src.test.results.clientpositive.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.llap.vector.decimal.expressions.q.out</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.java</file>
      <file type="M">itests.src.test.resources.testconfiguration.properties</file>
    </fixedFiles>
  </bug>
  <bug id="22511" opendate="2019-11-19 00:00:00" fixdate="2019-11-19 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix case of Month token in datetime to string conversion</summary>
      <description>Currently Hive doesn't allow month tokens with weird spelling like 'MONth', 'mONTH' etc. However, Oracle does and Hive should follow that approach.The rules: If the first letter is lowercase then the output is lowercase: 'mONTH' -&gt; 'may' If the first two letters are uppercase then the output is uppercase: 'MOnth' -&gt; 'MAY' If the first letter is uppercase and the second is lowercase then the output is capitalized: 'Month' -&gt; 'May'.Oracle:select to_char(to_timestamp('2019-05-10', 'YYYY-MM-DD'), 'MOnthYYYY') from DUAL;MAY 2019select to_char(to_timestamp('2019-05-10', 'YYYY-MM-DD'), 'mONTHYYYY') from DUAL;may 2019select to_char(to_timestamp('2019-05-10', 'YYYY-MM-DD'), 'MoNTHYYYY') from DUAL;May 2019Please check the same for 'Name of the day' tokens.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="2257" opendate="2011-7-5 00:00:00" fixdate="2011-7-5 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Enable TestHadoop20SAuthBridge</summary>
      <description>Looks like this test was accidentally disabled in HIVE-818.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">build.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22635" opendate="2019-12-12 00:00:00" fixdate="2019-1-12 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Disable scheduled query executor for unittests</summary>
      <description>HIVE-21884 missed to set the default to off; so it may sometime interfere with unit tests</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">data.conf.hive-site.xml</file>
    </fixedFiles>
  </bug>
  <bug id="22659" opendate="2019-12-18 00:00:00" fixdate="2019-1-18 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>JClouds needs to be updated to 2.1.3 in ptest</summary>
      <description>Since a couple of days ptest responded 404 to test queries coming in from jenkins side.I took a look into the issue and saw this exception on hiveptest-server-upstream side:Caused by: java.lang.IllegalStateException: zone https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-d not present in [https://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-east2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-northeast2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-south1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/asia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/australia-southeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-north1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west3-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/europe-west6-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/northamerica-northeast1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/southamerica-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-central1-fhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east1-dhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-east4-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-ahttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west1-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-chttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-bhttps://www.googleapis.com/compute/v1/projects/gcp-hive-upstream/zones/us-west2-a] Debugging into the issue, it seems like that when we we put our cloud context in place for test execution, some default values originating from default templates are matched with actual GCP capabilities - and I guess zone us-central-1d was decommissioned in real-life, hence the exception.I upgraded jclouds version from 2.1.0 to 2.1.3 on server side and retried running Tomcat with this new installation. It seems to have fixed the issue. NO PRECOMMIT TESTS </description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">testutils.ptest2.pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="2276" opendate="2011-7-8 00:00:00" fixdate="2011-7-8 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Fix Inconsistency between RB and JIRA patches for HIVE-2194</summary>
      <description>The RB and JIRA patches for HIVE-2194 were out of sync. An outdated patch for HIVE-2194 was committed. This patch updates that patch to include the changes from RB.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestMetaStoreEventListener.java</file>
    </fixedFiles>
  </bug>
  <bug id="22761" opendate="2020-1-22 00:00:00" fixdate="2020-1-22 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor fails to report query state as errored if session initialization fails</summary>
      <description>right now the info object is only initialized after the sessionstate is inited - which might get into trouble...</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="22762" opendate="2020-1-23 00:00:00" fixdate="2020-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Leap day is incorrectly parsed during cast in Hive</summary>
      <description>While casting a string to a date with a custom date format having day token before year and moth tokens, the date is parsed incorrectly for leap days.How to reproduceExecute select cast("29 02 0" as date format "dd mm rr") with Hive. The query results in 2020-02-28, incorrectly.Executing the another cast with a slightly modified representation of the date (day is preceded by year and moth) is however correctly parsed:select cast("0 02 29" as date format "rr mm dd")It returns 2020-02-29.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22763" opendate="2020-1-23 00:00:00" fixdate="2020-3-23 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>0 is accepted in 12-hour format during timestamp cast</summary>
      <description>Having a timestamp string in 12-hour format can be parsed if the hour is 0, however, based on the design document, it should be rejected.How to reproduceRun select cast("2020-01-01 0 am 00" as timestamp format "yyyy-mm-dd hh12 p.m. ss")It shouldn' t be parsed, as the hour component is 0.SpecPatternMeaningAdditional detailsHH12Hour of day (1-12)Same as HHHHHour of day (1-12) One digit inputs are possible in a string to datetime conversion but needs to be surrounded by separators. In a datetime to string conversion one digit hours are prefixed with a zero. Error if provided hour is not between 1 and 12. Displaying an unformatted timestamp in Impala uses the HH24 format regardless if it was created using HH12. If no AM/PM provided then defaults to AM. In string to datetime conversion, conflicts with SSSSS and HH24.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">common.src.test.org.apache.hadoop.hive.common.format.datetime.TestHiveSqlDateTimeFormatter.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.common.format.datetime.HiveSqlDateTimeFormatter.java</file>
    </fixedFiles>
  </bug>
  <bug id="22964" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>MM table split computation is very slow</summary>
      <description>Since for MM table we process the paths prior to inputFormat.getSplits() we end up doing listing on the whole table at once. This could be optimized.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.io.HiveInputFormat.java</file>
      <file type="M">common.src.java.org.apache.hadoop.hive.conf.HiveConf.java</file>
    </fixedFiles>
  </bug>
  <bug id="22966" opendate="2020-3-3 00:00:00" fixdate="2020-3-3 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>LLAP: Consider including waitTime for comparing attempts in same vertex</summary>
      <description>When attempts are compared within same vertex, it should pick up the attempt with longest wait time to avoid starvation.</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Improvement</type>
    </buginformation>
    <fixedFiles>
      <file type="M">llap-server.src.test.org.apache.hadoop.hive.llap.daemon.impl.comparator.TestShortestJobFirstComparator.java</file>
      <file type="M">llap-server.src.java.org.apache.hadoop.hive.llap.daemon.impl.comparator.ShortestJobFirstComparator.java</file>
    </fixedFiles>
  </bug>
  <bug id="23035" opendate="2020-3-17 00:00:00" fixdate="2020-3-17 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Scheduled query executor may hang in case TezAMs are launched on-demand</summary>
      <description>Right now the schq executor hangs during session initialization - because it tries to open the tez session while it initializes the SessionState</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.scheduled.ScheduledQueryExecutionService.java</file>
    </fixedFiles>
  </bug>
  <bug id="23178" opendate="2020-4-10 00:00:00" fixdate="2020-5-10 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Add Tez Total Order Partitioner</summary>
      <description></description>
      <version>3.1.0,3.1.1,3.1.2</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.org.apache.hadoop.hive.ql.exec.tez.TestTezTotalOrderPartitioner.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTotalOrderPartitioner.java</file>
    </fixedFiles>
  </bug>
  <bug id="23181" opendate="2020-4-11 00:00:00" fixdate="2020-4-11 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Remove snakeyaml lib from Hive distribution</summary>
      <description></description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.pom.xml</file>
      <file type="M">pom.xml</file>
    </fixedFiles>
  </bug>
  <bug id="23184" opendate="2020-4-13 00:00:00" fixdate="2020-5-13 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Upgrade druid to 0.17.1</summary>
      <description>Upgrade to druid latest release 0.17.1</description>
      <version>None</version>
      <fixedVersion>4.0.0-alpha-1</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.test.results.clientpositive.druid.druidmini.semijoin.reduction.all.types.q.out</file>
      <file type="M">ql.src.test.results.clientpositive.druid.druidkafkamini.delimited.q.out</file>
      <file type="M">pom.xml</file>
      <file type="M">itests.qtest-druid.pom.xml</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.ql.io.TestDruidRecordWriter.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestHiveDruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.TestDruidStorageHandler.java</file>
      <file type="M">druid-handler.src.test.org.apache.hadoop.hive.druid.serde.TestDruidSerDe.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidSelectQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.serde.DruidQueryRecordReader.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorTuningConfig.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.json.KafkaSupervisorSpec.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidRecordWriter.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidQueryBasedInputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.io.DruidOutputFormat.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandlerUtils.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidStorageHandler.java</file>
      <file type="M">druid-handler.src.java.org.apache.hadoop.hive.druid.DruidKafkaUtils.java</file>
      <file type="M">druid-handler.pom.xml</file>
      <file type="M">data.scripts.kafka.init.data.csv</file>
    </fixedFiles>
  </bug>
  <bug id="2319" opendate="2011-7-28 00:00:00" fixdate="2011-8-28 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Calling alter_table after changing partition comment throws an exception</summary>
      <description>Altering a table's partition key comments raises an InvalidOperationException. The partition key name and type should not be mutable, but the comment should be able to get changed.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">metastore.src.test.org.apache.hadoop.hive.metastore.TestHiveMetaStore.java</file>
      <file type="M">metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.java</file>
    </fixedFiles>
  </bug>
  <bug id="2326" opendate="2011-7-31 00:00:00" fixdate="2011-8-31 01:00:00" resolution="Fixed">
    <buginformation>
      <summary>Turn off bitmap indexing when map-side aggregation is turned off</summary>
      <description>Simply adding the CLUSTER BY clause on the ROW_OFFSET does not work with a GROUP BY clause, causing a SemanticException when trying to compile the the index builder task. Based on conversation with John Sichi, for now we will just turn off this feature.</description>
      <version>None</version>
      <fixedVersion>0.8.0</fixedVersion>
      <type>Bug</type>
    </buginformation>
    <fixedFiles>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEWAHBitmap.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.TableBasedIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler.java</file>
      <file type="M">ql.src.java.org.apache.hadoop.hive.ql.index.bitmap.BitmapIndexHandler.java</file>
    </fixedFiles>
  </bug>
</bugrepository>
